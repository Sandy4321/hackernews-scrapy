[{"url": "http://ww2.kqed.org/mindshift/2014/08/07/how-play-wires-kids-brains-for-social-and-academic-success/", "link_title": "How Play Wires Kids\u2019 Brains for Social and Academic Success", "sentiment": 0.15178469329412728, "text": "When it comes to brain development, time in the classroom may be less important than time on the playground.\n\n\u201cThe experience of play changes the connections of the neurons at the front end of your brain,\u201d says Sergio Pellis, a researcher at the University of Lethbridge in Alberta, Canada. \u201cAnd without play experience, those neurons aren\u2019t changed,\u201d he says.\n\nIt is those changes in the prefrontal cortex during childhood that help wire up the brain\u2019s executive control center, which has a critical role in regulating emotions, making plans and solving problems, Pellis says. So play, he adds, is what prepares a young brain for life, love and even schoolwork.\n\nBut to produce this sort of brain development, children need to engage in plenty of so-called free play, Pellis says. No coaches, no umpires, no rule books.\n\n\u201cWhether it\u2019s rough-and-tumble play or two kids deciding to build a sand castle together, the kids themselves have to negotiate, well, what are we going to do in this game? What are the rules we are going to follow?\u201d Pellis says. The brain builds new circuits in the prefrontal cortex to help it navigate these complex social interactions, he says.\n\nMuch of what scientists know about this process comes from research on animal species that engage in social play. This includes cats, dogs and most other mammals. But Pellis says he has also seen play in some birds, including young magpies that \u201cgrab one another and start wrestling on the ground like they were puppies or dogs.\u201d\n\nFor a long time, researchers thought this sort of rough-and-tumble play might be a way for young animals to develop skills like hunting or fighting. But studies in the past decade or so suggest that\u2019s not the case. Adult cats, for example, have no trouble killing a mouse even if they are deprived of play as kittens.\n\nSo researchers like Jaak Panksepp at Washington State University have come to believe play has a very different purpose: \u201cThe function of play is to build pro-social brains, social brains that know how to interact with others in positive ways,\u201d Panksepp says.\n\nPanksepp has studied this process in rats, which love to play and even produce a distinctive sound he has labeled \u201crat laughter.\u201d When the rats are young, play appears to initiate lasting changes in areas of the brain used for thinking and processing social interactions, Panskepp says.\n\nThe changes involve switching certain genes on and off. \u201cWe found that play activates the whole neocortex,\u201d he says. \u201cAnd we found that of the 1,200 genes that we measured, about one-third of them were significantly changed simply by having a half-hour of play.\u201d\n\nOf course, this doesn\u2019t prove that play affects human brains the same way. But there are good reasons to believe it does, Pellis says.\n\nFor one thing, he says, play behavior is remarkably similar across species. Rats, monkeys and children all abide by similar rules that require participants to take turns, play fair and not inflict pain. Play also helps both people and animals become more adept socially, Pellis says.\n\nAnd in people, he says, an added bonus is that the skills associated with play ultimately lead to better grades. In one study, researchers found that the best predictor of academic performance in eighth grade was a child\u2019s social skills in third grade.\n\nAnother hint that play matters, Pellis says, is that \u201ccountries where they actually have more recess tend to have higher academic performance than countries where recess is less.\u201d"},
{"url": "http://wpide.net", "link_title": "Show HN: WordPress Cloud IDE", "sentiment": 0.0, "text": "Enter your name and email below and we will let you know when we launch.\n\nWe will launch soon \n\nGet notified when we do"},
{"url": "http://afshinm.name/using-object-as-the-keys-of-another-object-in-javascript?hn", "link_title": "Using `object` as the keys of another `object` in JavaScript", "sentiment": 0.24242424242424243, "text": "Sometimes you need to use s for the keys of a dictionary (let\u2019s say, another ).\n\nSo the dictionary doesn\u2019t have two items because keys cannot be . You can get the item with string:\n\nOk, wait. Then how we can set objects as the keys of a dictionary?\n\nfeature in ES6 enables you to assign s as the keys of a dictionary (or another ).\n\nHere is the ES6 compatible of the previous example:\n\nAnd the variable should be:\n\nBesides, you can get the item with :\n\nOr set a new value for one of items:\n\nHere you can read more about Map feature in ECMAScript 6: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map"},
{"url": "http://en.wikipedia.org/wiki/Kernel_same-page_merging", "link_title": "Kernel same-page merging", "sentiment": 0.1549107142857143, "text": "Kernel same-page merging (abbreviated to KSM, and also known as kernel shared memory or memory merging) lets the hypervisor system share identical memory pages amongst different processes or virtualized guests. While not directly linked, KVM can use KSM to merge pages of Virtual machines.\n\nThis is done by scanning through the memory finding duplicate pages. The duplicate pair is then merged into a single page, and mapped into both original locations. The page is also marked as \"copy-on-write\", so the kernel will automatically separate them again should one process modify its data.[1]\n\nKSM was originally intended to run more virtual machines on one host by sharing memory between processes as well as virtual machines. Upon implementation, it was found to be useful for non-virtualized environments as well where memory is at a premium.[2][3] An experimental implementation of KSM by Red Hat found that 52 virtual instances of Windows XP with 1GB of memory, could run on a host computer that had only 16GB of RAM.[4]\n\nKSM first appeared in Linux kernel version 2.6.32.[2] To be effective, the operating system kernel must find identical memory pages held by different processes. As well it needs to decide whether the pages are going to update infrequently enough that SamePage Merging would be an efficient use of processor resources.[3] A concern is that although memory usage is reduced, CPU usage is increased, thus negating potential increases in performance.[1] Another concern is that KSM may pose security risks.[5]"},
{"url": "http://sarasoueidan.com/blog/svg-art-direction-using-viewbox/", "link_title": "Why We Need a ViewBox Property in CSS", "sentiment": 0.11448821167075134, "text": "The SVG attribute is easily one of SVG's most powerful features. Mastering this attribute can take your SVG skills to the next level, especially considering that a couple of the main SVG spriting techniques rely on this attribute to work. Because the attribute can be used to crop and extend the SVG canvas, it can be used for art-directing SVGs\u2014by using it to crop the SVG to the area that you want to display at a time. In this article, I want to go over how to do this, mention some tips that can save you some time doing it, and show the importance of having a property in CSS, in hopes that this article would serve as a practical use case that helps push this old SVGWG proposal forward.\n\nSince I've already covered everything you need to know about the attribute in a previous post, I will assume that you have a basic understanding of how the attribute works and what each of its values stands for. I will be mentioning some of the basics along the way, but I highly recommend you head to my other article and scan it before you move on with this article if you're not familiar with the attribute.\n\nThe attribute is used to specify the origin and dimensions of the user coordinate system of an SVG image. All the drawing inside of the SVG is done relative to this system. And since the SVG canvas is conceptually infinite in all directions, you can even draw shapes outside the boundaries of this coordinate system; but the position of those shapes relative to the SVG viewport can also be controlled by the position of the user coordinate system.\n\nThe attribute takes four parameters that specify the position of the origin of the system and its dimensions: . Initially, this system is identical to the initial viewport coordinate system established by the width and height of the SVG image, and its origin is at (0, 0)\u2014the top left corner of the SVG.\n\nBy changing the value of the and parameters you change the position of the origin. By changing the and , you change the dimensions of the system. This eventually allows you to extend and crop the SVG canvas using nothing but the attribute. Read along for examples.\n\nIMPORTANT NOTE: Throughout this article I won't be changing the default behavior (scale and position) of the inside the SVG viewport. Therefore, as per the default behavior of the attribute, the will scale as much as possible while still be entirely contained inside the viewport, and positioned at its center. Using the attribute, you can change the scale and position of the to your liking, but that is not required for the technique in this article to work, and therefore we won't be getting into that in this article.\n\nA while back, a client of mine requested that the SVG header photo of his website change on different screen sizes, so that only one portion of the full composition is visible on small screens, a bigger portion is visible on medium screens, and the full composition be visible on large screens. The first idea that crossed my mind when he requested that was to use the attribute to crop the SVG and only show the portions of the image he wanted on different screen sizes.\n\nBy changing the dimensions of the SVG coordinate system and the position of its origin, we can crop an SVG to only show the parts that we want inside the viewport.\n\nLet's see how that's done.\n\nSuppose we have the following SVG image in full composition, that we want to crop on smaller screen sizes. The image is a freebie House vector designed by Freepik and is licensed under Creative Commons Attribution 3.0 Unported License. For the sake of simplicity, we will assume that the image is going to be cropped only once to show one portion on small\u2013medium screens, and the full composition on larger screens, as shown in the image below.\n\nNow, there are a few considerations when cropping an SVG by changing the value of the attribute. We'll get to these shortly. But first, in order to change the coordinate system so that it matches the dashed rectangular area in the above image, we need to change the value from its initial parameters by translating the system's origin and changing the width and height.\n\nBut how do you know the new position and dimensions without having to go through a lot of trial and error?\n\nThere are a couple of ways. Since we're already in the graphics editor (Ai, in my example), we can use the editor's panels to retrieve the positions and dimensions of elements.\n\nThere is a reason why I drew that dashed rectangle to wrap the area I want to show on smaller screens: we can retrieve the position and dimensions of this rectangle and use them as values for the . Using Ai's Transform panel (see image below), we retrieve these values. By selecting the rectangle and then clicking the Transform link at the top right corner, we get the panel shown in the image below, with the , , and values that we are going to use.\n\nAs you have probably noticed, the values are not rounded, so we can do that ourselves. Using the above information, we change the value to: .\n\nSince the aspect ratio of the new is the same as the aspect ratio of the SVG viewport (both are square), the is going to scale up and only the selected area will be visible inside of the viewport. The result of changing the value is:\n\nWe need to ask a question here at this point:\n\nWell, in this case, there will be visible overflow. By visible overflow I don't mean overflow extending beyond the boundaries of the SVG viewport, but overflow relative to the new user coordinate system defined by the . The following image illustrates the problem.\n\nThe black border in the above image on the right is the area defined by the . As per the default behavior of the inside the viewport, it has been centered and scaled up as much as possible while remaining fully contained inside the viewport (blue border).\n\nSince the SVG canvas is conceptually infinite in all directions, you can draw outside the boundaries of the user coordinate system, and the content would simply overflow the system as shown in the above image.\n\nIf you change the aspect ratio of the SVG viewport (the SVG width and height) so they match those of the 's, you won't see that overflow anymore, since the will scale to fit the viewport as in the previous example.\n\nBut, in some scenarios, you may not be able or simply not want to change the aspect ratio of the Svg. An example is if you are using an SVG sprite to display images of a set of avatars on the page. In most cases, the avatars all have a fixed aspect ratio\u2014you won't be changing the size of each avatar to match the content of the image inside it. Or maybe you're embedding an icon system and want/need all icons to have the same dimensions all the time.\n\nTo clip off any excess (for example, part of another icon in the sprite that is showing inside the viewport), you can use a to clip that excess out. The clip path would be a element that covers the entire area, and is then applied to the root SVG.\n\nThere is, however, one thing to keep in mind here: make sure the and attributes of the are identical to those of the , otherwise the will be positioned relative to the old/initial system's origin and you will end up cropping and clipping an unexpected part of the SVG.\n\nOf course, clipping the excess out will mean that you're still using different aspect ratios and are thus going to end with that extra white space on either side of the content. If the SVG is a continuous scene as in our previous example, this will be unwanted and you will need to adjust the aspect ratio of the viewport. If the SVG is a bunch of icons that you're showing one at a time inside different viewports, this might not be an issue.\n\nThe important thing here to keep in mind that the aspect ratio of the is best kept the same as that of the viewport; else, you will have to apply a fix to avoid any excess unwanted space inside the SVG.\n\nSo, the can be used to crop the SVG and only show parts of it when needed. But how would that be done in a practical example?\n\nNothing new to add in this section, except the actual process with code. So, suppose you have the above SVG and you want to use it as a header image, for example, and you only want to show the cropped part on small\u2013medium screen sizes and the full composition on bigger screens.\n\nIn order to change the value of the SVG viewport's width and height, we can use CSS. Simple. But to change the value of the , we currently need to use JavaScript.\n\nNot all SVG presentation attributes are available as CSS properties; only the set of attributes that have CSS property equivalents can be set in CSS. You can see an overview of the set of attributes available as CSS properties in this table. In SVG2, more attributes (like , , , , , etc.) will be added to the list; but those are the properties we can work with today.\n\nIn order to show different parts of the SVG by changing the value based on different media queries, you can, for example, use Modernizr, check for media query conditions, and then change the accordingly, in JavaScript. An example of that might look like so:\n\nThis works. But wouldn\u2019t it be much more optimal if we could use CSS to do this?\n\nDISCLAIMER: At the time of writing of this article, there is no CSS property. This is just an example to demonstrate why such a property would be useful and an example of how I imagine it would be used.\n\nIdeally, we would be able to do something like this:\n\nThese styles would go inside (or outside) an SVG, and the SVG will then adapt its according to the viewport size\u2014be it the viewport of the page (in case of inline ), or the viewport established by the dimensions of whichever element is used to reference the SVG (which would lend us something practically identical to element queries).\n\nThis is currently not possible because we don't have a property in CSS.\n\nA while back, I asked an SVG spec editor about this, and he said that I could propose it to the SVGWG with a practical use case and example. After some discussion on Twitter, I learned that there already is a fairly old SVGWG proposal thread that goes a few years back. The initial proposal is still there today, and my hope is that, with a practical use case like this, this proposal could be pushed forward and the property implementation specced at some point in the near future. If you would like to see the property in CSS, please help make this happen by pushing this thread forward and commenting on it.\n\nWhile working on my client project, it took me literally less than a minute to art-direct the header image the way he wanted. However, we ended up going for three separate SVGs instead of using the same SVG with different viewBoxes on different screen sizes.\n\nThe reason we chose to go with three SVGs is that the size of the full composition was too big to serve on mobile\u2014reaching a whopping 100kb+ in size. The initial SVG was around 200kb, and I was able to slash the file size down to half by optimizing the SVG, but the resulting size was still too big to serve on mobile, so we ended up using three different images. This is something to keep in mind when art-directing SVGs: performance matters. A lot. So, if your SVG is too big, don't art-direct it using .\n\nNow, if you choose to serve three different SVG images, you can do so in one of many possible ways\u2014depending on the way you embed your SVG, and this also depends on what you want or don't want to do with it.\n\nThe ideal way to serve different SVG images would be to use the element. Not only does it allow us to provide different SVGs for the browser to choose from without needing JavaScript, but it also enables us to provide multiple optimized fallback images for non-supporting browsers (think IE8 and below) as well. is great when used with SVG, and you can read all about providing SVG fallback using in this article.\n\nAll this being said, will not be your best choice if you want to animate or interact with your SVG. Just like an SVG embedded using , the SVG cannot be styled and animated unless the styles and animations are defined inside the file, the SVG cannot be scripted (for security reasons), and any interactions (CSS or JS) \u2014 like hover, for example \u2014 will not work.\n\nSo, as I always say: SVG provides us with a lot of options to do almost everything; you need to weigh things, prioritize, sometimes maybe even make compromizes, and pick your best route based on that. But never compromise performance in favor of development convenience.\n\nBefore we finish up, and since we\u2019re on the subject of changing the SVG canvas\u2019 size using the attribute, let\u2019s take a look at another example where we can leverage the power of this attribute to save us some time and effort when working with SVG.\n\nJust like the attribute can be used to crop an SVG, it can be used to extend the SVG canvas as well.\n\nA few weeks ago I created a tool that allows you to generate circular menus in SVG. I created a few examples to show how a generated menu could be animated using JavaScript. The demos are embedded on the app page using the element. The boundaries of the define the boundaries of the SVG viewport, and anything that lies outside these boundaries is considered overflow and will be hidden by default.\n\nNote that the phrase \"outside these boundaries\" refers to content inside the SVG that is still drawn on the infinite SVG canvas, but that extends beyond the finite rectangle defined by the viewport.\n\nThe menus are created such that the size of the SVG is just big enough to contain the menu, not more, to avoid any excess and unwanted white space around the menu.\n\nI applied a staggering, bouncing animation to one of the menus as an example of how the menu can be animated. The bouncing effect \"stretched\" the menu items, and this led to the items being cut off while they animated.\n\nThe staggering bouncing animation is applied to the items such that it will scale an item from zero (items are initially not visible, scaled down) to 100% using a bouncing timing function, the effect of which will be scaling the item beyond 100% right before it is scaled back to 100%. This effect causes the item to scale up beyond the boundaries of the SVG and hence get cut off.\n\nThe following image shows the result of scaling the menu item up beyond the boundaries of the used to embed it (grey border). Illustration showing the menu item overflowing the boundaries of the SVG viewport when it is scaled up. The grey border represents the border of the SVG viewport (the ).\n\nSetting on the does not fix this, because is practically similar to an in behavior. What we need to do is extend the SVG canvas inside the viewport created by the so that the scaled items have enough space to \"bounce\" without exceeding its boundaries. We can do this using the attribute.\n\nTo extend the SVG canvas, you simply increase its dimensions. So, instead of 500px by 250px\u2014the original dimensions of the SVG menu, we use 700px by 350px; this will increase the height of the canvas visible inside of the viewport by 100px, and its width inside of the viewport by 200px. I chose these values based on how much the menu item is being scaled up in the bounce effect. Depending on your SVG and what you're working on, these values might be different.\n\nNow, to make sure the menu remains centered inside of the viewport, we're going to shift the position of the coordinate system by 100 pixels in the negative direction (upwards and to the left). Applying this shift to the origin of the coordinate system is practically the same as applying a translation transformation to the menu inside of the system. The result will be that the menu remains centered inside of the viewport. In this illustration, the blue borders represent the border of the SVG viewport (the ). The grey borders in this image show the initial dimensions of the user coordinate system. The blue numbers and arrows represent the extension of the coordinate system inside of the viewport.\n\nBy extending the dimensions of the user coordinate system, you are increasing the area of the canvas visible inside of the viewport. The result of doing this will also be that the contents of the canvas will look slightly smaller\u2014depending on how much you increase the canvas. In the case of the menu, the result was perfectly acceptable.\n\nThe following screen recording shows the result of extending the SVG canvas and how the animation now looks inside the buondaries of the SVG.\n\nChanging four values inside the attribute to extend the SVG canvas was all that was needed to troubleshoot and solve the issue of the items being cut off. Now that's pretty powerful, isn't it?\n\nThe attribute is awesome. It is literally SVG on steroids. By using this attribute, you can save a lot of time when working with SVG, troubleshoot SVG quickly without having to resort to a graphics editor, and, all in all, feel more comfortable editing SVG by hand.\n\nI highly recommend you learn all about this attribute if you haven't already, play with its values, and then leverage its power in your work. And if you do decide to use it to art-direct SVGs, don't forget to keep performance in mind.\n\nOne of the main objectives of this article was to also shed some light on the importance of having a property in CSS, so if you're convinced that this property is needed, please take the time to vote on / respond to the SVGWG thread and support the request.\n\nThank you very much for reading! :)"},
{"url": "https://arxiv.lateral.io", "link_title": "Discover research papers on Arxiv.org using ML", "sentiment": 0.5, "text": "This site uses cookies. If you continue to use the site it is assumed that you accept all cookies. Read more"},
{"url": "http://www.independent.co.uk/news/people/warren-buffett-thinks-that-the-poor-need-to-stop-blaming-inequality-on-the-rich-10271780.html?icn=puff-1", "link_title": "Warren Buffett thinks the poor should stop blaming inequality on the rich", "sentiment": 0.19032751760024488, "text": "In an opinion piece written for the Wall Street Journal Mr Buffett, who is estimated to be worth $71.3bn by Forbes magazine, claims that the depressing fact is that \u201cthe poor are most definitely not poor because the rich are rich\u201d.\n\nCiting the likes of Henry Ford and Steve Jobs as examples of innovation, Mr Buffett goes on to say that the rich are not undeserving and that \u201cmost of them have contributed brilliant innovations or managerial expertise,\u201d to America\u2019s economy.\n\nBut Mr Buffett claims that inequality cannot be solved by increases in the minimum wage, from $15 an hour, or through methods like improving the quality of education.\n\nHis solution is to increase the Earned Income Tax Credit, which the US government currently gives to millions of low-income workers, with the view that as wages increase the payments go down.\n\nWriting that the widening gap is an \u201cinevitable consequence\u201d of an advanced market-based economy, he said that pay first became unequal as not everybody could do the same job anymore.\n\nHe said: \u201cNo conspiracy lies behind this depressing fact: The poor are most definitely not poor because the rich are rich. Nor are the rich undeserving.\n\n\u201cMost of them have contributed brilliant innovations or managerial expertise to America\u2019s well-being. We all live far better because of Henry Ford, Steve Jobs, Sam Walton and the like.\n\n\u201cIt is simply a consequence of an economic engine that constantly requires more high-order talents while reducing the need for commodity-like tasks.\u201d"},
{"url": "https://www.jacobinmag.com/2015/05/slow-food-artisanal-natural-preservatives/", "link_title": "A Plea for Culinary Modernism", "sentiment": 0.08065139185654015, "text": "Modern, fast, processed food is a disaster. That, at least, is the message conveyed by newspapers and magazines, on television cooking programs, and in prizewinning cookbooks.\n\nIt is a mark of sophistication to bemoan the steel roller mill and supermarket bread while yearning for stone\u00ad ground flour and brick ovens; to seek out heirloom apples and pumpkins while despising modern tomatoes and hybrid corn; to be hostile to agronomists who develop high-yielding modern crops and to home economists who invent new recipes for General Mills.\n\nWe hover between ridicule and shame when we remember how our mothers and grand\u00admothers enthusiastically embraced canned and frozen foods. We nod in agreement when the waiter proclaims that the restaurant showcases the freshest local produce. We shun Wonder Bread and Coca-Cola. Above all, we loathe the great culminating symbol of Culinary Modernism, McDonald\u2019s \u2014 modern, fast, homogenous, and international.\n\nLike so many of my generation, my culinary style was created by those who scorned industrialized food; Culinary Luddites, we may call them, after the English hand workers of the nineteenth century who abhorred the machines that were destroying their traditional way of life. I learned to cook from the books of Elizabeth David, who urged us to sweep our store cupboards \u201cclean for ever of the cluttering debris of commercial sauce bottles and all synthetic flavorings.\u201d\n\nI progressed to the Time-Life Good Cook series and to Simple French Cooking, in which Richard Olney hoped against hope that \u201cthe reins of stubborn habit are strong enough to frustrate the famous industrial revolution for some time to come.\u201d I turned to Paula Wolfert to learn more about Mediterranean cooking and was assured that I wouldn\u2019t \u201cfind a dishonest dish in this book .\u00a0.\u00a0. The food here is real food .\u00a0.\u00a0. the real food of real people.\u201d Today I rush to the newsstand to pick up Saveur with its promise to teach me to \u201cSavor a world of authentic cuisine.\u201d\n\nCulinary Luddism involves more than just taste. Since the days of the counterculture, it has also presented itself as a moral and political crusade. Now in Boston, the Oldways Preservation and Exchange Trust works to provide \u201ca scientific basis for the preservation and revitalization of traditional diets.\n\nMeanwhile Slow Food, founded in 1989 to protest the opening of a McDonald\u2019s in Rome, is a self\u00ad-described Greenpeace for Food; its manifesto begins, \u201cWe are enslaved by speed and have all succumbed to the same insidious virus: Fast Life, which disrupts our habits, pervades the privacy of our homes and forces us to eat Fast Foods .\u00a0.\u00a0. Slow Food is now the only truly progressive answer.\u201d As one of its spokesmen was reported as saying in the New York Times, \u201cOur real enemy is the obtuse consumer.\u201d\n\nAt this point I begin to back off. I want to cry, \u201cEnough!\u201d But why? Why would I, who learned to cook from Culinary Luddites, who grew up in a family that, in Elizabeth David\u2019s words, produced their \u201cown home-cured bacon, ham and sausages .\u00a0.\u00a0. churned their own butter, fed their chickens and geese, cherished their fruit trees, skinned and cleaned their own hares\u201d (well, to be honest, not the geese and sausages), not rejoice at the growth of Culinary Luddism? Why would I (or anyone else) want to be thought \u201can obtuse consumer\u201d? Or admit to preferring unreal food for unreal people? Or to savoring inauthentic cuisine?\n\nThe answer is not far to seek: because I am an historian.\n\nAs an historian I cannot accept the account of the past implied by Culinary Luddism, a past sharply divided between good and bad, between the sunny rural days of yore and the gray industrial present. My enthusiasm for Luddite kitchen wisdom does not carry over to their history, any more than my response to a stirring political speech inclines me to accept the orator as scholar.\n\nThe Luddites\u2019 fable of disaster, of a fall from grace, smacks more of wishful thinking than of digging through archives. It gains credence not from scholarship but from evocative dichotomies: fresh and natural versus processed and preserved; local versus global; slow versus fast: artisanal and traditional versus urban and industrial; healthful versus contaminated and fatty. History shows, I believe, that the Luddites have things back to front.\n\nThat food should be fresh and natural has become an article of faith. It comes as something of a shock to realize that this is a latter-day creed. For our ancestors, natural was something quite nasty. Natural often tasted bad.\n\nFresh meat was rank and tough; fresh milk warm and unmistakably a bodily excretion; fresh fruits (dates and grapes being rare exceptions outside the tropics) were inedibly sour, fresh vegetables bitter. Even today, natural can be a shock when we actually encounter it. When Jacques Pepin offered free-\u00adrange chickens to friends, they found \u201cthe flesh tough and the flavor too strong,\u201d prompting him to wonder whether they would really like things the way they naturally used to be. Natural was unreliable. Fresh fish began to stink. Fresh milk soured, eggs went rotten.\n\nEverywhere seasons of plenty were followed by seasons of hunger when the days were short. The weather turned cold, or the rain did not fall. Hens stopped laying eggs, cows went dry, fruits and vegetables were not to be found, fish could not be caught in the stormy seas.\n\nNatural was usually indigestible. Grains, which supplied from fifty to ninety percent of the calories in most societies have to be threshed, ground, and cooked to make them edible. Other plants, including the roots and fibers that were the life support of the societies that did not eat grains, are often downright poisonous. Without careful processing green potatoes, stinging taro, and cassava bitter with prussic acid are not just indigestible, but toxic.\n\nNor did our ancestors\u2019 physiological theories dispose them to the natural. Until about two hundred years ago, from China to Europe, and in Mesoamerica, too, everyone believed that the fires in the belly cooked foodstuffs and turned them into nutrients. That was what digestion was. Cooking foods in effect pre-digested them and made them easier to assimilate. Given a choice, no one would burden the stomach with raw, unprocessed foods.\n\nSo to make food tasty, safe, digestible and healthy, our forebears bred, ground, soaked, leached, curdled, fermented, and cooked naturally occurring plants and animals until they were literally beaten into submission.\n\nTo lower toxin levels, they cooked plants, treated them with clay (the Kaopectate effect), leached them with water, acid fruits and vinegars, and alkaline lye. They intensively bred maize to the point that it could not reproduce without human help. They created sweet oranges and juicy apples and non-bitter legumes, happily abandoning their more natural but less tasty ancestors.\n\nThey built granaries for their grain, dried their meat and their fruit, salted and smoked their fish, curdled and fermented their dairy products, and cheerfully used whatever additives and preservatives they could \u2014 sugar, salt, oil, vinegar, lye \u2014 to make edible foodstuffs.\n\nIn the twelfth century, the Chinese sage Wu Tzu-mu listed the six foodstuffs essential to life: rice, salt, vinegar, soy sauce, oil, and tea. Four had been unrecognizably transformed from their naturally occurring state.\n\nWho could have imagined vinegar as rice that had been fermented to ale and then soured? Or soy sauce as cooked and fermented beans? Or oil as the extract of crushed cabbage seeds? Or bricks of tea as leaves that had been killed by heat, powdered, and compressed? Only salt and rice had any claim to fresh or natural, and even then the latter had been stored for months or years, threshed, and husked.\n\nProcessed and preserved foods kept well, were easier to digest, and were delicious: raised white bread instead of chewy wheat porridge; thick, nutritious, heady beer instead of prickly grains of barley; unctuous olive oil instead of a tiny, bitter fruit: soy milk, sauce, and tofu instead of dreary, flatulent soy beans; flexible, fragrant tortillas instead of dry, tough maize; not to mention red wine, blue cheese, sauerkraut, hundred-year-old eggs, Smithfield hams, smoked salmon, yogurt, sugar, chocolate, and fish sauce.\n\nEating fresh, natural food was regarded with suspicion verging on horror, something to which only the uncivilized, the poor, and the starving resorted. When the compiler of the Confucian classic, the Book of Rites (ca. 2oo BC), distinguished the first humans \u2014 people who had no alternative to wild, uncooked foods \u2013 from civilized peoples who took \u201cadvantage of the benefits of fire .\u00a0.\u00a0. [who] toasted, grilled, boiled, and roasted,\u201d he was only repeating a commonplace.\n\nWhen the ancient Greeks took it as a sign of bad times if people were driven to eat greens and root vegetables, they too were rehearsing common wisdom. Happiness was not a verdant Garden of Eden abounding in fresh fruits, but a securely locked storehouse jammed with preserved, processed foods.\n\nLocal food was greeted with about as much enthusiasm as fresh and natural. Local foods were the lot of the poor who could neither escape the tyranny of local climate and biology nor the monotonous, often precarious, diet it afforded. Meanwhile, the rich, in search of a more varied diet, bought, stole, wheedled, robbed, taxed, and ran off with appealing plants and animals, foodstuffs, and culinary techniques from wherever they could find them.\n\nBy the fifth century BC, Celtic princes in the region of France now known as Burgundy were enjoying a glass or two of Greek wine, drunk from silver copies of Greek drink\u00ading vessels. The Greeks themselves looked to the Persians, acclimatizing their peaches and apricots and citrons and emulating their rich sauces, while the Romans in turn hired Greek cooks. From around the time of the birth of Christ, the wealthy in China, India, and the Roman Empire paid vast sums for spices brought from the distant and mysterious Spice Islands.\n\nFrom the seventh century AD, Islamic caliphs and sultans transplanted sugar, rice, citrus, and a host of other Indian and Southeast Asian plants to Persia and the Mediterranean, transforming the diets of West Asia and the shores of the Mediterranean. In the thirteenth century, the Japanese had naturalized the tea plant of China and were importing sugar from Southeast Asia.\n\nIn the seventeenth century, the European rich drank sweetened coffee, tea, and cocoa in Chinese porcelain, imported or imitation, proffered by servants in Turkish or other foreign dress. To ensure their own supply, the French, Dutch, and English embarked on imperial ventures and moved millions of Africans and Asians around the globe. The Swedes, who had no empire, had a hard time getting these exotic food\u00adstuffs, so the eighteenth-century botanist Linnaeus set afoot plans to naturalize the tea plant in Sweden.\n\nWe may laugh at the climatic hopelessness of his proposal. Yet it was no more ridiculous than other, more successful, proposals to naturalize Southeast Asian sugarcane throughout the tropics, apples in Australia, grapes in Chile, hereford cattle in Colorado and Argentina, and Caucasian wheat on the Canadian prairie. Without our aggressively global ancestors, we would all still be subject to the tyranny of the local.\n\nAs for slow food, it is easy to wax nostalgic about a time when families and friends met to relax over delicious food, and to forget that, far from being an invention of the late twentieth century, fast food has been a mainstay of every society.\n\nHunters tracking their prey, fishermen at sea, shepherds tending their flocks, soldiers on campaign, and farmers rushing to get in the harvest all needed food that could be eaten quickly and away from home. The Creeks roasted barley and ground it into a meal to eat straight or mixed with water, milk, or butter (as the Tibetans still do), while the Aztecs ground roasted maize and mixed it with water to make an instant beverage (as the Mexicans still do).\n\nCity dwellers, above all, relied on fast food. When fuel cost as much as the food itself, when huddled dwellings lacked cooking facilities, and when cooking fires might easily conflagrate entire neighborhoods, it made sense to purchase your bread or noodles, and a little meat or fish to liven them up.\n\nBefore the birth of Christ, Romans were picking up honey cakes and sausages in the Forum. In twelfth-century Hangchow, the Chinese downed noodles, stuffed buns, bowls of soup, and deep-fried confections. In Baghdad of the same period, the townspeople bought ready-cooked meats, salt fish, bread, and a broth of dried chick peas. In the sixteenth cen\u00adtury, when the Spanish arrived in Mexico, Mexicans had been enjoying tacos from the market for generations. In the eighteenth century, the French purchased cocoa, apple turnovers, and wine in the boulevards of Paris, while the Japanese savored tea, noodles, and stewed fish.\n\nDeep-fried foods, expensive and dangerous to prepare at home, have always had their place on the street: doughnuts in Europe, churros in Mexico, andagi in Okinawa, and sev in India. Bread, also expensive to bake at home, is one of the oldest convenience foods. For many people in West Asia and Europe, a loaf fresh from the baker was the only warm food of the day.\n\nTo these venerable traditions of fast food, Americans have simply added the electric deep fryer, the heavy iron griddle of the Low Countries, and the franchise. The McDonald\u2019s in Rome was, in fact, just one more in a long tradition of fast food joints reaching back to the days of the Caesars.\n\nWhat about the idea that the best food was country food, handmade by artisans? That food came from the country goes without saying. The presumed corollary \u2014 that country people ate better than city dwellers \u2014 does not.\n\nFew who worked the land were independent peasants baking their own bread, brewing their own wine or beer, and salt\u00ading down their own pig. Most were burdened with heavy taxes and rents paid in kind (that is, food); or worse, they were indentured, serfs, or slaves.\n\nBarely part of the cash economy, they subsisted on what was left over. \u201cThe city dwellers,\u201d remarked the great Roman doctor Galen in the second century AD, \u201ccollected and stored enough grain for all the coming year immediately after the harvest. They car\u00adried off all the wheat, the barley, the beans and the lentils and left what remained to the countryfolk.\u201d\n\nWhat remained was pitiful. All too often, those who worked the land got by on thin gruels and gritty flatbreads north of the Alps. French peasants prayed that chestnuts would be sufficient to sustain them from the time when their grain ran out to the harvest still three months away. South of the Alps, Italian peasants suffered skin eruptions, went mad, and in the worst cases died of pellagra brought on by a diet of maize polenta and water.\n\nThe dishes we call ethnic and assume to be of peasant origin were invented for the urban, or at least urbane, aristocrats who collected the surplus. This is as true of the lasagne of northern Italy as it is of the chicken konna of Mughal Delhi, the mooshu pork of imperial China, the pilafs, stuffed vegetables, and baklava of the great Ottoman palace in Istanbul, or the mee krob of nineteenth-century Bangkok. Cities have always enjoyed the best food and have invariably been the focal points of culinary innovation.\n\nNor are most \u201ctraditional foods\u201d very old. For every prized dish that goes back two thousand years, a dozen have been invented in the last two hundred. The French baguette? A twentieth-century phenomenon, adopted nationwide only after World War II. English fish and chips? Dates from the late nineteenth century, when the working class took up the fried fish of Sephardic Jewish immigrants in East London. Fish and chips, though, will soon be a thing of the past.\n\nIt\u2019s a Balti and lager now, Balti being a kind of stir-fried curry dreamed up by Pakistanis living in Birmingham. Greek moussaka? Created in the early twentieth century in an attempt to Frenchify Greek food. The bubbling Russian samovar? Late eighteenth century. The Indonesian rijsttafel? Dutch colonial food. Indonesian padang food? Invented for the tourist market in the past fifty years.\n\nTequila? Promoted as the national drink of Mexico during the 1930s by the Mexican film industry. Indian tandoori chicken? The brain\u00adchild of Hindu Punjabis who survived by selling chicken cooked in a Muslim-style tandoor oven when they fled Pakistan for Delhi during the Partition of India. The soy sauce, steamed white rice, sushi, and tempura of Japan? Commonly eaten only after the middle of the nineteenth century.\n\nThe lomilomi salmon, salted salmon rubbed with chopped tomatoes and spring onions that is a fixture in every Hawaiian luau? Not a salmon is to be found within two thousand miles of the islands, and onions and tomatoes were unknown in Hawaii until the nineteenth century. These are indisputable facts of history, though if you point them out you will be met with stares of disbelief.\n\nNot only were many \u201ctraditional\u201d foods created after industrialization and urbanization, a lot of them were dependent on it. The Swedish smorgasbord came into its own at the beginning of the twentieth century when canned out-of-season fish, roe, and liver paste made it possible to set out a lavish table. Hungarian goulash was unknown before the nineteenth century, and not widely accepted until after the invention of a paprika-grinding mill in 1859.\n\nWhen lands were conquered, peoples migrated, populations converted to different religions or accepted new dietary theories, and dishes \u2014 even whole cuisines \u2014 were forgotten and new ones invented. Where now is the cuisine of Renaissance Spain and Italy, or of the Indian Raj, or of Tsarist Russia, or of medieval Japan? Instead we have Nonya food in Singapore, Cape Malay food in South Africa, Creole food in the Mississippi Delta, and Local Food in Hawaii. How long does it take to create a cuisine? Not long: less than fifty years, judging by past experience.\n\nWere old foods more healthful than ours? Inherent in this vague notion are several different claims, among them that foods were less dangerous, that diets were better balanced.\n\nYet while we fret about pesticides on apples, mercury in tuna, and mad cow disease, we should remember that ingesting food is, and always has been, inherently dangerous. Many plants contain both toxins and carcinogens, often at levels much higher than any pesticide residues. Grilling and frying add more.\n\nSome historians argue that bread made from moldy, verminous flour, or adulterated with mash, leaves, or bark to make it go further, or contaminated with hemp or poppy seeds to drown out sorrows, meant that for five hundred years Europe\u2019s poor staggered around in a drugged haze subject to hallucinations.\n\nCertainly, many of our forebears were drunk much of the time, given that beer or wine were preferred to water, and with good reason. In the cities, polluted water supplies brought intestinal diseases in their wake. In France, for example, no piped water was available until the 1860s.\n\nBread was likely to be stretched with chalk, pepper adulterated with the sweepings of warehouse floors, and sausage stuffed with all the horrors famously exposed by Upton Sinclair in The Jungle. Even the most reputable cookbooks recommended using concentrated sulphuric acid to inten\u00adsify the color of jams.\n\nMilk, suspected of spreading scarlet fever, typhoid, and diphtheria as well as tuberculosis, was sensibly avoided well into the twentieth century when the United States and many parts of Europe introduced stringent regulations. My mother sifted weevils from the flour bin; my aunt reckoned that if the maggots could eat her home-cured ham and survive, so could the family.\n\nAs to dietary balance, once again we have to distinguish between rich and poor. The rich, whose bountiful tables and ample girths were visible evidence of their station in life, suffered many of the diseases of excess.\n\nIn the seventeenth century, the Mughal Emperor, Jahangir, died of overindulgence in food, opium, and alcohol. In Georgian England, George Cheyne, the leading doctor, had to be wedged in and out of his carriage by his servants when he soared to four hundred pounds, while a little later Erasmus Darwin, grandfather of Charles and another important physician, had a semicircle cut out of his dining table to accommodate his paunch.\n\nIn the nineteenth century, the fourteenth shogun of Japan died at age twenty-one, probably of beriberi induced by eating the white rice available only to the privileged. In the Islamic countries, India, and Europe, the well-to-do took sugar as a medicine; in India they used butter; and in much of the world people avoided fresh vegetables, all on medical advice.\n\nWhether the peasants really starved, and if so how often, particularly outside of Europe, is the subject of ongoing research. What is clear is that the food supply was always precarious: if the weather was bad or war broke out, there might not be enough to go around. The end of winter or the dry season saw everyone suffering from the lack of fresh fruits and vegetables, scurvy occurring on land as well as at sea.\n\nBy our standards, the diet was scanty for people who were engaged in heavy physical toil. Estimates suggest that in France on the eve of the Revolution one in three adult men got by on no more than 1,800 calories a day, while a century later in Japan daily intake was perhaps 1,850 calories. Historians believe that in times of scarcity peasants essentially hibernated during the winter. It is not surprising, therefore, that in France the proudest of boasts was \u201cthere is always bread in the house,\u201d while the Japanese adage advised that \u201call that matters is a full stomach.\u201d\n\nBy the standard measures of health and nutrition \u2014 life expectancy and height \u2014 our ancestors were far worse off than we are. Much of the blame was due to the diet, exacerbated by living conditions and infections which affect the body\u2019s ability to use the food that is ingested. No amount of nostalgia for the pastoral foods of the distant past can wish away the fact that our ancestors lived mean, short lives, constantly afflicted with diseases, many of which can be directly attributed to what they did and did not eat.\n\nHistorical myths, though, can mislead as much by what they don\u2019t say as by what they do. Culinary Luddites typically gloss over the moral problems intrinsic to the labor of producing and preparing food. In 1800, 95\u00a0percent of the Russian population and 80\u00a0percent of the French lived in the country; in other words, they spent their days getting food on the table for themselves and other people.\n\nA century later, 88\u00a0percent of Russians, 85\u00a0percent of Greeks, and over 50\u00a0percent of the French were still on the land. Traditional societies were aristocratic, made up of the many who toiled to produce, process, preserve, and prepare food, and the few who, supported by the limited surplus, could do other things.\n\nIn the great kitchens of the few \u2014 royalty, aristocracy, and rich merchants \u2014 cooks created elaborate cuisines. The cuisines drove home the power of the mighty few with a symbol that everyone understood: ostentatious shows of more food than the powerful could possibly consume. Feasts were public occasions for the display of power, not private occasions for celebration, for enjoying food for food\u2019s sake. The poor were invited to watch, groveling as the rich gorged themselves.\n\nLouis XIV was exploiting a tradition going back to the Roman Empire when he encouraged spectators at his feasts. Sometimes, to hammer home the point while amus\u00ading the court, the spectators were let loose on the leftovers. \u201cThe destruction of so handsome an arrangement served to give another agreeable entertainment to the court,\u201d observed a commentator, \u201cby the alacrity and disorder of those who demolished these castles of marzipan, and these mountains of preserved fruit.\u201d\n\nMeanwhile, most men were born to a life of labor in the fields, most women to a life of grinding, chopping, and cooking. \u201cServitude,\u201d said my mother as she prepared home\u00adcooked breakfast, dinner, and tea for eight to ten people three hundred and sixty five days a year.\n\nShe was right. Churning butter and skinning and cleaning hares, without the option of picking up the phone for a pizza if something goes wrong, is unremitting, unforgiving toil. Perhaps, though, my mother did not realize how much worse her lot might have been.\n\nShe could at least buy our bread from the bakery. In Mexico, at the same time, women without servants could expect to spend five hours a day \u2014 one third of their waking hours \u2014 kneeling at the grindstone preparing the dough for the family\u2019s tortillas. Not until the 1950s did the invention of the tortilla machine release them from the drudgery.\n\nIn the eighteenth and early nineteenth centuries, it looked as if the distinction between gorgers and grovelers would worsen. Between 1557 and 1825 world population had doubled from 5oo million to a billion, and it was to double again by 1925.\n\nMalthus sounded his dire predictions. The poor, driven by necessity or government mandate, resorted to basic foods that produced bountifully even if they were disliked: maize and sweet potatoes in China and Japan, maize in Italy, Spain and Romania, potatoes in northern Europe.\n\nThey eked out an existence on porridges or polentas of oats or maize, on coarse breads of rye or barley bulked out with chaff or even clay and ground bark, and on boiled potatoes; they saw meat only on rare occasions. The privation continued. In Europe, 1840 was a year of hunger, best remembered now as the time of the devastating potato famine of Ireland.\n\nMeanwhile, the rich continued to indulge, feasting on white bread, meats, rich fatty sauces, sweet desserts, exotic hothouse-grown pineapples, wine, and tea, coffee, and chocolate drunk from fine china. In 1845, shortly after revolutions had rocked Europe, the British Prime Minister Benjamin Disraeli described \u201ctwo nations, between whom there is no intercourse and no sympathy .\u00a0.\u00a0. who are formed by a different breeding, are fed by a different food, are ordered by different manners, and are not governed by the same laws .\u00a0.\u00a0. THE RICH AND THE POOR.\u201d\n\nIn the nick of time, in the 1880s, the industrialization of food got under way long after the production of other common items of consumption such as textiles and clothing had been mechanized. Farmers brought new land into production, utilized reapers and later tractors and combines, spread more fertilizer, and by the 1930s began growing hybrid maize. Steamships and trains brought fresh and canned meats, fruits, vegetables, and milk to the growing towns. Instead of starving, the poor of the industrialized world survived and thrived.\n\nIn Britain the retail price of food in a typical workman\u2019s budget fell by a third between 1877 and 1887 (though he would still spend seventy-one percent of his income on food and drink). In 1898 in the United States a dollar bought forty-two percent more milk, fifty-one percent more coffee, a third more beef, twice as much sugar, and twice as much flour as in 1872. By the beginning of the twentieth century, the British working class were drinking sugary tea from china teacups and eating white bread spread with jam and margarine, canned meats, canned pineapple, and an orange from the Christmas stocking.\n\nTo us, the cheap jam, the margarine, and the starchy diet look pathetic. Yet white bread did not cause the \u201cweakness, indigestion, or nausea\u201d that coarse whole wheat bread did when it supplied most of the calories (not a problem for us since we never consume it in such quantities). Besides, it was easier to detect stretchers such as sawdust in white bread. Margarine and jam made the bread more attractive and easier to swallow. Sugar tasted good, and hot tea in an unheated house in mid-winter provided good cheer.\n\nFor those for whom fruit had been available, if at all, only from June to October, canned pineapple and a Christmas orange were treats to be relished. For the diners, therefore, the meals were a dream come true, a first step away from a coarse, monotonous diet and the constant threat of hunger, even starvation.\n\nNor should we think it was only the British, not famed for their cuisine, who were delighted with industrialized foods. Everyone was, whether American, Asian, African, or European.\n\nIn the first half of the twentieth century, Italians embraced factory-made pasta and canned tomatoes. In the second half of the century, Japanese women welcomed factory-made bread because they could sleep in a little longer instead of having to get up to make rice. Similarly, Mexicans seized on bread as a good food to have on hand when there was no time to prepare tortillas.\n\nWorking women in India are happy to serve commercially made bread during the week, saving the time-consuming business of making chapatis for the weekend. As supermarkets appeared in Eastern Europe and Russia, housewives rejoiced at the choice and convenience of ready-made goods.\n\nFor all, Culinary Modernism had provided what was wanted: food that was processed, preservable, industrial, novel, and fast, the food of the elite at a price everyone could afford. Where modern food became available, populations grew taller, stronger, had fewer diseases, and lived longer. Men had choices other than hard agricultural labor, women other than kneeling at the metate five hours a day.\n\nSo the sunlit past of the Culinary Luddites never existed. So their ethos is based not on history but on a fairy tale. So what? Perhaps we now need this culinary philosophy. Certainly no one would deny that an industrialized food supply has its own problems, problems we hear about every day. Perhaps we should eat more fresh, natural, local, artisanal, slow food. Why not create a historical myth to further that end? The past is over and gone. Does it matter if the history is not quite right?\n\nIt matters quite a bit, I believe. If we do not understand that most people had no choice but to devote their lives to growing and cooking food, we are incapable of comprehending that the foods of Culinary Modernism \u2014 egalitarian, available more or less equally to all, without demanding the disproportionate amount of the resources of time or money that traditional foodstuffs did \u2014 allow unparalleled choices not just of diet but of what to do with our lives.\n\nIf we urge the Mexican to stay at her metate, the farmer to stay at his olive press, the housewife to stay at her stove instead of going to McDonald\u2019s, all so that we may eat handmade tortillas, traditionally pressed olive oil, and home-cooked meals, we are assuming the mantle of the aristocrats of old. We are reducing the options of others as we attempt to impose our elite culinary preferences on the rest of the population.\n\nIf we fail to understand how scant and monotonous most traditional diets were, we can misunderstand the \u201cethnic foods\u201d we encounter in cookbooks, restaurants, or on our travels. We let our eyes glide over the occasional references to servants, to travel and education abroad in so-called ethnic cookbooks, references that otherwise would clue us in to the fact that the recipes are those of monied Italians, Indians, or Chinese with maids to do the donkey work of preparing elaborate dishes.\n\nWe may mistake the meals of today\u2019s European, Asian, or Mexican middle class (many of them benefiting from industrialization and contemporary tourism) for peasant food or for the daily fare of our ancestors. We can represent the peoples of the Mediterranean, Southeast Asia, India, or Mexico as pawns at the mercy of multinational corporations bent on selling trashy modem products \u2014 failing to appreciate that, like us, they enjoy a choice of goods in the market, foreign restaurants to eat at, and new recipes to try.\n\nA Mexican friend, suffering from one too many foreign visitors who chided her because she offered Italian, not Mexican food, complained, \u201cWhy can\u2019t we eat spaghetti, too?\u201d If we unthinkingly assume that good food maps neatly onto old or slow or homemade food (even though we\u2019ve all had lousy traditional cooking), we miss the fact that lots of industrial foodstuffs are better. Certainly no one with a grindstone will ever produce chocolate as suave as that produced by conching in a machine for seventy two hours. Nor is the housewife likely to tum out fine soy sauce or miso.\n\nAnd let us not forget that the current popularity of Italian food owes much to the availability and long shelf life of two convenience foods that even purists love, high-quality factory pasta and canned tomatoes. Far from fleeing them, we should be clamoring for more high-quality industrial foods.\n\nIf we romanticize the past, we may miss the fact that it is the modern, global, industrial economy (not the local resources of the wintry country around New York, Boston, or Chicago) that allows us to savor traditional, peasant, fresh, and natural foods.\n\nVirgin olive oil, Thai fish sauce, and udon noodles come to us thanks to international marketing. Fresh and natural loom so large because we can take for granted the preserved and processed staples \u2014 salt, flour, sugar, chocolate, oils, coffee, tea \u2014 produced by agribusiness and food corporations. Asparagus and strawberries in winter come to us on trucks trundling up from Mexico and planes flying in from Chile.\n\nVisits to charming little restaurants and colorful markets in Morocco or Vietnam would be impossible without international tourism. The ethnic foods we seek out when we travel are being preserved, indeed often created, by a hotel and restaurant industry determined to cater to our dream of India or Indonesia, Turkey, Hawaii, or Mexico. Culinary Luddism, far from escaping the modern global food economy, is parasitic upon it.\n\nCulinary Luddites are right, though, about two important things. We need to know how to prepare good food, and we need a culinary ethos. As far as good food goes, they\u2019ve done us all a service by teaching us to how to use the bounty delivered to us (ironically) by the global\u00a0economy.\n\nTheir culinary ethos, though, is another matter. Were we able to turn back the clock, as they urge, most of us would be toiling all day in the fields or the kitchen; many of us would be starving. Nostalgia is not what we need.\n\nWhat we need is an ethos that comes to terms with contemporary, industrialized food, not one that dismisses it, an ethos that opens choices for everyone, not one that closes them for many so that a few may enjoy their labor, and an ethos that does not prejudge, but decides case by case when natural is preferable to processed, fresh to preserved, old to new, slow to fast, artisanal to industrial.\n\nSuch an ethos, and not a timorous Luddism, is what will impel us to create the matchless modern cuisines appropriate to our time."},
{"url": "http://www.theguardian.com/commentisfree/2015/may/22/seven-myths-about-meditation", "link_title": "The Guardian \u2013 Seven Common Myths About Meditation", "sentiment": 0.11443997734036794, "text": "Meditation is becoming increasingly popular, and in recent years there have been calls for mindfulness (a meditative practice with Buddhist roots) to be more widely available on the NHS. Often promoted as a sure-fire way to reduce stress, it\u2019s also being increasingly offered in schools, universities and businesses.\n\nFor the secularised mind, meditation fills a spiritual vacuum; it brings the hope of becoming a better, happier individual in a more peaceful world. However, the fact that meditation was primarily designed not to make us happier, but to destroy our sense of individual self \u2013 who we feel and think we are most of the time \u2013 is often overlooked in the science and media stories about it, which focus almost exclusively on the benefits practitioners can expect.\n\nIf you\u2019re considering it, here are seven common beliefs about meditation that are not supported by scientific evidence.\n\nFact 1: It\u2019s easy to see why this myth might spring up. After all, sitting in silence and focusing on your breathing would seem like a fairly innocuous activity with little potential for harm. But when you consider how many of us, when worried or facing difficult circumstances, cope by keeping ourselves very busy and with little time to think, it isn\u2019t that much of a surprise to find that sitting without distractions, with only ourselves, might lead to disturbing emotions rising to the surface.\n\nHowever, many scientists have turned a blind eye to the potential unexpected or harmful consequences of meditation. With Transcendental Meditation, this is probably because many of those who have researched it have also been personally involved in the movement; with mindfulness, the reasons are less clear, because it is presented as a secular technique. Nevertheless, there is emerging scientific evidence from case studies, surveys of meditators\u2019 experience and historical studies to show that meditation can be associated with stress, negative effects and mental health problems. For example, one study found that mindfulness meditation led to increased cortisol, a biological marker of stress, despite the fact that participants subjectively reported feeling less stressed.\n\nFact 2: The idea that meditation is a cure-all for all lacks scientific basis. \u201cOne man\u2019s meat is another man\u2019s poison,\u201d the psychologist Arnold Lazarus reminded us in his writings about meditation. Although there has been relatively little research into how individual circumstances \u2013 such as age, gender, or personality type \u2013 might play a role in the value of meditation, there is a growing awareness that meditation works differently for each individual.\n\nFor example, it may provide an effective stress-relief technique for individuals facing serious problems (such as being unemployed), but have little value for low-stressed individuals. Or it may benefit depressed individuals who suffered trauma and abuse in their childhood, but not other depressed people. There is also some evidence that \u2013 along with yoga \u2013 it can be of particular use to prisoners, for whom it improves psychological wellbeing and, perhaps more importantly, encourages better control over impulsivity. We shouldn\u2019t be surprised about meditation having variable benefits from person to person. After all, the practice wasn\u2019t intended to make us happier or less stressed, but to assist us in diving deep within and challenging who we believe we are.\n\nFact 3: All global religions share the belief that following their particular practices and ideals will make us better individuals. So far, there is no clear scientific evidence that meditation is more effective at making us, for example, more compassionate than other spiritual or psychological practices. Research on this topic has serious methodological and theoretical limitations and biases. Most of the studies have no adequate control groups and generally fail to assess the expectations of participants (ie, if we expect to benefit from something, we may be more likely to report benefits).\n\nFact 4: There is very little evidence that an eight-week mindfulness-based group programme has the same benefits as of being in conventional psychological therapy \u2013 most studies compare mindfulness to \u201ctreatment as usual\u201d (such as seeing your GP), rather than one-to-one therapy. Although mindfulness interventions are group-based and most psychological therapy is conducted on a one-to-one basis, both approaches involve developing an increased awareness of our thoughts, emotions and way of relating to others. But the levels of awareness probably differ. A therapist can encourage us to examine conscious or unconscious patterns within ourselves, whereas these might be difficult to access in a one-size-fits-all group course, or if we were meditating on our own.\n\nFact 5: Meditation produces states of consciousness that we can indeed measure using various scientific instruments. However, the overall evidence is that these states are not physiologically unique. Furthermore, although different kinds of meditation may have diverse effects on consciousness (and on the brain), there is no scientific consensus about what these effects are.\n\nFact 6: In principle, it\u2019s perfectly possible to meditate and be uninterested in the spiritual background to the practice. However, research shows that meditation leads us to become more spiritual, and that this increase in spirituality is partly responsible for the practice\u2019s positive effects. So, even if we set out to ignore meditation\u2019s spiritual roots, those roots may nonetheless envelop us, to a greater or lesser degree. Overall, it is unclear whether secular models of mindfulness meditation are fully secular.\n\nFact 7: Meta-analyses show there is moderate evidence that meditation affects us in various ways, such as increasing positive emotions and reducing anxiety. However, it is less clear how powerful and long-lasting these changes are.\n\nSome studies show that meditating can have a greater impact than physical relaxation, although other research using a placebo meditation contradicts this finding. We need better studies but, perhaps as important, we also need models that explain how meditation works. For example, with mindfulness-based cognitive therapy (MBCT), we still can\u2019t be sure of the \u201cactive\u201d ingredient. Is it the meditation itself that causes positive effects, or is it the fact that the participant learns to step back and become aware of his or her thoughts and feelings in a supportive group environment?\n\nThere simply is no cohesive, overarching attempt to describe the various psychobiological processes that meditation sets in motion. Unless we can clearly map the effects of meditation \u2013 both the positive and the negative \u2013 and identify the processes underpinning the practice, our scientific understanding of meditation is precarious and can easily lead to exaggeration and misinterpretation.\n\n\u2022 Catherine Wikholm is the co-author, with Dr Miguel Farias, of The Buddha Pill\n\nCitation links have now been added to this article\n\n"},
{"url": "http://securityaffairs.co/wordpress/37075/hacking/ebay-reflected-file-download.html", "link_title": "EBay promptly fixed a reflected file download vulnerability", "sentiment": 0.10561728395061724, "text": "Ebay\u00a0is among the web services most targeted by cyber criminals and\u00a0phishers, over the years, security experts have spotted an impressive amount of attack exploiting techniques more or less sophisticated. Almost every attack relies on social engineering, attackers trick victims into clicking on malicious links or to visit websites hosting any kind of exploit kits used to steal victim\u2019s credentials.\n\nSecurity researcher David Sopas at WebSegura discovered a reflected file download vulnerability and reported it to eBay in March. Sopas\u00a0is a known expert that in the past discovered similar flaw affecting GitHub, Facebook and Instagram. The company promptly solved the problem and releases a fix a few days ago.\n\n\u201cWhat if malicious users could improve their phishing campaigns on eBay? What if malicious pages linked eBay to download malicious files and infect users?\u201d\u00a0Sopas\u00a0wrote in an advisory\u00a0that is temporary not available. \u201cWhen using eBay and inspecting it\u2019s requests I noticed a call to a JSON file that made me wonder a bit about a security vulnerability \u2013 Reflected Filename Download.\u201d\n\nThe company promptly solved the problem and released a fix a few days ago. The eBay website was affected by a reflected file download vulnerability that could be exploited by an attacker to trick victims into believing that they were downloading a file from the legitimate website. Sopas explained some browsers, including Internet Explorer 8 and 9, allow the attack by simply visiting a malicious URL, meanwhile other browsers make possible the exploitation of the flaw only by forcing the user to download the file.\n\nThe exploitation of the flaw in the eBay domain allow attackers to gain control of the victim\u2019s PC, the user in fact, have no perception of the attack because from their perspective a\u00a0harmless file is downloaded from the legitimate eBay website.\n\n\u201cPretty easy to implement but not easy to find. RFD attacks are still very common and many companies are not aware of it\u2019s real danger,\u201d\u00a0said Sopas, which connfirmend also that the attack is quite easy to carry out.\u00a0\u201cThe victim always thinks that the source file is on the trusted site.\u201d"},
{"url": "http://www.washingtonpost.com/posteverything/wp/2015/05/21/the-best-way-to-way-to-eliminate-the-gender-pay-gap-ban-salary-negotiations/", "link_title": "Eliminate the Gender Pay Gap by Banning Salary Negotiations", "sentiment": 0.15897591991341994, "text": "Ellen Pao, interim chief executive of Reddit, announced last month a ban on salary negotiations at the social media company. Her stated goal: to eliminate the persistent disadvantage that women have at the bargaining table.\n\nHer pronouncement came just days after Pao lost a high-profile sex-discrimination lawsuit against one of Silicon Valley\u2019s biggest venture-capital firms. Since then, she has insisted that companies \u201ccan\u2019t just hide\u201d from sexism in their workplaces and must be proactive in counteracting discrimination. Still, while it is true that women earn about 78 cents, on average, for every dollar a man makes for comparable work, Pao\u2019s no-negotiating policy has struck many as absurd.\n\nWhy take away an important tool for women to achieve equal pay? Why stop a woman with star qualifications from pushing for as much money as she can get?\n\nIn a perfect world, I would agree. Many people in the equal-pay debate argue that inferior negotiating skills are at the root of the gender pay gap. Teaching women to be better negotiators \u2014 or getting them to negotiate at all \u2014 would fix the issue. But the causes of this problem are more complicated than that. We have two decades of rigorous empirical research on how gender affects contract negotiations, and it all points in the same direction. Put simply: As we practice it in the United States, negotiation is a man\u2019s game with men\u2019s rules.\n\nAt bargaining tables, women\u2019s biggest obstacle isn\u2019t that they can\u2019t learn to be \u201cmore like men.\u201d The real problem is that most people, men and women alike, don\u2019t want them to be more like men.\n\nThe traits that both men and women associate with good negotiators are tied up with ideas of masculinity \u2014 such as rationality, assertiveness and self-assurance \u2014 rather than more feminine traits, such as emotionality and accommodation. That association automatically gives men the perceived upper hand in negotiations.\n\nIn a 2001 study I co-authored, we found that fewer than one-third of the surveyed business students believed that women had the advantage in negotiations, while 48 percent said men had the edge. The three most-cited reasons for men\u2019s supposed advantage were their strong and firm nature, aggressive and competitive instincts, and strong desire not to lose to a woman. I co-authored similar research last year that revealed that these perceptions have held steady. The respondents in that survey expected that women would be nicer negotiators than men, but also believed them to be less competent and more gullible.\n\nIf women aren\u2019t seen as tough enough at negotiating, why not just train them to \u201cman up\u201d? Unfortunately, even when they do employ traditionally male tactics, women still lose. Underlying our assumptions about what makes a good negotiator is the idea that it\u2019s okay \u2014 even necessary \u2014 to aggressively pursue one\u2019s self-interest when bargaining. It\u2019s not a sign of being selfish; it\u2019s what we expect. But we don\u2019t expect it in women.\n\nResearchers repeatedly have documented that people react more unfavorably to women who ask for more money, compared with men who do. A woman who negotiates is seen as especially demanding and therefore a less-than-ideal new colleague. In a series of controlled experiments in the 1990s, a Rutgers University study found that women risk being passed over for hire if they engage in self-promotion in job interviews, defying expectations of \u201cfeminine modesty.\u201d More than a decade later, Harvard and Carnegie Mellon researchers found that the effect persisted, with women facing backlash when behaving assertively in negotiations. To be demanding in a business setting is to be unfeminine, unseemly, shrewish or worse. This body of research underscores a cultural truth: Women are expected to be warm, empathetic and unselfish.\n\nThe Supreme Court ruled more than a quarter-century ago that this kind of gender stereotyping constitutes discrimination. In the landmark 1989 Price Waterhouse v. Hopkins case, a top-performing woman, Ann Hopkins, was denied a promotion for being too masculine in her interpersonal style. Supervisors at the accounting firm said Hopkins was \u201cmacho\u201d and \u201covercompensated for being a woman.\u201d She was advised to take a \u201ccourse at charm school\u201d and \u201cwalk more femininely, talk more femininely, dress more femininely, wear makeup, have her hair styled, and wear jewelry.\u201d\n\nThose genderized expectations persist today, with severe effects for women in negotiations. In 2014, Nazareth College rescinded a job offer to a woman for a faculty position after she attempted to negotiate a higher salary, maternity leave and a few other reasonable terms, according to Slate. \u201cThis is how I thought negotiating worked,\u201d she wrote in her account of the incident. It does, for some. But again and again, we find that women pay a price when they\u2019re assertive.\n\nSheryl Sandberg, Facebook\u2019s chief operating officer and one of the nation\u2019s most outspoken champions of women in business, has advised women to take an alternate route to winning in negotiations \u2014 playing up more feminine approaches. In their job interviews and salary negotiations, Sandberg advises women to always smile, to be \u201crelentlessly pleasant\u201d and \u201ccommunal,\u201d and to avoid taking a \u201ccritical stance.\u201d In other words, be \u201c\u2009\u2018appropriately\u2019 female.\u201d\n\nSadly, Sandberg isn\u2019t wrong. Our research at Berkeley supports the success of such tactics, by showing that women improve their chances in negotiations with men by using a touch of flirtation. In a 2012 study, American adults imagined selling a used car to a female buyer. When the buyer was flirtatious \u2014 playful, flattering and laughing \u2014 men gave her a lower price compared with a female buyer who adopted a neutral, \u201clet\u2019s get down to business\u201d demeanor. In a subsequent study, we found that feminine charm works its magic by putting men in a more positive mood.\n\nWhen it comes to playing hardball, women are damned if they do and damned if they don\u2019t. Training them to be tough negotiators won\u2019t overcome the cultural rules rigged against them in the workplace. And it\u2019s galling to think that women might need to employ a \u201cMad Men\u201d-era strategy of flirtation to get a fair shake. Given that salary negotiations ignite the gender pay gap at the starting gate,a gap fueled by small gender biases over time, negotiation-free workplaces are women\u2019s best option for getting the salaries they deserve.\n\nSuch policies do come with some risk. A ban on negotiations leaves a lot of power in the hands of employers, who may not be making equal salary offers to men and women in the first place. The solution is transparency. In an effort to encourage equity and trust, a growing number of companies reveal the salaries of all their employees, sometimes even posting them online. This makes it much harder to hoodwink job candidates and helps eliminate gender discrepancies.\n\nWill a no-negotiation policy hinder employers\u2019 ability to attract top talent? Quite the contrary. Research shows that companies that emphasize a culture of equality are attractive to workers. In a study I co-authored last year, we found that undergraduate business students, particularly aspiring businesswomen, are turned off by the prospect of working for employers who sacrifice fairness in exchange for profit. That\u2019s been chief executive Dane Atkinson\u2019s experience at analytics company SumAll, which has a transparent pay policy. \u201cWe\u2019ve gained far better people,\u201d he told Al Jazeera America last year, \u201cbecause people who think they\u2019ve earned their salary and have no issue with that tend to be better teammates.\u201d\n\nA no-negotiation policy implies that an employer pays based on a job\u2019s market value, rather than based on subjective individual characteristics. Laszlo Bock, chief of people operations at Google, recently extolled the virtues of this principle for eliminating the pay gap. Even making offers based on an individual\u2019s salary history can perpetuate the problem, he noted. \u201cWe figure out what the job is worth, not the person,\u201d he said during a talk in Washington.\n\nStop denying the gender pay gap exists. Even Jennifer Lawrence was shortchanged.\n\nMcDonald\u2019s was there for me when no one else was\n\nKeep Harriet Tubman \u2013 and all women \u2013 off the $20 bill"},
{"url": "https://www.python.org/dev/peps/pep-0484/", "link_title": "Type hints PEP in Python is accepted", "sentiment": 0.04987378246753248, "text": "Other approaches from which we have borrowed or to which ours can be compared and contrasted are described in PEP 482 .\n\nThe type system supports unions, generic types, and a special type named Any which is consistent with (i.e. assignable to and from) all types. This latter feature is taken from the idea of gradual typing. Gradual typing and the full type system are explained in PEP 483 .\n\nThe proposal is strongly inspired by mypy [mypy] . For example, the type \"sequence of integers\" can be written as Sequence[int] . The square brackets mean that no new syntax needs to be added to the language. The example here uses a custom type Sequence , imported from a pure-Python module typing . The Sequence[int] notation works at runtime by implementing __getitem__() in the metaclass (but its significance is primarily to an offline type checker).\n\nWhile these annotations are available at runtime through the usual __annotations__ attribute, no type checking happens at runtime . Instead, the proposal assumes the existence of a separate off-line type checker which users can run over their source code voluntarily. Essentially, such a type checker acts as a very powerful linter. (While it would of course be possible for individual users to employ a similar checker at run time for Design By Contract enforcement or JIT optimization, those tools are not yet as mature.)\n\nFor example, here is a simple function whose argument and return type are declared in the annotations:\n\nNote that this PEP still explicitly does NOT prevent other uses of annotations, nor does it require (or forbid) any particular processing of annotations, even when they conform to this specification. It simply enables better coordination, as PEP 333 did for web frameworks.\n\nThis PEP introduces a provisional module to provide these standard definitions and tools, along with some conventions for situations where annotations are not available.\n\nPEP 3107 introduced syntax for function annotations, but the semantics were deliberately left undefined. There has now been enough 3rd party usage for static type analysis that the community would benefit from a standard vocabulary and baseline tools within the standard library.\n\nIt should also be emphasized that Python will remain a dynamically typed language, and the authors have no desire to ever make type hints mandatory, even by convention.\n\nWhile the proposed typing module will contain some building blocks for runtime type checking -- in particular a useful isinstance() implementation -- third party packages would have to be developed to implement specific runtime type checking functionality, for example using decorators or metaclasses. Using type hints for performance optimizations is left as an exercise for the reader.\n\nOf these goals, static analysis is the most important. This includes support for off-line type checkers such as mypy, as well as providing a standard notation that can be used by IDEs for code completion and refactoring.\n\nThis PEP aims to provide a standard syntax for type annotations, opening up Python code to easier static analysis and refactoring, potential runtime type checking, and (perhaps, in some contexts) code generation utilizing type information.\n\nPEP 3107 added support for arbitrary annotations on parts of a function definition. Although no meaning was assigned to annotations then, there has always been an implicit goal to use them for type hinting [gvr-artima] , which is listed as the first possible use case in said PEP.\n\nType checkers are expected to attempt to infer as much information as necessary. The minimum requirement is to handle the builtin decorators @property , @staticmethod and @classmethod .\n\nA type checker is expected to check the body of a checked function for consistency with the given annotations. The annotations may also used to check correctness of calls appearing in other checked functions.\n\nIt is recommended but not required that checked functions have annotations for all arguments and the return type. For a checked function, the default annotation for arguments and for the return type is Any . An exception is that the first argument of instance and class methods does not need to be annotated; it is assumed to have the type of the containing class for instance methods, and a type object type corresponding to the containing class object for class methods. For example, in class A the first argument of an instance method has the implicit type A . In a class method, the precise type of the first argument cannot be represented using the available type notation.\n\nAny function without annotations should be treated as having the most general type possible, or ignored, by any type checker. Functions with the @no_type_check decorator or with a # type: ignore comment should be treated as having no annotations.\n\nThe syntax leverages PEP 3107 -style annotations with a number of extensions described in sections below. In its basic form, type hinting is used by filling function annotation slots with classes:\n\nThis states that the expected type of the argument is . Analogically, the expected return type is .\n\nExpressions whose type is a subtype of a specific argument type are also accepted for that argument.\n\nType hints may be built-in classes (including those defined in standard library or third-party extension modules), abstract base classes, types available in the module, and user-defined classes (including those defined in the standard library or third-party modules). While annotations are normally the best format for type hints, there are times when it is more appropriate to represent them by a special comment, or in a separately distributed stub file. (See below for examples.) Annotations must be valid expressions that evaluate without raising exceptions at the time the function is defined (but see below for forward references). Annotations should be kept simple or static analysis tools may not be able to interpret the values. For example, dynamically computed types are unlikely to be understood. (This is an intentionally somewhat vague requirement, specific inclusions and exclusions may be added to future versions of this PEP as warranted by the discussion.) In addition to the above, the following special constructs defined below may be used: , , , , , all ABCs and stand-ins for concrete classes exported from (e.g. and ), type variables, and type aliases. All newly introduced names used to support features described in following sections (such as and ) are available in the module.\n\nFrameworks expecting callback functions of specific signatures might be type hinted using . Examples: from typing import Callable def feeder(get_next_item: Callable[[], str]) -> None: # Body def async_query(on_success: Callable[[int], None], on_error: Callable[[int, Exception], None]) -> None: # Body It is possible to declare the return type of a callable without specifying the call signature by substituting a literal ellipsis (three dots) for the list of arguments: Note that there are no square brackets around the ellipsis. The arguments of the callback are completely unconstrained in this case (and keyword arguments are acceptable). Since using callbacks with keyword arguments is not perceived as a common use case, there is currently no support for specifying keyword arguments with . Similarly, there is no support for specifying callback signatures with a variable number of argument of a specific type.\n\nSince type information about objects kept in containers cannot be statically inferred in a generic way, abstract base classes have been extended to support subscription to denote expected types for container elements. Example: Generics can be parametrized by using a new factory available in called . Example: In this case the contract is that the returned value is consistent with the elements held by the collection. A expression must always directly be assigned to a variable (it should not be used as part of a larger expression). The argument to must be a string equal to the variable name to which it is assigned. Type variables must not be redefined. supports constraining parametric types to a fixed set of possible types. For example, we can define a type variable that ranges over just and . By default, a type variable ranges over all possible types. Example of constraining a type variable: The function can be called with either two arguments or two arguments, but not with a mix of and arguments. There should be at least two constraints, if any; specifying a single constraint is disallowed. Subtypes of types constrained by a type variable should be treated as their respective explicitly listed base types in the context of the type variable. Consider this example: The call is valid but the type variable will be set to and not . In effect, the inferred type of the return value assigned to will also be . Additionally, is a valid value for every type variable. Consider the following: def count_truthy(elements: List[Any]) -> int: return sum(1 for elem in elements if element) This is equivalent to omitting the generic notation and just saying .\n\nYou can include a base class to define a user-defined class as generic. Example: from typing import TypeVar, Generic T = TypeVar('T') class LoggedVar(Generic[T]): def __init__(self, value: T, name: str, logger: Logger) -> None: self.name = name self.logger = logger self.value = value def set(self, new: T) -> None: self.log('Set ' + repr(self.value)) self.value = new def get(self) -> T: self.log('Get ' + repr(self.value)) return self.value def log(self, message: str) -> None: self.logger.info('{}: {}'.format(self.name message)) as a base class defines that the class takes a single type parameter . This also makes valid as a type within the class body. The base class uses a metaclass that defines so that is valid as a type: from typing import Iterable def zero_all_vars(vars: Iterable[LoggedVar[int]]) -> None: for var in vars: var.set(0) A generic type can have any number of type variables, and type variables may be constrained. This is valid: Each type variable argument to must be distinct. This is thus invalid: You can use multiple inheritance with : Subclassing a generic class without specifying type parameters assumes for each position. In the following example, is not generic but implicitly inherits from :\n\nGeneric types like or cannot be instantiated. However, user-defined classes derived from them can be instantiated. Suppose we write a class inheriting from : Now there are two ways we can instantiate this class; the type inferred by a type checker may be different depending on the form we use. The first way is to give the value of the type parameter explicitly -- this overrides whatever type inference the type checker would otherwise perform: x = Node[T]() # The type inferred for x is Node[T]. y = Node[int]() # The type inferred for y is Node[int]. If no explicit types are given, the type checker is given some freedom. Consider this code: The inferred type could be , as there isn't enough context to infer a more precise type. Alternatively, a type checker may reject the line and require an explicit annotation, like this: A type checker with more powerful type inference could look at how is used elsewhere in the file and try to infer a more precise type such as even without an explicit type annotation. However, it is probably impossible to make such type inference work well in all cases, since Python programs can be very dynamic. This PEP doesn't specify the details of how type inference should work. We allow different tools to experiment with various approaches. We may give more explicit rules in future revisions. At runtime the type is not preserved, and the class of is just in all cases. This behavior is called \"type erasure\"; it is common practice in languages with generics (e.g. Java, TypeScript).\n\nis only valid as a base class -- it's not a proper type. However, user-defined generic types such as from the above example and built-in generic types and ABCs such as and are valid both as types and as base classes. For example, we can define a subclass of that specializes type arguments: from typing import Dict, List, Optional class Node: ... class SymbolTable(Dict[str, List[Node]]): def push(self, name: str, node: Node) -> None: self.setdefault(name, []).append(node) def pop(self, name: str) -> Node: return self[name].pop() def lookup(self, name: str) -> Optional[Node]: nodes = self.get(name) if nodes: return nodes[-1] return None is a subclass of and a subtype of . If a generic base class has a type variable as a type argument, this makes the defined class generic. For example, we can define a generic class that is iterable and a container: Now is a valid type. Note that we can use multiple times in the base class list, as long as we don't use the same type variable multiple times within . Also consider the following example: In this case MyDict has a single parameter, T.\n\nA type variable may specify an upper bound using . This means that an actual type substituted (explicitly or implictly) for the type variable must be a subclass of the boundary type. A common example is the definition of a Comparable type that works well enough to catch the most common errors: from typing import TypeVar class Comparable(metaclass=ABCMeta): @abstractmethod def __lt__(self, other: Any) -> bool: ... ... # __gt__ etc. as well CT = TypeVar('CT', bound=Comparable) def min(x: CT, y: CT) -> CT: if x < y: return x else: return y min(1, 2) # ok, return type int min('x', 'y') # ok, return type str An upper bound cannot be combined with type constraints (as in used , see the example earlier); type constraints cause the inferred type to be _exactly_ one of the constraint types, while an upper bound just requires that the actual type is a subclass of the boundary type.\n\nConsider a class with a subclass . Now suppose we have a function with an argument annotated with . Should we be allowed to call this function with a variable of type as its argument? Many people would answer \"yes, of course\" without even considering the consequences. But unless we know more about the function, a type checker should reject such a call: the function might append an instance to the list, which would violate the variable's type in the caller. It turns out such an argument acts _contravariantly_, whereas the intuitive answer (which is correct in case the function doesn't mutate its argument!) requires the argument to act _covariantly_. A longer introduction to these concepts can be found on Wikipedia [wiki-variance] ; here we just show how to control a type checker's behavior. By default type variables are considered _invariant_, which means that arguments for arguments annotated with types like must exactly match the type annotation -- no subclasses or superclasses of the type parameter (in this example ) are allowed. To facilitate the declaration of container types where covariant type checking is acceptable, a type variable can be declared using . For the (rare) case where contravariant behavior is desirable, pass . At most one of these may be passed. A typical example involves defining an immutable (or read-only) container class: from typing import TypeVar, Generic, Iterable, Iterator T = TypeVar('T', covariant=True) class ImmutableList(Generic[T]): def __init__(self, items: Iterable[T]) -> None: ... def __iter__(self) -> Iterator[T]: ... ... class Employee: ... class Manager(Employee): ... def dump_employees(emps: ImmutableList[Employee]) -> None: for emp in emps: ... mgrs = ImmutableList([Manager()]) # type: ImmutableList[Manager] dump_employees(mgrs) # OK The read-only collection classes in are all defined using a covariant type variable (e.g. and ). The mutable collection classes (e.g. and ) are defined using regular invariant type variables. The one example of a contravariant type variable is the type, which is contravariant in the argument type (see below). Note: variance affects type parameters for generic types -- it does not affect regular parameters. For example, the following example is fine: from typing import TypeVar class Employee: ... class Manager(Employee): ... E = TypeVar('E', bound=Employee) # Invariant def dump_employee(e: E) -> None: ... dump_employee(Manager()) # OK\n\nThere are three different builtin classes used for arrays of bytes (not counting the classes available in the module): , and . Of these, and have many behaviors in common (though not all -- is mutable). While there is an ABC defined in and a corresponding type in , functions accepting bytes (of some form) are so common that it would be cumbersome to have to write everywhere. So, as a shortcut similar to that for the builtin numeric classes, when an argument is annotated as having type , arguments of type or are acceptable. (Again, there are situations where this isn't sound, but we believe those are exceedingly rare in practice.)\n\nWhen a type hint contains names that have not been defined yet, that definition may be expressed as a string literal, to be resolved later. A situation where this occurs commonly is the definition of a container class, where the class being defined occurs in the signature of some of the methods. For example, the following code (the start of a simple binary tree implementation) does not work: To address this, we write: The string literal should contain a valid Python expression (i.e., should be a valid code object) and it should evaluate without errors once the module has been fully loaded. The local and global namespace in which it is evaluated should be the same namespaces in which default arguments to the same function would be evaluated. Moreover, the expression should be parseable as a valid type hint, i.e., it is constrained by the rules from the section Acceptable type hints above. It is allowable to use string literals as part of a type hint, for example: A common use for forward references is when e.g. Django models are needed in the signatures. Typically, each model is in a separate file, and has methods that arguments whose type involves other models. Because of the way circular imports work in Python, it is often not possible to import all the needed models directly: # File models/a.py from models.b import B class A(Model): def foo(self, b: B): ... # File models/b.py from models.a import A class B(Model): def bar(self, a: A): ... # File main.py from models.a import A from models.b import B Assuming main is imported first, this will fail with an ImportError at the line in models/b.py, which is being imported from models/a.py before a has defined class A. The solution is to switch to module-only imports and reference the models by their _module_._class_ name: # File models/a.py from models import b class A(Model): def foo(self, b: 'b.B'): ... # File models/b.py from models import a class B(Model): def bar(self, a: 'a.A'): ... # File main.py from models.a import A from models.b import B\n\nSince accepting a small, limited set of expected types for a single argument is common, there is a new special factory called . Example: from typing import Union def handle_employees(e: Union[Employee, Sequence[Employee]]) -> None: if isinstance(e, Employee): e = [e] ... A type factored by responds to checks for and any of its subtypes, and any of its subtypes, and so on. One common case of union types are optional types. By default, is an invalid value for any type, unless a default value of has been provided in the function definition. Examples: As a shorthand for you can write ; for example, the above is equivalent to: An optional type is also automatically assumed when the default value is , for example:\n\nA special kind of type is . Every type is a subtype of . This is also true for the builtin type . However, to the static type checker these are completely different. When the type of a value is , the type checker will reject almost all operations on it, and assigning it to a variable (or using it as a return value) of a more specialized type is a type error. On the other hand, when a value has type , the type checker will allow all operations on it, and a value of type can be assigned to a variable (or used as a return value) of a more constrained type."},
{"url": "https://github.com/skroutz/string_metric", "link_title": "A simple Ruby library with String Metric algorithms", "sentiment": 0.027777777777777773, "text": "A simple library with String Metric algorithms. If you want to read more about String Metric algorithms please read here.\n\nThis library wants to support MRI (1.9.3, 2.0.0, 2.1.0), JRuby and Rubinius.\n\nAdd this line to your application's Gemfile:\n\nOr install it yourself as:\n\nThe public api for Levenshtein Distance is the method .\n\n: It sets an upper limit for the calculated distance. Can be or . : It overrides the default (equals to 1) insertion penalty. Can be or . : It overrides the default (equals to 1) deletion penanty. Can be or . : It overrides the default (equals to 1) substitution penalty. Can be or . : The desired strategy for Levenshtein distance. Supported strategies are , , , and . The default strategy is for MRI and for other platforms One should not depend on strategy.\n\nYou can run benchmarks with\n\nor you can choose to benchmark a specific algorithm like:\n\nCurrently the set of fixtures is very small - ruby 2.1.0 is used\n\nstring_metric is licensed under MIT. See License"},
{"url": "http://www.reddit.com/r/rust/comments/36z2n0/rust_made_me_a_worse_c_programmer_right/", "link_title": "Rust made me a worse C++ Programmer", "sentiment": 0.04537037037037038, "text": "Yesterday something embarrassing happened to me:\n\nI've been coding Rust for half a year now and I really love it. Before that I coded in C++ for many years. Recently I had to code C++ for university again and I did a really dumb thing:\n\nI used pointers pointing inside a while pushing onto that vector.\n\nAfter a while I noticed my mistake and started to think about this saying \"Rust will make you a better C++ programmer\". Clearly that didn't work out so well for me. Is it me being stupid in this situation or is it normal that one gets used to the protection of the rust compiler?"},
{"url": "http://www.computerworld.com/article/2922909/emerging-technology/how-would-alan-turing-fix-ai.html", "link_title": "How would Alan Turing fix A.I.?", "sentiment": 0.07744432102984734, "text": "We want speaking machines because language is the best way to rapidly communicate ideas. Users want their lives made easier and Hollywood wants their 1960s predictions proven right.\n\nBut computers don\u2019t work for artificial intelligence (A.I.).\u00a0Alan Turing, the famous British mathematician, cryptologist and a founding father of today\u2019s computers would have pointed out that they weren't designed for it.\n\nComputers were designed to compress and duplicate information. They don\u2019t handle A.I. well because they leave too much work to programmers. Programmers were replaced by engineers using statistics in waves of hope from the 1970s, but the results remain inaccurate and limited. A.I. hasn't scaled and after nearly 60 years of effort, and a new approach is warranted.\n\nAlan Turing\u2019s paper from 1936 describes the computer process that is still in use today. He modeled a human who did computations with\u00a0a digital computer that emulates her. Human computers at the time supported many things including ballistics, banking and science.\n\nBoth human and digital computers calculate by blindly following procedures. Interestingly, human computers were usually thought of as women. The first computers were men\u00a0in the 1700s, but from the late 1800s and especially during the Second World War, computers (and the earliest programmers) were typically women.\n\nCan you imagine it? Turing emulated the 1930s human computer that used sheets of paper to process. He did not emulate what they did to sit at the table, drink coffee, see the sheets with their eyes, think about the next step or move their hand to write numbers.\n\nThis model has held back A.I. ever since, because what a person does and how they do it are radically different things. Human speech, for example, is a totally different problem to adding 1+1 on many levels.\n\nTuring defines the computer as storing a finite number of symbols -- like an alphabet and a set of punctuation and numerals. You cannot store anything else. This is the compression of information. And if you want to store the same word twice, you duplicate it by storing the same symbols twice in the same sequence. Compression and duplication are a design feature of computers and they are different to brains.\n\nThis design feature is amazingly powerful, with the Internet, iPhones, Windows and the general explosion in technology the beneficiary, but limiting for A.I.\n\nImagine how many times Wikipedia stores the most common English word: \"the\"? That is duplication on a massive scale. With A.I., the duplication creates the need for a search facility. Today, a search facility means we often guess answers because a single word can have many meanings. Worse, different words can mean the same things, such as when we store more than one language or dialect.\n\nMany in the field consider it heresy to suggest that computation is not at the core of the problems in the cognitive sciences, but that's the problem. Lack of progress means we need to consider alternatives.\n\nAlan Turing\u2019s tragic death at a young age in 1954 may have cost us getting intelligent machines much earlier because we lost his visionary guidance when we needed it most. Two years later, A.I. was founded at the Dartmouth Conference. Its computer-science supporters were familiar with the details underlying computers at the time. The trouble is, those computers were based on human computers. A.I. should be modeled on human brains, machines that can handle human languages, not on what human computers did.\n\nScience uses models to predict results. Bad models don\u2019t accurately predict results, but good ones do.\u00a0The targets that have been set in the world of A.I. aren't aligned with the expectations of customers.\n\nSiri, Dragon, Google, Bing Translate and others illustrate the gap: users want improvements. Despite an obvious lack of progress, engineers continue to try to leverage systems based on the old models.\n\nImagine if normal computers were like this. If your bank account were to change from time to time due to errors, you\u2019d be angry, while today\u2019s free Internet translation companies hand over totally wrong translations regularly.\n\nTuring would have told the A.I. community that computers, at their core, are not the right start for intelligent machines.\n\nThis article is published as part of the IDG Contributor Network. Want to Join?"},
{"url": "http://www.vertabelo.com/blog/technical-articles/web-app-development-with-flask-sqlalchemy-bootstrap-introduction", "link_title": "Flask web application development. Introduction", "sentiment": 0.25493827160493826, "text": "The tutorial is divided into four articles:\n\nThe aim for this tutorial is to build a simple TODO web application using Flask microframework, SQLalchemy,PostgreSQL 9.3 and Vertabelo and finally to deploy our existing app to the Heroku cloud. Please, don\u2019t treat this application as a final product for production. The main purpose is to show the process of developing a web app in Python.\n\nThis series follows the development of a simple small-sized CRUD app using Flask. The goal of this tutorial is to have a working application that will enable you to:\n\nThe finished code is hosted on Github.\n\nMy choice for the database during development was PostgreSQL, which is one the most frequently chosen open source databases.\n\nTo map our designed relational model into an object we use object relational mapper (ORM) SQLAlchemy. In this application we will use Flask-SQLAlchemy extension \u2013 a simple wrapper for SQLAlchemy.\n\nFlask is a Python microframework. It\u2019s main job is to take data from the database (by querying it with SQL using ORM) and embed the retrieved data into HTML using the Jinja2 template engine. This data can be displayed in a browser.\n\nBootstrap is the most popular HTML, CSS, and JS framework for developing responsive, mobile first projects on the web.\n\nBefore we dive into real application development we will touch on some of the principles associated with web development. Indeed, it isn\u2019t an easy task. Many libraries and frameworks provide some help. The plethora of choices and possibilities are enormous. That\u2019s why we\u2019ll start with a quick example of how to do this by looking at a typical Flask webapp architecture.\n\nAt the lowest level there\u2019s a database because we need a place to store data. The Flask framework, with the help of some extensions, retrieves the data from the database and renders into a viewable format.\n\nLet\u2019s start with retrieving data. The role of ORM (Object-relational Mapping) can is to map the relational model into an object. There are multiple implementations of ORM in many programming languages. In Python, the most popular is SQLAlchemy which is what we\u2019ll use for our example application.\n\nSo, the application instance talks to a database via SQLAlchemy. Now we can process the retrieved data. The application instance can\u2019t deliver the rendered web site on it\u2019s own. Instead, this task is passed to a web server - another entity that sits between the Flask application and the browser. The web server participates in the communication with the browser \u2013 it processes requests via HTTP/HTTPS and passes these requests to the application instance, using a protocol called Web Server Gateway Interface (WSGI).\n\nBy combining HTML, CSS and JavaScript, a browser is able to render a nice application. HTML stands for hyper text markup language. It\u2019s role is to define the structure of web site. CSS (Cascading Style Sheets) describes how a web site should look. In turn, JavaScript is used to implement the dynamic behavior of web site."},
{"url": "http://finance.yahoo.com/news/surprising-danger-being-good-job-211212286.html", "link_title": "The danger of being good at your job", "sentiment": 0.23792225534028807, "text": "Science confirms what high performers have known for years: It's not easy being so competent.\n\nA new study from Duke's Fuqua School of Business suggests that people with high self-control \u2014 the kind of people who remember birthdays, choose the salad instead of the fries, take on extra projects at work, and resolve conflicts easily \u2014 might actually pay a price for those virtues.\n\n\"People always talk about how having high self-control is a good thing,\" says researcher Christy Koval, a Ph.D. candidate and first author on the study, which was published in this month's Journal of Personality and Social Psychology. And in many ways, it is a good thing: \"Go-getters get what they go after,\" she points out. \"They're better at goal pursuits. They make very good relationship partners.\"\n\nThey're also better-off financially than their less-disciplined peers; they tend to be in better health, and they generally have higher-quality personal relationships.\n\nBut all that comes at a cost: High-self-control people, the researchers found, end up burdened by their own competence.\n\nFor one thing, people will expect more of you \u2014 whether or not that's actually a valid expectation. In one study, Koval and her colleagues asked undergraduates to rate how well they expected a fictional subject to do academically based on whether or not he accidentally binged on new music at the iTunes store.\n\nIn another, they asked people to assess how good that fictional subject was at his job based on how good he was at saving for a new apartment. The results were the same: The more self-control people exhibited, the more people expected of them.\n\nTo be fair to the rest of us, that's not an irrational assumption. The researchers point out that self-control does predict good performance. Nor is it all bad for the achievers \u2014 it is nice to be depended upon. But by expecting more of those people, we may be burning them out, Koval says.\n\nThat's because while the researchers found we tend to assign high-self-control people more work \u2014 which, again, makes sense \u2014 it's not actually any easier for them to do that work, even if it seems that way. Tasks are just as effortful for high-self-control people, Koval explains \u2014 it's just that they're better them. \"They persist longer,\" she says. \"They might adapt better strategies.\" But while science might appreciate you, your boss probably doesn't. People tend to underestimate how much effort go-getters put in.\n\nAll that leads to a problem: High-self-control people feel more burdened by their work relationships than their less-disciplined peers. They sacrifice more for the coworkers, the researchers found, even when those sacrifices come at the expense of their own goals. And that same dynamic plays out in romantic relationships. Being reliable is draining.\n\nWhich doesn't mean go-getters should stop go-getting. The benefits of high self-control still far outweigh the costs.\n\nBut managers (and coworkers and romantic partners) should take note: If you take those high-self-control people for granted, you may risk losing them. While relying on go-getters might be a good short-term strategy \u2014 they'll get stuff done \u2014 in the long run, Koval suggests, they \"might become dissatisfied with this burden we're placing on them.\"\n\nAccordingly, it's essential to recognize them for their (probably underestimated) efforts. They need to feel \"a return on the effort they're putting in,\" she says.\n\nNOW WATCH: How To Respond To 8 Illegal Interview Questions"},
{"url": "http://www.bbc.co.uk/news/world-europe-32856232", "link_title": "Ireland same-sex referendum set to approve gay marriage", "sentiment": 0.14793650793650798, "text": "Early indications suggest the Republic of Ireland has voted to legalise same-sex marriage in a historic referendum.\n\nMore than 3.2m people were asked whether they wanted to amend the country's constitution to allow gay and lesbian couples to marry.\n\nGovernment ministers have said they believe it will pass, while prominent \"no\" campaigners have conceded defeat.\n\nCounting started at 09:00 BST on Saturday morning. An \"unusually high\" turnout has been reported.\n\nA result is expected by mid to late afternoon on Saturday.\n\nIf the change is approved, the Republic of Ireland would become the first country to legalise same-sex marriage through a popular vote.\n\nMinister for Equality Aodhan O Riordain said on Twitter: \"I'm calling it. Key boxes opened. It's a yes. And a landslide across Dublin. And I'm so proud to be Irish today.\"\n\nMinister for Health Leo Varadkar, who earlier this year came out as the Republic of Ireland's first openly gay minister, said the campaign had been \"almost like a social revolution\".\n\nSpeaking from the Dublin count, he told Irish broadcaster RTE that it appeared about 75% of votes being counted there were in favour of legalising same-sex marriage.\n\nSome prominent \"no\" campaigners have already conceded defeat.\n\nDavid Quinn of the Iona Institute, a Catholic group, said it was \"obviously a very impressive victory for the 'yes' side\".\n\n\"Obviously there's a certain amount of disappointment, but I'm philosophical about the outcome,\" he told RTE.\n\n\"It was always going to be an uphill battle - there were far fewer organisations on the 'no' side, while all the major political parties were lined up on the 'yes' side and you had major corporations coming out for the first time to say how we should vote on a particular issue.\"\n\nDublin, Limerick and Waterford passed the 60% electorate turnout mark, while in Cork, Carlow, Kilkenny, Donegal, Tipperary, Kerry and Galway it was above 50%.\n\nThe upper courtyard of Dublin Castle is open to 2,000 people for people to view the declarations on a large screen.\n\nBefore Friday, votes had already been cast in some islands as well as hospitals, hospices and nursing homes. Irish citizens who are registered were allowed to vote, but there was no postal voting. Many people returned to Ireland to cast their votes.\n\nThey were asked whether they agreed with the statement: \"Marriage may be contracted in accordance with law by two persons without distinction as to their sex.\"\n\nThe referendum was being held 22 years after homosexual acts were decriminalised in Ireland.\n\nIn 2010, the Irish government enacted civil partnership legislation, which provided legal recognition for gay couples.\n\nBut there are some important differences between civil partnership and marriage, the critical one being that marriage is protected in the constitution while civil partnership is not.\n\nA constitutional convention established by the Irish government in 2013 considered the specifics of a proposal on extending marriage rights, as well as discussing other changes to the constitution.\n\nIt voted in favour of holding a referendum on same-sex marriage and the date was announced by Taoiseach (Prime Minister) Enda Kenny earlier this year.\n\nIf the measure is passed, Catholic churches will continue to decide for themselves whether to solemnise a marriage.\n\nThe leader of the Catholic Church in Ireland, Eamon Martin, has said the church may look at whether it continues to perform the civil side of solemnisation if the change comes in.\n\nA separate referendum, on whether the eligibility age of presidential candidates should be lowered from 35 to 21, was being held at the same time, along with a parliamentary by-election in the Carlow-Kilkenny constituency.\n\nSame-sex marriage is currently legal in 19 countries worldwide.\n\nDid you cast your vote? You can share your experiences by emailing haveyoursay@bbc.co.uk.\n\nIf you would be happy to speak further to a BBC journalist, please include a contact telephone number.\n\nEmail your pictures to yourpics@bbc.co.uk, upload them here, tweet them to @BBC_HaveYourSay or text 61124. If you are outside the UK, send them to the international number +44 7624 800 100 orWhatsApp us on +44 7525 900971"},
{"url": "http://www.zapcc.com/", "link_title": "Zapcc: A (much) faster C++ compiler", "sentiment": 0.04642857142857142, "text": "Zapcc is a Clang based C++ compiler, with unique header compilation technology that\u00a0speeds up\u00a0C++ compilations. Zapcc accelerates by compiling and caching C++ header files, saving the time-intensive re-compilations of the same header files\u00a0\u2013\u00a0similar to pre-compiled headers, but on steroids, resulting in much faster build times.\n\nZapcc has been in the cooking for a couple\u00a0years now. It\u2019s extremely complicated, but we are getting close to release. Contact us to join the private beta group already accelerating their C++ builds.\n\nWe believe that developers should not wait on compilers. Zapcc delivers exactly this."},
{"url": "http://m.youtube.com/watch?v=kt92-ZDm-HM", "link_title": "How Japan Cleans Its Bullet Trains in 7 Minutes Flat [video]", "sentiment": 0.12142857142857143, "text": "Rating is available when the video has been rented.\n\nThis feature is not available right now. Please try again later."},
{"url": "http://therealnews.com/t2/index.php?option=com_content&task=view&id=31&Itemid=74&jumival=13889", "link_title": "Five Big Banks Plead Guilty to Rigging Currency Markets and No One Goes to Jail", "sentiment": 0.35833333333333334, "text": "All original content on this site is copyright of The Real News Network. Click here for more\n\nProblems with this site? Please let us know"},
{"url": "http://classics.esquire.com/hell-sucks/", "link_title": "Hell Sucks (1968)", "sentiment": 0.027046226988745276, "text": "here is a map of Vietnam on the wall of my apartment in Saigon, and some nights, coming back late to the city, I\u2019ll lie out on my bed and look at it, too tired to do anything more than just get my boots off. The map is a marvel, especially absorbing because it is not real. For one thing, it is very old. It was left here years ago by a previous tenant, probably a Frenchman since the map was made in Paris. The paper has buckled, and much of the color has gone out of it, laying a kind of veil over the countries it depicts. Vietnam is divided into its older territories of Tonkin, Annam and Cochin China, and to the west, past Laos and Cambodge, sits Siam, a kingdom. That\u2019s old, I told the General. That\u2019s a really old map.\n\nThe General is drawn to it too, and whenever he stops by for a drink he\u2019ll regard it silently, undoubtedly noting inaccuracies which the maps available to him have corrected. The waters that wash around my Indochine are a placid, Disney blue, unlike the intense, metallic blues of the General\u2019s maps. But all of that aside, we both agree to the obsolescence of my map, to the final unreality of it. We know that for years now, there has been no country here but the war. The landscape has been converted to terrain, the geography broken down into its more useful components; corps and zones, tactical areas of responsibility, vicinities of operation, outposts, positions, objectives, fields of fire. The weather of Vietnam has been translated into conditions, and it\u2019s gone very much the same way with the people, the population, many of whom can\u2019t realize that there is an alternative to war because war is all they have ever known. Bad luck for them, the General says. As well as he knows them (and he knows them well), he seldom talks about them except to praise \u201ctheir complexity, their sophistication, their survivability.\u201d Endearing traits.\n\nEveryone is terribly sorry about what the war is doing to Vietnam and the Vietnamese, especially since the cities have been brought into it, although somehow most of the official expressions of grief have about them that taint of Presidential sorrow, turning a little grinny around the edges. The Tet Offensive changed everything here, made this an entirely different war, made it Something Else. (\u201cNonsense,\u201d a colonel told me. \u201cWe\u2019re just doing the same things in the cities that we\u2019ve done in the boonies, why \u2026 for years!\u201d He was not the same man who said, \u201cWe had to destroy Bentre in order to save it,\u201d but he might have been. He\u2019d be hip to that.) Before Tet, there was some clean touch to jungle encounters, some virtue to their brevity, always the promise of quick release from whatever horror there was. The war went on in bursts, meeting engagements; and covering it\u2014particularly in the Highlands and the Delta, II Corps and IV Corps\u2014you were always a tourist, a tripper who could summon up helicopters like taxis. You would taxi in, the war would break over you suddenly and then go away, and you would taxi out. Enough chances were taken to leave you exhilarated, and, except for the hangovers that any cheap thrill will give you, it was pleasant enough. Now, it is awful, just plain awful, awful without relief. (A friend on The New York Times told me that he didn\u2019t mind his nightmares so much as his waking impulse to file on them.) It has finally become that kind of conventional war that the Command so longed for, and it is not going well. And for every month that it continues not going well, the scope of its destruction is enlarged. We are not really a particularly brutal people, certainly no more brutal now than we\u2019ve been in other wars, acquiring it as the war goes on. But our machine is devastating. And versatile. It can do almost everything but stop.\n\nAnd after all these years, we were caught in midwinter with the blunt truth that our achievement in Vietnam had been less than epic, a fact that touched everyone but the men who run the war. It became finally clear that General Westmoreland did not understand this war (\u201cThis is a different war than Americans have ever been asked to fight,\u201d he told the Examining Angels. \u201cHow is it different?\u201d they asked. \u201cWell, you know, it\u2019s just \u2026 different\u201d), and he was asked to leave it. The immediate official response was manic; after years and years of posing along the rim, the Mission joined hands and leapt through the Looking Glass. It was as though Swift\u2019s vault had been plundered to meet the public doubt. They trotted out their kill ratios, their curious estimates of enemy morale (there wasn\u2019t any), their poor, salvaged shards of Pacification (that good American idea; it would have worked wonderfully in New Mexico), strange redemption profiles of the countryside\u2019s lost security. The same incantations, the heavies, moderates and lights of this statistic-obsessed war, were sung again, and optimism was spent at the same excessive rate which we had previously maintained in the expending of ordnance. This antic Thumbs-Upmanship was best pegged by a British correspondent who compared it all to the captain of the Titanic announcing, \u201cThere\u2019s nothing to be alarmed about, ladies and gentlemen. We\u2019re only stopping briefly to take on some ice.\u201d And I remembered an Indian lady I once knew who shipped a trunk to her family in Calcutta. She had lost the key but found another, the key to one of her closets, and she mailed that on after the trunk. She knew it wouldn\u2019t open the trunk, but she so wanted it to work that she sent it anyway. Strange story, but I expect it might touch our Ambassador, and possibly even our former Commanding General.\n\nhen the battle for Hu\u00e9 was all over, they entered it into the records, gauged its terrible cost and battlegrammed it, so that it took on the dry, tactical stamp of the West Point Atlas of American Wars. When future observers come to it, it will seem that some order had been apparent during the twenty-seven days that it took to get the North Vietnamese and Vietcong forces out of the Imperial City, and that will not be exactly the truth. Hu\u00e9 was not the bloodiest battle of the Vietnam war (unless you enter in the more than four thousand civilian dead and the tens of thousands who were wounded, not likely in any forthcoming revised-edition of the W.P.A.O.A.W.), but it was the hardest and the bitterest, and for those of us who were a part of it, even the coldest chronicles will be enough to recall the texture of its dread. If the war was changing, Hu\u00e9 was that turn of the screw which locked the new terms into place for good, taking you beyond that cutoff point where one war becomes just like all other wars. You would get twinges of this feeling any time that you were on the line, with the troops; but still, before Hu\u00e9, you thought of yourself as a dove or a hawk, felt that our involvement was criminal or proper, obscene or clean. After Hu\u00e9, all of your lines of reasoning turned into clumsy coils, and all of the talk got on your nerves. Hu\u00e9 finally gave you what you had expected, half yearned for, in the days of the war that ended with the Offensive. It got up memories, vicarious enough, stored from old copies of Life magazine, old movie newsreels, Path\u00e9 sound tracks whose dirge-disaster music still echoed: the Italian Campaign, the fight for the Reservoir, gruesome camp, evocations of \u201944 and \u201950.\n\nGoing in, there were sixty of us packed in a deuce-and-a-half, one of eight trucks moving in convoy from Phubai, bringing in over three hundred replacements for the casualties taken in the earliest fighting south of the Perfume River. There had been a harsh, dark storm going on for days, and it turned the convoy route into a mud bed. It was terribly cold in the trucks, and the road was covered with leaves that had either been blown off the trees by the storm or torn away by our heavy artillery. The artillery had done a job here, touched everything. Many of the houses had been completely collapsed, and not one had been left without at least heavy pitting from shell fragments. Hundreds of refugees held to the side of the road as we passed. Many of them had been wounded during the shelling. The kids would laugh and shout the standard, \u201cYou you you! Okay?\u201d The old would look on with that quiet tolerance for misery that makes so many Americans uneasy, which is usually misread as indifference. But the younger men and women would often give us looks of unmistakable contempt, pulling their cheering children back from the trucks.\n\nSo we sat there, grinning at the bad weather and the discomfort, sharing the first fear, glad that we were not riding point or closing the rear because, man, the middle is good. They had been hitting our convoys regularly, and a lot of the trucks had been turned back. The houses that we were passing so slowly made the best kind of cover for snipers, and one B-40 rocket could have made casualties out of a whole truckload of us. All the Grunts were whistling and no two were whistling the same tune, and it sounded like a locker room just before a big game. A friend of mine, Sergeant Dale Dye, a Marine correspondent, sat with a tall yellow flower sticking out of his helmet cover, a really outstanding target, the kind of idiosyncracy the Marines will indulge in. His eyes rollicked, and below his big moustache his wicked, shy smile said, \u201cOh yes, Charlie\u2019s got his shit together here, this will be oh-so-bad, indubitably.\u201d It was the same smile I saw later when a sniper\u2019s bullet tore up the wall two inches above his head inside the Citadel. Odd cause for merriment in anyone but a Grunt.\n\nThere\u2019s something you see in the faces of Marines that you\u2019ll never see in the Army, some extra character etched in by the training and by more hard times than you\u2019d believe, by constant intimidation, by the widespread conviction that you will get yours if you hang in there long enough. They\u2019re each of them like the hardest man on the block (You ain\u2019t been cut, you ain\u2019t my man) and they all have that wild, haunted, going-West look that says it is perfectly correct to be here where the fighting is worst, where you won\u2019t have half of what you\u2019ll need, where it is colder than the Nam ever gets. To pass the time, I started reading the stuff they\u2019d written on their helmet covers and flak jackets. There were the names of campaigns and the names of their girls, nicknames (The Entertainer, The Avenger, Short Time Safety Moe), the slogans that touch on their lonely, severe fantasies (Born to Lose, Born to Raise Hell), and general graffiti (Hell Sucks, Time is On My Side, Yossarian Lives, Just You and Me God\u2014Right?). There was nothing on the truck as good as the scrawl on the wall in Khesahn that said, \u201cI Think I\u2019m Falling In Love With Jake,\u201d but it passed the time.\n\nAnd they are all giving you that mock-astonished look. \u201cYou mean you don\u2019t have to be here? And you\u2019re here!\u201d But they are glad you\u2019re here, really very grateful. \u201cHey, Esquire! Hey, you want a story, man ? Write this: I\u2019m up there on S81, this was May, I\u2019m up there walkin\u2019 the ridgeline an\u2019 this Zip jumps up smack into me, lays this AK-47 fuckin\u2019 right into me, only he\u2019s so surprised I got my whole clip off \u2019fore he knew how to thank me for it. Grease one.\u201d After twenty kilometers of this, in spite of the roiling dark sky ahead, we could see the smoke coming up from the far side of the river, from the Citadel of Hu\u00e9.\n\nThe bridge was down that spanned the canal dividing the village of An Cu and the southern sector of Hu\u00e9, blown the night before by the Vietcong, and the forward area beyond the opposite bank was not thought to be secure, so we bivouacked in the village for the night. It had been completely deserted, and we set ourselves up in empty hootches, laying our poncho liners out over the litter of shattered glass and brick. At dusk, while we were all stretched out along the canal bank eating dinner, two Marine gunships came down on us, strafing us, sending burning tracers up along the canal, and we ran for cover, more astonished than scared. \u201cWay to go, mother-lover, way to pinpoint the motherin\u2019 enemy,\u201d one of the Grunts screamed, and he set up his M-60 machine gun in case they came back. \u201cI don\u2019t guess we gotta take that shit.\u201d Patrols were sent out, guards posted, and we went to the hootches to sleep.\n\nFor some reason, we were not even mortared that night.\n\nhe next morning we knew that the area must have been secured beyond a reasonable doubt, because the A.R.V.N, were there. Good little fighters, the A.R.V.N.; ask any U.S. adviser in the field. Most of them here were not even armed. They needed both hands free for their work that morning, which consisted of thoroughly combing every house and store in the village, turning out drawers, tipping over chests and urns, raiding chicken coops and liquor cabinets, kicking in all the glass cases they could find, and forcibly relieving refugees on the road of radios, wine, ducks, clothing, anything. What they couldn\u2019t carry, they wore. One soldier moved up the road in an old felt hat that fell down over his eyes and a blue gabardine overcoat at least eight sizes too large, so that it trailed around him in the mud as he walked. I thought he was going to ask me the way to Floogle Street, but he only smiled proudly at his good luck and ducked into one of the shops.\n\nIt was the same after we\u2019d crossed the canal on a two-by-four and started walking in. We tried to flag down a lift, but the jeeps all seemed to be driven by A.R.V.N. officers out on organized looting parties. We walked along in the open toward the river, talking in an offhanded way about how superb the N.V.A. snipers were supposed to be, until we came across the very first of the hundreds of civilian dead that we were to see in the next weeks: a little girl who had been hit while riding her bicycle and an old man who lay arched over his straw hat. They\u2019d been lying out like that for over a week, and for the first time I was grateful for the cold.\n\nAlong the Perfume River\u2019s south bank there is a long, graceful park that separates Hu\u00e9\u2019s most pleasant avenue, Le Loi, from the riverfront. People will talk about how they\u2019d sit out there in the sun and watch the sampans moving down the river, or watch the girls bicycling up Le Loi, past the villas of officials and the French-architected university buildings. Many of those villas had been destroyed and much of the university permanently damaged. In the middle of the street a couple of ambulances from the German Mission had been blown up, and the Cercle Sportif was covered with bullet holes and shrapnel. In the park itself, four fat green dead lay sprawled around a tall, ornate cage, inside of which sat a small, shivering monkey. One of the correspondents along stepped over the corpses to feed it some fruit. (Days later, I came back to the spot. The corpses were gone, but so was the monkey. There had been so many refugees and so little food then, and someone must have eaten him.) The Marines of 2/5 had secured almost all of the central south bank and were now fanning out to the west, fighting and clearing one of the major canals. We were waiting for some decision on whether or not U.S. Marines would be going into the Citadel itself, but no one had any doubts about what that decision would be. Didn\u2019t it always come to that with the Grunts? Didn\u2019t it, every goddam time? We sat there taking in the dread by watching the columns of smoke across the river, receiving occasional sniper rounds, infrequent bursts of .50 caliber, watching the Navy L.C.U.\u2019s on the river getting shelled from the wall. One Marine next to me was saying that it was just a damned shame, all them poor people, all them nice-looking houses. He was looking at the black napalm blasts and the wreckage along the wall. \u201cLooks like the Imperial City\u2019s had the schnitz,\u201d he said.\n\nIt stayed cold for the next ten days, cold and dark, and that damp gloom was the background for the footage that we all took out of the Citadel. The little sunlight there was caught the heavy motes of dust that blew up from the wreckage of the East Wall, held it until everything you saw was filtered through it. And most of what you saw was taken in from unaccustomed angles, prone positions or quick looks from a crouch; lying flat out, hearing the hard dry rattle of shrapnel scudding against the debris around you, listening to the Marine next to you who didn\u2019t moan, \u201cOh my God, Oh Sweet Jesus, Oh Holy Mother save me,\u201d but who sobbed, instead, \u201cAre you ready for this? I mean, are you ready for this?\u201d Once, when the noise from a six-round mortar attack stopped, I heard some singing in back of me. There were three Grunts huddled together holding onto their helmets, looking more mischievous than scared. \u201cWe gotta get out of this place,\u201d they sang, \u201cif it\u2019s the las\u2019 thing we ever do-woo.\u201d With all of that dust blowing around, the acrid smell of gunpowder would hang in the air for a long while after fire fights, and there was also some CS gas that we\u2019d fired at the N.V.A. blowing back in over our positions. It was impossible to get off a clean breath with all of that going on, and of course there was that other smell too, that most special of all smells that came up from shallow graves and from shattered heaps of stone wherever an air strike had passed. It held to the lining of your nostrils and worked itself into the weave of your fatigues, and weeks later, miles away, you would wake up from a dream in the middle of the night and it would be there in the room with you. The N.V.A. had dug themselves so deeply into the wall that air strikes had to destroy it meter by meter, dropping napalm as close as three hundred meters from our positions. Up on the highest point of the wall, on what had once been a tower, I looked across the Citadel\u2019s moat and saw the N.V.A. moving quickly among the rubble of the opposing wall. We were close enough to be able to see their faces. A rifle went off a few feet to my right, and one of the figures across the moat started forward and then dropped. A Marine sniper leaned back from his cover and grinned at me.\n\nBy the end of that week, the wall had cost the Marines roughly one casualty for every meter taken, a quarter of them K.I.A. 1/5, which came to be known as the Citadel Battalion, had been through every tough battle the Marines had had in the past six months, and now some of its companies were down to below platoon strength. They all knew how bad it was, the novelty of fighting in the streets had become a nasty, spooky joke, and not many of them really believed they\u2019d ever get out alive. Everyone wanted to get wounded.\n\nThere was a tough, quiet Negro who called himself Philly Dog. He\u2019d been a gang lord in the streets of North Philadelphia, and in Hu\u00e9 he was the best man to be with, the only one who really understood how it was when you had no cover and no rear. He was better here than the hottest jungle fighter, better than those lean, mean Nam veterans with their proficiency badges for coaxing water out of palm roots, filleting snakes and reading moss. Philly Dog was the only scout you could feel right about in Hu\u00e9.\n\n\u201cJust hold onto it, man,\u201d he\u2019d say. \u201cYou doan go out there. That\u2019s Charlie.\u201d He pointed up the road.\n\nWe were in among the makings of a former villa, with only the rear wall still standing. I couldn\u2019t see anything up the road past one of our tanks, only a few houses, scattered trees and wires and a gigantic portion of collapsed wall.\n\n\u201c \u2019Cause if I was Charlie, that\u2019d be my spot.\u201d And he was right, almost every time.\n\nAt night, in the battalion C.P., the Major in command would sit reading his maps, staring vacantly at the trapezoid of the Citadel. It could have been a scene in a Norman farmhouse twenty-five years ago, with candles burning on the tables, bottles of red wine arranged along damaged shelves, the cold in the room, the high ceilings, the heavy ornate cross on the wall. The Major had not slept for five nights, and for the fifth night in a row he assured us that tomorrow would get it for sure, the final stretch of the wall would be taken, and he had all the Marines he needed to do it. And one of his aides, a tough mustang of a first lieutenant, would pitch a hard, ironic smile above the Major\u2019s stare, a smile that rejected good news and opted for doom, and it was like hearing him say, \u201cThe Major here is full of shit, and we both know it.\u201d\n\nWe found a villa near the C.P. and set ourselves up in it for the night. We never stayed in the same area two nights in a row, since it never took the N.V.A. very long to get us zeroed in. In the living room of the villa there were photographs of a Vietnamese family that had been taken in the States; the father in a dark business suit standing somewhere in New York; Mom, Dad and the kids at Disneyland. The Grunts could never get over the fact that there were wealthy Vietnamese, and these pictures filled them with awe. Dale Dye was there (after the sniper had barely missed him, he had gotten rid of that flower), and some of the guys had found a bottle of Veuve Clicquot. Usually they\u2019d scarfe up the 7 Crown or the Calvert\u2019s and leave four-star cognacs sitting on the shelves, but the champagne intrigued them. Most of them had never tasted it. One tall kid was saying that where he came from, it only got poured at weddings. Dye popped the cork, and one of them went chasing after it, giggling at how gala this was getting to be. Dye passed the bottle to the tall boy, who put it to his lips as if it might go off before drinking it. \u201cIt tickles m\u2019 nose,\u201d he said, and Dye broke up, shaking his head. \u201cIt\u2019s a good champagne,\u201d he said. \u201cNot a great champagne, but a good champagne.\u201d\n\nWe slept so soundly that night that a sixty-round mortar barrage a little before dawn failed to wake us.\n\nAfter the Catholic chaplain was killed, the Protestant had to give communion. His name was Takesian, an Armenian from Boston, one of those hip, blunt clerics who loved to talk, as though talking itself contained ritual powers of redemption. He wasn\u2019t one of your grizzled battle chaplains, but he was very brave, and very much affected by the particular ugliness of the Hu\u00e9 fighting. It was not physical fear that put him off, but the mood of bitterness that no one seemed to be able to shake, and he would sit for long stretches by himself, staring at the wounded through his thick steel-rimmed glasses. He was using sliced C-ration white bread and canteen water to deliver the sacraments, and some of the Grunts were skeptical about receiving them from a Protestant. \u201cListen, you silly bastards,\u201d Takesian said. \u201cYou could all get your ass shot off any time now out there. Do you think God gives a damn how you\u2019ve been blessed?\u201d\n\nometimes one of the companies would find itself completely cut off, and it would take hours for the Marines to get their casualties out. I remember one Marine with a head wound who finally made it up to the Battalion C.P., only to find himself stuck in a stalled jeep. He finally jumped out of the jeep and started to push it, knowing it was the only way out of there. Most of the tanks and trucks which carried casualties had to move up a long, straight road with no cover and they began calling it Rocket Alley. Every tank the Marines had had been hit at least once there. An epiphany of Hu\u00e9 turned up in John Olson\u2019s great photograph in Life, the wounded from Delta Company piled hurriedly on the tank. Sometimes, on the way out to the Battalion Aid Station, the more seriously wounded would take on that bad color, the grey-blue fishbelly promise of death that would spread upwards from the chest and cover the face. There was one Marine who had been shot through the neck, and all the way out the corpsmen massaged his chest. By the time they reached the station, though, he was so bad that the doctor triaged him, passed him over to treat the ones that he knew could be saved, and when they put him into the green canvas body bag there was some chance that he was still clinically alive. The doctor had never been in a position before where he had had to choose like that\u2014there were so many wounded\u2014and he never got used to it. During the lulls, he\u2019d step outside for some air, but it was no better out there. The bodies were stacked together, and there was always a crowd of A.R.V.N. standing around staring, death-enthralled like all Vietnamese. Since they did not know what else to do, and not knowing how it would look to the Marines, they would smile vacantly at the bodies there, and a couple of ugly incidents occurred. The Marines who had volunteered for the body details were overworked and became snappish, ripping packs off of corpses angrily, cutting gear away with bayonets, heaving bodies into the green bags. One of the dead Marines had become stiff and they had trouble getting him to fit. \u201cDamn,\u201d one of them said, \u201cdidn\u2019t this mother have big feet on him?\u201d And he finally forced the legs into the canvas. In the station, there was the youngest-looking Marine I\u2019d ever seen, so young that his parents must have had to sign for him at enlistment. He\u2019d been caught in the knee by a large piece of shrapnel, and he had no idea at all of what they would do with him now that he\u2019d been wounded. He lay out on the stretcher while the doctor explained how he would be choppered back to the Phubai hospital and then put on a plane for Danang, and then flown back to the States for what would probably be the rest of his hitch. At first the boy was sure that the doctor was kidding him, then he started to believe a little of it, and when he knew that it was true, that he was actually getting out, he couldn\u2019t stop smiling, and enormous tears of happiness ran down into his ears.\n\nIt was at this point that I began to recognize almost every casualty, remember conversations we\u2019d had days and even hours earlier, and that\u2019s when I got out, riding a Medevac chopper with a lieutenant who was covered with blood-soaked bandages. He\u2019d been hit in both legs, both arms, the chest and head, his ears were filled with caked blood, and he asked a photographer named Art Greenspon who was in the chopper if he\u2019d get a picture of him like this so he could mail it to his wife.\n\nBut at this point, the battle for Hu\u00e9 was almost over. The Cav was working the northwest corner of the Citadel, and elements of the 101st had come in through what had formerly been an N.V.A. resupply route. Vietnamese Marines and some of the 1st A.R.V.N. Division, who had fought well from the beginning, had been moving the remaining N.V.A. down toward the wall. The N.V.A. flag that had flown for so long over the South Wall had been brought down, and in its place an American flag had been put up, a sight which must have thrilled those most xenophobic of all Vietnamese, the people of Hu\u00e9. Two days later the Hoc Bao, Vietnamese Rangers for whom this privilege had been reserved, stormed through the walls of the Imperial Palace, but there were no enemy troops left inside. Except for a few corpses that bobbed sluggishly in the moat, most of the dead had been buried. Nearly seventy percent of Vietnam\u2019s one lovely city was destroyed, and if the landscape seemed desolate, imagine how the figures in that landscape looked.\n\nThere were two official ceremonies marking the expulsion of the N.V.A., both flag raisings. On the south bank of the Perfume River, two hundred refugees from one of the camps were recruited to stand in the rain and watch the G.V.N. flag being run up. But the rope snapped, and the crowd, thinking the V.C. had shot it down, broke up in panic. (There was no rain in the stories that the Saigon papers ran, there was no trouble with the rope, and the cheering crowd numbered thousands.) As for the other ceremony, the Citadel was still thought by most people to be insecure, and when the flag finally went up there was no one there to watch it except a handful of Vietnamese troops.\n\nn the first weeks after the Tet Offensive began, the curfew began early in the afternoon, and was strictly enforced. By two-thirty each day Saigon had the look of the final reel of On the Beach, a desolate city whose long avenues held nothing but refuse, windblown papers, small, distinct piles of human excrement and the dead flowers and spent firecracker casings of the Lunar New Year. Alive, Saigon had been depressing enough, but once the Offensive began it became so stark that, in an odd way, it was invigorating. The trees along the main streets all looked like they\u2019d been struck by lightning, and it became unusually, uncomfortably cold; one more piece of bad luck in a place where nothing was in its season. With so much filth growing in so many streets and alleys, an epidemic of plague was feared, and if there was ever a place that suggested plague, demanded it, it was Saigon in the Emergency. Large numbers of American civilians, the construction workers and engineers who were making it here like they\u2019d never made it at home, began openly carrying weapons, 45\u2019s and grease guns and AK\u2019s, and no mob of Mississippi sheriff\u2019s boys ever promised more bad news. You\u2019d see them at ten in the morning on the terrace of the Continental Hotel, waiting for the bar to open, unable to light their own cigarettes until it did. The crowds on Tu Do Street looked like Ensor processions, and there was a corruption in the air that had nothing to do with government officials on the take. After seven in the evening, when the curfew became total, nothing but police vehicles and M.P. jeeps moved in the streets, except for a few very young children who raced up and down over the rubbish, running newspaper kites up into the chilling wind. Shortly after dark, I could expect to see the headlights of the General\u2019s jeep coming up the street toward my apartment.\n\nThe General is a great favorite of the press here. He is commonly thought to be candid, articulate and accessible, which is absolutely the highest compliment the press corps can pay to any member of the American Mission. He is less accessible now that the war has begun to go badly, but he still finds time most nights to drop around for a quick drink, before returning to his headquarters and, I imagine, a late night\u2019s work. I have never really understood our growing friendship, since there is not a single point touching the war that we agree on. It is thought by outsiders that the General and I spend our evenings playing chess, but in fact I never learned the game, and its abstractions make the General nervous. My colleagues think that he drinks with me instead of them because I am accredited to a monthly, but of course there\u2019s more to it than that. For one thing, the General never condescends to me, while I take a lot of trouble trying not to understand him too quickly. I suppose that we are both, in our own way, aesthetes. The General is an aesthete of insurgency and counterinsurgency, a choreographer of guerrilla activity, and he has been at it a long, long time. Some of the older hands here remember seeing him in Vietnam at the time of the Indochina War. He was a captain then, and he would turn up in odd, remote corners of the country dressed in black pajamas. He is supposed to have spoken fluent Vietnamese then, although he now flatly denies any familiarity with that language, which he will actually mimic quite cruelly, breaking into protracted fits of laughter. It\u2019s said that he took a break in service during the early Sixties, two years in which he all but completely disappeared. He has no command designation as such, but is connected vaguely with something called Special Operations, about which he refuses to speak.\n\nOne is immediately struck by the clean-lined ruggedness of his features, although the longer you observe him the more you notice something delicate there, some softness behind the eyes that is almost feminine. The eyes are ice-blue but not cold, and they suggest his most interesting trait, an originality of mind that one never associates with the Military, and which constantly catches you off balance. It\u2019s impossible to guess his age to a certainty (I\u2019d never think of asking him), but he is probably around fifty.\n\n\u201cHow\u2019s the war going, General?\u201d I\u2019ll ask him.\n\n\u201cOh, how does it ever go, Mike? Slowly. Damned slowly.\u201d\n\nHe accepts his drink, lights a Bastos and sinks into a chair.\n\n\u201cWe\u2019re hurting him,\u201d he says. \u201cWe know we\u2019re hurting him. What we don\u2019t know is how much more of it he can take. We\u2019re killing him.\u201d He raises his glass. \u201cTo absent friends.\u201d\n\nWe talk about many things: Blake, Mexico, the Beethoven Quartets, Oriental women, the Saints, wines, the Elizabethans, classic automobiles and, obviously, Vietnam, which I don\u2019t really understand that well. Before the Offensive, we would argue about whether the American position here was morally defensible. I believed it was not, the General believed it was beside the point. In fact (we never said this, but somehow mutually acknowledged it) the subject bored us both. Now, since Tet, I\u2019ve been more concerned with whether or not our position is even militarily defensible, and the General is optimistic there. Sometimes, he worries about me, about my safety and, even more, about my sanity. I have what he refers to as \u201cthis thing about death,\u201d an unhealthy fascination with so much of what I\u2019ve had to see here. He respects it intellectually (one of our other constant topics is suicide) but finally he finds it morbid and unprofitable. Worst of all, he finds that I have a tendency, when discussing the dead, to not only dwell on them, but to personalize them as well. \u201cThat way lies you-know-what,\u201d he says, tapping his temple; but he lets me get it out, lets me talk about the victims, about the dead and the disposition of the dead.\n\nThe first dead I saw in Vietnam was a Cambodian Mercenary serving with the Special Forces in the Seven Mountains Region of the Delta. He had accidentally shot himself in the head while cleaning his .30-caliber rifle. Mercenaries live in a compound with their families, and this one had his parents, his grandparents and his wife with him at the time. The medics bandaged his entire head so that he looked like something you\u2019d see in relief on an old temple wall, some dead prince, very dignified in repose. The women squatted over his body, and their moaning built up into a terrible wail, falling off and beginning again, hour after hour. Some blood and brine from the wound had seeped through the bandages and filled a small dent in the canvas, so that when they carried him from the stretcher some of it spilled over my boots. \u201cSorry,\u201d one of the medics said. \u201cGot some on you.\u201d The next dead that I saw were in a mass of over one hundred, Vietcong who had tried to overrun the perimeter of an outpost of the 25th Infantry Division near Tayninh. They had been stopped by 105\u2019s firing fl\u00e9chette canisters, thousands of steel shafts that cut them up in the most incredible way, leaving them almost unrecognizable as human beings, although you could see that some of them were very young and some were women. In Cantho, on the morning after the Offensive began, there were around forty V.C. piled up at the end of the airstrip, and one of them was a medic who had died huddled over his aid case. One of the Americans worked him loose with his feet, jammed a cigar into the clenched teeth and photographed it. Another American was screaming at a very young dead, almost sobbing, \u201cThere, you silly bastard, there! You got it now? You got it?\u201d Americans often admonish the dead like that, particularly the young ones. The bodies were all loaded into the back of a truck, where they lay all day, growing stiff in the positions which they had taken in death. When the truck finally started, one of them fell off the back. He was so rigid that he landed exactly on both knees and one elbow (a perfect three-pointer, one of the guys called it) with no other part of him touching the ground, and he had to be lifted up into the truck like a heavy wrought-iron figure.\n\nLater that day, in the provincial hospital at Cantho, the friendly dead began accumulating in the corridors. The Vietcong shelled the hospital for over seven hours from Cantho University, which they had captured and held until it was finally bombed flat, and even if we had wanted to leave we couldn\u2019t have. They needed help desperately that day. Over four hundred civilian wounded were brought in, most of them children (\u201cWho shot you, V.C. or U.S.?\u201d the Psywar types kept asking), and we had to cleanse the wounds, cut away dead tissue, or just lean on them, hold them still while the surgeons worked. Outside of the operating rooms there were all of those who were beyond saving, already going grey before death. They just waited there, and you could see they knew. There was an odd piece of graffiti up on the wall of one of the hospital rooms, and I passed it a hundred times that day, always meaning to find out who had written it, and why. It said, \u201cHow do you feel about decay, Senator?\u201d In spite of the mortars, a number of Vietnamese came into the hospital carrying wounded children, strangers whom they\u2019d found lying out in the streets, and a number of others came in simply to help. At this point in the fighting the IV Corps Commander, General Maim, was absorbed in constructing a solid five-block perimeter around his house, a strange sort of defense plan considering that the most precious region of the country was at stake, and we were not permitted to drive our jeeps through this perimeter because they drew fire. We had to drive instead through sniper-infested sections of the city, frequently through ambushes along the road, and there were certainly a lot of dead to be seen there.\n\nOf course it is much closer to you when the dead are Americans, and closer still when you\u2019ve known them. I\u2019m always being told about our comparatively heightened regard for human life, and a lot of us here think that it\u2019s exclusively American. I knew a G.I. in Bu Dop who could look at blown-up Vietnamese all day, V.C. or friendly, men, women and children, it didn\u2019t much matter. \u201cHell, they ain\u2019t people,\u201d he said. \u201cThem\u2019re Slopes.\u201d But the sight of one quite cleanly killed American made him vomit. The war was a very simple one for him, and you can bet that he had a solution for ending it. But we did agree that it was a bad thing that Americans were getting killed. I\u2019d spent enough hours flying out of combat LZ\u2019s in choppers shared with the dead. Often enough, they had no faces left at all, and some died with that wincing sucking-in of breath that shows the full pain of it, some with the dreamy smiles of the drowned, and one that I particularly remember with full staring eyes and a look of mighty outrage, like some Old Testament picture of wrath at the injustice of it. Some just get Blown Away, and sometimes, if they can\u2019t reassemble a more or less total corpse from the found parts, they will enter it as Missing. \u201cShitty way to buy the farm,\u201d one kid told me.\n\nFor me, though, the very worst dead was a Vietnamese who had been killed near a canal in southern Hu\u00e9, on the road leading to the Hotel Company C.P. The very top of his head had been shaved off by a piece of debris, so that only the back of his scalp remained connected to the skull. It was like a lidded container whose contents had poured out onto the road to be washed away by the rains. Perhaps something had driven over it, or perhaps it had just collapsed during the ten days or more that it had lain there, but I couldn\u2019t get the image of it out of my mind. I spent that afternoon with the commander of Hotel, checking their defense perimeter. He was a great, decent Marine named Captain Christmas. This was not a wealthy section of Hu\u00e9. The homes were modest, sometimes nothing more than elaborate hootches, but walking around Hotel\u2019s positions you could see that the entire section had been planned and landscaped, its arranged pathways decorated with statues, its gardens formally designed. Christmas was very moved by this, and his men had strict orders to respect the homes, the grounds and the people. But when it came to spending the night there, my nerves gave out. The Grunts probably assumed that I was afraid of a mortar attack, which was ridiculous since one could be and usually was mortared almost anywhere in Hu\u00e9 at any time. It was that dead out there with his hinged scalp. I knew that if I stayed here he would drift in over me that night, grinning and dripping, all rot and green-black bloat. After I\u2019d decided to go, I knew that I\u2019d have to pass him again on the way out, and when the time came I forgot my promise and looked back at him one more time. \u2026\n\nThe General holds up his hand. He\u2019s been leaning forward, listening like a crack therapist, and his eyes have gone narrow. He\u2019s been tuned into it, all right.\n\n\u201cYes. Of course,\u201d he says. \u201cIt\u2019s terrible. I didn\u2019t really expect it to happen this way. If they\u2019d listened to me then.\u201d\n\nHe shakes his head and a guarded smile comes over his face. Outside, there is a gecko chittering and screaming, and a cluster of magnesium flares are settling down over the perimeter beyond Tan Son Nhut. The General\u2019s driver, a giant Khmer called Lurch, is sleeping behind the wheel of the jeep down by the curb.\n\n\u201cSometimes,\u201d the General says, \u201cI think I\u2019m the only man in the world who understands this thing.\u201d\n\n\u201cIt must be very lonely for you.\u201d\n\n\u201cMike, it comes with the job. But you. If you hate this all so much, why do you stay?\u201d\n\nHe has me there. I wait a moment before answering. \u201cBecause, General, it\u2019s the only war we\u2019ve got.\u201d\n\nAnd he really smiles now. After all that talk, we\u2019re speaking the same language again.\n\nMichael Herr, a relative unknown in the late sixties, persuaded Esquire editor Harold Hayes to send him to Vietnam for more than a year as a war correspondent in 1967. His Esquire stories were compiled in Dispatches, published in 1977. He has cowritten several screenplays, including Full Metal Jacket, and he provided the narration for Apocalypse Now."},
{"url": "http://www.wired.com/2015/05/silk-road-untold-story/", "link_title": "Silk Road: The Untold Story", "sentiment": -0.05069444444444446, "text": "In October 2013, a young entrepreneur named Ross Ulbricht was arrested at the Glen Park branch of the San Francisco Public library. It was the culmination of a two-year investigation into a vast online drug market called Silk Road. The authorities charged that Ulbricht, an idealistic 29-year-old Eagle Scout from Austin, Texas, was the kingpin of the operation. They said he\u2019d reaped millions from the site, all transacted anonymously with Bitcoin. They said he\u2019d devolved into a cold-blooded criminal, hiring hit men to take out those who crossed him.\n\nThe story of how Ulbricht founded Silk Road, how it grew into a $1.2 billion operation, and how federal law enforcement shut it down is complicated, dark, and utterly fascinating. This two-part series tells that story."},
{"url": "http://www.trakaid.com/gas-suppliers-need-tracking-solution/", "link_title": "This Is Why Tracking Solution Is Important for Gas Suppliers", "sentiment": 0.07970227051859706, "text": "After the global recession, the economy is steadily recovering. So, improving the supply chain efficiency to maintain profits and spirit during the good and bad economic times has become the focal point for all businesses. The expanding markets, multiplying suppliers, and inflating consumers has changed the scenario from tip to toe. The margins are shrinking, and competition is growing. There is an increase in the exigency to slick operations and curtail costs.\n\nThe companies dealing with gas supply business acknowledge the essentiality of retrenching cost, ameliorating service and efficiency and accelerating return on investments from one pole of the supply chain to the other. The gas suppliers invest a huge sum of money in cylinders that remain the main supply asset for delivering gas. These cylinders need to be fully utilized, maintained and tracked. The loss and theft of these tankers are expensive and troublesome.\n\nIn the complex environment, asset tracking solutions have become important to track the tankers throughout their lifecycle. Below are some reasons why gas suppliers need tracking solutions.\n\nThe optimization of an expansive supply chain tends to take up a lot of time. This is because the company follows manual procedures; the data acquired remains prone to errors due to the human inefficiency and costs ample of money. To eradicate these issues out of the businesses, the gas suppliers depend on asset tracking system that utilizes RFID technology to its full depth.\n\nMachine-to-machine data acquisition uploads all the information automatically without the need for much human intervention. The data status remains up-to\u2013date within lesser time. According to a survey, RFID raises inventory accuracy from an average of 63% to 95%. Moreover, the inventory labor productivity improves by 96%.\n\nToday, the gas supply organization is fully aware that the visibility of asset, inventory and shipment has become critical. RFID tracking system provides an automated way of knowing what the gas suppliers own and where it is in the rotation process. Unlike barcodes, RFID does not require a direct line of sight to be read and can read many tags simultaneously at once.\n\nA tracking solution such as Trakaid\u2019s CyTrack provides end-to-end visibility. It gives real-time tracking status and allows verification of the history and location of the cylinders. Such a visibility based on RFID reading can give the gas supply companies a better analysis of when inventory problems are likely to occur so that prior actions to resolve them are undertaken seriously.\n\nThe gathered information makes the suppliers view the sleeping and lost cylinders, eliminating buying of new cylinders and recognizing the spots for recovery of the cylinders. They can share the tracking information with their distributors to provide value added services, for example- internal tracking.\n\nCustomers\u2019 sustainability remains at stake if the gas is not rearranged at the right time. Maintaining the process of monitoring gas available in tanks requires constant training and review. Ultimately, emergency calls for a gas delivery result in a hustle-bustle in the gas company. The delivery of the gas consumes time and non-availability of gas harm the revenue.\n\nThe gas suppliers require a smooth sailing process for the delivery of gases. To provide hassle free services to the customers, cylinder suppliers depend on technology. Gas monitoring equipment such as Trakaid\u2019s LgMonitor removes out-of-stock issues with gas supplies via latest sensing and telemetry technology to sense and measure average daily consumption and transfer information to a remote location.\n\nIn this way, the gas suppliers can plan for a delivery and arrange for the gas. As a result, with a gas monitoring system, the deliveries will be scheduled and the possibility of emergency requests at odd timings will end.\n\nDue to the lack of specifications, it becomes bothersome for the gas supply company to identify the cylinder that must go for testing. Because of this, the damaged cylinders at times are used for the supply of gases, evoking dangers at all levels in the process. As a result, the gas supply companies face the loss of bonds with the distributors, partners and customers.\n\nUnlike barcodes, tracking devices such as RFID provides full specification on the cylinder tags. The date of testing is mentioned on the RFID tag that makes the gas suppliers aware of its testing needs. Due to this, only the fittest cylinders are used for supplying gas, making the association strongest.\n\nYou can also growth hack your gas supply business by using Trakaid\u2019s CyTrack and LgMonitor."},
{"url": "https://www.bignerdranch.com/blog/music-visualization-with-d3-js/", "link_title": "Music Visualization with D3.js", "sentiment": 0.25644208037825067, "text": "In my previous post, we created a bar chart using D3.js.\n\nWhile brainstorming ideas on how to elaborate on my first post, I had a flashback to music visualizers in WinAmp where shapes would beautifully expand and contract with the frequency of the music. I thought this would be a great opportunity for me to learn the Web Audio API as well as display what D3 can do. You can view the finished visualizer here.\n\nFirst, let\u2019s add our HTML source and controls.\n\nPoint the source of the element to your audio file. In this case, I created an audio folder and added an mp3 of Odesza\u2019s song \u201cAbove The Middle,\u201d available for free from their website. But if you think my taste in music is horrendous, feel free to use your own!\n\nIf you were to open up your index.html now, you should be able to play, pause and increase/decrease the volume of your audio file. All you have to do now is slap a 500 page Terms of Service agreement on it and you got yourself an app!\n\nTo be able to make a visualization, we need to grab the data from the song and then interpret it. Introducing the\u2026\n\nThe Web Audio API gives you access to control, modify and interpret audio within the browser. I was surprised by how much data you can retrieve about an audio file\u2014and there are endless possibilities for modification. You can grab the frequency data, time info, create an oscilloscope or a voice modulator and much more. Once you have the streaming frequency data, you could make whatever visualization you can imagine! It\u2019s pretty exciting for me (and hopefully for you, too).\n\nFor a full explanation on the Web Audio API, MDN has some pretty great documentation. But think of it as the middle-man between an audio input and output, effectivaly hijacking your audio through a context and network of nodes before it reaches its output (i.e., you speakers).\n\nHere we are creating our AudioContext, getting our element, turning it into a MediaElementAudioSourceNode so that we can manipulate the audio from its source, and creating our AnalyserNode so that we can retrieve frequency data. Finally, we use the audioSrc.connect() method to connect the output of our element to the input of our analyser, and then connect to the audioCtx.destination (our speakers).\n\nWe need to make a few changes to the D3 code from my first blog post.\n\nThe array will be where we will copy our audio frequency data to. I also increased the size of the svg to accomodate the larger amount of data points. Lastly, I removed the rectangle\u2019s height and Y attributes, because they will be blank until the song is played, and we are only concerned with adding 200 rectangles and setting their x and width values at initial screen load.\n\nNow comes the fun part: constantly streaming audio data to the browser and dynamically updating the D3 bar chart.\n\nLet\u2019s break it down. will loop continuously and update our frequencyData and chart. It will loop because tells the browser to run again before repainting the screen.\n\nMost importantly, we need to get the audio data. will use our attached AnalyserNode to grab the frequency data of the audio and copy it to the Uint8Array frequencyData that we created earlier. If you were to set the length of the frequencyData array to , it would default to 1024. I opted to limit the length to 200 to make the chart a little easier to see. If you want data on the entire audio spectrum, feel free to use the total length, but 200 gives us a pretty good sample to work with.\n\nIf you were to put a inside the function and looked at the output, you\u2019ll see a Matrix-like stream of numbers.\n\nWith that data, you can make any visualization you want! For now, let\u2019s bind the streaming data to the D3 chart. Since we just need to update the data on our chart, there\u2019s no need to .append() anything; we just need to update the data, y, height and fill values. Since the range of frequency data is 0-255, it\u2019s perfect for using as the rgb value. This will make the color a brighter blue for frequencies that are higher.\n\nThat\u2019s it! You can view the source code here, and the finished demo here."},
{"url": "http://fabric8.io/", "link_title": "Fabric8 \u2013 an open source set of microservices run on top of Kubernetes and OS3", "sentiment": 0.2, "text": "fabric8 is an open source set of microservices that run on top of Kubernetes and OpenShift V3 to provide:\n\nfabric8 is an open source community project (sponsored by Red Hat, Inc.) fabric8 is being closely integrated into OpenShift V3 but it is also designed to run on top of any Kubernetes cluster"},
{"url": "https://www.eff.org/deeplinks/2004/12/9-11-legislation-launches-misguided-data-mining-and-domestic-surveillance-schemes", "link_title": "9/11 Legislation's Misguided Data-Mining and Domestic Surveillance Schemes", "sentiment": 0.02692550505050505, "text": "On Friday President Bush signed into law the Intelligence Reform and Terrorism Prevention Act of 2004 (IRTPA; PDF), launching several flawed \"security\" schemes that EFF has long opposed. The media has focused on turf wars between the intelligence and defense communities, but the real story is how IRTPA trades basic rights for the illusion of security. For instance:\n\nA clause authorizing the creation of a massive \"Information Sharing Environment\" (ISE) to link \"all appropriate Federal, State, local, and tribal entities, and the private sector.\"\n\nThis vast network links the information in public and private databases, which poses the same kind of threat to our privacy and freedom that the notorious Terrorism Information Awareness (TIA) program did. Yet the IRTPA contains no meaningful safeguards against unchecked data mining other than directing the President to issue guidelines. It also includes a definition of \"terrorist information\" that is frighteningly broad.\n\nA number of provisions that provide the statutory basis for \"Secure Flight,\" the government's third try at a controversial passenger-screening system that has consistently failed to pass muster for protecting passenger privacy.\n\nThe basic concept: the government will force commercial air carriers to hand over your private travel information and compare it with a \"consolidated and integrated terrorist watchlist.\" It will also establish a massive \"counterterrorist travel intelligence\" infrastructure that calls for travel data mining (\"recognition of travel patterns, tactics, and behavior exhibited by terrorists\").\n\nIt's not clear how the government would use the travel patterns of millions of Americans to catch the small number of individuals worldwide who are planning terrorist attacks. In fact, this approach has been thoroughly debunked by security experts. What is clear is that the system will create fertile ground for constitutional violations and the abuse of private information. The latest Privacy Act notice on Secure Flight shows that the Transportation Security Administration (TSA) still doesn't have a plan for how long the government will keep your private information, nor has it mapped out adequate procedures for correcting your \"file\" if you are wrongly flagged as a terrorist.\n\nStraight from the infamous \"PATRIOT II\" draft legislation leaked to the public last year comes a provision that allows the government to use secret foreign intelligence warrants and wiretap orders against people unconnected to any international terrorist group or foreign nation. This represents yet another step in the ongoing destruction of even the most basic legal protections for those the government suspects are terrorists.\n\nJust as EFF, the ACLU, and a number of other civil liberties groups feared, IRTPA creates the basis for a de facto national ID system using biometrics. Driven by misguided political consensus, the law calls for a \"global standard of identification\" and minimum national standards for birth certificates, driver's licenses and state ID cards, and social security cards and numbers. It also directs the Secretary of Homeland Security to establish new standards for ID for domestic air travelers.\n\nIdentification is not security. Indeed, the 9/11 Commission report revealed that a critical stumbling block in identifying foreign terrorists is the inability to evaluate *foreign* information and records. Yet we are placing disproportionate emphasis on pervasive domestic surveillance, opening the door to a standardized \"internal passport\" -- the hallmark of a totalitarian regime.\n\nIf you care about preserving your privacy and basic consitutional freedoms, help us fight the good fight by joining EFF today."},
{"url": "https://www.youtube.com/watch?v=Q2aEzeMDHMA", "link_title": "JPEG DCT (Discrete Cosine Transform)", "sentiment": -0.0625, "text": "DCT is the secret to JPEG's compression. Image Analyst Mike Pound explains how the compression works.\n\n\n\nColourspaces: https://youtu.be/LFXN9PiOGtY \n\nJPEG 'files' & Colour: https://youtu.be/n_uNPbdenRs \n\nComputer That Changed Everything (Altair 8800): https://youtu.be/6LYRgrqJgDc \n\nProblems with JPEG: COMING SOON\n\nUpside Down Trees (Huffman Encoding): https://youtu.be/umTbivyJoiI \n\n\n\nColourspaces: https://youtu.be/LFXN9PiOGtY \n\nJPEG isn't a file format - JPEG pt1: https://youtu.be/n_uNPbdenRs \n\nUpside Down Trees (Huffman Encoding): https://youtu.be/umTbivyJoiI \n\nProblems with JPEG: COMING SOON!\n\nComputer That Changed Everything (Altair 8800): https://youtu.be/6LYRgrqJgDc \n\n\n\n\n\nhttp://www.facebook.com/computerphile\n\nhttps://twitter.com/computer_phile\n\n\n\nThis video was filmed and edited by Sean Riley.\n\n\n\nComputer Science at the University of Nottingham: http://bit.ly/nottscomputer\n\n\n\nComputerphile is a sister project to Brady Haran's Numberphile. More at http://www.bradyharan.com"},
{"url": "https://github.com/prikhi/pencil", "link_title": "Libre, multiplatform wireframing with Pencil", "sentiment": 0.16252705627705635, "text": "This project was originally hosted on https://code.google.com/p/evoluspencil/ & was abandoned around 2013. This fork was started for new development on March 13th, 2015.\n\nAdditional collections are available on the Original Stencil Download Page.\n\nYou will need version 36 or higher of to run Pencil as a Firefox Extension. Linux users will need version 36 of either , or . The Windows installer has everything you need built-in.\n\nWindows, Linux & Firefox Packages are available for download from the Releases Page.\n\nPencil can be installed as Firefox Extension, instead of a standalone application. To build the extension's file:\n\nA package will also be created. This contains Pencil nested within the directory structure that most Linux distributions expect(under and ) along with an executable, a desktop entry & mimetype information. This can be used for creating distribution-specific packages.\n\nYou'll need installed so you can pull the Windows XULRunner runtime and to compile the installer.\n\nThis should place an installer in the folder.\n\nYou'll need installed to fetch the OS X XULRunner runtime.\n\nThis will create a in the folder.\n\nTo build the docs locally you'll need and the , which are easily installable using :\n\nThe output files will be put in .\n\nYou don't have to be a programmer to contribute! All feature requests & bug reports are appreciated. You can also update or improve the docs, package Pencil for your Linux distribution, write some stencils, attempt to reproduce bug reports or just spread the word :)\n\nIf you make changes that affect users, please update .\n\nIf you set the environmental variable, the script will enable debugging features like printing calls to to the console or to the javascript console:\n\nSetting will cause also Pencil to start a remote debugging server on port . This lets you use Firefox's DOM Inspector to debug Pencil. You can connect Firefox to the debugging server by going to . You may need to enable Remote Debugging under Firefox's Settings( then click the gear icon in the upper-right).\n\nThe script is responsible for building everything. Each build is usually in two steps: copying & modifying files common to all builds then customizing those files for the specific build(by removing files, embedding xulrunner, creating the expected directory structure, etc.).\n\nThe build script uses the file to hold variables such as the current version & the minimum/maximum firefox/xulrunner versions. The script uses to replace all instances of with the value of in the file passed to it.\n\nIf you add a variable to you must modify the script to replace the variable. If you add a variable to a file, you must make sure that file is processed by at some point(usually in the function).\n\nuses the to remove all the text between and . This can be used to enable code only when building for development. If you add and to a file, make sure passes the file to (again, this usually happens in the function).\n\nYou can pass the argument to to remove all the outputs. You can use to remove any XULRunner downloads as well.\n\nStart off by changing the version numbers in & and sectioning off the changes in . Then run & test the built packages in .\n\nIf everything looks OK, update the & variables in the file & commit all the changes(with a message like ).\n\nCreate a new tag, merge it into master & push:\n\nUpload the packages in to the new Release created on Github. Update the GNU/Linux distro-specific packages or ping their maintainers.\n\nThis fork is released under GPLv2 like it's parent codebase."},
{"url": "http://pastebin.com/ArF54v6G", "link_title": "ZoneEdit: The New Boss Is Not Like the Old Boss", "sentiment": 0.16103780000331722, "text": "Greetings Zoneedit User; (You are receiving this message because you have at least one domain on ZoneEdit's service: ...) I had an experience last week which echoed what many of you probably experienced when you received word that Zoneedit had been purchased (yet again). You see I went downtown to Murray's organic produce in St. Lawrence Market in Toronto. Murray has been a fixture there for 35 years, behind the counter are photos of celebs and luminaries who have visited there, ranging from Catherine Zeta Jones to the Dalai Lama. Murray's been great, we send him an email list in advance and he has bags of healthy fruits and veggies waiting for us by the time we get there. At Christmas our families exchange gift baskets. This has been going on for 10 years. This time Murray introduced me to somebody new: \"Mark, I want you to meet Anthony. He and his brother will be taking over the shop.\" It was taking a few extra seconds for my brain to parse what this meant exactly, but when Murray added \"and I'm sure they're going to take very good care of you and the girls\" (referring to my wife and daughter). This latter statement cinched it - with a jolt I realized what was happening here. Regime change. End of an era. Murray was out, and this Anthony guy and his brother were in. Anthony seemed nice. He assured me nothing would change, and that he'll be happy to take my orders and have them ready for me, just as Murray had been doing for the previous decade. But as I walked away I recognized an unmistakable sense of loss and doubt. I would miss Murray, and would the service sustain that same level we'd taken for granted over the years? Then I realized that what I was feeling then was possibly very similar to what you experienced when you found out that Zoneedit had been acquired by easyDNS. I know it's been a long road for you as a Zoneedit user. When it launched, Erik Aronesty and Michael Krebs were \"Murray\". They were very engaged and personally invested into the success of Zoneedit. In those early years Erik and I would occasionally speak on the phone, comparing notes on what we were seeing - DDoS attacks, spam gangs and any other manner of what were becoming near constant existential threats to centralized managed DNS platforms. After Erik and Michael sold, Zoneedit entered a kind of long, wandering phase. I know when Dotster bought Zoneedit they had the best of intentions, (around the same time Dotster also approached easyDNS but we turned them down. They were buying a lot of companies in those days). Perhaps what ensued could best surmised as that Zoneedit was in a sense \"left behind\". It was never consolidated into the main platform with all the other acquisitions and additionally it did not have the benefit of having somebody or some team personally invested and at the helm. Then Dotster itself was acquired and the problem was exacerbated. We had the idea to \"carve out\" Zoneedit from the even larger acquiring entity because we realized that there was quite literally \"nobody at the wheel\" at Zoneedit. A couple of truly heroic engineers there were assigned part-time duties to keep it on life support, but under the new regime there were no new signups being allowed, there was no capital spending on infrastructure maintenance, even the twitter account was deleted. I'm personally stunned when I hear Zoneedit customers tell me now that \"the system ran flawlessly for over 10 years\" because since the serial acquisitions started there were numerous outages. Those who were somehow never affected were truly blessed with a unique combination of fortuitous name server delegations and outright luck. I know that since this last acquisition there have been some hiccups and glitches. But what has changed that there is now somebody who is personally invested in making Zoneedit work and succeed, and there is a team actively working on it. In this sense, the new boss isn't the same as the old boss (but perhaps we're similar to the original boss). The point of this email today is to thank you for sticking with Zoneedit after all these years and to let you know that the long dark tea time of the soul is over for Zoneedit. Since taking over we've: \u2022 completely rebuilt the entire Zoneedit infrastructure from scratch, including replacement of all nameservers (which were previously running unpatched bind8 on Red Hat boxes which were years past end-of-life). \u2022 Brought back the 5-free DNS zones for all legacy users \u2022 Deployed a completely new user control panel (some of you hate it, that's ok, because we're listening - tell us how to improve it). \u2022 Fixed the URL forwarding (half the old forwarders were broken) \u2022 improved the functionality of host monitoring and DNS failover. \u2022 Rolled out The Support Zone user community. \u2022 Added Registrar transfers for .com, .net .org, .biz, .info domains - including free Whois privacy \u2022 Added support for .CH domains \u2022 Added RapidSSL certs And over the next few weeks and months we will continue to add more useful stuff for running your domains: \u2022 Support for .CA domains \u2022 Outbound SMTP / Mailout \u2022 Geotargeted URL Forwarding This is just the beginning of a steady stream of useful domain tools to make running you more effective at managing your online presence. Thanks for taking the time to read this. If you do reply please understand that sometimes the volume of replies precludes me from replying individually, but I do read every single email I get. I'd love to hear your feedback. Please post it to this thread, so the team and I can address each and every one of them. Sincerely, Mark Jeftovic President/CEO ZoneEdit The easyZone Corp, 300A-219 Dufferin St, Toronto, ON, M6K 3J1"},
{"url": "http://www.cloudways.com/blog/partnership-program-digital-agencies/", "link_title": "How Web Development Agencies Can Make More Money", "sentiment": 0.1264887672240613, "text": "The work culture at a creative digital and web agency is pretty relaxed. It seems every day is a casual Friday. You need such leniency because creative jobs are as tough as manual labor. You have to come up with unique ideas and innovative solutions. When these agencies work, they make magic. They are the brains and brawns behind hundreds of web apps and websites. Without them, the internet would be quite \u2018uncool\u2019.\n\nBut! Whenever an agency builds something, its clients ask them to host the finished app or website. This is where an agency looks uncool when it \u00a0either has to refuse or deal with hosting technicalities. If they agree to host, they are left with decisions of picking a hosting medium that delivers uninterrupted, blazing-fast performance. Nowadays, this is not an easy decision to make as many providers promise a lot, but fail to deliver satisfactory results.\n\nIn this post, I will explain how we are helping digital agencies by providing a powerful Cloud hosting solution that will not only keep the customers happy, but it will create an additional stream of revenue.\n\nCloudways loves to work with agencies. We want them to stay supercool. We want them to keep providing interactive designs and building awesome websites for their clients.\n\nWe want Web Agencies to stay productive in their specialized areas. We want them to write great pieces of code and develop extraordinary designs without worrying about their hosting provider.\n\nThis is only possible if they don\u2019t invest their valuable hours in deciding where to host, how to configure a server, how to move a site on that server and then how to manage a site of their customer. Why should they think about this? Because, these agencies want their customers to stay satisfied\u2014and not get frustrated if their website goes offline frequently or stays slow all the time.\n\nCloudways Cloud Platform will take care of hosting needs without much fuss. We truly understand that if these smart designers and developers start worrying about the hosting, they may divert their focus to troubleshooting. This can be frustrating \u00a0as they are not made to handle hosting hardships.\n\nHere, let me clear up one thing: We don\u2019t make agencies our customers, rather we make them our long term Partners!\n\nYou can become our partner by choosing any of the following options which you think is useful for you.\n\nIf you are currently providing design/dev services for your clients, then you can add value to your services by offering managed hosting services as well. This way your customers will have a \u201cone window\u201d solution and they don\u2019t have to rush anywhere else for finding a high performing, reliable host.\n\nFeatures that will be beneficial for your Customers\n\nYou can learn more by visiting the Features page.\n\nYou will simply own an account at Cloudways and under that account, you will be deploying multiple applications or servers of your customers, depending upon their needs. We will be charging you on the basis of number of servers that you are going to host with us, not applications. On the other hand, you can charge each customer individually and we will not be contacting any of your customers or sites directly. In other words, you will be managing your customers\u2019 site from our platform.\n\nYou may think we are bragging. How about some testimonials then?\n\nLimesharp.net, is one of the leading Magento agencies based in UK. They have hosted several ecommerce stores with us. Look what the CEO of Limesharp has to say about Cloudways:\n\nJoslex Studios is a US-based digital creative agency specializing in Responsive Website Design and WordPress Development. This is what Jeff Weese, the founder of Joslex Studios, has to say about us:\n\nThrough a business partnership, we allow our Partners to offer our Cloud Hosting services directly from their websites with their brand name. By simply adding a performance-focused Cloud Hosting platform, they get the dual benefits of fostering a long-term relationship with their clients while generating a subsidiary stream of revenue.\n\nThey take orders from their clients\u2019 websites. While at the backend, they host everything with Cloudways where we already have a managed platform with 1-click hosting platform with 1-click optimized installations, 24/7 live support, automated backups, pre-configured Varnish, and Memcached optimization layers, and servers built on Apache and Nginx.\n\nAnd, while they are busy in doing this stuff, they get some additional perks through Cloudways:\n\nInterested? Sign up now to start your journey with Cloudways."},
{"url": "http://hacksoflife.blogspot.com/2015/05/underestanding-powervr-gpus-via-metal.html", "link_title": "Underestanding PowerVR GPUs via Metal", "sentiment": 0.1409412929197101, "text": "In graphics terms, the texture is immutable. You can change the contents of its image, but you can't change the object itself in such a way that the underlying hardware resources and shader instructions associated with the texture have to be altered.\n\nThis is a win for the driver: when you go to use an MTLTexture, whatever was true about the texture last time you used it is still true now, always.\n\nCompare this to OpenGL. With OpenGL, you can bind the texture id to a new texture - not only with different dimensions, but maybe of a totally different type. Surprise, OpenGL - that 2-d texture I used is now a cube map! Because anything can change at any time, OpenGL has to track mutations and re-check the validity of bound state when you draw.\n\n\n\n Commands Are Assembled in Command Buffers and Then Queued for the GPU\n\nHow do your OpenGL commands actually get\u00a0to the GPU? The OpenGL way involves a fair amount of witchcraft:\n\nOn every modern GPU where I've been able to find out how command processing works, the GPU follows pretty much the same design:\n\nWhen you issue OpenGL commands, the encoder is built into the context, is \"discovered\" via your current thread, and commands are sent to a command buffer that is allocated on the fly. (If you really push the API hard, some OpenGL implementations can block in random locations because the context's encoder can't get a command buffer.)\n\nThe OpenGL context also has access to a queue internally, and will queue your buffer when (1) it fills up or (2) you call one of glFlush/glFinish/swap. This is why your commands might not start executing until you call flush - if the buffer isn't full, OpenGL will leave it around, waiting for more commands.\n\nOne last note: the race condition between the CPU writing commands and the GPU reading them is handled by a buffer being in only one place at a time, whether it's the CPU (encoding commands) or GPU (executing them) - this is true for both Metal and GLES. So while you are encoding commands, the GPU has not started on them yet.\n\n\n\n \n\n The GPU Does Work When You Start and End Rendering to a Surface As I am sure you have read 1000 times, the PowerVR GPUs are tiled deferred renderers. What this means is that rasterizing and fragment shading are done on tiny 32x32 pixel tiles of the screen, one at at time. (The tile size might be different - I haven't found a good reference.) For each rendering pass, the GPU iterates on each tile of the surface and renders everything in the rendering pass that intersects that tile.\n\n \n\n The PowerVR GPUs are designed this way so that they can function without high-speed VRAM tied to a high-bandwidth memory bus. Normal desktop GPUs use a ton of memory bandwidth, and that's a source of power consumption. \u00a0The PowerVR GPUs have a tiny amount of on-chip video memory; for each tile the surface is loaded into this cache, fully shaded (with multiple primitives) and then saved back out to shared memory (e.g. the surface itself).**\n\n \n\n This means the driver has to understand the bounded set of drawing operations that occur for a single surface, book-ended by a start and end. The driver also has to understand the life-cycle of this rendering pass: do we need to load the surface from memory to modify it, or can we just clear it and draw? What results actually need to be saved? \u00a0(You probably need your color buffer when you're done drawing, but maybe not the depth buffer. If depth was just used for hidden surface removal, you can skip saving it to memory.) Optimizing the start and end of a surface rendering pass saves a ton of bandwidth.\n\n \n\n Metal lets you specify how a rendering pass will work explicitly: an \n\n \n\n To get a command encoder to render (a \n\n \n\n Compare this to OpenGL ES; when you bind a new surface for drawing, the driver must note that it doesn't know how you want the pass started. It then has to track any drawing operation (which will implicitly load the surface from memory) as well as a clear operation (which will start by clearing). Lots of book-keeping.\n\n \n\n Normally this is not a problem - you queue up a ton of work and the GPU always has a long todo list. But in the non-ideal case where GPU latency matters (e.g. you want the answer as fast as possible), in OpenGL ES you might have to issue a flush so the GPU can start - OpenGL will then get you a new command buffer. (This is why the GL spec has all of that language about glFlush ensuring that commands will complete in finite time - until you flush, the command buffer is just sitting there waiting for the driver to add more to it.)As I am sure you have read 1000 times, the PowerVR GPUs are tiled deferred renderers. What this means is that rasterizing and fragment shading are done on tiny 32x32 pixel tiles of the screen, one at at time. (The tile size might be different - I haven't found a good reference.) For each rendering pass, the GPU iterates on each tile of the surface and renders everything in the rendering pass that intersects that tile.The PowerVR GPUs are designed this way so that they can function without high-speed VRAM tied to a high-bandwidth memory bus. Normal desktop GPUs use a ton of memory bandwidth, and that's a source of power consumption. \u00a0The PowerVR GPUs have a tiny amount of on-chip video memory; for each tile the surface is loaded into this cache, fully shaded (with multiple primitives) and then saved back out to shared memory (e.g. the surface itself).**This means the driver has to understand the bounded set of drawing operations that occur for a single surface, book-ended by a start and end. The driver also has to understand the life-cycle of this rendering pass: do we need to load the surface from memory to modify it, or can we just clear it and draw? What results actually need to be saved? \u00a0(You probably need your color buffer when you're done drawing, but maybe not the depth buffer. If depth was just used for hidden surface removal, you can skip saving it to memory.) Optimizing the start and end of a surface rendering pass saves a ton of bandwidth.Metal lets you specify how a rendering pass will work explicitly: an MTLRenderPassDescriptor describes the surfaces you will render to and exactly how you want them to be loaded and stored. You can explicitly specify that the surface be loaded from memory, cleared, or whatever is fastest; you can also explicitly store the surface, use it for an FSAA resolve, or discard it.To get a command encoder to render (a MTLRenderCommandEncoder ), you have to pass a MTLRenderPassDescriptor describing how a pass is book-ended and what surfaces are involved. You can't not answer the question.Compare this to OpenGL ES; when you bind a new surface for drawing, the driver must note that it doesn't know how you want the pass started. It then has to track any drawing operation (which will implicitly load the surface from memory) as well as a clear operation (which will start by clearing). Lots of book-keeping.\n\nThe Entire Pipeline Is Grafted Onto Your Shader OpenGL encourages us to think of the format of our vertex data as being part of the vertex data, because we use \n\n \n\n This view of vertex fetching is misleading; glVertexAttribPointer really wraps up two very different bits of information:\n\n \n\n Where to get the raw vertex data (we need to know the VBO binding and base pointer) and How to fetch and interpret that data (for which we need to know the data type, stride, and whether normalization is desired). The trend in recent years is for GPUs to do vertex fetch \"in software\" as part of the vertex shader, rather than have fixed function hardware or registers that do the fetch. Moving vertex fetch to software is a win because the hardware already has to support fast streamed cached reads for compute applications, so some fixed function transistors can be thrown overboard to make room for more shader cores.\n\n \n\n On the desktop, blending is still fixed function, but on the Power VR, blending and write-out to the framebuffer is done in the shader as well. \u00a0(For a really good explanation of why blending hasn't gone programmable on the desktop, read on chip\u00a0on the PowerVR, you can see why the arguments about latency and bandwidth from desktop don't apply here, making blend-in-shader a reasonable idea.)\n\n \n\n The sum of these two facts is: your shader actually contains a bunch of extra code, generated by the driver, on both the front and back.\n\n \n\n Metal exposes this directly with a single object: \n\n \n\n Every time you change the vertex format (or even pretend to by changing the vertex base pointer with glVertexAttribPointer), every time you change the color write mask, or change blending, you're requiring a new underlying pipeline to be built for your GLSL shader. Metal exposes the actual pipeline, allowing for greater efficiency. (In X-Plane, for example, we always tie blending state to the shader, so a pipeline is a pretty good fit.)\n\n \n\n \n\n If there's a summary here, it's that GLES doesn't quite match the PowerVR chip, and we can see the mismatch by looking at Metal. In almost all cases, the driver has to do more work to make GLES fit the hardware, inferring and guessing the semantics of our application.\n\n \n\n I'll do one more post in this series, looking at Mantle, and some of the terrifying things we've never had to worry about when running OpenGL on AMD's GCN architecture.\n\n \n\n OpenGL encourages us to think of the format of our vertex data as being part of the vertex data, because we use glVertexAttribPointer to tell OpenGL how our vertices are read from a VBO.This view of vertex fetching is misleading; glVertexAttribPointer really wraps up two very different bits of information:The trend in recent years is for GPUs to do vertex fetch \"in software\" as part of the vertex shader, rather than have fixed function hardware or registers that do the fetch. Moving vertex fetch to software is a win because the hardware already has to support fast streamed cached reads for compute applications, so some fixed function transistors can be thrown overboard to make room for more shader cores.On the desktop, blending is still fixed function, but on the Power VR, blending and write-out to the framebuffer is done in the shader as well. \u00a0(For a really good explanation of why blending hasn't gone programmable on the desktop, read this . \u00a0Since the currently rendered tile is cachedon the PowerVR, you can see why the arguments about latency and bandwidth from desktop don't apply here, making blend-in-shader a reasonable idea.)The sum of these two facts is: your shader actually contains a bunch of extra code, generated by the driver, on both the front and back.Metal exposes this directly with a single object: MTLRenderPipelineState . This object wraps up the actual complete GPU pipeline with all of the \"extra\" stuff included that you wouldn't know about in OpenGL. Like most GPU objects, the pipeline state is immutable and is created with a separate MTLRenderPipelineDescriptor object. We can see from the descriptor that the pipeline locks down not only the vertex and fragment functions, but also the vertex format and anti-aliasing properties for rasterization. Color mask and blending is in the color attachment descriptor, so that's part of the pipeline too.Every time you change the vertex format (or even pretend to by changing the vertex base pointer with glVertexAttribPointer), every time you change the color write mask, or change blending, you're requiring a new underlying pipeline to be built for your GLSL shader. Metal exposes the actual pipeline, allowing for greater efficiency. (In X-Plane, for example, we always tie blending state to the shader, so a pipeline is a pretty good fit.)If there's a summary here, it's that GLES doesn't quite match the PowerVR chip, and we can see the mismatch by looking at Metal. In almost all cases, the driver has to do more work to make GLES fit the hardware, inferring and guessing the semantics of our application.I'll do one more post in this series, looking at Mantle, and some of the terrifying things we've never had to worry about when running OpenGL on AMD's GCN architecture.\n\n* Technically the real API objects are all ObjC protocols, while the lighter-weight struct-like entities are objects. I'll call them all objects here - to client code they might as well be. The fact that API-created objects are protocols stops you from trying to alloc/init them.\n\n \n\n ** Besides saving bus bandwidth, this technique also saves shading ops. Because the renderer ha access to the entire\u00a0rendering pass before it fills in a tile, it can re-order opaque triangles for perfect front-to-back rendering, leveraging early Z rejection.\n\nIn my previous post I suggested that OpenGL and OpenGL ES, as APIs, don't always fit the underlying hardware. One way to understand this is to read GPU hardware documentation - AMD is pretty good about posting hardware specs , e.g. ISAs, register listings, etc. You can also read the extensions and see the IHV trying to bend the API to be closer to the hardware (see NVidia's big pile of bindless this and bindless that ). But both these ways of \"studying\" the hardware are time consuming and not practical if you don't do 3-d graphics full time.Recently there has been a flood of new low-level, close-to-the-hardware APIs: metal (Apple, PowerVR), Mantle (AMD, GCN), Vulkan (Khronos, everything), DirectX 12 (Microsoft, desktop GPUs). This provides us another way of understanding the hardware: we can look at what the graphics API would look like if it were rewritten to match today's hardware.Let's take a look at some Metal APIs and see what they tell us about the PowerVR hardware."},
{"url": "https://epic.org/privacy/terrorism/usapatriot/sunset.html#expire", "link_title": "USA Patriot Act: Expiring Sections", "sentiment": 0.07815049533799534, "text": "The Uniting and Strengthening America by Providing Appropriate Tools Required to Intercept and Obstruct Terrorism Act of 2001 introduced a plethora of legislative changes which significantly increased the surveillance and investigative powers of law enforcement agencies in the United States. Legislative proposals were introduced less than a week after the terrorist attacks of September 11, 2001. President Bush signed the final bill, the USA PATRIOT Act, into law on October 26, 2001. Though the Act made significant amendments to over 15 important statutes, it was introduced with great haste and passed with little debate, and without a House, Senate, or conference report.\n\nThough it significantly expanded the government's investigative authority, the USA PATRIOT Act did not provide for the system of checks and balances that traditionally safeguard civil liberties in the face of such legislation. The Act does, however, contain a sunset provision that terminates on December 31, 2005, several of the sections that enhance law enforcement search and electronic surveillance authority. Because the law gives government a great deal more surveillance capability, a sunset is crucial to determine how well the tools work, how effective they have been, and how responsibly the government has applied the laws.\n\nThe provisions of the USA PATRIOT Act subject to sunset generally modified two existing laws. Title III governs law enforcement interception of and access to communications in ordinary criminal investigations. The Foreign Intelligence Surveillance Act (FISA) regulates the FBI's collection of \"foreign intelligence\" information for intelligence purposes. Under the Fourth Amendment, a Title III warrant to intercept a communication must be based on probable cause to believe that a crime has been or is being committed. This is not the general rule under FISA: surveillance under FISA is permitted based on a finding of probable cause that the surveillance target is a foreign power or an agent of a foreign power, irrespective of whether the target is suspected of engaging in criminal activity. The USA PATRIOT Act expanded law enforcement authority to conduct searches and obtain communications under Title III, and also rolled back the already lax restrictions on the FBI's ability to gather information about individuals under FISA.\n\nAs it considered the legislation that would become the USA PATRIOT Act in the weeks after 9/11, Congress understood a sunset provision would provide an opportunity to review the most extreme of the government's new investigative powers at a later, less chaotic time. As Washington Post journalist Robert O\u0092Harrow Jr. has reported:\n\nIn its final report, the 9/11 Commission recommended (pdf) that the the burden of proof for showing that Congress should renew USA PATRIOT Act powers subject to sunset should be on President Bush. Specifically, the president must show that each power actually materially enhances security and that there is adequate supervision of the use of such powers to ensure that civil liberties are protected. If the power is granted, the Commission emphasized, there must be adequate guidelines and oversight to properly confine its use. The Commission further stated, \"[b]ecause of concerns regarding the shifting balance of power to the government, we think that a full and informed debate on the Patriot Act would be healthy.\"\n\nHowever, the Department of Justice has revealed very little information to the public about its use of investigative powers under the USA PATRIOT Act. The agency provided some information to the House Judiciary Committee in response to oversight questions in July 2002 (pdf) and May 2003 (pdf),but considers much information concerning its use of USA PATRIOT Act authorities to be classified and therefore available only to Congress' intelligence committees.\n\nSection 206 expanded FISA to permit \"roving wiretap\" authority, which allows the FBI to intercept any communications made to or by an intelligence target without specifying the particular telephone line, computer or other facility to be monitored. Prior law required third parties (such as common carriers and others) \"specified in court-ordered surveillance\" to provide assistance necessary to accomplish the surveillance. Under Section 206, that obligation has been extended to unnamed and unspecified third parties. Such generic orders could have a significant impact on the privacy rights of large numbers of innocent users, particularly those who access the Internet through public facilities such as libraries, university computer labs and cybercafes. Upon the suspicion that an intelligence target might use such a facility, the FBI can now monitor all communications transmitted at the facility. The problem is exacerbated by the fact that the recipient of the assistance order (for instance, a library) would be prohibited from disclosing the fact that monitoring is occurring. Generic roving wiretap orders raise significant constitutional issues, as they do not comport with the Fourth Amendment's requirement that any search warrant \"particularly describe the place to be searched.\" That deficiency becomes even more significant when where the private communications of law-abiding American citizens might be intercepted incidentally. When asked by the House Judiciary Committee in 2002 how many times it has used Section 206 to conduct roving surveillance, the Department of Justice refused to answer (pdf), stating that the number is classified. On its public web site, the agency says it has used Section 206 49 times as of March 30, 2005.\n\nSection 207: Duration of FISA surveillance of non-United States agents of a foreign power\n\nSection 207 extended the duration of FISA wiretap orders relating to an agent of foreign power from 90 days to 120 days, and now allows an extension of one year intervals instead of 90 day increments as required under pre-USA PATRIOT Act law. In addition, the section amends timelines for FISA physical search orders. Such searches targeting an agent of a foreign power may now be issued for up to 120 days, with extensions for up to one year. Before the USA PATRIOT Act, such orders could be issued for 45 days and extended for up to year. The section also extends the duration of other general physical search orders from 45 to 90 days. Because the FBI could seek extensions for FISA orders before the USA PATRIOT Act was passed, it is unclear why this authority is necessary. No information has been made public about how the Department of Justice has used the expanded timelines under Section 207.\n\nSection 214 made it easier for the FBI to use FISA to obtain information through pen register/trap and trace devices, which capture information about the sender and recipient of a communication. First, this provision extended FISA pen register/trap and trace authority to electronic communications, such as email and Internet communications, as well as wire communications, such as telephone calls. Second, Section 214 removed the pre-existing legal requirement that the government show the surveillance target is \"an agent of a foreign power\" before obtaining a pen register/trap and trace order under the FISA. The government can now obtain a pen register/trap and trace device \"for any investigation to gather foreign intelligence information,\" without a showing that the device has, is or will be used by a foreign agent or by an individual engaged in international terrorism or clandestine intelligence activities. However, the section prohibits the use of FISA pen register/trap and trace surveillance against a United States citizen where the investigation is conducted \"solely on the basis of activities protected by the First Amendment.\" This amendment significantly eviscerates the constitutional rationale for the relatively lax requirements that apply to foreign intelligence surveillance. That laxity is premised on the assumption that Congress and the courts should not unduly restrain the Executive Branch, in pursuit of its national security responsibilities to monitor the activities of foreign powers and their agents. The removal of the \"foreign power\" predicate for pen register/trap and trace surveillance upsets that delicate balance. Though a Department of Justice web site makes clear that the FBI has used its authority under Section 214, the Department has refused to provide (pdf) the House Judiciary Committee the number of times the FBI has used this expanded authority to obtain a pen register or trap and trace order, stating the answer is classified.\n\nBefore the USA PATRIOT Act, FISA allowed the FBI the ability to request orders to access business records related to hotels and motels, automobile rental agencies and storage rental facilities. These requests had to include \"specific and articulable facts giving reason to believe the person to whom the record pertain [was] a foreign or an agent of a foreign power.\" 50 U.S.C. \u00a7 1862(b)(2). Section 215 expands this authority, now including any tangible item, regardless of who is holding it, and now only requires that the FBI is seeking records relevant to an investigation of foreign intelligence or terrorist activities. The requirement that access to records must be granted by a court order remains in effect. This section permits the FBI to compel production of any record or item without a showing of \"probable cause\" (the existence of specific facts to support the belief that a crime has been committed or that the items sought are evidence of a crime). Individuals served with a search warrant issued under FISA rules may not disclose, under penalty of law, the existence of the warrant or the fact that records were provided to the government. Section 215 prohibits investigation of a United States person solely on the basis of activities protected by the First Amendment. In September 2003, the Department of Justice reported that it had never used Section 215 to seek business records. However, the Department now states that federal judges have granted 35 requests for Section 215 orders as of March 30, 2005. The orders were used to obtain driver's license records, public accommodation records, apartment lease records, credit card records, and telephone subscriber records for phone numbers captured under pen register and trap and trace authority.\n\nSection 218 expanded the application of FISA to those situations where foreign intelligence gathering is merely \"a significant\" purpose of the investigation, rather than the sole or primary purpose, which was previously the required standard. This change allows the FBI to obtain FISA wiretap orders in situations where an investigation is primarily criminal in nature. The more lenient standards that the government must meet under FISA (as opposed to the stringent requirements of Title III) are justified by the fact that FISA's provisions facilitate the collection of foreign intelligence information, not criminal evidence. This traditional justification is eliminated where the lax FISA provisions are applicable to the interception of information relating to a domestic criminal investigation. The change is a serious alteration to the delicate constitutional balance reflected in the prior legal regime governing electronic surveillance. When asked by the House Judiciary Committee how many more FISA applications had been approved as a result of Section 218, the Department of Justice responded (pdf) in July 2002, \"[b]ecause we immediately began using the new 'significant purpose' standard after passage of the PATRIOT Act, we had no occasion to make contemporaneous assessments on whether our FISAs would also satisfy a 'primary purpose' standard. Therefore, we cannot respond to this question with specificity.\"\n\nSection 220 expands the geographic scope of where the FBI can obtain search warrants or court orders for electronic communications content and customer record information. Before the passage of the USA PATRIOT Act, a search warrant or court order for access to electronic communications content and customer record information had to been issued in the judicial district where the ISP or telephone company possessing the evidence was located. Section 220 makes it possible for any federal court to approve such a search warrant or court order, regardless of where the evidence is. This makes it possible for the FBI to seek out judges that are sympathetic to law enforcement interests to approve warrants for electronic evidence. Furthermore, this arrangement makes it more difficult for an ISP or telephone company to challenge a search warrant or court order from a court far from where the company is located. When asked by the House Judiciary Committee how many search warrants have been served under Section 220 in jurisdictions other than that of the court issuing the warrant, the Department of Justice responded in July 2002, \"[a]lthough the exact number of search warrants for electronic evidence that have been executed outside the issuing district is unknown, the impact of Section 220 has plainly been significant.\""},
{"url": "http://www.bbc.com/news/technology-32826353", "link_title": "UK sells off unused net addresses", "sentiment": 0.05607638888888889, "text": "The UK government has started selling off internet addresses that it no longer uses.\n\nThe first group of 150,000 addresses has been snapped up by a Norwegian firm called Altibox for about \u00a3600,000.\n\nThe addresses are becoming valuable because the net has almost outgrown the addressing scheme it adopted in the 1970s.\n\nIf the UK government sells off all the surplus addresses it owns it could get up to \u00a315m.\n\nHowever, some fear that as the addresses are shared out more widely, data could go astray.\n\nThe surplus addresses are part of a much bigger block of 16 million addresses given to the Department of Work and Pensions in 1993. Earlier this year, the DWP started a project to see how many of these IP addresses could be freed.\n\nAn official report produced before the DWP began its investigation suggested that 70% of the massive block was used for the UK government's internal network, leaving about five million free for disposal.\n\nA government spokesman said: \"Government periodically reviews all its assets to consider their financial value, including options to release income from those that are not used to their fullest potential.\n\n\"The scope of the value of these assets is commercially sensitive and protected by standard legal confidentiality agreements.\"\n\nThe addresses are known as IP Version 4 (IPv4) addresses and are valuable because of a hard limit in the numbering system they use. This caps the total number of IPv4 addresses at 4.3 billion. In practice there are fewer available because some are reserved for other uses.\n\nThe net is in the process of moving to IP Version 6 (IPv6), which has an almost inexhaustible supply of addresses. However, technical incompatibilities between the two versions means many firms are seeking to expand their existing IPv4 networks instead of switching.\n\nRegional caches of IPv4 addresses have all but run dry, meaning many firms have to look elsewhere for them, said Sandra Brown, president of address broker IPV4 Market Group.\n\nTrading in IPv4 had been brisk in Europe, said Ms Brown, because the organisation that oversees net addresses in the region had approved policies that allowed transfers. In the busiest months, about two million IPv4 addresses were being traded in Europe.\n\n\"Supply has met demand but we are reaching a point where supply is about to fall short and we have seen prices escalate because of that,\" she said.\n\nEach individual IP address was worth up to $11 (\u00a37), she said, but prices were lower when big deals were done.\n\nTrading was likely to continue for years as firms were only slowly migrating to IPv6.\n\n\"Most of the people I talk to say it will take five to 10 years to convert,\" said Ms Brown.\n\nThat might spell trouble, said Doug Madory from network specialist Dyn, because there were concerns about what happened when that finite stock of addresses was divided very finely.\n\n\"People typically try to deal with addresses in contiguous blocks to keep the binary math from getting unwieldy leading to errors,\" he said.\n\n\"As you slice it thinly the number of routes gets larger and larger and it's computationally expensive to look up where each packet has to go.\"\n\nIn addition, he said, delays in transferring ownership had already led to some data going astray.\n\n\"We see this as a transition period,\" said Andrew de la Haye, chief operating officer of the Ripe agency that oversees net addresses in Europe.\n\nHe added that some European companies were analysing how they use IPv4 as a way to help them move to the larger addressing system.\n\n\"It's a bit early to say but I have spoken to a few of our members and they are freeing up IPv4 address spaces to fund their IPv6 migration,\" he said. \"The long-term strategy should be IPv6.\""},
{"url": "https://crypto.stackexchange.com/questions/11629/public-key-cryptosystems-without-poly-time-quantum-attacks", "link_title": "Public-key cryptosystems without poly-time quantum attacks", "sentiment": -0.5, "text": "It's well-known that Shor's algorithm can solve integer factorization, discrete logarithm and discrete log over elliptic curves in cubic time. This implies that cryptosystems like RSA, ElGamal, and Elliptic Curve Diffie-Hellman (ECDH) are vulnerable to quantum computers.\n\nAre there any existing public-key cryptosystem that are NOT known to have a polynomial-time quantum attack?\n\nThis question was inspired by this blog post."},
{"url": "https://securityinnovation.com/products/encryption-libraries/ntru-crypto/ntru-challenge/", "link_title": "Calling All Cryptographers \u2013 NTRU Challenge", "sentiment": 0.09285714285714287, "text": "Security Innovation is pleased to present the NTRU Challenge, which has been created to increase the world's understanding of the shortest vector problem in NTRU lattices, and to encourage and\u00a0stimulate further research in the security analysis of NTRU-based cryptosystems.\u00a0 We hope it will provide additional information to users of NTRU public-key cryptosystems to help them select suitable key lengths for a desired level of security.\n\nThe Challenge is to compute the NTRU private keys from the given list of public keys and associated system parameters. This is the type of problem a hacker faces who wishes to defeat an NTRU-based cryptosystem.\u00a0 There will be several challenges comprised of a number of security levels, some of which can be solved in a day, some in a few months and some which are considered to be computationally intractable.\n\nClick here for a complete set of rules.\n\nThe first correct submission for each NTRU Challenge will receive the following prizes (see contest rules for details)\n\nOnce you submit the form below, you will receive an email confirming your entry. To complete your submission, please reply to that email and attach your solution, as well as a description of the techniques you used. The latter should be about 500 words."},
{"url": "http://america.aljazeera.com/opinions/2015/5/yanis-varoufakis-is-more-than-his-clothes.html", "link_title": "Yanis Varoufakis is more than his clothes", "sentiment": 0.002426532404793281, "text": "Yanis Varoufakis may or may not end up saving Greece\u2019s economy \u2014 but he\u2019s certainly giving the media a lot to gossip about. At a meeting in Riga, Latvia, with his European counterparts, Varoufakis, the Greek finance minister was reportedly \u201csavaged\u201d and later labeled a \u201cgambler\u201d and an \u201camateur\u201d by an unnamed source in Bloomberg. This report was then repeated in most of the European press. The Greek negotiating team was subsequently reshaped, with alternate minster Eyclid Tsakalotos taking the helm of the Brussels team and technocrat George Chouliarakis tackling the day-to-day technical level negotiations. Varoufakis, it seemed, was on his way out, and he appeared to acknowledge his new marginal position. Channeling Franklin Delano Roosevelt, he tweeted a few weeks ago:\n\nOnly his sidelining now seems to have been purely for appearances\u2019 sake. Two weeks after sending his beleaguered tweet, he was back in Brussels.\u00a0And Varoufakis denies that the insults in Riga ever took place. Financial Times journalist Peter Spiegel felt the need on May 21, after almost a month, to say that no one attacked Varoufakis in the meeting. But this small detail was distorted by stories in\u00a0Business Insider,\u00a0The Guardian\u00a0and\u00a0The Telegraph. \u201cThe media went into a frenzy of obfuscations and lies, which I am sure they are not entirely responsible for,\u201d Varoufakis told The New York Times in a\u00a0profile\u00a0published this week. \u201cAll these reports that I was abused, that I was called names, that I was called a time waster and all that \u2014 let me say that I deny this with every fiber of my body.\u201d This denial and the somewhat late clarifications, call into question the reliability of the outlets that examine his every move and reproduce often distorting, one-sided statements by anonymous sources. Can we trust the information coming out of Brussels? And why is this one man, once propped up as a European revolutionary, getting so viciously torn down? The media\u2019s treatment of Varoufakis is a symbol of how they, and the European Union, have approached the Greek question since the crisis began. An anonymous, ill-informed and one-sided opinion claiming the Greek positions are unrealistic is given more merit than an entire country\u2019s demand for an end to austerity. Like Varoufakis in this case, a stereotype of the Greek national character is put before facts and figures.\n\nThe problem, from the start, was the cult of personality that the media built up around the him and the symbolism it carried. The Varoufakis narrative isn\u2019t about one man; it\u2019s a stand-in for an entire nation.\u00a0 While his intellectual credentials and the honesty of his positions are hard to deny \u2014 he has stuck by his views since the beginning of the Greek crisis and published books and dozens of articles in support of his analysis \u2014 what got him the attention of the international media was his disdain for convention, with his leather jacket, shaved head and motorbike. When he visited the U.K. to meet Chancellor of the Exchequer George Osborne, the Brits loved him. (Londoners did, after all, elect the markedly more outrageous Boris Johnson as their mayor.) There was hardly any international outlet that didn\u2019t carry a profile or an interview of his since Syriza won in the elections in January. \u201cRebel,\u201d \u201crock star\u201d and \u201cerratic Marxist\u201d (which he chose for himself) were just a few of the labels following him around. But the technocrats in Brussels were less impressed. This is what also made him the ideal scapegoat, onto which journalists and their readers could project their ambivalence (and perhaps lack of understanding) about the entire eurozone crisis.\n\nVaroufakis went to Brussels in January with a very simple mandate: to renegotiate Greece\u2019s bailout deals, which had brought little more to the country other than a deep recession and what the country\u2019s leaders repeatedly refer to as a humanitarian crisis. Even if he wanted to, he couldn\u2019t capitulate to the demands of the country\u2019s lenders and continue with the austerity program. He has been clear about his dim view of the austerity regime, calling it tantamount to leaving the euro, but he doesn\u2019t believe a Grexit is a good move, nor does the Greek electorate that voted for him. However, his reasonable (and decidedly nonradical) position never ended up being the focus of the talks or the coverage thereof. For example, when the Greeks ask why the country should privatize profit by selling public assets, such as regional airports, the only answer available to them in Brussels is \u201cBecause you have to.\u201d When Syriza officials suggest other ways out of the slump, the only reply available seems to them be \u201cYou can\u2019t do that, because we say so,\u201d regardless of the plethora of evidence that suggests the Greek economy can\u2019t handle more cuts and recessionary measures. Instead of taking his ideas seriously, Varoufakis\u2019 European counterparts obsessed over his clothes and his manners \u2014 perhaps to avoid the hard work of restructuring Greece\u2019s unviable debt and backing down on measures that, on the basis of how they have worked over the past five years, will sink the country further into recession. Similarly, when Syriza brought up the issue of war reparations that Germany might owe Greece, it was almost universally shunned.\n\nAs the media picked apart Varoufakis, from his smirk to his casual footwear, ugly stereotypes about Greeks resurfaced. In a German daily, the reporter wrote that while \u201cthe other finance ministers looked pale and tired, Varoufakis looked as if he had just come back from vacation.\u201d The fallacy of hardworking northerners and lazy southerners should have been put to rest with the 20th century but is still around in 2015. The press briefings cited in most media \u2014 which come almost exclusively from unofficial, anonymous sources \u2014 said that the discussions that took place over the past few months were no better. They spoke of Greece having no viable proposals and of living in an alternative reality. They accused Varoufakis of being an ideologue, as though German Chancellor Angela Merkel\u2019s notion of \u201cexpansionary contraction,\u201d which was used to justify the austerity dogma, wasn\u2019t in and of itself an ideological intervention (let alone a contradiction in terms). They even complained that Varoufakis was lecturing them on macroeconomics. He was, in a way. \u201cOne of the great ironies of the Eurogroup is that there is no macroeconomic discussion. It\u2019s all rules based, as if the rules are God given and as if the rules can go against the rules of macroeconomics,\u201d Varoufakis said in The Irish Times, in response to a criticism by Ireland\u2019s finance minister that he was \u201ctoo theoretical.\u201d For now, Varoufakis, like Greece, enjoys too much unwanted attention. While support for Syriza is growing and the party is now leading with 21 percentage points\u00a0over New Democracy, everyone from everyday supporters of the party (as recent polls have shown) to Greek businesses agrees that the negotiations have gone on too long. But it\u2019s also becoming obvious that when close to a solution, the EU goes back to the table with demands that make no sense in the current context. Earlier this week The Wall Street Journal reported that German Finance Minister Wolfgang Sch\u00e4uble \u201cshowed no willingness to compromise in the negotiations to unlock the final installment of Greece\u2019s 245 billion euro ($272 billion) bailout.\u201d And in late-night talks on Thursday, Greek Prime Minister Alexis Tsipras met with his counterparts from France and Germany; the atmosphere, Bloomberg reported, was convivial, but the team failed to reach an agreement to release additional bailout funds. We\u2019re running out of time, as we\u2019re only a few weeks before Greece is forced to default on billions of euros in debt repayments. Europe and the international media should stop talking about its finance minister\u2019s clothes and address his nation\u2019s needs and the ideas that he is putting on the table."},
{"url": "https://firstlook.org/theintercept/2015/05/22/sen-jim-inhofe-says-patriot-act-opponents-dont-understand-north-korea-path-kill-everyone/", "link_title": "Sen.Inhofe: Patriot Act Critics Forget; N.Korea, Iraq on Path to \u201cKill Everyone\u201d", "sentiment": 0.23833333333333334, "text": "Sen. Jim Inhofe, R-Okla., says his office is getting deluged with phone calls in support of his colleague Sen. Rand Paul\u2019s, R-Ky., campaign to end dragnet surveillance enabled by the Patriot Act.\n\nBut in an interview on Friday with radio station KTOK in Oklahoma City, Inhofe dismissed his constituents, claiming that privacy advocates don\u2019t understand that \u201cwe\u2019re in the most threatened position in the history of this country.\u201d\n\nThe senator argued \u201ccountries like North Korea, Iran, Iraq, Syria, all of them are on the path to getting bombs and delivery systems that would reach the United States of America and could have the effect of killing everyone who is listening now.\u201d\n\nInhofe went on to say that \u201ceveryone in the leadership except the president of the United States\u201d recognizes the threat he was describing, adding, \u201cwhen you stop and think and make a choice between having a complete city bombed out and privacy, my choice is easy.\u201d\n\nThe future of the Patriot Act is in flux as the House of Representatives leaves Washington for a Memorial Day recess week. Several important sections of the Patriot Act expire at the end of the month.\u00a0Senate Majority Leader Mitch McConnell, R-Ky., has called for a series of votes this weekend on separate proposals, including a full reauthorization of the Patriot Act and the USA Freedom Act, which would codify the bulk collection of metadata while adding privacy reforms to the process.\n\nInhofe later moved on from the NSA and privacy during the interview, telling KTOK that he is focused on working to \u201cstop the EPA over-regulation that\u2019s killing our farmers and a lot of our businesspeople.\u201d Notably, the Environmental Protection Agency has not killed any farmers or business people."},
{"url": "http://www.npr.org/2015/05/22/408025154/can-aging-be-cured", "link_title": "Can Aging Be Cured?", "sentiment": 0.06666666666666667, "text": "More From This Episode\n\nPart 2 of the TED Radio Hour episode The Fountain Of Youth\n\nCambridge researcher Aubrey de Grey argues that aging is merely a disease \u2014 and a curable one at that.\n\nAubrey de Grey is the chief science officer and co-founder of the SENS Research Foundation. He argues that aging can be \"cured\" if it's approached as an \"engineering problem.\" His plan calls for identifying the components that cause human tissue to age, and designing remedies for each\u2014 forestalling disease and prolonging healthy life."},
{"url": "http://securityaffairs.co/wordpress/37065/hacking/android-factory-reset-flaw.html", "link_title": "Android Factory reset fails to wipe sensitive user data, million devices at risk", "sentiment": 0.0019379844961240345, "text": "Researchers at Cambridge University, Laurent Simon and Ross Anderson, revealed that more than half a billion Android devices could have data recovered due to flaws in the default wiping process.\n\nThe experts have analyzed\u00a0Android 21 devices from 5 different vendors, including Samsung, HTC, and Nexus running Android versions 2.3 to 4.3, discovering that more than 500 Million Android devices\u00a0fail to completely wipe data after the factory reset. The researchers demonstrated that it is possible to retrieve encryption keys and master tokens for Google and Facebook in 80 percent of cases.\n\nUser data, including SMS, photos, and videos, can be recovered because the factory reset process in Android 4.3 Jellybean and below versions is affected by flaws.\u00a0The researchers explained \u00a0that the factory reset doesn\u2019t completely wipe data, in fact, they are able to retrieve encryption keys and master tokens for Google and Facebook in 80 percent of cases.\n\nUser data, including SMS, photos, and videos, can be recovered because the factory reset process in Android 4.3 Jellybean and below versions is affected by flaws.\u00a0The researchers published a paper titled\u00a0Security Analysis of Android Factory Resets (PDF) that includes the detailed results of the study.\n\n\u201cAfter the reboot, the phone successfully re-synchronised contacts, emails, and so on,\u201d researchers reported. \u201cWe recovered Google tokens in all devices with flawed Factory Reset, and the master token 80% of the time. Tokens for other apps such as Facebook can be recovered similarly. We stress that we have never attempted to use those tokens to access anyone\u2019s account.\u201d\u00a0 states the paper.\n\nAfter running factory reset on every Android device, the researchers were able to retain at least some piece of data previously stored in the Smartphone, Google account credentials, including text messages, conversations on apps such as WhatsApp and Facebook.\n\n\u201cWe estimate that up to 500 million devices may not properly sanitise their data partition where credentials and other sensitive data are stored, and up to 630 million may not properly sanitise the internal SD card where multimedia files are generally saved.We found we could recover Google credentials on all devices presenting a flawed factory reset. Full-disk encryption has the potential to mitigate the problem, but we found that a flawed factory reset leaves behind enough data for the encryption key to be recovered.\u201d explained the duo.\n\nThe experts already have reported the flaw to Google, they also highlighted that remote wiping feature\u00a0for lost and stolen Android devices is affected by the same flaws.\n\nEveryone that has sold used Android devices could be exposed to the risk of disclosure of their data, the problem is serious if we consider devices that are\u00a0put aside by private firms.\n\nCyber spies and thieves can buy or steal the mobile devices and obtain user sensitive data, including bank account information.\n\nAn attacker can also obtain the list of installed apps and use it to profile the previous Android owner, for example discovering if he is a corporate executive or simply a school kid.\n\nGoogle has yet to comment the factory reset issue, it suggests Android users to:\n\nbut researchers explained that this solution is not 100 percent reliable.\n\nLets me suggest enabling full disk encryption and used strong passwords to make hard brute forcing of master keys."},
{"url": "https://medium.com/@preslavrachev/sell-what-your-product-can-do-not-what-it-is-877be282d175", "link_title": "Sell What Your Product Can Do, Not What It Is", "sentiment": 0.05483630952380953, "text": "A couple of days ago, I watched this session with Des Traynor, a cofounder of Intercom. He gives lots of startup advice, and I warmly recommend you to watch it or give it a listen. One of the many helpful things he says, stuck in my head:\n\nIt\u2019s a common issue in startups, driven by technical and product-oriented founders. They usually start pitching the product, spending tons of effort describing its features in detail. Instead, what the customer cares about, are the problems that the product is going to help her solve. Pitching a product by listing its features is hardly ever going to resonate in her mind.\n\nTypical use cases involve product features used in combination. In simpler words, a product is more than the sum of its features:\n\nSounds simple and make total sense, right? Yet, looking back at all the projects I\u2019ve made and worked on, I admit that I have spent way too much time over-focusing on their features. It\u2019s a no-brainer then that some of those projects faded away or pivoted. Not because they were inferior in any product sense, but because I couldn\u2019t explain what they do, the way the customers expected.\n\nThink about it, the next time you pitch your products to customers."},
{"url": "http://yanirseroussi.com/2014/09/07/building-a-recommender-system-on-a-shoestring-budget/", "link_title": "Building a recommender system on a shoestring budget", "sentiment": 0.1418807641633729, "text": "\n\n This is the second part of a series of posts on my BCRecommender \u2013 personalised Bandcamp recommendations project.\n\n Check out the first part for the general motivation behind this project.\n\n\n\nBCRecommender is a hobby project whose main goal is to help me find music I like on Bandcamp. Its secondary goal is to serve as a testing ground for ideas I have and things I\u2019d like to explore.\n\n One question I\u2019ve been wondering about is: how much money does one need to spend on infrastructure for a simple web-based product before it reaches meaningful traffic?\n\n The answer is: not much at all. It can easily be done for less than $1 per month.\n\n This post discusses my exploration of this question by describing the main components of the BCRecommender system, without getting into the algorithms that drive it (which will be covered in subsequent posts).\n\nThe general flow of BCRecommender is fairly simple: crawl publicly-available data from Bandcamp (fan collections and tracks/albums = tralbums), generate recommendations based on this data (static lists of tralbums indexed by fan for personalised recommendations and by tralbum for similarity), and present the recommendations to users in a way that\u2019s easy to browse and explore (since we\u2019re dealing with music it must be playable, which is easy to achieve by embedding Bandcamp\u2019s iframes).\n\nThe first iteration of the project was implemented as a Django project. Having never built a Django project from scratch, I figured this would be a good way to learn how it\u2019s done properly. One thing I was keen on learning was using the Django ORM with an SQL database (in the past I\u2019ve worked with Django and MongoDB). This ended up working less smoothly than I expected, perhaps because I\u2019m too used to MongoDB, or because SQL forces you to model your data in unnatural ways, or because I insisted on using SQLite for simplicity. Whatever it was, I quickly started missing MongoDB, despite its flaws.\n\nI chose AWS for hosting because my personal account was under the free tier, and using a micro instance is more than enough for serving a website with no traffic. I considered Google App Engine with its indefinite free tier, but after reading the docs I realised I don\u2019t want to jump through so many hoops to use their system \u2013 Google\u2019s free tier was likely to cost too much in pain and time.\n\nWhile an AWS micro instance is enough for serving the recommendations, it\u2019s not enough for generating them. Rather than paying Amazon for another instance, I figured that using spare capacity on my own laptop (quad-core with 16GB of RAM) would be good enough. So the backend worker for BCRecommender ended up being a local virtual machine using one core and 4GB of RAM.\n\nAfter some coding I had a nice setup in place:\n\nThis system wasn\u2019t going to scale, but I didn\u2019t care. I just used it to discover new music, and it worked. I didn\u2019t even bother registering a domain name, so it was all running for free.\n\nA few months ago, Facebook announced that Parse\u2019s free tier will include 30 requests / second. That\u2019s over 2.5 million requests per day, which is quite a lot \u2013 probably enough to run the majority of websites on the internet. It seemed too good to be true, so I had to try it myself.\n\nIt took a few hours to convert the Django webserver/frontend code to Parse. This was fairly straightforward, and it had the added advantages of getting rid of some deployment scripts and having a more solid development environment. Parse supplies a command-line tool for deployment that constantly syncs the code to an app that is identical to the production app \u2013 much better than the Fabric script I had.\n\nThe disadvantages of the move to Parse were having to rewrite some of the backend in JavaScript (= less readable than Python), and a more complex data sync command (no longer just copying a big SQLite file). However, I would definitely use it for other projects because of the generous free tier, the availability of APIs for all major platforms, and the elimination of most operational concerns.\n\nWith the Django webserver out of the way, there was little use left for Django in the project. It took a few more hours to get rid of it, replacing the management commands with Commandr, and the SQLite database with MongoDB (wrapped with the excellent MongoEngine, which has matured a lot in recent years). MongoDB has become a more natural choice now, since it is the database used by Parse. I expect this setup of a local Python backend and a Parse frontend to work quite well (and remain virtually free) for the foreseeable future.\n\nThe only fixed cost I now have comes from registering the bcrecommender.com domain and managing it with Route 53. This wasn\u2019t required when I was running it only for myself, and I could have just kept it under bcrecommender.parseapp.com, but I think it would be useful for other Bandcamp users. I would also like to use it as a training lab to improve my (poor) marketing skills \u2013 not having a dedicated domain just looks bad.\n\nIn summary, it\u2019s definitely possible to build simple projects and host them for free. It also looks like my approach would scale way beyond the current BCRecommender volume. The next post in this series will cover some of the algorithms and general considerations of building the recommender system."},
{"url": "http://blog.gaijinpot.com/food-allergies-and-dietary-restrictions-in-japan/", "link_title": "Food allergies and dietary restrictions in Japan", "sentiment": 0.10641456582633052, "text": "I have been living in Japan for nearly a decade and in that time, I have met many vegans, vegetarians, and people who otherwise have to avoid certain types of food for medical, religious, or other reasons. It\u2019s not impossible to cut certain products out of your life, but it is far more difficult to do in Japan than many other countries.\n\nCommunicate Your Needs in Every Way Possible\n\nIt\u2019s quite surprising to some newcomers that a country which prides itself in soy products and healthy living actually makes meatless living quite difficult. I remember one case in particular when I went out to have pasta with several friends. One of our friends was a vegetarian. We scoured the menu, and the pepper pasta seemed vegetarian friendly. Not taking any chances, we asked the waitress if it contained meat, and she told us that it didn\u2019t. Relieved, my friend ordered that dish. When it arrived with the rest of our meals, we all sat there looking dumbfound at the dish our friend was given. It was topped with a layer of ham.\n\nWe argued with the waitress and the manager. They said that because we didn\u2019t specifically ask for the dish without meat, they weren\u2019t obligated to change our order. We still had to pay for the pasta, plus an additional one without meat. Luckily, we had a friend who was late to the restaurant and was happy to eat the otherwise unwanted pasta. While most restaurants would have made the effort to accommodate us, especially since we did ask about the contents prior to ordering, the story still stands as an example of the worst case scenario. If you have an allergy or dietary restriction, make sure to specify that you cannot have any such ingredients. Ask the server to check with the chef if there is any question about what is in your food.\n\nThere are some cases, as in the story above, where servers have no idea what is in the dish they are serving. They may give you the wrong information. I have one friend who cannot have ketchup. We were having lunch at McDonalds, and my friend asked the employee at the register whether the specialty sandwich had ketchup or any ketchup based sauce in it. The employee said no, that there was only a mayonnaise-based sauce, so my friend ordered the sandwich. Upon checking her order, my friend found that the sauce was a combination of ketchup and mayonnaise. Luckily, she was able to return the meal for a sauceless version. Always check your food before you eat it.\n\nReading labels is very important if you have any restrictions on what you can eat. There are many ingredients in traditional Japanese products and dishes that could catch people off guard.\n\nGaijinPot blogger James Darnbrook\u2019s article on supermarket shopping in Japan has a comprehensive list of food terminology. There is also a card you can print out if you need to avoid gluten. Finding gluten free products in general can be very difficult. Soy sauces, noodles, and a lot of Japanese dishes typically contain gluten. The word for gluten, \u30b0\u30eb\u30c6\u30f3 (guruten), is rarely if ever listed on product labels. That means you need to look out for the various types of \u9ea6 (mugi). There\u2019s \u5c0f\u9ea6 (komugi) \u30e9\u30a4\u9ea6 (raimugi) \u5927\u9ea6 (oomugi) and \u30aa\u30fc\u30c4\u9ea6 (ootsumugi), meaning wheat, rye, barley, and oats respectively.\n\nA big surprise for many foreign people is the fact that most of the ground beef in Japan is actually mixed with pork. If you need to avoid pork, make sure that the kanji for pork, \u8c5a (buta), is not on the label. 100% ground beef is very rare and typically only comes in small containers, unless you buy it at Costco or specifically ask for it from a butcher.\n\nBe very careful of nomenclature. Just because something sounds vegetarian or meatless does not mean that it doesn\u2019t have meat. The Starbucks veggie wrap had a good helping of chicken in it. Tully\u2019s four cheese focaccia has a layer of ham. Be very careful to check labels for contents and ask servers to not only verify that something does not have meat, but to make note that you do not want meat.\n\nIn Japan, \u201csoy burgers\u201d and \u201cveggie burgers\u201d do not necessarily mean that the item is vegetarian. It could simply mean that the main ingredient in the meat burger is soy or a veggie mix. Many patties sold at supermarkets as \u201csoy burgers\u201d also contain fish, squid, octopus, and/or egg.\n\nRecently, almond milk and coconut milk have been popping up at various supermarkets and convenience stores. However, I have found items marketed as almond milk or coconut milk that were actually just milk with almond or coconut flavoring. The fact that it was actually milk was not noticeable anywhere except on the list of ingredients. This could be potentially disastrous for someone with dairy allergies. Always read the labels carefully.\n\nAlso be careful at cafes. There are many cafes that allow for drink customization, though it\u2019s not as widespread as in other countries. If you ask for something made with soy milk, you will also need to specify that you don\u2019t want whipped cream or any sort of cream on top. Nothing is more upsetting and frustrating than specifically asking for a soy non-dairy drink, and getting that perfectly non-dairy drink topped with a tower of cream. Apparently, it is considered health conscious and trendy to order soy milk but still have the drink made as usual with other dairy products. It\u2019s the Japanese equivalent to ordering a \u201chealthy\u201d diet soda with your burger and fries."},
{"url": "http://pastebin.com/BMSpLPJY", "link_title": "Lizard Squad Press Release", "sentiment": 0.16242424242424244, "text": "Hello, Many news agencies are reporting about Obnoxious' arrest. We decided to make this just to clear the dust in the air. This is important because the \"You can't arrest a lizard\" is still a valid statement. However, people are starting to think otherwise, because of incidents like this. The reason the statement still stands is because Obnoxious and many others aren't a part of the core members. They're public figures that like to play. We've said it so many times it's getting annoying, the grand total of indictments of lizard squad members is ZERO. Side note: There's a reason Obnoxious was smiling at the hearing. He was trying to get arrested, hence why he live streamed it. We had a bet that he couldn't get arrested within the month and he won. Side note #2: Our 1 year anniversary is coming up. So be ready for what we have in store for you. With love, - Lizard Squad [0] - http://arstechnica.com/tech-policy/2015/05/teen-pleads-guilty-to-23-charges-of-swatting-harassing-online-game-rivals/ [1] - http://www.businessinsider.com/lizard-squad-hacker-was-just-arrested-for-swatting-female-league-of-legends-gamers-2015-5 MIRROR: shenron.su/media/obnoxious.txt"},
{"url": "http://www.zsoltnagy.eu/eslint-for-better-productivity-and-higher-accuracy/", "link_title": "ESLint for Better Productivity and Higher Accuracy", "sentiment": 0.1206941779811098, "text": "What is focus? Concentration of power. When writing Javascript code, most of us focus on solving very complex problems. While dedicating intellectual capacity to the solution, small code quality mistakes are often made. It takes time and effort to fix these mistakes, provided that you notice them. If you don\u2019t, your teammates will definitely point them out. On your worst days, you may end up feeling embarrassed: despite the fact that you stick to the highest possible standards, you are still human and you make mistakes you should not be making.\n\nMany people have realized in the past that code quality checks have to be automated. For this reason, Javascript linters were invented. Some of you may already use a linter like JsLint or JsHint. They are all great tools for increasing your accuracy. In this article, I will introduce you to an alternative: ESLint. ESLint provides you with full control over the linting process, including the application of your own custom rules.\n\nExcellent code quality is a necessary condition for excellent maintainability. If you work in a team, in order to avoid confusing each other, you have to stick to standards. If you think you don\u2019t work in teams, think again! Unless you create hobby projects for yourself, you have to write code under the assumption that someone else will contribute to your code base. Your coding standards have to be enforced and automatized. This is where ESLint comes in: it gives you the freedom to focus on the problem you would like to solve, and let the rest happen on autopilot. You may make mistakes such as:\n\nThis list is not exhaustive and can never be exhaustive. Some teams have specific agreements that no-one has ever thought about. ESLint gives you the chance to create and enforce these agreements via custom rules.\n\nSimilarly to my last post on ES6 modules and Webpack, you will need NodeJs to install ESLint.\n\nYou will also need a configuration file , wherever you would like to run ESLint. The documentation describes the possible settings you can include in your JSON object in your configuration file. It would be useless to include a list here as new options are continuously added as new versions come out.\n\nI will write an example file with a couple of injected errors ESLint should catch. This function should calculate according to the recursive definition , including the exit criterion .\n\nHow many errors can you spot in this code? Regardless of the quality standards you stick to, there is one error that does not even let the code execute.\n\nAutomatic semicolon insertion and the return statement: Semicolons are inserted automatically at the end of each line where the statement is not supposed to continue. This happens to our first statement too. The code\n\nafter the semicolon insertion. This results in a syntax error. Let\u2019s verify the error with ESLint:\n\nThe solution to this problem is that the open brace of the returned object has to be on the same line as the return statement. After making this change, ESLint can start detecting other problems:\n\nEleven linting errors. We have some work to do.\n\nFirst of all, I did not use strict mode. We can easily address this problem by adding to the beginning of the file. It is good practice anyway.\n\nWe were also told that was never used in the code. ESLint is intelligent enough to detect that a recursive call does not count as a usage. For the sake of simplicity, I will encapsulate function in a self-invoking function.\n\nThe variable is only there for demonstration purposes. Let\u2019s continue with the errors.\n\nAn unexpected statement was found in the code. I have seen people push statements to remote repositories and it\u2019s not a nice experience to load an application just to find out that it enters debug mode under some circumstances. ESLint is intelligent enough to detect debuggers, therefore you will never have to worry about pushing a forgotten statement.\n\nWe have two errors with the name . ESLint does not like code of format\n\nThe reason is that it is very easy to shut off our brain and write the following:\n\nIn practice, is executed regardless the value of the condition as only belongs to the if statement. Although this error has never happened to me, it could happen to others, so I mostly respect the rule. The only exception could be the shorthand\n\nThis shorthand is more compact, more readable, and the above reasoning does not apply as we cannot trick ourselves with wrong indentation.\n\nThe errors indicate missing semicolons. Although Javascript inserts them automatically, it is still good practice to go the extra mile (extra millimeter to be exact) and write those semicolons explicitly. Just like free text, Javascript code should also be readable. Imagine reading this article without full stops. It would not be convenient, right?\n\nThe rule is not a big deal. It just indicates the lack of terminating new line at the end of the file. Let\u2019s add it.\n\nWe are almost done. If you prefer strict equality to , don\u2019t turn off! I erroneously wrote the condition . Let\u2019s correct it to .\n\nOur beautified code looks like this:\n\nAnother linting reveals that most errors are gone:\n\nLet\u2019s make the conscious decision that this rule is not for us because we prefer single quotes to double quotes. We can easily change our preferences by adding a rule to .\n\nThe new rule specifies that we have to use single quotes for strings. Double quotes are only tolerated inside a string for the purpose of avoiding escaping. Let\u2019s run the linter again!\n\nSurprisingly, we used single quotes as well as double quotes in this function. Mixing the two is not advised as it indicates lack of consistency. The guilty double codes were added with the string . After changing the quotes and running ESLint again, all errors will disappear.\n\nLet\u2019s add a built in ESLint rule to our configuration file. As you might have noticed, I used the form . Some coding standards require a space between the keyword and the open parenthesis so that it is easier to tell the difference between a statement and a function. Let\u2019s add this rule to the file.\n\nYou can find the rule in the list of all ESLint rules in the official documentation.\n\nRe-linting the code gives you two new errors.\n\nFixing the indentation errors will make linting succeed. Let\u2019s have a look at the final result.\n\nThe code is a lot nicer to read than at the start of the article, isn\u2019t it?\n\nAutomation has a key role in increasing productivity and accuracy. If you want to write maintainable code, make linting a must for yourself and your team! A safety net that catches your errors gives you the freedom to focus on finding the most efficient, elegant and maintainable solution to the problems you face with.\n\nI have very ambitious goals with ESLint, namely covering my coding standards as much as possible. This will definitely include writing some custom rules. If you get stuck with ESLint or you would like to read about a specific topic related to linting, write a comment below."},
{"url": "http://www.designer-daily.com/google-reveals-literata-a-new-typeface-for-google-play-books-53240", "link_title": "Google reveals Literata, a new typeface for Google Play books", "sentiment": 0.09564393939393938, "text": "So far, Droid Serif was the default font for reading books on Google Play. Earlier in May, Google quietly introduced a new typeface to replace it along with the release of the latest version of PlayBooks. Google introduced Literata, a well-balanced serif font family that was designed for readability on screen.\n\nThe new typeface was designed by Type Together, a firm specialized in creating tailored fonts for the corporate world. Literata makes the junction between the literary world, with a more textured look-and-feel, and the world of tablets and smartphones with its focus on on-screen readability.\u00a0Literata features two weights and the matching italics, and it has been\u00a0adapted for various other languages."},
{"url": "http://www.theguardian.com/world/2015/may/22/france-to-force-big-supermarkets-to-give-away-unsold-food-to-charity?CMP=fb_gu", "link_title": "France to force big supermarkets to give unsold food to charities", "sentiment": -0.036569865319865315, "text": "French supermarkets will be banned from throwing away or destroying unsold food and must instead donate it to charities or for animal feed, under a law set to crack down on food waste.\n\nThe French national assembly voted unanimously to pass the legislation as France battles an epidemic of wasted food that has highlighted the divide between giant food firms and people who are struggling to eat.\n\nAs MPs united in a rare cross-party consensus, the centre-right deputy Yves J\u00e9go told parliament: \u201cThere\u2019s an absolute urgency \u2013 charities are desperate for food. The most moving part of this law is that it opens us up to others who are suffering.\u201d\n\nSupermarkets will be barred from deliberately spoiling unsold food so it cannot be eaten. Those with a footprint of 4,305 sq ft (400 sq m) or more will have to sign contracts with charities by July next year or face penalties including fines of up to \u20ac75,000 (\u00a353,000) or two years in jail.\n\n\u201cIt\u2019s scandalous to see bleach being poured into supermarket dustbins along with edible foods,\u201d said the Socialist deputy Guillaume Garot, a former food minister who proposed the bill.\n\nIn recent years, French media have highlighted how poor families, students, unemployed or homeless people often stealthily forage in supermarket bins at night to feed themselves, able to survive on edible products which had been thrown out just as their best-before dates approached.\n\nBut some supermarkets doused binned food in bleach to prevent potential food-poisoning by eating food from bins. Other supermarkets deliberately binned food in locked warehouses for collection by refuse trucks to stop scavengers.\n\nThe practice of foraging in supermarket bins is not without risk \u2013 some people picking through rotten fruit and rubbish to reach yoghurts, cheese platters or readymade pizzas have been stopped by police and faced criminal action for theft. In 2011, a 59-year-old father of six working for the minimum wage at a Monoprix supermarket in Marseille almost lost his job after a colleague called security when they saw him pick six melons and two lettuces out of a bin.\n\nPressure groups, recycling commandos and direct action foraging movements have been highlighting the issue of waste in France. Members of the Gars\u2019pilleurs, an action group founded in Lyon, don gardening gloves to remove food from supermarket bins at night and redistribute it on the streets the next morning to raise awareness about waste, poverty and food distribution.\n\nThe group and four others issued a statement earlier this year warning that simply obliging supermarket giants to pass unsold food to charities could give a \u201cfalse and dangerous idea of a magic solution\u201d to food waste. They said it would create an illusion that supermarkets had done their bit, while failing to address the wider issue of overproduction in the food industry as well as the wastage in food distribution chains.\n\nThe law will also introduce an education programme about food waste in schools and businesses. It follows a measure in February to remove the best-before dates on fresh foods.\n\nThe measures are part of wider drive to halve the amount of food waste in France by 2025. According to official estimates, the average French person throws out 20kg-30kg of food a year \u2013 7kg of which is still in its wrapping. The combined national cost of this is up to \u20ac20bn.\n\nOf the 7.1m tonnes of food wasted in France each year, 67% is binned by consumers, 15% by restaurants and 11% by shops. Each year 1.3bn tonnes of food are wasted worldwide.\n\nThe F\u00e9d\u00e9ration du Commerce et de la Distribution, which represents big supermarkets, criticised the plan. \u201cThe law is wrong in both target and intent, given the big stores represent only 5% of food waste but have these new obligations,\u201d said Jacques Creyssel, head of the organisation. \u201cThey are already the pre-eminent food donors, with more than 4,500 stores having signed agreements with aid groups.\u201d\n\nThe logistics of the law must also not put an unfair burden on charities, with the unsold food given to them in a way that is ready to use, a parliamentary report has stipulated. It must not be up to charities to have to sift through the waste to set aside squashed fruit or food that had gone off. Supermarkets have said that charities must now also be properly equipped with fridges and trucks to be able to handle the food donations.\n\nThe French law goes further than the UK, where the government has a voluntary agreement with the grocery and retail sector to cut both food and packaging waste in the supply chain, but does not believe in mandatory targets.\n\nA report earlier this year showed that in the UK, households threw away 7m tonnes of food in 2012, enough to fill London\u2019s Wembley stadium nine times over. Avoidable household food waste in the UK is associated with 17m tonnes of CO2 emissions annually."},
{"url": "http://marcelgruenauer.com/terminal-screencasts-with-asciinema/", "link_title": "Terminal Screencasts with asciinema", "sentiment": 0.14427759740259738, "text": "Screencasting the terminal is useful for demos. On OS X, you could use QuickTime Player to create a screen recording of the terminal window, then upload the video to a site like Vimeo. The disadvantage is that a video file is rather large for this purpose, and since it\u2019s hosted on some video site, it makes offline blogging burdensome. Furthermore, if you don\u2019t watch it at its original size, the text might be blurry.\n\nasciinema is a lightweight, open source alternative. It consists of two parts: first, a recorder, which creates a new shell and records everything that goes on in there including timing, and saves it to a JSON file. The second part is the JavaScript player, which takes the recording and replays it in the browser, making it look like a video. But it\u2019s all pure text, so it looks great.\n\nYou can upload recordings to the asciinema site and replay them from there, but I prefer to keep everything local so I can work offline, streamline the workflow and be independent of an external hosting site.\n\nSetting up asciinema for local development is a bit involved, so here is a step-by-step guide.\n\nOn OS X, you can install with :\n\nWhen I tried to run it, it complained about an UTF-8 issue:\n\nJust make sure that your contains something like:\n\nNow you\u2019re ready to record a screencast. Save it to a file called .\n\nThis will open a new shell. Everything you do in that shell, including timing information, will be recorded to .\n\nMy bash prompt and vim configuration are quite involved, so I\u2019d like to simplify them while recording screencasts. sets the environment variable for the new shell, and I\u2019m testing for that in :\n\nAlso, in , I\u2019m using a simpler colorscheme and turn off syntax highlighting during recording:\n\nThe asciinema JavaScript player doesn\u2019t understand the format produced by the recorder. You have to convert it to a simpler format. Clone the converter:\n\nAs explained there, the converter needs . It says that it only compiles on Linux, but I was able to install it without problems on OS X.\n\nThe converter also need a file from another repository.\n\nThe binary must be in the same directory as from the gist. And you have to run from that directory, or it won\u2019t find .\n\nNow you can convert the recording:\n\nClone the player repository and build it:\n\nNow make a directory that will contain the mini-site for the demo and copy CSS and JavaScript files:\n\nAdjust the parameters as described in the player repository\u2019s .\n\nNow view the finished recording in a browser. Run a basic webserver and open :\n\nAll this seems like a lot of work, but once it\u2019s set up, you just have to record your screencast, convert it and copy it to your site where the player can find it.\n\nTalk to me on Twitter."},
{"url": "https://github.com/dunglas/DunglasApiBundle", "link_title": "Create JSON-LD / Hydra / Schema.org API in Minutes (Symfony Inside)", "sentiment": 0.38074074074074077, "text": "DunglasApiBundle is an easy to use and powerful system to create hypermedia-driven REST APIs for the Symfony framework and API Platform. It embraces JSON for Linked Data (JSON-LD) and Hydra Core Vocabulary web standards.\n\nBuild a working and fully-featured CRUD API in minutes. Leverage the awesome features of the tool to develop complex and high performance API-first projects.\n\nHere is the fully-featured REST API you'll get in minutes, I promise:\n\nEverything is fully customizable through a powerful event system and strong OOP. This bundle is documented and tested with Behat (take a look at the directory).\n\nDunglasApiBundle is part of the API Platform project. It is developed by K\u00e9vin Dunglas, Les-Tilleuls.coop and awesome contributors."},
{"url": "http://www.wsj.com/articles/indian-newspapers-balk-at-moms-ad-seeking-groom-for-gay-son-1432351828", "link_title": "Indian Newspapers Balk at Mom\u2019s Ad Seeking Groom for Gay Son", "sentiment": 0.5, "text": "Like many parents in India, Padma Iyer decided to place an ad in the matrimonial sections of some national Indian newspapers to look for a spouse for her son, Harish Iyer. But three of the four papers refused to run the advertisement, according to Mr. Iyer.\n\nThe ad read: \u201cSeeking 25-40, well-placed, animal-loving, vegetarian groom for my son (36, 5\u201911) who works with an NGO. Caste no bar (Though Iyer Preferred).\u201d Iyer is a Brahmin..."},
{"url": "http://petapixel.com/2015/05/21/high-school-forces-student-to-remove-online-photos-under-threat-of-suspension/", "link_title": "Student Pressured by School to Remove Photography Portfolio", "sentiment": 0.048186730527156066, "text": "Imagine assembling a portfolio of over 4,000 photographs and then being forced to make it disappear or face life-altering consequences; that\u2019s the situation sophomore Anthony Mazur is currently facing at Flower Mound High School in his Texas hometown. After discovering the love of sports photography, the Lewisville Independent School District is now claiming that Anthony\u2019s photographs are theirs and that he has no right to use them.\n\n \n\n Mazur tells PetaPixel that he joined a yearbook class at the beginning of his sophomore year, which resulted in a school trip to a seminar lead by the Texas Association of Journalism Educators. The event inspired Mazur to pursue his passion for photography; he began capturing sporting events and the world around him using a camera and equipment provided by the school. Supported by parents, players, and friends, it seemed that everyone praised the work he was doing.\n\nHowever, the Flower Mound High School felt differently than others. One day, Mazur was pulled from one of his classes and whisked down to the principal\u2019s office where he found his \u201cFlickr website where [he] posted all of [his] photographs\u201d displayed on the administrator\u2019s computer screen. The principal insisted that Mazur remove the photos from Flickr, calling the usage illegal, and threatening Mazur with in-school suspension and a ban from all extracurricular activities.\n\nAccording to Anthony Mazur, the threat went further, the principal then made a \u201cconcealed threat\u201d to receive all money that Mazur possibly made from his photography endeavor. Attempting to avoid a scholastic disaster, Mazur was pressured into signing an Administrative Directive with his parents, forcing him to take down his entire portfolio.\n\nMazur has since posted an account of this dispute on his Flickr page, which has been scrubbed clean of the disputed photos.\n\nAs of now, Mazur\u2019s family has already filed and lost an appeal to the principals of Flower Mound High School. The school claimed that Mazur\u2019s work \u201cviolated students\u2019 privacy by posting their image\u201d. Currently, the family has filed an appeal to the Superintendent of Schools.\n\nMazur told us in an interview that the second appeal has lapsed behind its seven-day response time, and the family is now seeking to appeal to the school board directly.\n\nThe real question is whether or not Anthony Mazur has a case on his hands. In his support stands Title 17 of the United States Copyright Law, which denotes that the \u201cCopyright in a work protected under this title vests initially in the author or authors of the work\u201d; in the case of photography, the individual who presses the shutter is the \u2018author\u2019.\n\nIn addition, the District\u2019s Board Policy Manual explicitly states \u201ca student shall retain all rights to work created as part of the instruction or using District technology resources.\u201d\n\nThe Lewisville Independent School District may have a case citing the expectation of privacy within a school environment. As we recently noted in our article, \u201cHow To Protect Your Rights As a Photographer in the Modern World,\u201d there are areas that photographs may be restricted in due to their private or secure nature. For the district to prove that there is a reasonable expectation of privacy, they must either demonstrate that the school is generally recognized by society as private.\n\nWe reached out to the Lewisville Independent School District multiple times but they have not yet responded to our request for comment.\n\nUpdate on 5/22/15: Here\u2019s a statement provided to us by the Lewisville Independent School District via email:\n\nAccording to Lewisville ISD\u2019s Acceptable Use Policy, the electronic communications system is defined as the district\u2019s network (including the wireless network), servers, computer workstations, mobile technologies, peripherals, applications, databases, online resources, Internet access, email and any other technology designated for use by students, including all new technologies as they become available. The district considers the use of said technologies to be inappropriate when a student electronically posts data (including but not limited to audio recordings, video recordings, images and personal information) about others or oneself when it is not related to a class project and/or without the permission of all parties. Lewisville ISD\u2019s practice is if anyone attending a public district event takes photos using their own device from an area accessible to the public, the district would not interfere with those photos being posted to a third-party site. The district is not at liberty to share student information pertaining to this situation due to the Family Educational Rights and Privacy Act [FERPA].\n\nImage credits: Photographs by Anthony Mazur and used with permission. Photograph of Anthony Mazur was taken by\u00a0Aahlad Madireddy."},
{"url": "http://www.theguardian.com/technology/2015/may/23/amazon-to-begin-paying-corporation-tax-on-uk-retail-sales", "link_title": "Amazon to begin paying corporation tax on UK retail sales", "sentiment": 0.06675463912306018, "text": "Amazon has become the first technology company to abandon controversial corporate structures that divert sales and profits away from UK in the face of a clampdown imposed by George Osborne.\n\nFrom the start of this month the online retailer has started booking its sales through the UK, meaning resulting profits will be taxed by HMRC. The group made $8.3bn (\u00a35.3bn) of worldwide sales from British online shoppers but for 11 years all these internet transactions have been booked in Luxembourg.\n\nA spokesman said Amazon was \u201cnow recording retail sales made to customers in the UK through the UK branch. Previously, these sales were recorded in Luxembourg\u201d.\n\nThe move will allow Amazon to avoid being caught by chancellor George Osborne\u2019s new diverted profits tax, which came into law from April. It imposes a punitive 25% tax on groups deemed to be artificially routing profits overseas.\n\nAmazon had for years denied that its UK corporate structures were artificial or tax-motivated. The move will be greeted as a victory for the chancellor who last September singled his determination to rein in technology firms going to extraordinary lengths to avoid UK tax. \u201cYou are welcome here in Britain with open arms,\u201d he said.\n\n\u201cWhile we offer some of the lowest business taxes in the world, we expect those taxes to be paid.\u201d\n\nHinting at the diverted profits tax \u2013 which has popularly become known as the Google Tax \u2013 he added: \u201cIf you abuse our tax system, you abuse the trust of the British people.\u201d A number of other countries are considering copying Osborne\u2019s diverted profits tax.\n\nLatest moves by Amazon will put pressure on others to follow suit \u2013 particularly Google, which routes its sales through Ireland. Representatives from Amazon and Google suffered two bruising encounters with parliament\u2019s public accounts committee over these arrangements.\n\nCommittee chair Margaret Hodge told Matt Brittin, Google\u2019s northern Europe boss, that his company\u2019s behaviour on tax was \u201cdevious, calculated and, in my view, unethical\u201d.\n\nShe added: \u201dYou are a company that says you \u2018do no evil\u2019. And I think that you do do evil.\u201d Hodge was referring to Google\u2019s long-standing corporate motto, \u201cDon\u2019t be evil,\u201d which appeared in its $23bn US stock market flotation prospectus in 2004.\n\nThe dressing down from Hodge, who last week announced she was stepped down from the committee, is widely credited with stirring widespread public outcry, which, in turn, put pressure on the chancellor to act. Amazon confirmed the changes to its tax affairs after the Guardian reported on Friday that its top two executives in the UK had quietly quit as directors of the online retailer\u2019s main British company in the face of Osborne\u2019s clampdown.\n\nChristopher North, Amazon\u2019s managing director for the UK, and Rob McWilliam, who joined from Asda as finance director two years ago, resigned from the board of Amazon.co.uk Ltd on 1 May.\n\nNorth and McWilliam remain senior executives despite relinquishing their directorships, with the former continuing as country manager and head of Amazon in the UK, while McWilliam is the UK\u2019s vice president of the consumables division.\n\n\n\nThe company said: \u201cWe regularly review our business structure to ensure that we are able to best serve our customers and provide additional product and services. More than two years ago we began the process of establishing local country branches of Amazon EU Sarl, our primary retail operating company in Europe.\u201d\n\nSales are still being recorded by Amazon EU Sarl, a Luxembourg-registered company, but \u2013 crucially for tax purposes \u2013 will be booked in a UK branch of that company, for which a tax return must be filed with HMRC.\n\nA year ago, North claimed in an interview with the Guardian that Amazon\u2019s European corporate structure was not determined by tax avoidance strategies, insisting it would be impossible to route sales to UK customers through a British company paying tax to HMRC. \u201cWe just couldn\u2019t do that,\u201d he said. \u201cAnd a single European business is going to need a single European headquarters.\u201d\n\nLatest accounts for Amazon.co.uk Ltd show sales of just \u00a3449m for 2013 and a tax charge of \u00a34.2m. Elsewhere in its corporate filings, however, Amazon attributed $7.29bn (\u00a34.71bn) of worldwide net sales to the UK for the same year.\n\nThe UK business employs thousands of staff, many on low wages, in its network of warehouses, and also large number in sales, procurement and marketing activities. However, its revenue comes from services provided to Amazon EU Sarl, and it does not transact with British online customers."},
{"url": "http://www.presentationzen.com/presentationzen/2015/05/the-key-to-story-structure-in-two-words-therefore-but.html", "link_title": "The Key to story structure in two words: Therefore and But", "sentiment": 0.13494468494468498, "text": "There are a ton of storytelling-related books and websites in the cosmos. And there is no shortage of people giving story advice and tips. Much of the advice is helpful, but the enormous volume of information related to writing or telling better stories can be overwhelming. Therefore, when someone credible comes along who offers free, insanely simple yet effective advice for improving one's story, he will find a very large audience indeed. This is exactly what happened just a few years ago, all quite by accident it would seem.\n\n\n\n In 2011 Comedy Central began shooting a documentary about the process behind the creation of a typical South Park episode. The short film\u2014\"Six Days to Air: The Making of South Park\"\u2014 focuses on the co-creators and lead writers for the show, Matt Stone and Trey Parker, as they and their team brainstorm ideas, write, rewrite, record dialog, and finally animate one entire show in just six days. The documentary begins as Matt and Trey return from New York City where their first Broadway musical, The Book of Mormon, had just opened to rave reviews. Now back in Colorado, they find themselves with no ideas for the next episode of South Park and with the pressure of producing a show that will air in less than a week. This, they say, is all quite normal for them. The process is intense and the pressure is palpable, but without the crazy deadline, says Trey Parker, the episodes would never get finished. At first, Matt and Trey and a few other writers and producers sit in a room with a large whiteboard and bounce ideas around. Often Usually the ideas are absurd, but if it makes others in the room laugh, then they may be on to something. \"For every good idea we get, there are a hundred not so good ones,\" Matt Stone says. (You can find the Six Days to Air documentary as an extra on the complete 15th Season of South Park DVD.)\n\n\n\nTherefore & But\n\nThe entire documentary is insightful, but there is one 45-second bit that popped out to anyone who is interested in writing or telling stories. When talking about the frantic rewriting process of their script, Trey reveals his simple rule for rewriting and improving the story. \"I call it the rule of replacing ands with either buts or therefores.\" Trey says that a common trap a lot of writers fall into is describing actions and events in a typical \"this happened, and then this happened, and then this happened, and then this happened....\" This kind of X and then Y and then Z progression\u2014similar to creating a list of things\u2014is not engaging. This approach to writing (or speaking) is dull and does not generate momentum, let alone sustain it. Therefore, Trey says, \"whenever I can go back in the writing and change that to \"this happened, therefore this happens. But this happens...\" In other words, says, Trey, \"Whenever you can replace your 'ands' with 'buts' and 'therefores,' it makes for better writing.\"\n\n\n\nLater that year in 2011, Trey and Matt surprised a\u00a0\u201cStorytelling Strategies\u201d class at NYU as part of a mtvU series and offered up story advice, once again explaining the \"Replacing ands with therefores and buts\" story structure tip. Watch the 6-min clip below.\n\nHere's the transcript from the key part of the video above where Trey Parker explains their simple but oh so effective rewriting tip.\n\n\"Each individual scene has to work as a funny sketch. You don\u2019t want to have one scene and go \u2018well, what was the point of that scene?\u2019 So we found out this rule that maybe you guys have all heard before, but it took us a long time to learn it. But we can take these beats, which are basically the beats of your outline. And if the words \u2018and then\u2019 belong between those beats\u2026 you\u2019re f****d. Basically. You got something pretty boring. What should happen between every beat that you\u2019ve written down is either the word 'Therefore' or 'but,' right? So what I\u2019m saying is that you come up with an idea and it\u2019s like \u2018okay, this happens\u2019 and then \u2018THIS happens.\u2019 No no no. It should be \u2018this happens\u2019 and THEREFORE \u2018this happens.\u2019 BUT \u2018this happens\u2019 THEREFORE \u2018this happens.\u2019 \u2026 And sometimes we will literally write it out to make sure we\u2019re doing it. We\u2019ll have our beats and we\u2019ll say okay \u2018this happens\u2019 but \u2018then this happens\u2019 and that affects this and that does to that and that\u2019s why you get a show that feels okay.\"\n\nBut, there's more...\n\nRight, I'm sure you've got it, but here's one more explanation of the Trey Parker story tip. Below is a wonderful video essay by Tony Zhou where he explains how important Buts and Therefores are in creating a tight, well structured story. As Tony says, as much as possible, we want to avoid the dreaded \"and then, and then, and then...\" Tony also touches on\u00a0Alfred Hitchcock's story structure technique called \"Meanwhile, back at the ranch.\" This is where you have two (or more) things going in parallel. When you reach the peak of one then you can move to the other. You see this in films a lot. Tony Zhou is a remarkable video essayist. Checkout all of his video essays. He's a great teacher.\n\n\n\n \n\nF for Fake (1973) - How to Structure a Video Essay from Tony Zhou on Vimeo.\n\nI first came across this therefore/but story structure tip in a great screenwriting book called Screenwriting 101 by Film Crit Hulk! This is one of the freshest screenwriting books I have ever read (and there are a gazillion screenwriting books). In the book, Hulk talks for a couple of pages about the Trey Parker and Matt Stone simple tip of changing ands to therefores and buts. After reading this I went out and purchased the South Park season 15 DVD just so I could get the documentaries which are included as extras. It was worth it.\n\n\n\nRemember, there are no panaceas, but looking again at your writing\u2014or your presentation structure\u2014and going back and changing your 'and then' to a 'but' or 'therefore' can make a huge difference as you continue to tighten your story, giving it tension and momentum."},
{"url": "http://vilcins.lv/blog/2015/CSS-hacks-you-may-not-know/", "link_title": "CSS hacks you may not know", "sentiment": 0.1951388888888889, "text": "You can use unicode characters for class names in your HTML, and even use write CSS selectors with those same characters.\n\nAs you know older browsers have different ways of controlling the opacity or transparency.\n\nCurrently works only in Firefox.\n\nCorrect syntax for all background properties is:\n\nSetting the property to 62.5% in makes 1em equal to 10px.\n\nEasily target IE8 and older versions. Yes, I know it\u2019s 2015, but I recently used this hack :D\n\nP.S. If you know of any other good ones, please contact me via twitter or email, i will add."},
{"url": "https://www.linkedin.com/pulse/using-vrin-model-evaluate-web-platforms-joni-salminen", "link_title": "Using the VRIN model to evaluate web platforms", "sentiment": 0.13841946341946346, "text": "In this article, I discuss how the classic VRIN model can be used to evaluate modern web platforms.\n\nIt's one of the most cited models of the resource-based view of the firm. Essentially, it describes how a firm can achieve\u00a0sustainable competitive advantage through resources that fulfill certain criteria.\n\nThese criteria for resources that provide a sustainable competitive advantage are:\n\nBy gaining access to this type of resources, a firm can create a lasting competitive advantage. Note that this framework takes one perspective to strategy, i.e. the resource-based view. Alternative ones are e.g. Porter's five forces\u00a0and power-based frameworks, among many others.\n\nThe \"resource\" in resource-based view\u00a0can be defined as some form of input which can be transformed into tangible or intangible output that provides utility or value in the market. In a competitive\u00a0setting, a firm competes with its resources against other players; what resources it has and how it uses them are key variables in determining the competitive outcome, i.e. success or failure in the market.\n\nIn each business environment, there are certain resources that are particularly important. An orange juice factory, for example,\u00a0requires different resources to be successful than a consulting business (the former needs a good supply of oranges, and the latter bright consultants; both rely on good customer relationships, though).\n\nSo, what kind of resources are relevant for online platforms?\n\nI first give a general overview of the VRIN dimensions in online context. This is done by comparing online environment with offline environment.\n\nThe term 'value' is tricky because of its definition: if we define it as something useful, we easily end up in a tautology (circular argument): a resource is valuable because it is useful for some party.\n\nThe specific resources for online platforms are discussed later on.\n\nOne of the key preoccupations in economic theory is scarcity: raw materials are scarce and firms need to compete over their exploitation.\n\nOffline industries are characterized by rivalry - once oil is consumed, it cannot be reused. Knowledge products on the web, on the other hand, are described as non-rivalry products: if one consumer downloads an MP3 song, that does not remove the ability for another consumer to download as well (but if a consumer buys a snickers bar, there is one less for others to buy). Scarcity is usually associated to startups so that they are forced to innovate due to liability of smallness.\n\nThis deals with how well the business idea can be copied.\n\nin \"traditional\" industries, such as manufacturing, patents and copyrights (IPR) are important. They protect firms against infringement and plagiarism. without them, every innovation could be easily copied which would quickly erode any competitive advantage. Intellectual property rights therefore enable the protection of \"innovations\" against imitation.\n\nImitation is less of a concern online. In most cases, the web technologies are public knowledge (e.g., open source). Even large players contribute to public domain. Therefore, rather than being something that competitors could not imitate, the emphasis on competition between web platforms tends to be on acquiring users rather than patents. (There are also other sources of resource advantage we'll discuss later on.)\n\nThe difference between imitation and substitution is that in the former you are being copied whereas in the latter your product is being replaced by another solution. For example, Evernote can be replaced by paper and pen.\n\nHowever, I would argue the source of resource advantage comes from something else than immunity of subsitution: after all, there are tens of search-engines and hundreds of social networks but still the giants overcome them.\n\n'Why' is the question we're going to examine next.\n\nHere's what I think is important:\n\nKnowledge means holding the \"smartest workers\" - this is obviously a highly important resource. As Steve Jobs said, they're not hiring smart people to tell them what to do, but so that the smart workers could tell Apple what to do.\n\nStorage/server capacity\u00a0is crucial for web firms. The more users they have, the more important this resource is in order to provide a reliable user experience.\n\nUsers are crucial given that the platform condition of critical mass is achieved. Critical mass is closely associated with network effects, meaning that the more there are users, the more valuable the platform is.\n\nContent is important as well -- content is a complement to content platforms, whereas users are complements of social platforms (for more on this typology, see my dissertation).\n\nComplementors are antecedents to getting users or content - they are third parties that provide extensions to the core platform, and therefore add its usefulness to the users.\n\nAlgorithms are proprietary solutions platforms use to solve matching problems.\n\nCompany culture is a resource which can be turned into an efficient deployment machine.\n\nA great\u00a0company culture may be hard to imitate because its creation requires tacit knowledge.\n\nFinancing is an antecedent to acquiring other resources, such as the best team and storage capacity (although it's not self-evident that money leads to functional a team, as examples in the web industry demonstrate).\n\nFinally, location is important because can provide an access to a network of partner companies, high-quality employees and investors (think Silicon valley)\u00a0that, again, are linked to the successful use of other resources.\n\nA location is not a rare asset because\u00a0it's always possible to find an office space in a given city; similarly, you can follow where your competitors go.\n\nWhat can be learned from this analysis?\n\nFirst, the \"value\" in the VRIN framework is self-evident and not very useful in finding out differences between resources, UNLESS the list of resources is really wide and not industry-specific. That would be case when exploring the ; here, the list creation was\n\nMy list highlights intangible resources as a source of competitive advantage for web platforms. Based on this analysis, company culture is a resource the most compatible with the VRIN criteria.\n\nAlthough it was argued that substitutability is less of a concern in online than offline, the risk of disruption touches equally well the dominant web platforms. Their large user base protects them against incremental innovations, but not against disruptive innovations. However, just as the concept of \"value\" has tautological nature, disruption is the same - disruptive innovation is disruptive because it has disrupted an industry - and this can only be stated in hindsight.\n\nOf course, the best executives in the world have seen disruption beforehand, e.g. Schibstedt and digital transformation of publishing, but most companies, even big ones like Nokia have failed\u00a0to do so.\n\nLet's take a look at the three big: Google, Facebook and eBay. Each one is a platform: Google combines searchers with websites (or, alternatively, advertisers with publisher websites (AdSense); or even more alternatively, advertisers with searchers (AdWords)), Facebook matches users to one another (one-sided platform) and advertisers with users (two-sided platform). eBay as an exchange platform\u00a0matches buyers and sellers.\n\nIt would be useful to assess how well each of them score in the above resources and how the resources are understood in these companies.\n\nI\u2019m into digital marketing, startups, platforms. Download my dissertation on startup dilemmas: http://goo.gl/QRc11f"},
{"url": "http://www.xaprb.com/blog/2015/05/22/percona-mongodb-mysql-history-repeat/", "link_title": "History Repeats: MySQL, MongoDB, Percona, and Open Source", "sentiment": 0.14639724310776941, "text": "History is repeating again. MongoDB is breaking out of the niche into the mainstream, performance and instrumentation are terrible in specific cases, MongoDB isn\u2019t able to fix all the problems alone, and an ecosystem is growing.\n\nThis should really be a series of blog posts, because there\u2019s a book\u2019s worth of things happening, but I\u2019ll summarize instead.\n\nAt the same time that history is repeating in the MongoDB world, a tremendous amount of stuff is happening quietly in other major communities too. Especially MySQL, but also in PostgreSQL, ElasticSearch, Cassandra and other opensource databases. I\u2019m probably only qualified to write about the MySQL side of things; I\u2019m pretty sure most people don\u2019t know a lot of the interesting things that are going on that will have long-lasting effects. Maybe I\u2019ll write about that someday.\n\nIn the meanwhile, I think we\u2019re all in for an exciting ride as MongoDB proves me right."},
{"url": "http://www.makeuseof.com/tag/people-contribute-open-source-projects/", "link_title": "Open source development is the future of software", "sentiment": 0.12924983966650636, "text": "Open source development is the future of software. It\u2019s great for users like you and me because open source software is usually free (not always) and often safer to use because malicious code is less likely to be implemented.\n\nBut what compels developers to contribute code for free? After all, writing code requires time, effort, and expertise. And while it\u2019s true that open source developers can make money, it\u2019s certainly easier through proprietary channels.\n\nIn fact, many proprietary businesses are jumping on the train. For example, Microsoft recently decided to open source their .NET Framework. Quite a surprise, if you ask me! So the question remains: what benefits are there to contributing open source? As it turns out, the motives rarely involve money.\n\nThe beauty of an open source project is that anyone can contribute code. Of course measures are put in place to prevent abuse (submitted code needs to be approved by a project leader) but if the code works, it doesn\u2019t matter who wrote it.\n\nAs such, many intermediate programmers will seek out open source projects that they find interesting and look for areas where they can make a difference.\n\nFor example, many projects use a bug tracker to keep track of issues that need resolving. These issues range from trivial to complex, so while the experts work on fixing a deeply critical bug, novices can tackle the trivial stuff.\n\nThe benefits are three-fold: 1) it\u2019s a more efficient use of time since multiple bugs can be fixed in parallel, 2) the experts stay engaged because they don\u2019t have to waste effort dealing with trivial-but-time-consuming fixes, and 3) novices gain valuable experience at no risk to anybody.\n\nIn fact, if you\u2019re a newbie programmer with at least one year of solid programming education (even if it\u2019s self-taught learning), contributing to an open source project can be one way to learn programming faster.\n\nAt some point in an open source developer\u2019s career, the \u201cgain experience\u201d phase turns into a \u201cportfolio of skills\u201d phase. If you\u2019re ever in need of a job, this portfolio won\u2019t replace a traditional resume or CV, but it can be a great supplement.\n\nEvery line of code that you contribute to an open source project is publicly accessible. The more you contribute, the more you shape the project. If that project ends up being a success, it reflects well on you. If it flops, it still shows your work ethic and coding expertise.\n\nArtists always point to their portfolio when applying for a position. Photographers show their portfolio when looking for new clients. The programming field is starting to head in this direction, at least in some ways.\n\nIf you\u2019re interviewing for a multinational corporation with several legacy systems still running on COBOL or Fortran, your portfolio of modern development probably won\u2019t matter much. But if you\u2019ve developed free Django tools and you\u2019re interviewing for a backend development position, you bet it\u2019s going to help.\n\nSo, if you want a career in programming, this is one big reason why you should contribute to open source projects.\n\nLet\u2019s say you\u2019re an avid programmer who uses a lot of open source tools as part of your workflow. You love the tools, you believe in the tools, and you can\u2019t imagine switching to anything else. But one day, you run into a critical bug that halts your productivity.\n\nIn the case of proprietary software, you\u2019d be stuck. Sure, you could submit a ticket to the developing company and hope they implement a fast patch, but there\u2019s no guarantee. In fact, it could take months (or years!) before they get around to it. You\u2019d be at their mercy.\n\nBut in an open source project, you could browse through the code, find the error, fix it, and recompile yourself. Or you could submit the fix to the project leader for review, and if it looks good he\u2019ll push out a patch.\n\nThe process might still take a few days or weeks, but it\u2019s much better than in the proprietary example.\n\nOne of the biggest reasons why people contribute open source code is because they believe in open source philosophy. Sounds pretty obvious, right? But you\u2019d be surprised how many developers truly believe in the open software ideology.\n\nWith regard to this, a lot of open source enthusiasts tend to be evangelistic. If everyone adopted \u201copen source thinking\u201d, the world would be a better place \u2014 and to a certain degree, I can hop aboard. The fact that anyone can fork a project is something I really appreciate because it promotes competition and innovation.\n\nIn other words, these people contribute open source code because they simply enjoy it. Open source is their identity and they\u2019re just living out what they believe. And you know what? There\u2019s nothing wrong with that.\n\nTangentially, by contributing to open source projects, these people end up collaborating with other like-minded people. The religious have churches, hobbyists have clubs, and open source developers have open source projects. Community fellowship alone is a great reason to get involved.\n\nWhile we\u2019re on the subject of community, let\u2019s not forget that there are more ways to contribute to an open source project than through code. Code is important, but other forms of support are certainly welcome.\n\nLet\u2019s say you absolutely love a particular program that you use regularly, whether it be for personal or commercial reasons. It has changed your life and you want to help the project in some way, but you have no coding experience. What can you do?\n\nDonations are always an option. For a one-man team, a $10 donation could buy that developer a few coffees or a six-pack of beer. It could fund a domain name renewal for one year. It could extend web hosting for a few months. No matter how small, a donation can help a lot.\n\nDocumentation is another big area where developers usually need help. This includes things like technical writing (e.g. manuals) or community knowledgebases (e.g. wikis). For example, community-produced tutorials can really help the developer by allowing them to focus all their attention on coding itself.\n\nOther methods of support include software localization, website translations, or simply spreading awareness of the product through word of mouth and social media\n\nRegardless, these kinds of \u201cloyalty contributions\u201d stem from die-hard appreciation of the product, which is something open source projects seem to cultivate well.\n\nHave you ever contributed to an open source project? If so, in what capacity? If not, what would convince you to do so? Share your thoughts with us in the comments below!\n\nImage Credits:\u00a0Hands On Laptop Via Shutterstock, Programming Skills Via Shutterstock, Lots of Coding Projects Via Shutterstock, Clean Workspace Via Shutterstock, Coding Buddies Via Shutterstock, Project Analysis Via Shutterstock"},
{"url": "http://www.independent.co.uk/news/uk/home-news/cashstrapped-councils-forced-to-switch-off-cctv-cameras-across-uks-towns-and-cities-10271267.html", "link_title": "Cash-strapped councils switch off CCTV cameras across UK", "sentiment": 0.08293182628856785, "text": "Tony Porter, the Surveillance Camera Commissioner, told The Independent that a rising number of local authorities in England and Wales were shutting down CCTV networks to cope with the Government\u2019s austerity cuts, raising the prospect of a \u201cpostcode lottery\u201d for crime detection.\n\nThe former counter-terrorism officer, who is responsible for the regulation of around 100,000 publicly-operated CCTV cameras, called for the creation of inspection teams to ensure that councils\u2019 networks are being managed effectively while maintaining people\u2019s privacy.\n\nLast night the Police Federation said the deactivation of CCTV cameras would introduce \u201cvulnerabilities\u201d to counter-terrorism operations and \u201cdeny justice\u201d to the victims of sexual offences and street violence. But civil liberties groups said there was little evidence of the cameras\u2019 effectiveness and that councils were right to keep them under constant review.\n\n\n\n Tony Porter, the Surveillance Camera Commissioner \n\n\u201cThere are an increasing number of examples where councils and employees are citing a lack of money as being the rationale to reduce the service or completely change its composition \u2013 and that does concern me,\u201d Mr Porter said. \u201cBecause CCTV isn\u2019t a statutory function, it is something a lot of councils are looking at.\u201d\n\nHe continued: \u201cMost people recognise the utility of CCTV for supporting law enforcement. To degrade the capacity may have an impact on police \u2013 and given that both police and local authorities aren\u2019t protected in terms of their funding, it is potentially going to have an impact on how the police gather evidence. It may well be that they find it increasingly difficult to acquire the imagery that will help them investigate crimes.\u201d\n\nMr Porter, who was appointed to the independent role just over a year ago, warned in a speech earlier this week that councils in Blackpool and Derby had stopped \u201cmonitoring their systems 24/7\u201d to save money. A wider \u201cdeterioration of standards and training\u201d was likely as local authorities hired inexperienced CCTV operators, he told a conference in Kenilworth.\n\nPaul Ford, secretary of the Police Federation\u2019s national detectives\u2019 forum, said CCTV was crucial in ensuring public safety and that the \u201carbitrary\u201d deactivation of cameras by councils carried \u201cmassive consequences\u201d for policing, as it would create blind spots around towns and cities.\n\n\u201cIt\u2019s a vital tool for identifying offenders and bringing them to justice. If you switch the cameras off, you deny justice to victims and have offenders likely to go out and commit further crimes and cause more damage and misery to the public,\u201d he said.\n\n\u201cWe shouldn\u2019t see it in isolation. You also have to link it to the turning off of street lighting, which local authorities are doing to save money as well, the closure of police stations and the reduction of 17,000 police officers in England and Wales. It\u2019s quite a toxic mix.\u201d\n\n\n\n There have been calls for a creation of camera inspection teams \n\nHe added that local authority CCTV was particularly important at a time of police cuts, as it allowed officers to \u201cvirtually patrol\u201d large areas from their control rooms. \u201cYou\u2019ve got lots of areas that will suddenly lose that safety net,\u201d he said.\n\nHowever, Mr Porter stressed that some councils were switching off CCTV cameras in a responsible way, by studying crime statistics and identifying areas where they were not needed. One West Midlands council had deactivated a third of its cameras in this way, saving \u00a3250,000 in the process. \u201cI think that\u2019s to be celebrated, because it reduces civic surveillance but leaves in position a camera network that the public can have trust in,\u201d he added.\n\nUnder legislation passed in 2012, councils and police forces are legally obliged to pay \u201cdue regard\u201d to the Surveillance Camera Code of Practice \u2013 but breaching it does not mean they automatically face sanctions. Mr Porter is due to deliver a report on the state of the country\u2019s public CCTV network to Home Secretary Theresa May in the autumn.\n\nHe told The Independent that his current role, which only permits him to \u201cencourage, review and advise\u201d, should be strengthened to allow him to send inspection teams into local councils. \u201cI do think there\u2019s a step-up point where public authorities should be held to greater account. If that is some form of inspection and enforcement notice, I think that can be achieved with a fairly light-touch set up,\u201d he said.\n\nAccording to some estimates there are up to six million CCTV cameras across the UK, one of the highest totals in the world. Emma Carr, director of civil liberties group Big Brother Watch, said Britain\u2019s crime rate was \u201cnot significantly lower\u201d than comparable countries which did not have such high levels of surveillance.\n\n\u201cCouncils should therefore be regularly reviewing whether their CCTV systems, which are often outdated and ineffective, are necessary,\u201d she added. \u201cEvidence repeatedly shows that rather than CCTV, measures like better street lighting and effective policing, are what keep the public safe.\u201d\n\nA spokesman for the Local Government Association, which represents councils in England and Wales, said: \u201cCouncils have never had to monitor CCTV 24 hours a day to be effective, with most systems automatically recording footage. Residents value such surveillance and where it is cost effective and makes an impact, councils will continue to invest in it. Whilst councils pay for most CCTV cameras, the main users of the recorded footage are the police and Crown Prosecution Service during criminal investigations.\u201d\n\nA Government spokesman said crime had fallen by more than a quarter since 2010, making citizens and communities \u201csafer than at any point\u201d since the Crime Survey for England and Wales began in 1981.\n\n\u201cPublic safety is paramount and the majority of local authorities have continued to balance their budgets and increased or maintained public satisfaction with services,\u201d he added. \u201cDecisions on CCTV provision should be a local decision by elected local councillors, reflecting local circumstances and the views of local residents \u2013 especially in relation to any concerns about crime.\u201d\n\n100,000 \u2013 The number of publicly-operated CCTV cameras in England and Wales\n\n300 \u2013 The average number of times someone in a British city will be filmed by a CCTV camera during the course of a single day, according to Mr Porter\n\n\u00a3250,000 \u2013 The amount of money saved by a council which cut a third of its cameras\n\n\u00a32.6 billion \u2013 The total savings councils must find in this financial year alone\n\n84% \u2013 The proportion of Britons who support the use of CCTV in a public space"},
{"url": "http://nikita-volkov.github.io/if-haskell-were-strict/", "link_title": "If Haskell were strict, what would the laziness be like?", "sentiment": 0.13380192502532925, "text": "Recently a question by Chris Done on Reddit has spawned yet another debate on the subject of whether Haskell\u2019s laziness is actually a good thing.\n\nWith this post I\u2019m not going to state my take on the matter, instead I\u2019ll speculate on what Haskell could be like were it a strict language and how it would approach the standard problems.\n\nBefore we go on, please note that all the following code will go with an implication that Haskell is strict.\n\nLet\u2019s take a look at the function, which essentially implements exactly the same thing as the \u201cif-then-else\u201d block:\n\nOf course the first question coming to mind is about dealing with the evaluation of alternative branches, because we don\u2019t want to waste resources on evaluation of a failed branch.\n\nBut look at this:\n\nIn other words, all we need to do is just to defer the evaluation of each branch using a function on a unit.\n\nAlready the type seems like a concept of its own, so we can as well give it a name:\n\nNext comes a thought \u201cIt suspiciously looks like something that could be a Monad\u201d. Yes it does and yes it is! I would have \u201cnewtyped\u201d the thing and declared the instances, but there\u2019s actually no need, since there already are instances of , and for a type , of which is just a special case.\n\nSo, with the intent of our condition function becomes clearer:\n\nFeels like we\u2019re coming closer to the laziness. But we\u2019re yet lacking the preservation of already computed results. I mean, we\u2019re in a pure language after all, so a specific function of type is guaranteed to always produce the same result, so wouldn\u2019t it be enough to compute it once and then just return a stored result? And that\u2019s exactly the thing that Haskell\u2019s Thunk is meant to solve.\n\nThunk is a basic mechanism that drives Haskell\u2019s laziness. The internet is filled with excellent tutorials about it, if you don\u2019t know about it yet. Following is how we could implement it explicitly in a library if Haskell were strict:\n\nThis datatype evidently forms a , and , but I\u2019ll let your imagination figure the instances out.\n\nNow, having thunks at hand we can easily implement any lazy data-structure. For instance, here is the lazy stream, which we call \u201clist\u201d:\n\nLooking at it we can easily extract a general piece, turning it into a more general List Monad Transformer:\n\nWhich after refactoring becomes just\n\nAnd, since forms a monad, we get a lazy pure stream for free:\n\nAs well as the strict linked list:\n\nThere\u2019s also an extra thing to consider: a compiler could implement automatic function memoization, or make a special case for functions of type , turning them into implicit thunks, such as the ones Haskell has already. Then there\u2019d be no need for the explicit type facade. Whether that would be a good thing though is a subject for another debate.\n\nLooking at the above I see a code which has clear semantics about when and whether things get evaluated - it\u2019s all there in the type signatures. I also see the language\u2019s magical constructs implemented as libraries. Say what you will, but I would love to see that in an actual language."},
{"url": "http://inessential.com/2015/05/22/how_not_to_crash_4_threading", "link_title": "How Not to Crash in Cocoa: Threading", "sentiment": 0.15673722771937054, "text": "Here\u2019s a simple rule: do everything on the main thread. Machines and devices are so fast these days that you can do more on the main thread than you think you can.\n\nIt\u2019s a kind of paradise when you don\u2019t have to think about concurrency because there isn\u2019t any.\n\nI\u2019m a performance junkie. Or, more to the point, I\u2019m a user experience junkie, and the only thing worse than something being slow is being slow and noticeably blocking the main thread. Don\u2019t do that.\n\nI\u2019ll get to that. But let\u2019s start with the main thread.\n\nAll the code I write expects to run on the main thread and on the main thread only, with exceptions. (We\u2019ll get to the exceptions in a minute.)\n\nThis solves a bunch of problems. For instance, I wrote in an earlier post about unregistering for notifications in dealloc. A few people pointed out that you can\u2019t guarantee on which thread dealloc will be called \u2014 but you can, actually, for any given object that runs on the main thread and is referenced only on the main thread.\n\nIt also means that any KVO changes are posted on the main thread, and any observing objects are also running on the main thread and expecting notifications on the main thread.\n\nThe benefits of not having to deal with concurrency are tremendous. I strongly suggest writing your entire app this way, and then testing to see if there\u2019s anything that blocks the main thread. If not, then it\u2019s golden, and you should ship. (Test with suitably large data, of course.)\n\nIf \u2014 and only if \u2014 you find that the main thread is noticeably blocked should you look for ways to un-block it.\n\nThe first candidates are transformations that can be completely isolated from the rest of your app. I\u2019ll use the example of processing JSON.\n\nWhen I get JSON from a server, I like to turn it into intermediate objects that later get merged into model objects. Reasons:\n\nSo I use an NSOperationQueue or GCD queue (usually the latter, these days) to turn NSData returned by a server into intermediate objects.\n\nThese intermediate objects will be accessed by one thread at a time. They\u2019re created on a background thread and then passed to the main thread, where they\u2019re used to update the model and then discarded.\n\nBecause they are referenced on different threads during their lifetimes, I make sure that these objects never know about anything except themselves and what\u2019s passed into their init methods. Once created on the queue, they\u2019re immutable. They don\u2019t observe anything and nobody should observe them (they don\u2019t change, after all).\n\nSometimes a number of objects work together. Instead of JSON, think of an RSS parser. In this example, there are three main objects involved: a SAX parser wrapper, its delegate, and the intermediate objects the delegate creates. (Conceptually exactly like the objects from the example above.)\n\nThe SAX parser wrapper and its delegate live for the length of the operation. They don\u2019t need to be thread-safe, even though the code is run on a separate thread \u2014 because they are accessed only on that thread. While they\u2019re working, they know nothing about the outside world, and the outside world knows nothing about them.\n\nSo these objects work together, but, importantly, they never use KVO or notifications in any way. Instead they use the delegate pattern (whether via blocks or methods isn\u2019t conceptually important).\n\nThe objects work together, but as loosely as possible while still keeping the group isolated to its task.\n\nIn the end, only the intermediate objects survive \u2014 they\u2019re passed to the main thread, where they\u2019re used to update the model. And then they\u2019re discarded.\n\nI\u2019ve used the phrase \u201cupdate the model\u201d several times and mentioned doing it on the main thread. A few years ago I would never have dreamed of that \u2014 but computers and devices have gotten so much faster that it\u2019s worth going main-thread-only at first, and considering alternatives only after dealing with everything else that can and should be safely moved to a queue.\n\nYou really don\u2019t want to update model objects on background threads. It\u2019s a crash-making machine. But testing and profiling may tell you that you need to.\n\nTry to break down the problem. If updating the model is okay except for this one thing \u2014 something that involves turning NSData into a UIImage or NSImage, for instance \u2014 then move just that slow part to a background task. (Creating an image from data or a file is a perfectly good thing to move off the main thread. It\u2019s easily isolatable.)\n\nIt could be that the problem is the database: perhaps you find that it\u2019s otherwise fast to create objects and update properties in memory, even a whole bunch of them. In that case, you might do what I do, which is de-couple the database calls from the main thread. (It\u2019s not that hard: the database code needs to run on a serial background queue, and it should do everything in the exact some order that things happen in the main thread.)\n\nWhich is to say: there are options.\n\nBut if you still find that you have to update the model on a background thread, then you just have to do it. Remember that the rest of your app is on the main thread, so when posting notifications and so on, do so on the main thread.\n\nDo everything on the main thread. Don\u2019t even think about queues and background threads. Enjoy paradise!\n\nIf, after testing and profiling, you find you do have to move some things to a background queue, pick things that can be perfectly isolated, and make sure they\u2019re perfectly isolated. Use delegates; do not use KVO or notifications.\n\nIf, in the end, you still need to do some tricky things \u2014 like updating your model on a background queue \u2014 remember that the rest of your app is either running on the main thread or is little isolated things that you don\u2019t need to think about while writing this tricky code. Then: be careful, and don\u2019t be optimistic. (Optimists write crashes.)"},
{"url": "http://www.businessinsider.com/elon-musk-creates-a-grade-school-2015-5", "link_title": "Elon Musk didn't like his kids' school, so he made his own without grades", "sentiment": 0.062155248133509006, "text": "Elon Musk didn't like his kids' school, so he started his own, the inventor and entrepreneur said in an interview on Beijing Television.\n\nThe school is called Ad Astra \u2014 which means \"To the stars\" \u2014 and is small and relatively secretive. It doesn't have its own website or a social media presence.\n\nChristina Simon, who writes about private elementary schools in Los Angeles, has done some digging around Ad Astra.\n\nShe says she's been in contact with a mother whose child attends Musk's school. The mother told Simon that the relatively new Ad Astra School is \"very small and experimental,\" and caters to a small group of children whose parents are primarily SpaceX employees.\n\nMusk says in the interview that Ad Astra, which is a year old, currently has 14 kids and will increase to 20 in September. His grand vision for the school involves removing grade levels, so there's no distinction between students in 1st grade and 3rd. Musk is \"making all the children go through the same grade at the same time, like an assembly line,\" he says in the interview.\n\n\"Some people love English or languages. Some people love math. Some people love music. Different abilities, different times,\" he says. \"It makes more sense to cater the education to match their aptitudes and abilities.\"\n\nMusk pulled his kids out of their school and even hired one of their teachers away to start Ad Astra. \"I didn't see the regular schools doing the things I thought should be done,\" he says.\n\n\"It's important to teach problem solving, or teach to the problem and not the tools,\" Musk says. \"Let's say you're trying to teach people about how engines work. A more traditional approach would be saying, 'we're going to teach all about screwdrivers and wrenches.' This is a very difficult way to do it.\"\n\nInstead, Musk says it makes more sense to give students an engine and then work to disassemble it.\n\n\"How are we going to take it apart? You need a screwdriver. That's what the screwdriver is for,\" Musk explains. \"And then a very important thing happens: The relevance of the tools becomes apparent.\"\n\nSo far, Ad Astra \"seems to be going pretty well,\" according to Musk. \"The kids really love going to school.\"\n\n\"I hated going to school when I was a kid,\" Musk told his interviewer. \"It was torture.\"\n\nWhen Musk was a child living in Pretoria, South Africa, he was viciously bullied as a student. His classmates pushed him down a concrete stairwell. In one instance, he was beaten so badly that he needed to go to the hospital.\n\n\u201cThey got my best [expletive] friend to lure me out of hiding so they could beat me up. And that [expletive] hurt. For some reason they decided that I was it, and they were going to go after me nonstop. That\u2019s what made growing up difficult. For a number of years there was no respite. You get chased around by gangs at school who tried to beat the [expletive] out of me, and then I\u2019d come home, and it would just be awful there as well.\u201d\n\nHis difficult experiences both at home \u2014 where he had a strained relationship with his father \u2014 and at school would eventually lead Musk to leave South Africa for the United States.\n\nYou can watch Musk's full video interview below."},
{"url": "http://graphics.wsj.com/hillary-clinton-emails/", "link_title": "WSJ Crowdsources Investigation of Hillary Clinton Emails", "sentiment": 0.43125, "text": "As Wall Street Journal reporters start sifting through the first 900 pages of Hillary Clinton's 55,000 pages of State Department emails \u2014 which are being released on a rolling fashion through the rest of the year \u2014 we welcome your help in tagging pages you find interesting.\n\nAfter you read a document, you can tag it using descriptions such as \"Sensitive\" or \"Surprising,\" and you can also input your own keywords as you see fit. (Then go to another page and do it again!) When you submit your tags, you are helping us build a database of what readers have discovered amid these thousands of pages."},
{"url": "http://www.fiercevaccines.com/story/genoceas-genital-herpes-vaccine-succeeds-ph-ii/2015-05-21", "link_title": "Genocea's genital herpes vaccine succeeds in Phase II Clinical Trials", "sentiment": 0.2477225672877847, "text": "There is no cure for genital herpes, but Genocea ($GNCA) is getting closer to marketing an immunotherapy that could control herpes symptoms. The Massachusetts-based company announced Wednesday that it met its goals in a Phase II study of its investigational GEN-003 vaccine.\n\nThe candidate consists of a protein and adjuvant, and works by targeting a T cell response against the HSV-2 virus. The study's aim was to find the best combination of the protein and adjuvant to reduce viral activity, measured in the viral shedding rate and the genital lesion rate.\n\nThe study put 310 herpes-infected patients into 7 groups, administering 6 different dosages as well as a placebo. Every dose of GEN-003 showed a statistically significant improvement in the viral shedding rate vs. baseline, and compared to placebo, every dose but the weakest of GEN-003 showed a statistically significant improvement in the viral shedding rate, the rate at which the virus, which lives in nerve cells, makes its way to the skin and physically sheds.\n\n\"We replicated results from the first trial and confirmed the antiviral effect of GEN-003, putting to rest any doubt that it's a viable product,\" Dr. Seth Hetherington, Genocea's chief medical officer, told FierceVaccines. \"We found a dose that is better than the best dose from the first trial, which increases the value of the profile for this product.\"\n\nThe best performer was 60 \u00b5g per protein / 75 \u00b5g of Matrix-M2, which cut viral shedding by 55% after 28 days, compared with baseline. This is an improvement over results reported in a Phase I/IIa trial, which reported a 52% reduction in viral shedding at the same point.\n\n\"We made improvements in the manufacturing process and invested in preparing for Phase III,\" Hetherington said. Genocea is producing material that could be scaled up for commercialization, and will conduct a small bridging study to ensure that the current GEN-003 profile can be maintained on a large scale, he said.\n\nThe CDC estimates that in the U.S., one out of every 6 people aged 14 to 49 years has genital herpes. GEN-003 will not be a cure for herpes. It was developed as a therapy, CEO Chip Clark told FierceVaccines.\n\n\"What we know about the biology of the disease is that if you have viral shedding, you can have outbreaks and can transmit it to others. Showing we have this profound effect on viral shedding is crucial. We may show in later clinical trials that we can prevent transmission as well,\" Clark said.\n\n- here's the announcement\n\n - and FierceBiotech's take\n\nRelated Articles:\n\n Analyst: Genocea's genital herpes vaccine better positioned than Agenus' to enter market\n\n Biotechs angling for T-cell response in race for next-gen herpes vaccine"},
{"url": "https://www.eff.org/deeplinks/2015/05/clock-still-running-neither-nsa-reform-nor-reauthorization-advances-senate", "link_title": "Neither NSA Reform nor Reauthorization Advances in Senate", "sentiment": -0.14722222222222225, "text": "Tonight, the US Senate failed to move ahead with the USA Freedom Act, an NSA reform bill that would address phone record surveillance and FISA Court transparency and fairness. It also was unable to muster votes for a temporary reauthorization of Section 215 of the Patriot Act, the section of law used to justify the mass phone records surveillance program. That\u2019s good news: if the Senate stalemate continues, the mass surveillance of everyone\u2019s phone records will simply expire on June 1.\n\nSection 215 of the Patriot Act has been wrongly interpreted in secret by the government for years. We commend every Senator who voted against reauthorizing the unconstitutional surveillance of millions of law-abiding Americans.\n\nIn the wake of tonight\u2019s vote, Congress must stop stalling and address the surveillance and secrecy abuses of our government.\n\nThe battle isn't over. Senator Majority Leader Mitch McConnell is calling for another attempt to reauthorize Section 215 on Sunday May 31, only hours before the provision is set to expire.\n\nEFF urges Congress to again reject Section 215 reauthorization, and then turn to addressing other surveillance abuses by the US government, including mass surveillance of the Internet, the secretive and one-sided FISA Court, and the problems of secrecy and over-classification that have created the environment that allowed such spying overreach to flourish."},
{"url": "https://f-droid.org/repository/browse/?fdid=org.mozilla.firefox", "link_title": "F-Droid Dropping Firefox", "sentiment": 0.09116161616161614, "text": "Mobile version of the Firefox web browser. Uses the Gecko layout engine to render web pages, which implements current and anticipated web standards. Features include: bookmark sync, custom search engines, support for addons and the ability to enable or disable the search suggestions.\n\nAnti-features: Non-free Addons: The license of the addons/modules on addons.mozilla.org may be seen in the version notes of the addon and often the license is Custom or other non-free. However there is no such license info for most apps from the marketplace. Tracking: Stats are sent back regularly to the developers, but that can be disabled via settings.\n\nNote that this package is planned to be dropped from the F-Droid repo soon. Please consider using Fennec FDroid instead to be sure of continued update support.\n\nTracks You\n\nThis application tracks and reports your activity to somewhere. more...\n\nFor full details and additional technical information, see this application's page on the F-Droid wiki.\n\nAlthough APK downloads are available below to give you the choice, you should be aware that by installing that way you will not receive update notifications, and it's a less secure way to download. We recommend that you install the F-Droid client and use that.\n\nThis version uses native code and will only run on: armeabi-v7a\n\nThis version is built and signed by the original developer.\n\nYOUR ACCOUNTS\n\n Act as an account authenticator [ ]\n\nAllows an application to use the account authenticator capabilities of the AccountManager, including creating accounts and getting and setting their passwords.\n\n\n\n Manage the accounts list [ ]\n\nAllows an application to perform operations like adding, and removing accounts and deleting their password.\n\n\n\n Use the authentication credentials of an account [ ]\n\nAllows an application to request authentication tokens.\n\n\n\nEXTRA/CUSTOM\n\n Take pictures and videos [ ]\n\nAllows application to take pictures and videos with the camera. This allows the application at any time to collect images the camera is seeing.\n\n\n\nYOUR LOCATION\n\n Fine (GPS) location [ ]\n\nAccess fine location sources such as the Global Positioning System on the tablet, where available. Malicious applications can use this to determine where you are, and may consume additional battery power.\n\n\n\nEXTRA/CUSTOM\n\n Record audio [ ]\n\nAllows application to access the audio record path.\n\n\n\nNETWORK COMMUNICATION\n\n Change Wi-Fi state [ ]\n\nAllows an application to connect to and disconnect from Wi-Fi access points, and to make changes to configured Wi-Fi networks.\n\n\n\n Full Internet access [ ]\n\nAllows an application to create network sockets.\n\n\n\n Control Near Field Communication [ ]\n\nAllows an application to communicate with Near Field Communication (NFC) tags, cards, and readers.\n\n\n\nSTORAGE\n\n Modify/delete USB storage contents [ ]\n\nAllows an application to write to the USB storage.\n\n\n\nYOUR ACCOUNTS\n\n Discover known accounts [ ]\n\nAllows an application to get the list of accounts known by the tablet.\n\n\n\nEXTRA/CUSTOM\n\n Control vibrator [ ]\n\nAllows the application to control the vibrator.\n\n\n\n Prevent tablet from sleeping [ ]\n\nAllows an application to prevent the tablet from going to sleep.\n\n\n\nEXTRA/CUSTOM\n\n Automatically start at boot [ ]\n\nAllows an application to have itself started as soon as the system has finished booting. This can make it take longer to start the tablet and allow the application to slow down the overall tablet by always running.\n\n\n\nNETWORK COMMUNICATION\n\n View network state [ ]\n\nAllows an application to view the state of all networks.\n\n\n\n View Wi-Fi state [ ]\n\nAllows an application to view the information about the state of Wi-Fi.\n\n\n\nSTORAGE\n\n [ ]\n\n\n\nEXTRA/CUSTOM\n\n Read sync settings [ ]\n\nAllows an application to read the sync settings, such as whether sync is enabled for Contacts.\n\n\n\n Read sync statistics [ ]\n\nAllows an application to read the sync stats; e.g., the history of syncs that have occurred.\n\n\n\n Write sync settings [ ]\n\nAllows an application to modify the sync settings, such as whether sync is enabled for Contacts.\n\n\n\nSYSTEM TOOLS\n\n Modify global system settings [ ]\n\nAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.\n\n\n\nEXTRA/CUSTOM\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n\n\nThis version uses native code and will only run on: armeabi-v7a\n\nThis version is built and signed by the original developer.\n\nYOUR ACCOUNTS\n\n Act as an account authenticator [ ]\n\nAllows an application to use the account authenticator capabilities of the AccountManager, including creating accounts and getting and setting their passwords.\n\n\n\n Manage the accounts list [ ]\n\nAllows an application to perform operations like adding, and removing accounts and deleting their password.\n\n\n\n Use the authentication credentials of an account [ ]\n\nAllows an application to request authentication tokens.\n\n\n\nEXTRA/CUSTOM\n\n Take pictures and videos [ ]\n\nAllows application to take pictures and videos with the camera. This allows the application at any time to collect images the camera is seeing.\n\n\n\nYOUR LOCATION\n\n Fine (GPS) location [ ]\n\nAccess fine location sources such as the Global Positioning System on the tablet, where available. Malicious applications can use this to determine where you are, and may consume additional battery power.\n\n\n\nEXTRA/CUSTOM\n\n Record audio [ ]\n\nAllows application to access the audio record path.\n\n\n\nNETWORK COMMUNICATION\n\n Change Wi-Fi state [ ]\n\nAllows an application to connect to and disconnect from Wi-Fi access points, and to make changes to configured Wi-Fi networks.\n\n\n\n Full Internet access [ ]\n\nAllows an application to create network sockets.\n\n\n\n Control Near Field Communication [ ]\n\nAllows an application to communicate with Near Field Communication (NFC) tags, cards, and readers.\n\n\n\nSTORAGE\n\n Modify/delete USB storage contents [ ]\n\nAllows an application to write to the USB storage.\n\n\n\nYOUR ACCOUNTS\n\n Discover known accounts [ ]\n\nAllows an application to get the list of accounts known by the tablet.\n\n\n\nEXTRA/CUSTOM\n\n Control vibrator [ ]\n\nAllows the application to control the vibrator.\n\n\n\n Prevent tablet from sleeping [ ]\n\nAllows an application to prevent the tablet from going to sleep.\n\n\n\nEXTRA/CUSTOM\n\n Automatically start at boot [ ]\n\nAllows an application to have itself started as soon as the system has finished booting. This can make it take longer to start the tablet and allow the application to slow down the overall tablet by always running.\n\n\n\nNETWORK COMMUNICATION\n\n View network state [ ]\n\nAllows an application to view the state of all networks.\n\n\n\n View Wi-Fi state [ ]\n\nAllows an application to view the information about the state of Wi-Fi.\n\n\n\nSTORAGE\n\n [ ]\n\n\n\nEXTRA/CUSTOM\n\n Read sync settings [ ]\n\nAllows an application to read the sync settings, such as whether sync is enabled for Contacts.\n\n\n\n Read sync statistics [ ]\n\nAllows an application to read the sync stats; e.g., the history of syncs that have occurred.\n\n\n\n Write sync settings [ ]\n\nAllows an application to modify the sync settings, such as whether sync is enabled for Contacts.\n\n\n\nSYSTEM TOOLS\n\n Modify global system settings [ ]\n\nAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.\n\n\n\nEXTRA/CUSTOM\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n\n\nThis version uses native code and will only run on: armeabi-v7a\n\nThis version is built and signed by the original developer.\n\nYOUR ACCOUNTS\n\n Act as an account authenticator [ ]\n\nAllows an application to use the account authenticator capabilities of the AccountManager, including creating accounts and getting and setting their passwords.\n\n\n\n Manage the accounts list [ ]\n\nAllows an application to perform operations like adding, and removing accounts and deleting their password.\n\n\n\n Use the authentication credentials of an account [ ]\n\nAllows an application to request authentication tokens.\n\n\n\nEXTRA/CUSTOM\n\n Take pictures and videos [ ]\n\nAllows application to take pictures and videos with the camera. This allows the application at any time to collect images the camera is seeing.\n\n\n\nYOUR LOCATION\n\n Fine (GPS) location [ ]\n\nAccess fine location sources such as the Global Positioning System on the tablet, where available. Malicious applications can use this to determine where you are, and may consume additional battery power.\n\n\n\nEXTRA/CUSTOM\n\n Record audio [ ]\n\nAllows application to access the audio record path.\n\n\n\nNETWORK COMMUNICATION\n\n Change Wi-Fi state [ ]\n\nAllows an application to connect to and disconnect from Wi-Fi access points, and to make changes to configured Wi-Fi networks.\n\n\n\n Full Internet access [ ]\n\nAllows an application to create network sockets.\n\n\n\n Control Near Field Communication [ ]\n\nAllows an application to communicate with Near Field Communication (NFC) tags, cards, and readers.\n\n\n\nSTORAGE\n\n Modify/delete USB storage contents [ ]\n\nAllows an application to write to the USB storage.\n\n\n\nYOUR ACCOUNTS\n\n Discover known accounts [ ]\n\nAllows an application to get the list of accounts known by the tablet.\n\n\n\nEXTRA/CUSTOM\n\n Control vibrator [ ]\n\nAllows the application to control the vibrator.\n\n\n\n Prevent tablet from sleeping [ ]\n\nAllows an application to prevent the tablet from going to sleep.\n\n\n\nEXTRA/CUSTOM\n\n Automatically start at boot [ ]\n\nAllows an application to have itself started as soon as the system has finished booting. This can make it take longer to start the tablet and allow the application to slow down the overall tablet by always running.\n\n\n\nNETWORK COMMUNICATION\n\n View network state [ ]\n\nAllows an application to view the state of all networks.\n\n\n\n View Wi-Fi state [ ]\n\nAllows an application to view the information about the state of Wi-Fi.\n\n\n\nSTORAGE\n\n [ ]\n\n\n\nEXTRA/CUSTOM\n\n Read sync settings [ ]\n\nAllows an application to read the sync settings, such as whether sync is enabled for Contacts.\n\n\n\n Read sync statistics [ ]\n\nAllows an application to read the sync stats; e.g., the history of syncs that have occurred.\n\n\n\n Write sync settings [ ]\n\nAllows an application to modify the sync settings, such as whether sync is enabled for Contacts.\n\n\n\nSYSTEM TOOLS\n\n Modify global system settings [ ]\n\nAllows an application to modify the system's settings data. Malicious applications can corrupt your system's configuration.\n\n\n\nEXTRA/CUSTOM\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n [ ]\n\n\n\n"},
{"url": "https://medium.com/startup-blink/tech-story-you-should-know-within-a-week-19b10d3ef886", "link_title": "Tech Stories You Should Know a Week", "sentiment": 0.0, "text": "Tech Stories You Should Know A Week"},
{"url": "http://www.psychologicalscience.org/index.php/news/releases/self-promoters-tend-to-misjudge-how-annoying-they-are-to-others.html#.VV_dj_UtOHY.twitter", "link_title": "Self-Promoters Tend to Misjudge How Annoying They Are to Others", "sentiment": 0.11203503286836618, "text": "Bragging to coworkers about a recent promotion, or posting a photo of your brand new car on Facebook, may seem like harmless ways to share good news.\u00a0But new research\u00a0published in Psychological Science, a journal of the Association for Psychological Science, shows that self-promotion or a \u201chumblebrag\u201d often backfires.\n\nResearchers Irene Scopelliti,\u00a0George Loewenstein,\u00a0and Joachim Vosgerau wanted to find out why so many people frequently get the trade-off between self-promotion and modesty wrong. They found that self-promoters overestimate how much their self-promotion elicits positive emotions and underestimate how much it elicits negative emotions. As a consequence, when people try to increase the favorability of the opinion others have of them, they excessively self-promote, which has the opposite of the intended effect.\n\n\u201cMost people probably realize that they experience emotions other than pure joy when they are on the receiving end of someone else\u2019s self-promotion. Yet, when we engage in self-promotion ourselves, we tend to overestimate others\u2019 positive reactions and underestimate their negative ones,\u201d said Scopelliti, the study\u2019s lead author and a lecturer in marketing at City University London who conducted the research while a postdoctoral fellow at Carnegie Mellon.\n\n\u201cThese results are particularly important in the Internet age, when opportunities for self-promotion have proliferated via social networking. The effects may be exacerbated by the additional distance between people sharing information and their recipient, which can both reduce the empathy of the self-promoter and decrease the sharing of pleasure by the recipient,\u201d she said.\n\nThe researchers ran two online experiments to find evidence of the misperception. They asked some participants to describe a time they had bragged about themselves (self-promoters) and asked others to describe a time that they were on the receiving end of someone else\u2019s bragging (recipients).\n\nTaken together, the data from the two experiments indicated that self-promoters overestimate the extent to which people on the receiving end of their stories are likely to feel happy for them and proud of them. At the same time, they also seem to underestimate the extent to which recipients are likely to feel annoyed with them.\n\nA third experiment examined the consequences of the miscalibration, revealing that recipients of excessive self-promotion view self-promoters as less likeable and as braggarts.\n\n\u201cThis shows how often, when we are trying to make a good impression, it backfires,\u201d said Loewenstein, the Herbert A. Simon University Professor of Economics and Psychology in CMU\u2019s\u00a0Dietrich College of Humanities and Social Sciences. \u201cBragging is probably just the tip of the iceberg of the self-destructive things we do in the service of self-promotion, from unfortunate flourishes in public speeches to inept efforts to \u2018dress for success\u2019 to obviously insincere attempts to ingratiate ourselves to those in power.\u201d\n\nThe researchers believe knowing this could be valuable for both braggers and self-promotion recipients.\n\n\u201cIt may be beneficial for people who plan to engage in self-promotion to try to realize that others may actually be less happy than they think to hear about their latest achievement. Recipients of such self-promotion who find themselves annoyed might likewise try to bolster their tolerance in the knowledge that braggarts genuinely underestimate others\u2019 negative reactions to their bragging,\u201d said Vosgerau, professor of marketing at Bocconi University.\n\nAll data and materials have been made publicly available through Open Science Framework and can be accessed at https://osf.io/chm94/ and https://osf.io/evikb/, respectively. The complete Open Practices Disclosure for this article can be found at http://pss.sagepub.com/content/by/supplemental-data. This article has received badges for Open Data and Open Materials. More information about the Open Practices badges can be found at https://osf.io/tvyxz/wiki/view/ and\u00a0http://pss.sagepub.com/content/25/1/3.full."},
{"url": "http://daeken.com/2012-12-06_Responsible_Disclosure_Can_Be_Anything_But.html", "link_title": "Responsible Disclosure Can Be Anything But (2012)", "sentiment": 0.11285909903867641, "text": "The last few weeks were full of mixed emotions for me. Reliable reports of hotel break-ins seemingly utilizing the vulnerability I disclosed at BlackHat are coming out, and I'm both horrified and vindicated.\n\nAs I explained in my paper, the decision to release this information in the way I did was not an easy one. I've been full of doubt ever since I first made the call, and while I was inclined to think I did the right thing, I really couldn't tell for sure. But now, my doubt is gone.\n\nEver since the first story broke about these issues, I've taken significant flak from the security community and others for my lack of adherence to Responsible Disclosure. Hopefully this post will show that that's not always the responsible approach.\n\nWhen I set out to reverse-engineer the Onity HT system, my goal wasn't to examine or undermine its security at all. Rather, the goal was to understand how it works and create a replacement for the Onity front desk system (primarily the encoder that makes the keycards). In the course of developing that, I found the vulnerabilities but didn't think much about them for a while.\n\nIn 2010, we (the startup I was running with friends at the time, UPM) decided to license the opening technology to a locksmithing company for law enforcement purposes. Quite honestly, I don't know what I can or can't say beyond that, due to contracts; at some point I'll be able to talk more openly about it.\n\nIn 2012, we decided that it was time to make these vulnerabilities public, but we were unsure how.\n\nThese were the characteristics of the vulnerability that were considered when weighing the release:\n\nThe vulnerability itself, in a vacuum, is quite severe. Any Onity HT lock can be opened in less than a second with a piece of hardware costing effectively nothing. The hardware can be built by someone with no special skills for only a few dollars and utilized with no real 'training'.\n\nCombine this with the fact that there are 4-10 million of these locks (Onity's own estimates -- my guess is that this is locks in use versus locks shipped) in use protecting people for 20 years now, and you have the perfect storm.\n\nSince the locks are not flashable, the only real way to fix them is to either prevent access to the jack (a non-solution) or replace the circuit boards in the locks. Either way, you're talking tens of millions of dollars to fix all the locks. Neither the individual hotels -- primarily independently owned with very low margins -- nor Onity can afford this.\n\nThe major vulnerability (the memory read) is trivial to discover -- it's used as part of the normal operations of the lock. That combined with the time these locks have been on the market gave me a strong feeling that I was not the first to discover it. In fact, it's blindingly obvious to anyone looking at the way the lock's communication functions that this vulnerability would be present; how did Onity not know of it?\n\nI've since heard from various people that someone inside of Onity may have built an opening device themselves at one point, but I can't confirm that, let alone that it operated on the same principles as mine.\n\nThe standard 'Responsible Disclosure' approach would be to notify Onity and give them X months to deal with the issue before taking it public. While this is tried and true, there are several issues with this approach.\n\nOnity, after 20 years and 4-10M locks, has a vested interest in this information not getting out, as it makes them look bad and costs them a significant amount of money. As such, it's likely that without public pressure -- which we've seen in the form of unrelenting press coverage -- they would have attempted to cover this up. Cases of security researchers being sued by vendors are well known in the industry and not uncommon.\n\nDue to the difficulty in mitigating the issue, it's entirely possible that only a tiny fraction of hotels would've been fixed by the publication deadline, and without such a deadline applying pressure, there's no reason for Onity to continue to make strides to fix the issue.\n\nThis was a genuine option for a long while. While it's likely that it's been discovered and exploited long before I even looked at these locks, it was not a well-known attack.\n\nHowever, I decided that the long-term benefits of this being fixed outweighed the problems certain to be faced in the short term while the flaws were being mitigated.\n\nThe last approach is to simply release all information to the public in the most visible way possible. This dramatically increases the odds that someone will use the attack for malicious purposes, which is why it was always a big concern for me.\n\nHowever, by making it as visible as possible, it puts significant pressure on Onity and the hospitality industry as a whole to fix the issues and get hotel guests back to a safe position. At the end of the day, this seemed like the approach most likely to get a swift response to the problem.\n\nThe key problem with Responsible Disclosure -- aside from the implication that any other means of disclosing vulnerabilities is irresponsible -- is that it puts the focus on security researchers' responsibility to vendors rather than customers. In most cases, the vendor of a vulnerable product is not the victim of attacks against that product, which makes this focus misguided in many cases.\n\nIn the case of the Onity vulnerabilities, the vendor (Onity) is not the victim, and neither are the hotel owners (Onity's customers). In this case, the hotel's customers are the true victims. While hotels or Onity could be liable in certain cases as a result of these vulnerabilities, hotel guests are the ones with their property and lives on the line.\n\nPutting the focus on responsibility to the vendor, as nice a situation as it may be for vendors in many cases, can leave the true victims in a dangerous spot.\n\nThe focus for security researchers should always be on the customers, not the vendor; this is what Responsible Disclosure gets wrong. While it's true that in the majority of cases -- especially when dealing with software -- the quickest way to make customers safe is to work with the vendor to fix the issue, Responsible Disclosure advocates tend to ignore the edge cases in favor of a dogmatic adherence to this method.\n\nThis is a hard one, as Responsible Disclosure is usually the best approach and knowing when it falls short is tough. In general, I'd say that if the majority of the conditions below are met, Responsible Disclosure might not be the best way to minimize customer risk:\n\nIn cases where all or most of these are met, Responsible Disclosure may not be a responsible approach and may lead to customers remaining insecure and unsafe for years to come.\n\nThe response to the Onity HT vulnerabilities has been bigger than I ever thought it would be. The press has been unrelenting for months -- it was even featured on the Today Show this morning (December 6th, 2012), more than 4 months since the original release.\n\nOnity has been vague on how and when they're fixing the issue. In August they published a plan for mitigating the flaws, which I responded to; Forbes picked up on this and Onity subsequently removed all trace of this plan from their site. They are now stating that they are paying for/heavily subsidizing their fix, only after months of battering from the press.\n\nThis would never have happened without public pressure; hotel guests would have remained vulnerable for a long time, rather than a few months. When all is said and done, disclosing this fully and publicly will have lead to increased safety for hotel guests the world over.\n\nCases like this are never easy. In fact, it downright sucks to have to go through this with a vulnerability that could cause severe harm to customers. But at the end of the day, you have to decide one thing: is the customer more important than the vendor? In most cases, I'd say that they are.\n\nThe world is not black and white, and a dogmatic adherence to Responsible Disclosure makes us less secure and less safe as a whole. I urge all security researchers to think long and hard about how to disclose vulnerabilities, for the sake of everyone impacted, not just the vendor."},
{"url": "http://www.fastcoexist.com/3046505/more-than-250-tech-companies-release-letter-to-congress-opposing-trans-pacific-partnership", "link_title": "250 Tech Companies Release Letter to Congress Opposing Trans-Pacific Partnership", "sentiment": 0.021764520202020205, "text": "President Obama's massive trade agreement with 11 other countries in the Asia-Pacific region, the Trans-Pacific Partnership, has hit a number of roadblocks. It has stumbled in the Senate and received opposition from medical professionals, environmental groups, and labor unions. Proponents say the TPP will help grow the American economy and open up new markets for American exports. Detractors believe it will result in more low-wage American jobs being moved overseas. But it's hard to say exactly what its effects will be because the full text of the agreement has been kept secret and Congress is not allowed to disclose the details to the public.\n\nNow one more group is throwing its hat into the ring: technology companies. In an open letter to Congress, over 250 technology companies large and small expressed their opposition to the trade deal. The companies include project management website Basecamp, web hosts Dreamhost and Namecheap, cell phone company Credo Mobile, hardware company for the maker movement Adafruit, and others such as Mediafire, Imgur, and Boing Boing.\n\nWe write to you as a community representing thousands of our nation\u2019s innovators, entrepreneurs, job-creators, and users to express our concern over trade agreements such as the Trans-Pacific Partnership (TPP). Despite containing many provisions that go far beyond the scope of traditional trade policy, the public is kept in the dark as these deals continue to be negotiated behind closed doors with heavy influence from only a limited subset of stakeholders.\n\nThe letter goes on to state that the TPP would create limits to fair use by making copyright law more strict, make online enforcement of copyright infringement expensive and onerous for startups and small companies, criminalize journalism and whistleblowing, and harm consumer and user rights.\n\n\"The TPP makes a mockery of democratic legislative ideals,\" says David Heinemeier Hansson, creator of the Ruby on Rails web development framework and a partner at Basecamp. \"It's shrouded in secrecy exactly because it would wither in sunlight. It's a terrible piece of overreach to endow a few special interests with enormous and unsavory power. The whole thing needs to be scrapped and started over. International trade is too important to have it hitched to this collection of wishful thinking by a select few.\"\n\n\"Democracies make their laws in public, not in smoke-filled rooms,\" says Cory Doctorow, co-editor of Boing Boing. \"The fact that TPP backers went to extreme, unprecedent measure to stop anyone from finding out what was going\u2014even going so far as to threaten Congress with jail if they spoke about it\u2014tells you that this is something being done to Americans, not for Americans.\"\n\nYou can read the full text of the letter and list of companies signing below:"},
{"url": "http://blog.floatalk.com/can-we-update-the-tool-on-how-to-comment-sharing-comments-and-pin/", "link_title": "Can We \u201cUPDATE\u201d the Tool on How to Comment, Sharing Comments and PIN?", "sentiment": 0.14881422924901183, "text": "Comments and reviews are where we engage in meaningful conversations with others. However, most of the time, we have to click on the content first, wait to go to another page, scroll to the bottom of that page and sometimes log-in before you are able to start typing your thoughts.\n\nWho has time for that in today\u2019s age of information overload?\n\nI believe internet allows us to create any community we like globally and the future of marketing is all about peer-to-peer marketing. Comments, reviews, and the ability to share is going to be important for content creators and brands.\n\nMedium does a pretty good job allowing readers to comment on the articles they read and where they would like to comment in the article. The only downside is, it\u2019s only within Medium \u2026 can we expand that idea into all other websites?\n\nI came across Floatalk, www.floatalk.com I think this is a new tool that could seriously revolutionize instant commenting, sharing comments and PIN. It can be easily installed into any Web browser\u2019s Bookmark Bar (just drag and drop). Once installed, turn it on and you can start commenting or PIN on anything you see on the page (an article, a sentence, an image, a video). The comment will also float to wherever the content goes so other Floatalk-ers can see your comment \u2026 or should we say engage with you. The comment or PIN will also be published publically or to your followers only on Floatalk homepage or your own Floatalk homepage.\n\nThis speeds things up for today\u2019s busy people no?\n\nI say I\u2019m behind this new Floatalk tool and lets all Floatalk and be a Floatalk-er. I am taking the dive and I hope to find your comments/reviews/PIN somewhere on Floatalk sometime soon."},
{"url": "http://www.warplife.com/jobs/computer/united-states/new-york/new-york/", "link_title": "Find a Computer Job in New York City", "sentiment": 0.13479338842975205, "text": "When Hitler attacked the Jews I was not a Jew, therefore I was not concerned. And when Hitler attacked the Catholics, I was not a Catholic, and therefore, I was not concerned. And when Hitler attacked the unions and the industrialists, I was not a member of the unions and was not concerned. Then Hitler attacked me and the Protestant church - and there was nobody left to be concerned. -- Pastor Martin Niem\u00f6ller\n\nThis site totally sucks when viewed on a smartphone. I'll fix this Real Soon Now.\n\nI just started building this index to New York's many software publishers and electronics companies. If your company is not yet listed, or you know of a company in or near New York that offers software development or electrical engineering work, please let me know by emailing mdcrawford@gmail.com.\n\nIt is best if you can send me, at the very least, the name of the employer or client, and the URL - that is, the link - to their website homepage. If you can't find their website, then tell me where they are located.\n\nI Am Eternally In Your Debt. -- Mike"},
{"url": "http://www.npr.org/2015/05/23/408927009/senate-blocks-patriot-act-extension", "link_title": "Senate Blocks Patriot Act Extension", "sentiment": -0.02295780100658149, "text": "The Senate struggled to prevent an interruption in critical government surveillance programs early Saturday, rejecting both a House-passed bill and a short-term extension of the USA Patriot Act.\n\nThe back-to-back votes left lawmakers without a clear fallback, although current law doesn't expire until midnight May 31.\n\nThe White House has pressured the Senate to back the House bill, which would end the National Security Agency's bulk collection of domestic phone records. Instead, the records would remain with telephone companies subject to a case-by-case review.\n\nThe vote was 57-42, short of the 60-vote threshold to move ahead.\n\nThat was immediately followed by rejection of a two-month extension to the existing programs. The vote was 45-54, again short of the 60-vote threshold.\n\nRepublican officials said Senate Majority Leader Mitch McConnell, R-Ky., intended to try again, this time with an even shorter renewal of current law.\n\nWhatever the Senate approves must be passed by the House, which has already left Washington for a weeklong Memorial Day break.\n\nComplicating McConnell's efforts was an attempt by fellow Kentuckian Sen. Rand Paul, who has vowed to do everything he can to prevent the renewal of the bulk phone records collection.\n\n\"My filibuster continues to end NSA illegal spying,\" tweeted the Republican presidential contender.\n\nThe legal provisions authorizing the programs will expire at midnight May 31, and officials say they will lose valuable surveillance tools if the Senate fails to go along with the House. But key Republican senators oppose the House approach.\n\nAt issue is a section of the Patriot Act, Section 215, used by the government to justify secretly collecting the \"to and from\" information about nearly every American landline telephone call. For technical and bureaucratic reasons, the program was not collecting a large chunk of mobile calling records, which made it less effective as fewer people continued to use landlines.\n\nWhen former NSA contractor Edward Snowden revealed the program in 2013, many Americans were outraged that NSA had their calling records. President Barack Obama ultimately announced a plan similar to the USA Freedom Act and asked Congress to pass it. He said the plan would preserve the NSA's ability to hunt for domestic connections to international plots without having an intelligence agency hold millions of Americans' private records.\n\nSince it gave the government extraordinary powers, Section 215 of the Patriot Act was designed to expire at midnight on May 31 unless Congress renews it.\n\nUnder the USA Freedom Act, the government would transition over six months to a system under which it queries the phone companies with known terrorists' numbers to get back a list of numbers that had been in touch with a terrorist number.\n\nBut if Section 215 expires without replacement, the government would lack the blanket authority to conduct those searches. There would be legal methods to hunt for connections in U.S. phone records to terrorists, said current and former U.S. officials who spoke on condition of anonymity because they were not authorized to discuss the matter publicly. But those methods would not be applicable in every case.\n\nThe Justice Department has said the NSA would begin winding down its collection of domestic calling records this week if the Senate fails to act because the collection takes time to halt."},
{"url": "https://leanpub.com/thereactnativebook", "link_title": "The React Native Book", "sentiment": 0.12083333333333332, "text": "The author is letting you set the price you'll pay for this book! The suggested price is $15.99 , and the minimum price is $14.99.\n\nThis book is a practical and quick guide for you to develop iOS application using React-Native Framework. You'll learn how to use Flex Layout, basic React-Native components, develop custom modules, custom views.\n\nWithin 45 days of purchase you can get a 100% refund on any Leanpub purchase, in two clicks. We process the refunds manually, so they may take a few days to show up. \n\nSee full terms"},
{"url": "http://techcrunch.com/2015/05/16/why-im-still-wearing-my-apple-watch/", "link_title": "Why I\u2019m Still Wearing My Apple Watch", "sentiment": 0.15029798591239268, "text": "As TechCrunch\u2019s resident watch nerd, I have been asked\u00a0many times if I\u2019m swapping my Omegas and Seikos and JLCs for the Apple Watch.\ufffc\n\nAnd I have. I honestly have. I\u2019ve worn the Apple Watch every day since I got it and I don\u2019t know when I\u2019m going to strap on a mechanical next. It\u2019s the saddest thing in the world for me to say but, after years of calling each and every smartwatch nice but not necessary, I\u2019ve finally succumbed to this shiny bauble for a number of reasons.\n\nI\u2019ve worn multiple Android Wear devices, as well as a Pebble. I\u2019ve also seen other devices like Martian and Geak come and go and none of them did exactly what smartwatches are supposed to do: send me notifications that I can either act on or ignore. Pebble came closest to that goal (and I did pre-order the color Pebble because I love the company) but it lacked most of the features that make things like Samsung and Apple\u2019s smartwatches superior \u2014 including workout monitoring and simple \u201capps.\u201d\n\nThat said, why don\u2019t I just wear a Fitbit and my Speedmaster? Because I think the Apple Watch does the best of both of those objects in a package that, in some ways, gives the Speedmaster a run for its money. It\u2019s that simple.\n\nI know how watches are built. I\u2019ve repaired a few mechanicals, and I\u2019ve visited a number of watch factories. I know for certain that there are a number of Swiss companies that are arguing over their Montrachet and claims about how Apple pulled it off. They have been able to build a steel case with curved sides and crystal for a price that comes in far below anything any Swiss house could offer.\n\nConsider the much-beloved Ressence Type 3. The primary difference between it and a $2,500 Swiss Army Alpnach Automatic Chronograph (arguably the cheapest mechanical chronograph you\u2019re going to find that isn\u2019t absolute garbage) is the case and the face. It took lots of expensive people lots of time to design that case, but I doubt any of those people were as expensive as Apple engineers.\n\nIn short, the biggest-selling point on a $34,000 Ressence \u2013 a watch with a pillowy, almost organic case and dial style \u2013 has been improved upon and mass-manufactured by a company that makes cellphones. I would never compare the Ressence, which is legitimately a work of art, to\u00a0the Apple Watch, but from a manufacturing standpoint, there is little difference.\n\nAs an aside, I don\u2019t think the band removal system on the Apple Watch is particularly novel. I\u2019ve seen Jaeger Le Coltre and Cartier watches with the same push-button system for years. Apple steals!\n\nAs I mentioned, the Apple Watch replaces my Fitbit. While many would argue having a step tracker on your body is wishful thinking, I like to know that I\u2019m moving and grooving during the day. Furthermore, having my notifications and health data show up on my wrist rather than on my phone is an amazing benefit.\n\nI\u2019ve noticed myself checking my watch versus whipping out my phone and, thanks to the solid notifications, I can spend more time looking up rather than rooting though apps only to find out that an email wasn\u2019t what I thought it was.\n\nI\u2019ve been using the Chronograph face and it supplies me with just enough daily info to keep me interested. I now miss the Apple Watch when I\u2019m not wearing it and I found myself wanting to slide the face of my Bell & Ross today.\n\nWatches are tantalizing. When I was truly collecting, back from 2004-2006, I had 60\u00a0watches at any one time. There is an itch that watch collectors get that compels them almost endlessly to search for new watches. This neophilia is not healthy and, for the most part, it doesn\u2019t last.\n\nMost watch collectors settle in at about 20 \u2013 I know I did \u2013 and buy and sell watches to afford new additions. But the Apple Watch will probably satisfy that neophiliac itch for a long time. Watches are items that are endlessly admired by their owners.\n\nI remember taking my watches off and examining every corner and service, marveling at the balance wheel twisting behind the exhibition back and noting the motion of the seconds hand as it swept around the face. The Apple Watch allows for that same fascination but through software, which is obviously a dangerous thing to someone addicted to the new.\n\nWill I ever go back to mechanicals? Probably. I know the value of a solid mechanical watch and I know the history and provenance of watchmaking. I know watches were once our crowning achievement as humans, and I also know that technology has knee-capped the watch industry multiple times in the last century alone.\n\nThe great makers \u2013 Rolex, Omega, Breguet \u2013 existed in a lucrative bubble until the late 1970s when they were nearly destroyed in a very real way by Seiko, Casio and Texas Instruments. The first quartz watches cost as much as cars, but once electronics manufacturers got better at stamping out parts, they were able to reduce the price to dollars \u2013 or pennies. The Swiss watch industry reacted by going upscale, a move that has priced many of us out of the hobby.\n\nAm I worried for Zurich at this point? Yes and no. Switzerland has long rested on its many-geared laurels and ignored the average consumer. I started my site WristWatchReview in 2004 because I was confused. I\u2019d open the pages of GQ and Esquire and see models wearing, for example, a Prada jacket ($900), Zegna shirt ($400), and a Breitling watch ($20,000). I understood implicitly that if you paid $900 for a jacket you were probably just too rich but I didn\u2019t understand why the watch cost as much as a house in a nicer part of Detroit these days.\n\nWhat watchmakers never told us \u2013 and still don\u2019t tell us \u2013 is that watchmaking in the 21st century is as sensical as making horse-drawn carriages or developing photos using the gelatin silver process. Yet what is better than a moonlight ride on a horse-drawn carriage or a beautiful print of a baby just born?\n\nTechnology has replaced much of the watchmaker\u2019s art with a robotic soldering gun and an underpaid factory worker which is why it became, as a watch lover, important to share the history, the majesty and the importance of watchmaking with the world. I wanted people to understand the art of the thing.\n\nBut Switzerland never got that. Take the Speedmaster Professional, for example. Switzerland markets this primarily as the first watch on the moon. But the storied Speedmaster Pro is important not because it went to the moon once but because it is one of the most legible and reliable mechanical chronographs in existence.\n\nThe great minds of the 1950s and 1960s wore them, and we owe the creators and wearers of early chronographs a debt of gratitude for the rise of the Information Age. It is a striking artifact of an earlier time, forged in an era of unique design and mechanical mastery.\n\nSwitzerland finds a dead horse and keeps beating it. Hublot, a company that I love, offers essentially one watch in different permutations. Rolex hasn\u2019t changed much in the last century (but don\u2019t tell the Rolex nerds that) except the price. This staid and stolid ethic worked for them for decades, even after the quartz crisis. They make one watch and make delicate versions of it over and over, ad infinitum, adding more zeroes to the price because that\u2019s what collectors will pay. It\u2019s an affront to the sane consumer, and without context \u201cgood\u201d watches seem obscene rather than desirable.\n\n\u201cVintage mechanical watches are among the very finest fossils of the pre-digital age. Each one is a miniature world unto itself, a tiny functioning mechanism, a congeries of minute and mysterious moving parts. Moving parts! And consequently these watches are, in a sense, alive. They have heartbeats,\u201d wrote William Gibson over fifteen years ago. \u201cThey seem to respond, Tamagotchi-like, to \u2018love,\u2019 in the form, usually, of the expensive ministrations of specialist technicians. Like ancient steam-tractors or Vincent motorcycles, they can be painstakingly restored from virtually any stage of ruin.\u201d\n\nI agree with him completely. There is something magical about the purely mechanical, an object so complex that it takes an expert a lifetime to master the steps needed to build it. In an era of commodity hardware and easy interactivity, that means something.\n\nBut even Gibson, that cybernetic seer, couldn\u2019t foresee the rise of another, far more enticing Tamagotchi. The Apple Watch doesn\u2019t quite respond to love in the same way \u2013 it is cold and calculated \u2013 but instead engenders love through a weird melding of design and desire, of technology and fashion, of unity and connectedness. And those meldings are exactly why Switzerland needs to watch out."},
{"url": "http://theprojector.sg", "link_title": "That's a really beautiful movie theatre website", "sentiment": 0.3, "text": "Join our mailing list for updates and invites\n\nFind us on Level 5 of Golden Mile Tower on Beach Road"},
{"url": "http://www.sherbit.io/about", "link_title": "More information about Sherbit", "sentiment": 0.26507177033492824, "text": "More and more information is being generated about us everyday. As more of our everyday activities are connected to the internet, we need a way of making sense of that huge stream of data. Sherbit is a platform for monitoring the information that\u2019s being collected about you, while putting it to use for your own benefit. Your data doesn\u2019t just belong to internet companies \u2014 it belongs to you.\n\nWe\u2019re developing applications that help you turn the \u201cnumbers\u201d into meaningful insights you can put into action: like discovering ways to manage your time more effectively, live with less stress, or get a better night\u2019s sleep. By creating tools for taking control of\u00a0your data, we want to help make internet services more transparent \u2014 so we take our own standards for privacy and security very seriously. We encourage you to take a look at our privacy policy\u00a0(coming soon), and stay tuned to our blog to keep informed about the web applications\u00a0you use everyday.\n\nWe hope you enjoy using Sherbit \u2014 please make sure to send us your feedback\u00a0at info@sherbit.io."},
{"url": "http://www.buzzfeed.com/johanabhuiyan/florida-agency-classifies-uber-driver-as-employee-says-he-is#.ppKMwvNmdb", "link_title": "Florida Classifies Uber Driver as Employee, Says He Is Eligible for Unemployment", "sentiment": 0.026041666666666664, "text": "The state of Florida has taken a position on the ride-hail industry\u2019s recent labor debate. Yesterday, the Florida Department of Economic Opportunity decided that former Uber driver Darrin McGillis was an employee of the company, not a contractor as the company contends, and is thus eligible for unemployment insurance.\n\nWhen McGillis, a Florida resident and a 2010 candidate for state governor, began driving for Uber in November he went after the big money promised by the company\u2019s lucrative UberXL service. He bought a seven-seat Mitsubishi Outlander and was soon ferrying passengers around Miami, collecting UberXL\u2019s higher base fare.\n\nBut this past March, McGillis\u2019 Outlander \u2014 his primary source of income at the time \u2014 was damaged when a passing scooter hit one of its back doors as an Uber passenger exited it. \u201cUber was crickets when I asked for help,\u201d McGillis told BuzzFeed News.\n\nAfter several weeks of back-and-forth on email and phone conversations with driver operations staffers in both the local Uber office and later at headquarters, McGillis decided to file for unemployment. \u201cI told them, \u2018You guys need to take care of this, you have this so-called insurance,\u2019\u201d he explained. \u201cAt that point, I said, \u2018You need to buy this SUV from me. I bought this in January to be an Uber XL driver.\u2019 They told me to take it up with Uncle Sam. I said fine, I\u2019ll file unemployment.\u201d\n\nSo in the second week of April, McGillis filed a claim against Uber and Rasier LLC \u2014 a subsidiary of the company \u2014 with the Florida Department of Economic Opportunity. So began a two-pronged process that requires the DEO to determine whether McGillis is an independent contractor, and the Department of Revenue has to determine whether he is eligible for unemployment."},
{"url": "http://www.theguardian.com/us-news/2015/may/23/usa-freedom-act-fails-as-senators-reject-bill-to-scrap-nsa-bulk-collection", "link_title": "USA Freedom Act fails as senators reject bill to scrap NSA bulk collection", "sentiment": 0.08682342290832859, "text": "For the second time in less than a year, US senators rejected a bill to abolish the National Security Agency\u2019s bulk collection of American phone records.\n\nBy a vote of 57-42, the USA Freedom Act failed on Friday to reach the 60-vote threshold needed to advance in the Senate after hours of procedural manoeuvering lasted into the wee hours Saturday morning.\n\nThe result left the Senate due to reconvene on May 31, just hours before a wellspring of broad NSA and FBI domestic spying powers will expire at midnight.\n\nArchitects of the USA Freedom Act had hoped that the expiration at the end of May of the Patriot Act authorities, known as Section 215, provided them sufficient leverage to undo the defeat of 2014 and push their bill over the line.\n\nThe bill was a compromise to limit the scope of government surveillance. It traded the end of NSA bulk surveillance for the retention through 2019 of Section 215, which permits the collection of \u201cbusiness records\u201d outside normal warrant and subpoena channels \u2013 as well as a massive amount of US communications metadata, according to a justice department report.\n\nAlthough the bill passed the House of Representatives by a massive 338-88 margin last week, it was unable to overcome concerns from Republicans about the process of letting telecom companies take responsibility about the collection data from the NSA.\n\nRepublican leadership was hoping for a short-term extension of the Patriot Act which would push debate into early June, once the Senate returns from its Memorial Day recess.\n\nThis was considered far more likely than a two-month extension of the legislation, which was considered a forlorn hope and failed by a 45-54 vote shortly after the USA Freedom Act failed to reach cloture on Saturday morning.\n\nNevada Republican Dean Heller, a co-sponsor of the bill, told reporters early on Friday: \u201cWe\u2019re losing the \u2018politics of going home\u2019 argument with our conference.\u201d\n\nHe added that proponents of a short term extension were able to argue that supporting the bill meant staying on Capitol Hill all week. \u201cSo how do you win that argument?\u201d Heller said.\n\nThe answer was by making senators stay regardless of how they voted as Kentucky Republican Rand Paul, a virulent opponent of NSA surveillance, torpedoed any attempt to kick the can down the road.\n\nOn Saturday morning, after both cloture votes failed, Senate majority leader Mitch McConnell asked for unanimous consent to extend the Patriot Act for a week. Paul objected. Objections were then heard from Paul, as well as from Oregon Democrat Ron Wyden and New Mexico Democrat Martin Heinrich on four-day, two-day and one-day extensions. Eventually McConnell gave up and announced that the Senate would adjourn until 31 May, the day before the key provisions of the Patriot Act expire.\n\nThe failure of the USA Freedom Act leaves the Senate in an impasse.\n\nRepublican whip John Cornyn, a strident supporter of extending the Patriot Act, divided the Senate into three groups on Friday.\n\nAs he put it, there are those who want a \u201cstraight extension, those who like USA Freedom and those who like nothing\u201d.\n\nThose who want a straight extension of the Patriot Act are in a distinct minority and supporters of the USA Freedom Act still cannot muster the necessary super majority to advance the bill. The result means those who are more than happy to simply let Section 215 expire on May 31 are in the driver\u2019s seat.\n\nWhen reporters asked Paul on Saturday morning whether he was concerned about the provisions of the Patriot Act expiring at the end of the month, the Kentucky Republican seemed unworried \u201cWe were liking the constitution for about 200 years and I think we could rely on the constitution.\u201d\n\nThere still is some room for compromise. Arizona Republican John McCain, when asked if the USA Freedom Act was better than a lapse, said: \u201cThere are some programs that are affected by \u2018Freedom USA\u2019 that I would be very concerned about shutting down.\u201d He added \u201cbut obviously anything is better than shutting down the whole operation.\u201d\n\nMcCain also noted that \u201cyou can argue whether we should be doing the mega data thing but you can\u2019t argue that it\u2019s a good idea to shut down the whole thing.\u201d\n\nHowever, that shouldn\u2019t be seen as any sort of endorsement of the NSA reform bill by hawks in Senate GOP caucus. Representative Tom Massie, a Kentucky Republican who came to the Senate floor to witness the vote Saturday morning, told reporters he was surprised at how strongly many of his fellow Republicans felt about the compromise reform bill. \u201cThey really don\u2019t like the Freedom Act,\u201d he said.\n\nIn the meantime, barring a breakthrough in the coming days, \u201cthe whole operation may be shutdown regardless\u201d as the May 31 deadline looms closer.\n\nMitch McConnell may still be majority leader but for now, it\u2019s Rand Paul\u2019s Senate.\n\n"},
{"url": "http://www.wsj.com/article_email/amazon-targets-etsy-with-handmade-marketplace-1432332301-lMyQjAxMTE1NDI5MjMyMTI2Wj", "link_title": "Amazon Targets Etsy with \u2018Handmade\u2019 Marketplace", "sentiment": 0.10881542699724518, "text": "Having vanquished scores of brick-and-mortar retailers, Amazon.com Inc. AMZN -0.93 % has a new target: Etsy Inc. ETSY -1.94 %\n\nThe Seattle Web retailer is prepping a marketplace for artisan goods it is calling Handmade. Etsy sellers received invites reviewed by The Wall Street Journal over the past few days to sign up for the new section of the Amazon site.\n\n\u201cWe\u2019re offering artisans like you a first peek at Handmade, a new marketplace for handcrafted goods,\u201d the Amazon email states. The invite doesn\u2019t provide additional specifics such as when it will be rolled out or fee details.\n\nThe invite points to a questionnaire on Amazon\u2019s site that asks sellers which primary product category they fall under, listing 11 general subjects such as apparel, baby, and pet supplies. The poll lists subcategories for jewelry and home and kitchen in particular.\n\nRepresentatives for Amazon and Etsy declined to comment.\n\nAmazon\u2019s new site could present a new challenge for Etsy, whose stock has fallen about 43% since the close of its first day of trading last month. Unlike Amazon, Etsy doesn\u2019t have its own Prime loyalty program, nor does it oversee a shipping network to expedite deliveries.\n\nStill, Etsy\u2019s 3.5% commission and 20-cent listing fee are potentially much less expensive than Amazon\u2019s cut. Today, Amazon\u2019s commission varies depending on the product sold, but for many categories it charges sellers a 15% fee on the price of each sale.\n\nEtsy, which went public on April 16, reported a $36.6 million loss in this year\u2019s first quarter, wider than the $463,000 loss a year earlier, despite a 44% jump in revenue to $58.5 million. The Brooklyn, N.Y.-based company said the loss was due mostly to restructuring costs.\n\nThe number of Etsy\u2019s active buyers\u2014those who have made at least one purchase in the past 12 months\u2014rose to 20.8 million from 15.3 million a year earlier. But that pales in comparison with Amazon\u2019s 278 million active accounts.\n\nDar Garfield, who sells handmade jewelry on Etsy through her SheekyDoodle site, said she was surprised to get an invite from Amazon, since she didn\u2019t have a seller account with the Seattle retailer.\n\n\u201cAmazon has such huge traffic numbers on their website already, it\u2019s pretty appealing,\u201d said Ms. Garfield. \u201cI am probably going to do it.\u201d\n\nMs. Garfield, of New Plymouth, Idaho, said she has about 1,300 products for sale on Etsy at any one time, noting that is the only outlet where she sells her wares.\n\nMonica Estrada, who runs a Manila, Philippines-based custom document and planner store on Etsy, said she would also consider joining Amazon\u2019s Handmade site.\n\n\u201cIt\u2019s a good idea not to put all your eggs in one basket, and joining another handmade marketplace would make me feel like I\u2019m not so dependent on Etsy for income,\u201d she said in an email.\n\nOn Etsy.com message boards, some Etsy sellers expressed concern about Amazon\u2019s shipping guarantees, which include two days for Prime members. Many Etsy sellers make their goods on demand, which means they wouldn\u2019t be able to meet strict shipping deadlines.\n\nIt isn\u2019t clear if Amazon would hold artisans on the new marketplace site to the same shipping standards as other sellers."},
{"url": "http://blogs.wsj.com/digits/2015/05/22/munchery-valued-at-about-300-million-amid-food-fight/", "link_title": "Munchery Valued at About $300M Amid Food Fight", "sentiment": 0.1674325674325674, "text": "Munchery is the latest food-delivery startup to raise cash from hungry venture capitalists.\n\nThe San Francisco-based service for ordering quick microwavable meals is closing a funding round of $85 million that would value Munchery at about $300 million, according to a person briefed on the deal. Two previous investors in Munchery, SherpaVentures and Menlo Ventures, led the deal, the person said. A Munchery spokeswoman declined to comment.\n\nMunchery is one of several young startups providing an alternative to take-out with gourmet meals prepared by local chefs and delivered in 20 to 40 minutes. Sprig, another ready-made meal delivery startup, raised $45 million from investors last month; Blue Apron, a make-it-your-self food kit service, is currently seeking funding at a valuation of around $2 billion, people familiar with the matter told The Wall Street Journal earlier this month.\n\nUnlike Sprig and other rivals including SpoonRocket, Munchery\u2019s meals are delivered cold and need to be heated up. That makes it easier for the company to deliver a large number of meals over a wider area without fear of food spoiling.\n\nMunchery is also unique in employing a staff of drivers who are full-time employees with benefits, unlike the majority of on-demand, app-based services who rely on fleets of independent contractors to for labor. While that adds to Munchery\u2019s costs, it could help the startup build more loyalty with staff and customers at a time when the legality of large contract workforces is being called into question.\n\nThe business model of delivering food is coming under more scrutiny as VC firms consider writing large checks with the goal of turning the startups into sustainable businesses.\n\nThe new round of funding is the single largest check written by SherpaVentures, the VC firm co-founded in 2013 by early Uber investor Shervin Pishevar, said the person familiar with the deal.\n\nMunchery prices entrees between $9 and $14 each and the company charges about $3 per delivery no matter how many meals are included in an order.\n\nMunchery is available in San Francisco, New York, Seattle and Los Angeles. The company previously raised more than $30 million in funding since it was founded in 2010.\n\n______________________________________________________\n\n\n\n For the latest news and analysis, follow @wsjd\n\n\n\n Get breaking news and personal-tech reviews delivered right to your inbox. \n\n\n\n More from WSJ.D: And make sure to visit WSJ.D for all of our news, personal tech coverage, analysis and more, and add our XML feed to your favorite reader."},
{"url": "http://images3.com", "link_title": "Show HN: ImageS3 supports animated GIF resizing now", "sentiment": 0.15833333333333333, "text": "ImageS3 is a free and open image hosting service for developers. It is designed to store, resize and manage images for all your web and mobile apps in one place.\n\nJava - ImageS3 developed in Java. You can download the latest JRE7 build from here. Play Framework - Play Framework is a lightweight and highly-scalable application server. ImageS3 released with Play as a standalone package which lets you don't need to download different jar files separately. Follow this instruction to install 2.3.x Play Framework. MongoDB - MongoDB is a leading NoSQL database with amazing features. ImageS3 uses MongoDB to store image metadata and other objects information. Follow this instruction to install the latest MongoDB. Amazon S3 -- Amazon S3 is a secure, durable and highly-scalable cloud storage service. ImageS3 uses Amazon S3 to store image files. If you already have Amazon AWS account, you can skip this, otherwise, sign up one at here.\n\nAll the configuration files are located in image-play-[version]/conf.\n\nYou need to set the following properties before running ImageS3:\n\nIf you have a different location for image files, then setup the following configurations.\n\nUse the following command to run ImageS3:\n\nAnd then, open http://localhost:9000 in your browser, you will see the admin tool:\n\nYou can also run ImageS3 on different port, for example 8080:"},
{"url": "http://www.motherjones.com/politics/2014/01/blackstone-rental-homes-bundled-derivatives", "link_title": "Wall Street's New Financial Product: Your Rent Check", "sentiment": 0.13290043290043285, "text": "Toward the end of 2012, Mark Alston, a real estate broker in Los Angeles, began noticing something strange. Home prices were starting to rise, and fast\u2014about 20 percent annually. Normally, higher home prices would signal increased demand from homebuyers and indicate that the economy was rebounding. But the home ownership rate was still dropping. Somehow, the real estate market was out of whack.\n\nThen there were the buyers themselves. \"I went two years without selling to a black family, and that wasn't for lack of trying,\" recalls Alston, whose business is concentrated in inner-city neighborhoods where the majority of residents are African American and Latino. Now all his buyers were businessmen in suits. And weirder yet, they were all paying in cash.\n\nOver the last two years, private equity firms and hedge funds have amassed an unprecedented real estate empire, snapping up Spanish revivals in Phoenix, adobes in Los Angeles, Queen Anne Victorians in Atlanta, and brick-faced bungalows in Chicago. In total, Wall Street investors have bought more than 200,000 cheap, mostly foreclosed houses in some of the cities hardest hit by the economic meltdown. But they're not simply flipping these houses. Instead, they've started bundling some of them into a new kind of financial product that could blow up the housing market all over again.\n\nNo company has bought more houses than the Blackstone Group, one of the world's largest private equity firms. (Its many investments include Hilton Hotels, the Weather Channel, and SeaWorld. Among its institutional investors are Goldman Sachs, Morgan Stanley, Citigroup, Bank of America, Deutsche Bank, and JPMorgan Chase.) Through its subsidiary, Invitation Homes, Blackstone has picked up houses through local brokers, at foreclosure auctions, and in bulk purchases. Last April, it bought 1,400 houses in Atlanta in a single day. In Phoenix, some neighborhoods have a Blackstone-owned home on just about every block. As of November, Blackstone had acquired 40,000 houses, most of them foreclosures, worth $7.5 billion. Today, it is the largest owner of single-family rental homes in the nation.\n\nBlackstone's deep pockets\u2014$248 billion in assets under management and a $3.6 billion credit line arranged by Deutsche Bank for buying houses\u2014allow it to outbid individual buyers, driving up local real estate prices and pushing families out of the market. \"You can't compete with a company that's betting on speculative future value when they're playing with cash,\" says Alston. \"Institutional investors are siphoning the wealth and the ability for wealth accumulation out of underserved communities,\" adds Henry Wade, cofounder of the Arizona Association of Real Estate Brokers.\n\nBut buying houses cheap and then waiting for them to appreciate isn't the only way Blackstone is making money on these deals. It wants your rent check, too. In November, after many months of hype, the firm released the first-ever rated bond backed by securitized rental payments. Joining forces with Credit Suisse, Deutsche Bank, and JPMorgan (which recently paid a record $13 billion fine to settle accusations of ripping off mortgage investors), Blackstone has bundled the rental payments from more than 3,200 single-family houses, offering investors its mortgages on the underlying properties as collateral. After investors tripped over themselves to buy into the $479 million bond, Blackstone's competitors announced that they, too, would develop similar securities.\n\nAsked why the public should expect rental-backed securities to be safe, the hedge fund investor responds, \"Trust me.\"\n\n\"It's just like a residential mortgage-backed security,\" says one hedge fund investor whose company does business with Blackstone. Yet some analysts and observers are uneasy about the idea of a new market for securitized mortgage debt backed by rent checks. Dean Baker, an economist and codirector of the Center for Economic and Policy Research, is concerned that Wall Street firms are overlooking the risks of these untested investments. \"You kind of just hope they know what they're doing,\" he says. In documents sent to investors, Blackstone has stated that it expects that 95 percent of its homes will be occupied at all times, with an average monthly rent of around $1,300. Real estate professionals say that those assumptions may be overly ambitious for single-family rentals.\n\nPlying investors with such upbeat projections creates intense pressure to keep houses occupied\u2014even as residents are squeezed by higher rents and strict collections policies. In Charlotte, North Carolina, Invitation Homes raised rents by as much as a third and filed eviction proceedings against nearly 10 percent of its renters, according to the Charlotte Observer.\n\nCaDonna Porter moved into an Invitation Homes property outside Atlanta with her children in September. When part of her monthly payment was rejected because she tried to use a debit card, the company demanded that she deliver the remaining amount in person, via certified funds, by 5 p.m. the following day or incur a $200 fee and face eviction. Porter took time off from work to deliver a money order in person, only to be informed that the payment had been rejected because it didn't include the late fee and an additional $75 insufficient funds fee.\n\nIn a maddening string of emails, Invitation Homes repeatedly reminded Porter that it could file to evict her unless she paid the penalties. When she finally said that she would seek legal counsel, Invitation Homes agreed to accept her payment as \"a one-time courtesy.\" Andrew Gallina, Invitation Homes' vice president for marketing, says it treats all of its renters equally: \"Under the law, we're not allowed to make changes or exceptions. That's just basic fair housing.\"\n\nInvitation Homes has described its strategy as \"a bet on America.\" Rather than pricing buyers out of the market, Gallina says, the company is helping families who can't get mortgages.\n\nBut what if the security blows up? Investors could demand their collateral back, forcing renters out of their homes, even if they never missed a payment. \"We could well end up in that situation where you get a lot of people getting evicted\u2014not because the tenants have fallen behind, but because the landlords have fallen behind,\" says Baker.\n\nAsked why the public should expect rental-backed securities to be safe, the hedge fund investor responds, \"Trust me.\"\n\nA longer version of this article was previously published by TomDispatch."},
{"url": "http://www.pcworld.com/article/2925897/alibabas-uc-browser-found-leaking-users-data.html", "link_title": "Alibaba's UC Browser found leaking users' data", "sentiment": 0.02587719298245614, "text": "A mobile browser owned by China\u2019s Alibaba Group contained privacy risks that could have exposed users\u2019 personal data, according to a security group.\n\nThe flaws were found in UC Browser, a third-party app that\u2019s been expanding in popularity across the globe, and has over 100 million daily active users. Last year, Chinese e-commerce giant Alibaba Group acquired UC Browser\u2019s parent company.\n\nThe Citizen Lab at the University of Toronto\u2019s Munk School of Global Affairs investigated the mobile browser, and found that it was sending user data unencrypted, or with not enough encryption. This included transmitting phone numbers and device serial numbers, in addition to user search queries and geolocation data.\n\nThe lack of encryption would let anyone with access to the data traffic to identify a user\u2019s phone number, the device, and the person\u2019s location, Citizen Lab said in its report released Thursday.\n\nBoth UC Browser\u2019s Chinese language and English language versions contained the reported flaws. On Friday, Alibaba said it investigated the issues and has fixed the problem with a version update users can now download. \u201cWe have no evidence that any user information has been taken,\u201d the company said in an email.\n\nThe privacy risk is that governments such as China and India often require telecommunication providers to hand over their data traffic, Citizen Lab said. Any personal data leaked through UC Browser could be used by governments or other third parties, it added.\n\nCitizen Lab investigated the UC Browser after a leak of a 2012 document by Edward Snowden, disclosed by Canada\u2019s CBC News, showed that Canada\u2019s intelligence agency and its partners were allegedly aware of security flaws in UC Browser, and wished to exploit them for electronic surveillance.\n\nAlthough Citizen Lab found problems with UC Browser\u2019s software, the group could not confirm if they were the same vulnerabilities detailed in the document disclosed by the former National Security Agency contractor."},
{"url": "https://medium.com/@rustem/31-reasons-why-you-should-not-miss-maker-faire-b33a3f328656", "link_title": "31 Reasons Why You Should Not Miss Maker Faire", "sentiment": 0.525, "text": "Hi! My name is Rustem and I am the founder of Robo. We are developing a robot called Robo Wunderkind to teach children the basics of programming and robotics.\n\nToday I\u2019ll tell you about my trip to Maker Faire 2015 in San Mateo, California. In this post you will see a huge robotic giraffe, an acrobat dancing on a flaming pole, R2-D2 robots, and a lot of happy children.\n\nMaker Faire is an annual two-day exhibition for makers from all over the world to show off their inventions. Some call it the greatest Show & Tell Show on the Earth! This year, Maker Faire Bay Area is celebrating its 10th anniversary. Over 100 thousand visitors attended the event, and more than 50% of them brought their children. Maker Faire was the perfect opportunity to show our amazing Robo Wunderkind to the world.\n\nThe exhibition is divided into 10 zones. For me, two days were not enough to explore all the pavilions:"},
{"url": "http://thewire.in/2015/05/23/isro-keeps-up-steady-trickle-of-photos-from-mars-orbiter/", "link_title": "ISRO Keeps Up Steady Trickle of Photos from Mars Orbiter", "sentiment": 0.0987702922077922, "text": "On May 22, the Indian Space Research Organization released two\u00a0new\u00a0pictures snapped by the Mars Orbiter Mission, currently in orbit around the red planet. They were taken\u00a0by the Mars Colour Camera on-board the orbiter in February and April, and follow a heftier batch of photos released in the third week of March. On the same day, ISRO was given\u00a0the Pioneer Award by the International Space Development Conference, organized by the American National Space Society.\n\nOne\u00a0picture shows Tyrrhenus Mons (above), a major volcanic\u00a0elevation located in the southern hemisphere of Mars. While it isn\u2019t major in the same sense Mount Olympus Mons is \u2013 as the tallest mountain in the Solar System,\u00a0located almost on the opposite side of the planet\u00a0\u2013 Tyrrhenus is significant for its age and formative history. It is one of the oldest volcanoes on Mars, being 3.7-3.9 billion years old, and formed by hot clouds of ash being blown through the surface by molten rock that suddenly encountered steam or a cloud of gas.\n\nThe second picture shows the oddly shaped Pital Crater\u00a0(below), located near the Valles Marineris canyons below the equator. The picture appears to have been taken in April, from a height of 808 km.\n\nThe Mars Colour Camera that took the picture is among five scientific payloads. It has an instantaneous field of view ranging from 19.5 m to 4 km and a 2048 x 2048 squared-pixel detector. Its imaging capabilities are also complemented by MOM\u2019s highly elliptical orbit around Mars, which takes it 77,000 km from the planet at one point (its closest orbital approach is at 421 km). The great\u00a0separation allows the Colour Camera and others to take pictures with large fields of view.\n\nThe camera has multiple\u00a0objectives: to image features on the planet\u2019s surface, to map the geological features surrounding probable sources of methane (which are determined by a companion payload called the Methane Sensor for Mars), to image dust-storms over six months and to map the polar ice caps.\u00a0As an aside, the camera will also provide important context to the data logged by the four other instruments.\n\nThose instruments are a Methane Sensor for Mars, Exospheric Neutral Composition Analyser (to determine composition of particles), Lyman-Alpha photometer (to measure the relative abundance of hydrogen and deuterium in the upper atmosphere), and Thermal Infrared Imaging Spectrometer (to measure thermal radiation).\n\nOn March 24 this year, the orbiter\u00a0completed its original six months around Mars, and the mission was promptly extended by six months (it is now on the verge of completing its eighth month). On the same day, ISRO released a batch of pictures from the Colour Camera, including stunning\u00a0snaps of the blue-tinged\u00a0Valles Marineris.\n\nYou may also like:"},
{"url": "http://www.theregister.co.uk/2015/05/22/factory_reset_fails_in_half_a_billion_android_phones/", "link_title": "Factory reset memory wipe fails in 500 MEEELLION Android phones", "sentiment": 0.025591397849462363, "text": "Cambridge University boffins Laurent Simon and Ross Anderson say half a billion Android phones could have data recovered and Google accounts compromised thanks to flaws in the default wiping feature.\n\nThe gaffe allows tokens for Google and Facebook among others to be recovered in 80 percent of cases. Encryption keys can also be recovered and can with some brute-force password guessing allow attackers to access previously wiped data.\n\nThose keys, along with a host of data including SMS, photos, and videos, can be recovered because the factory reset process in Android 4.3 Jellybean and below is flawed, the pair say in the paper Security Analysis of Android Factory Resets (PDF).\n\nHere's the gist of it:\n\nSimon and Anderson tested 21 phones from five vendors including Samsung, HTC, and Nexus running Android versions 2.3 to 4.3.\n\nIt is unknown how Android versions above 4.3 are affected. Google has been contacted for comment.\n\nThe duo say in the work considered the first comprehensive analysis of Android factory reset functions that remote wiping features for lost and stolen phones are subject to the same flaws.\n\nEnterprises that sell off used phones that once contained or were linked to systems that store sensitive corporate information should be most concerned about the flaws given the option for skilled data thieves to purchase recycled phones and trawl through system partitions.\n\nThieves can also buy phones on sites such as eBay in a bid to obtain bank account information, should tried and tested bank trojans prove uninspired.\n\nThe pair match the most likely crimes to each data set suggesting images and browsing histories be used for blackmail and credentials be sold off on the criminal hacking underground.\n\nLists of installed apps could be useful to determine the value of the previous Android user, such as corporate executives as opposed to school kids, while the 'phone owner' facility is useful to contact the victim.\n\nBecause encrypted systems could be attacked only after the salted key is cracked, this author suggests users may be able to counter the attack by enabling full disk encryption and creating very long complex passwords just before handsets are wiped for sale in a bid to make brute forcing of keys impractical. \u00ae"},
{"url": "https://www.youtube.com/watch?v=_wXHR-lad-Q", "link_title": "TeraDeep: 10M images deep neural network", "sentiment": 0.12142857142857143, "text": "Rating is available when the video has been rented.\n\nThis feature is not available right now. Please try again later."},
{"url": "http://www.iftf.org/future-now/article-detail/the-blue-city-a-scenario-about-control-in-human-machine-futures/", "link_title": "The Blue City: A Human+Machine Futures Scenario", "sentiment": 0.0530879446640316, "text": "If you paused and observed, you could almost make out the invisible hand guiding people on their hurried but merry marches around the city. The system doesn\u2019t like dawdlers and certainly doesn\u2019t allow pausing long enough to actually observe the system in action. Within a few moments, your auditory cone would fill with irritating noise. You might receive an olfactory-gram from the corner bakery, inducing an intense craving for something sweet. And if you managed to resist those attempts at mobile persuasion, the solar sidewalk beneath your feet would begin to heat to an uncomfortable temperature. If the system wanted you to move, you were going to move eventually, but of course it would always be your choice.\n\nLife in the city is smooth\u2014you almost never feel the system working. The individual is sacred, at least that\u2019s what the great corporate paeans to choice assert. And messages reminding you of your right to choice are constantly generated and splashed across both physical and virtual space by the government\u2019s newest narrative algorithm. Even with its incessant flow of brainwave readings, the system always protects individual free will, and it makes a persuasive case.\n\nIt\u2019s as if the city is designed for you. A self-driving car always appears when you need it, one that automatically routes you where you need to go and emits calming eucalyptus on particularly busy days. In the evenings, your Navigator shoes lead you to the subway, the long day of work fleeing from memory as you watch each passenger\u2019s mood-illuminated clothes magically fade into matching shades with each stop\u2014not thinking about your own transformation to complement the whole. You often run into friends for an impromptu dinner at the neighborhood restaurant, so happy for an unplanned outing that you don\u2019t notice your EyeHealth contacts obscuring desserts from the menu. Between courses, sweeping music suddenly plays when a potential mate with complementary DNA and disposition passes by\u2014 music only the two of you can hear.\n\n\u2026Yet is it the life you want to live?\n\nYour habits and little behavioral quirks always converge with those of others. The system\u2019s reminders are so good, and so subtle, you forget they\u2019re even there. Like a knife in the hand of an expert chef, the little pings and pokes from the things on your body are a natural extension as the line between you and the rest of the world\u2014between human and machine\u2014blurs. Yet sometimes you wonder if it\u2019s possible to experience a moment outside the system, to rise above the people, goods, and information constantly on the move and see the mysterious central-control map your mother says is just an urban legend. These thoughts are only fleeting, but you entertain them more and more. That sharp pain rising from your neural-networked shoe hurts only for a second when you do.\n\nThis scenario is one of the four possibilities we imagined in our new map, Human+Machine Futures in Full Color: Extending Our Senses and Ourselves, the first in our 2015 Technology Horizons program\u2019s series of projects about Human+Machine Futures\u2014the blurring lines between human and machine, natural and artificial. The map forecasts how emerging technology is creating what are today vast and unknown territories of human sensory experience. Rooted in the color-based systems mythology tool IFTF developed in 2014, we intended for the map\u2019s forecasts, scenarios, and artifacts from the future to help us imagine how the future of human-machine symbiosis will be colored by our priorities, concerns, and values.\n\nThis blue scenario and artifact from the future are about control and efficiency\u2014a subtle and seamless engineering of the lived experiences of both humans and machines. We can see the blue color\u00a0archetype's focus on control in the Blue City's\u00a0clean, clear, and logical interface. The goal of this system is as \u201cblue\u201d as its interface; the whole point of all its routing and coordination is to create order from complexity to ensure efficiency for urban residents, even if it means we might give up a bit agency here and there.\n\nSignals of this blue future from today\n\nWhile the technologies we see in the scenario mostly do not exist in those forms or are not currently so interconnected, we found signals from today that point to the possibility for elements of the blue scenario to emerge in the next decade.\n\nThe extremes of these signals might be uncomfortable, especially when put together, in a blue future. But how would a \u201cred\u201d approach, which creates winners and losers, use this technology? Might a \u201cgreen\u201d approach, which prioritizes connection, seem more natural? These are the kinds of questions we designed these scenarios to provoke. Over the next several weeks, we\u2019ll post the other three scenarios and artifacts, painting a picture of the full color spectrum of emerging technologies shaping the human-machine relationship, in the hopes that it helps you design for the future in which you\u2019d like to live."},
{"url": "https://xkcd.com/1508/", "link_title": "Xkcd: Operating Systems", "sentiment": 0.075, "text": "[[A timeline comic.]] Title: Operating systems running in my house. In 1985, begins running MS DOS. In 1993, begins running Windows. In 1994, begins running Mac OS. In 1998, stops running MS DOS. In 2000, starts running Linux In 2001, stops running Mac OS. In 2007, stops running Windows. In 2009, starts running OSX and Android. In 2013, starts running iOS. \"Now\" is marked as 2015. In 2016, stops running Android. In 2018, stops running Linux, starts running SOMETHING.JS In 2019, iOS and OSX merge into one. In 2023, starts running Tinder OS. In 2024, stops running OSX and starts running NEST. In 2028, stops running SOMETHING.JS and starts running Elon Musk Project. In 2029, stops running Tinder OS and starts running DOS, but ironically. In 2032, stops running NEST. In 2034, starts running Blood DRone. In 2036, stops running DOS, but ironically. In 2041, stops running Blood Drone and Elon Musk Project as HUMAN CIVILIZATION ENDS IN FIRE. In 2059, starts running GNU HURD. {{Title text: One of the survivors, poking around in the ruins with the point of a spear, uncovers a singed photo of Richard Stallman. They stare in silence. \"This,\" one of them finally says, \"This is a man who BELIEVED in something.\"}}\n\nPermanent link to this comic: http://xkcd.com/1508/Image URL (for hotlinking/embedding): http://imgs.xkcd.com/comics/operating_systems.png"},
{"url": "http://owaislone.org/blog/webpack-plus-reactjs-and-django/", "link_title": "Using Webpack transparently with Django and hot reloading React components", "sentiment": 0.07358777997364956, "text": "Using Webpack transparently with Django + hot reloading React components as a bonus\n\nIf you don\u2019t already know webpack, you\u2019ve some catching up to do.\n\nWe\u2019ll be setting up webpack and keeping it decoupled from django\u2019s staticfiles system. Read my earlier post explaining why we we\u2019ll be handling things this way and not integrating with staticfiles. We\u2019ll be using webpack-bundle-tracker to extract information from webpack and django-webpack-loader to use the extracted information for django integration.\n\nWe\u2019ll use npm to manage our frontend dependencies instead of managing them manually in one of the static files directories. You can optionally use bower as well in addition to npm.\n\nFirst let\u2019s setup npm in the root of your django project. This will generate a file called in your project root. It serves 2 purposes. Imagine requirements.txt and setup.py merged into one. That is packages.json for npm packages. If you use the or flag when installing a package, it\u2019ll save the packages as dependencies in the packages.json file. To reinstall the packages, all you need to is run . Awesome, right? It gets better. The packages will be installed locally specific to your project under a directory called node_modules like virtualenv. To install a package globally, all you need to do is to use with npm install.\n\nSo, let\u2019s generate a packages.json file in our project root using npm init\n\nIn addition to webpack, we\u2019ll at least need the webpack-bundle-tracker plugin to extract useful information from webpack and store it in as json in a file. This file will act as the link between webpack and django.\n\nSince we\u2019ll be setting up webpack with an example reactjs app, we\u2019ll also need babel. Babel is a great Javascript compiler that compiles ES6 into ES5 among other things. This lets use write next generation javascript today and still have run work in current browsers. Babel also supports react\u2019s JSX language so we don\u2019t need an additional compiler for that.\n\nWe\u2019ll also need babel-loader to integrate babel with webpack. Webpack supports pluggable libraries called loaders that add support for different types of files and languages. Loaders can also be chained. For example, you can make a less file go through a less loader to compile it to css and then pass the output through a css loader. More on loaders here.\n\nsaves the packages you install as dependencies of your package. The packages that must be installed in order to run your package. saves the packages as build dependencies, the packages that must be installed to hack on your package. Since we are not going to be publishing a real npm package, either one works. I like to use \u2014save-dev as I only need the packages to build my bundles. Whatever the bundle depends on is included in the bundle it self.\n\nLet\u2019s create a simple webpack config to load files using babel and use the plugin to extract information to . More on webpack configuration here.\n\nAt this point, our directory structure will look something like this.\n\n\n\nBinaries shipped with node packages are installed to and it not added to automatically so we need to use fullpaths to the binaries. Installing binaries globablly like will add them to one of the binary location on in .\n\nLet\u2019s go ahead and compile our first bundle\n\n\n\nThis should create bundle at . This is good but we don\u2019t want to create bundles manually every time we make changes to our code.\n\nThis will leave the compiler running and compile bundles automatically when you change any of your source files. You\u2019ll need to restart it if you make any changes to the webpack configuration though.\n\nSkip this part if you already have a react app running.\n\nLet\u2019s write a simple \u201chello, world\u201d react app and use webpack to compile it. We refer to as the entry point of our app in which will look for , or because we\u2019ve added these three extensions to our webpack config under the key .\n\nIf you left webpack running in watch mode, it should automatically pick up the changes and compile a new bundle.\n\n\n\n\n\nNow that we\u2019ve handed off the build process webpack, only thing we need on the django side is to know which bundle to include in our html pages. This is where comes in. It\u2019ll also raise exceptions when webpack fails to build a bundle and django will show some useful information when in debug mode. To make sure django loads latest bundles during development, webpack loader will block any requests whenever it find webpack compiling code if .\n\nwill render the required and tags in the template.\n\nEverything we need is in place now. Bundles will be automatically generated (provided you start webpack with \u2014watch) and django will automatically pick up latest bundles from directory. In development, django will also block any requests while the bundles are being compiled. It\u2019ll also raise raise if webpack fails to build at which point we can turn to our webpack process for more details.\n\nNow that we\u2019ve webpack working with django, let\u2019s make things a little more fun by setting up hot reloading for our react components.\n\nSince we are using pure webpack without any abstraction, we are free to use it however we want without the need to integrate any special with django. Whenever something new comes up for webpack, we can immediately use it without worrying if staticfiles, pipeline or compressor will support it or not. Decoupling FTW!\n\nWe\u2019ll use a library called react-hot-loader by Dan Abramov. We\u2019ll also need webpack-dev-server to build and serve our bundles if we want to hot reload any modules.\n\nLet\u2019s modify to use webpack-dev-server and react-hot-loader\n\n var path = require(\"path\");\n\nvar webpack = require('webpack');\n\nvar BundleTracker = require('webpack-bundle-tracker');\n\n\n\n\n\nmodule.exports = {\n\n context: __dirname,\n\n entry: [\n\n 'webpack-dev-server/client?http://localhost:3000',\n\n 'webpack/hot/only-dev-server',\n\n './js/index'\n\n ],\n\n\n\n output: {\n\n path: path.resolve('./assets/bundles/'),\n\n filename: \"[name]-[hash].js\",\n\n publicPath: 'http://localhost:3000/assets/bundles/', \n\n },\n\n\n\n plugins: [\n\n new webpack.HotModuleReplacementPlugin(),\n\n new webpack.NoErrorsPlugin(), \n\n new BundleTracker({filename: './assets/webpack-stats.json'}),\n\n ],\n\n\n\n module: {\n\n loaders: [\n\n \n\n { test: /\\.jsx?$/, exclude: /node_modules/, loaders: ['react-hot', 'babel'], },\n\n ],\n\n },\n\n\n\n resolve: {\n\n modulesDirectories: ['node_modules', 'bower_components'],\n\n extensions: ['', '.js', '.jsx']\n\n },\n\n}\n\n\n\nInstead of running , we\u2019ll run webpack-dev-server to both compile and serve our bundles. The server will run on port 3000. in our webpack config refers to this server. Note that the server will by default keep the bunles in memory and not write to disk, so don\u2019t be surprised if you don\u2019t see anything new in . Let\u2019s use webpack-dev-server\u2019s API to create a new instance of the server and pass webpack initialized with out config file to it. We\u2019ll store this as in our project root and use node run the server.\n\nNow instead of running webpack in watch mode, we run webpack-dev-server like this\n\n\n\nDone! Any changes made to the react components will reflect in the browser. No reload needed. Magic! right?\n\nIf you are interested in hot reloading react components, you should read this https://medium.com/@dan_abramov/the-death-of-react-hot-loader-765fa791d7c4\n\nProduction bundles are different than local ones for various reason. I like to have slightly different webpack config for production, generate them locally and commit the bundle(s) and stats file to the codebase. As we store our bundles in assets and django is configured to look for static files in the assets directory, we don\u2019t need to do anything special to with the bundles once they are generated. If you run collectstatic on your production servers after fetching new code, new bundles will be\n\nthere. If you configure static storage to use S3, then the bundles will be synched automatically to S3 ready for use in production.\n\nImporant: Make sure production config doesn\u2019t use react-hot-loader or webpack-dev-server. Also make sure you use something like Uglify to compress your code and strip off any code only meant to be used in development.\n\nNote: You should add our local webpack stats file and local bundles to .gitignore as they serve no purpose outside your local environment.\n\n\n\n var config = require('./webpack.config.js');\n\n\n\nconfig.output.path = require('path').resolve('./assets/dist');\n\nconfig.output.pathName = '/production/path/to/bundle/directory'; // either this or define WEBPACK_LOADER['BASE_URL'] in settings.py\n\n\n\nconfig.plugins = [\n\n new BundleTracker({filename: './assets/webpack-stats-prod.json'}),\n\n\n\n // removes a lot of debugging code in React\n\n new webpack.DefinePlugin({\n\n 'process.env': {\n\n 'NODE_ENV': JSON.stringify('production')\n\n }}),\n\n\n\n // keeps hashes consistent between compilations\n\n new webpack.optimize.OccurenceOrderPlugin(),\n\n\n\n // minifies your code\n\n new webpack.optimize.UglifyJsPlugin({\n\n compressor: {\n\n warnings: false\n\n }\n\n })\n\n];\n\n}\n\n\n\nGenerate production bundles by invoking webpack one time with prod config\n\n\n\nThis will create production bundles in and"},
{"url": "http://tonyday567.github.io/blog/mvc-todo/", "link_title": "Haskell as a JavaScript MVC framework", "sentiment": 0.05151360544217687, "text": "It\u2019s a bit rough around the edges, but I\u2019ve found the mvc-todo project to be quite fun so far. It contains a coding of the todoMVC standardized app using haskell via ghcjs. The resulting todo list page looks just like all the others , but hidden in this boring are some tiny slices of awesomeness.\n\nThe first thing to like is that the page is 99% haskell . It\u2019s not a web page describing haskell code, nor haskell being used to generate a page, nor even a subset or language inspired by haskell. Full-blown haskell code smashed directly into a web page for anyone to run just feels a little bit special.\n\nMore concretely, the project is an opportunity to directly compare functional and imperative style, within the context of a well defined, popular problem domain (single-page front-end MVC development).\n\nOne interesting comparator is how actions are treated in functional versus imperative styles. I started the spec grind with this basic plan in mind:\n\nHere\u2019s the data type for UI actions, which represents all the various ways a user can produce an effect:\n\nChecking in with the other frameworks I discovered that only one of the 68 frameworks defined an Action, because javascript doesn\u2019t have a sum type . In imperative land, actions tend to be functions rather than things. So saying this:\n\nis unnatural, because there is no such thing as an action.\n\nSum types are oh so neglected in mainstream programming. Search for sum types in wikipedia and you\u2019re actually redirected to tagged union ! Why the blind spot? One clue comes from this quote from the inventors of MVC:\n\nSimplistic maybe, but also simply wrong - no linguist would ever sign up to this rigid demarcation between action words and naming words. Category theory leads us to a better definition: objects can be anything (any thing). Even action words!\n\nThus armed with sum types, a language can completely represent an covering any and all UI effects, and overall design tightens up as a consequence. In this todoMVC space:\n\nAll of which makes functional programming an interesting newcomer in the one-page front-end development biz. It\u2019s going to become much harder for haskell to avoid success with this level of real world interaction coming within reach."},
{"url": "https://medium.com/@jtwarren/event-processing-in-rails-83b76f33b973", "link_title": "Event processing in Rails", "sentiment": 0.053524831649831635, "text": "When I think about implementing something like this in Rails, I want it to be clean and out of the way. In the example above, I could imagine a naive approach calling two separate service objects after creating a post\u200a\u2014\u200aone call for sending an email, and one call for checking content and flagging posts. This to me seems like poor design. What I\u2019d really like to do is just trigger an Event on a post creation and have configuration around event listeners out of my API. This allows anyone to listen (or consume) an event without continuously adding code to the API. All that is really needed here is to add an event consumer and have it listen to the appropriate events.\n\nWorking backwards with this problem, we have a few different consumers that need to somehow be called when an event is triggered. Looking to Rails for help we see that ActiveSupport::Notifications is used by Rails itself for a similar problem, logging and instrumenting various events. The next step is to figure out how to tie each consumer to a given event. This is super easy with Notifications and writing the initializer is trivial.\n\nHere we have a POST_CREATION event being mapped to several consumers\u200a\u2014\u200aone for emails, one for spam, and another for logging to Kafka. The functionality of the consumers is understood, and this nice thing is that they\u2019re logically different and do not need to interact with each other. I do want to note that the above functionality could be abstracted into the consumers themselves, but I prefer explicitness of defining the mapping in this scenario.\n\nBecause we want ultimate flexibility with our consumers, we define them to take in a Ruby Hash called a payload. We\u2019ll define a method named `call` on consumers, one that will be called when an event is triggered.\n\nNow, any consumer can do with the payload whatever they want or need to. Writing a new consumer for an event becomes really easy and doesn\u2019t need interaction or coordination with the API controllers at all.\n\nLet\u2019s say that we wanted to add a new consumer to do something when a post is created. Perhaps we want to do a bit of text analysis (a little tf-idf trickery) and determine what the post is about so that we can either add it to a category or surface it in recommendations.\n\nHere we have written a simple consumer that enqueues the analysis for asynchronous execution. While there\u2019s obviously post analysis code missing from here, the only changes we really needed were to write the consumer and update our EVENT_CONSUMER_MAPPING.\n\nThe final step is to define what an event is and create an easy way to trigger them. Here, I have defined a base class for such an event, and defined a few methods to help keep things clean.\n\nNow we can finally define our post creation event."},
{"url": "https://www.youtube.com/watch?v=2-3wcYXJm3Y", "link_title": "10 Lessons for Foreign Entrepreneurs in Sillicon Valley by Tytus Cytowski", "sentiment": 0.3458333333333334, "text": "1:4:95: 95% of the people in the world have no clue where to go in life, 4% know where to go but don't know how to get there, 1% do know, and also know how to get there.\n\n\n\nTytus Cytowski walks us through the how and why to enter Silicon Valley as entrepreneurs. He uses his legal expertise to walk us through several catch-22 situations it's best we're made aware of and in the end formulates a simple set of lessons and \"silicon valley secrets\".\n\n\n\n---------------------------------------- \u00ad\u00ad------------------------------\n\nThis, that, and much more from us, at http://courses.platzi.com\n\n\n\nSubscribe to us!\n\nTwitter: http://twitter.com/IamPlatzi\n\nFacebook: http://facebook.com/IAmPlatzi"},
{"url": "http://shoryuken.com/2015/05/21/human-reaction-times-and-fighting-games-or-why-blocking-isnt-always-easy/", "link_title": "Human Reaction Times and Fighting Games, or Why Blocking Isn\u2019t Always Easy", "sentiment": 0.06826574605335665, "text": "Frame data matters to some extent in every competitive game. Many of these games, however, lack a static framerate against which to compare these numbers, so any gathered frame values are, at best, a rough estimate of the times that players can expect before the attack in question connects with their character.\n\nSince fighting games do run at a static 60 frames-per-second, we are able to discern the exact fraction of a second that we have to react to any given attack. Infil, on their blog\u00a0dedicated to helping Street Fighter players transition to Killer Instinct, recently put up an excellent post\u00a0discussing how human reaction time ties into the latter\u00a0in particular. Through some simple math, they were able to discern that the average human reaction time of about 265 milliseconds equates to roughly 16 frames in a 60fps game. If you\u2019d like to test your own reaction time to see how you compare to the rest of humanity, you can do so by following this link.\n\nThis average reaction time means that any attack with a startup time above 16 frames should afford enough of a gap for a reasonable reaction\u2014blocking, in particular. But Infil makes it a point to note that this reaction speed relies upon a single input from an expected source, like clicking when the screen turns green or, in fighting game terms, blocking an overhead. As soon as other stimuli begin piling onto this single source\u2014lows, throws, potential unblockables that require an entirely different form of defense\u2014things get a lot hairier.\n\nWith this in mind, here\u2019s a more FGC-friendly (and I use the word \u201cfriendly\u201d quite loosely) reaction speed benchmarking tool designed by Teyah and posted to the Dustloop forums back in January of 2011. Instead of providing\u00a0a simple red-to-green shift\u2014as offered in the test linked above\u2014to denote when one should press an appropriate button, users of this application are tasked with blocking Millia Rage\u2019s overhead options. That\u2019s all. However, these animations push human reaction times to the absolute limit, thanks to their sheer speed. Don\u2019t flinch when she goes for a low, either, as pressing anything against a low will also count as a hit for Millia. It also counts what Teyah appropriately calls \u201cyomi blocking\u201d as a fault, so you have to actually be reacting to Millia\u2019s animations, not simply guessing.\n\nIt\u2019s hard. It\u2019s genuinely hard. Try it for yourself.\n\nThese tight reaction times and the number of actual reactions possible is why, in a game like Ultra Street Fighter IV, we occasionally see players not use an overhead for an entire set, and then throw it out after a long string of hits to secure a stun in the final round to take the tournament. The overhead wasn\u2019t even on the radar for the opponent that found themselves on its receiving end, so it hit.\n\nWith regards to Killer Instinct in particular, Infil cites various bits of frame data for comparison against the average human reaction time, and there are plenty of things to choose from. From the grab bag of potential examples, they pulled Sabrewulf\u2019s Jumping Slash overhead because, \u201cyou execute the move out of a running attack, which also has a low option (Hamstring).\u201d So immediately, a defender has two sources (high and low) to potentially block on reaction. Infil goes on to state that \u201cJumping Slash has 15 frames of startup, which is already on the low side of the reaction limit, but it also doesn\u2019t have a particularly obvious visual tell until the move is almost ready to hit you. This means Jumping Slash is designed to be mostly unreactable to the average gamer.\u201d\u00a0In other words, defenders have to read their opponents and predict which option they\u2019ll take and block appropriately before the attack even begins.\n\n These same sort of tight reaction windows occur within the game\u2019s combo-breaking system as well, with light auto-doubles allowing only 14-15 frames to react and manuals offering as few as 5 frames.\n\nThis got me thinking about Mortal Kombat X\u2014as plenty of things do at the moment\u2014and the way that 50/50 okizeme dominates the current playing field. This is one of the reasons that a character\u2019s strength (or lack thereof) is so heavily influenced by their ability (or inability) to execute fast overhead and low-hitting attacks that\u00a0convert into combos.\n\n[The frame data used below is taken from the current PC build of Mortal Kombat X; these numbers may have shifted slightly in either direction due to corrections or slight changes implemented in the most recent console patch.]\n\nThere are certain characters\u2014Bojutsu Kung Jin is an excellent example\u2014who can easily dish out over 30% damage from landing a low or an overhead. Here are the startup times for two of his openers:\n\nMuch like Sabrewulf\u2019s Jumping Slash, these attacks are awfully close to the 16 frames of average reaction time. Add in the threat of a throw and those defending are better off spending meter in order to escape pressure through an armored attack than actually try to block, unless their opponent seems to always go for the same okizeme option.\n\nThe takeaway here is that we\u2019ve all probably got faster-then-normal reaction times by now, but we can\u2019t react to everything. This drives home the importance of a varied approach to the offensive side of whatever fighting game you prefer. Don\u2019t always go for a low. Don\u2019t always throw. Be sure to mixup your approach, but also know when to keep some of your options out of the opponent\u2019s mind\u2014like the overhead in the Ultra Street Fighter IV\u00a0example above\u2014to throw out when they least expect them. And if you get hit with the occasional mixup, don\u2019t get discouraged; we\u2019re all only human.\n\nAnd don\u2019t forget to head over to Infil\u2019s blog to check out their full post regarding Killer Instinct and reaction times. It\u2019s a great read."},
{"url": "http://www.wired.com/2015/04/ex-machina-turing-bechdel-test/", "link_title": "Ex Machina Has a Serious Fembot Problem", "sentiment": 0.12567312152133578, "text": "The Turing test detects if a machine can truly think like a human. The Bechdel Test detects gender bias in fiction. If you were to mash the two together to create a particularly messy Venn diagram, the overlap shall henceforth be known as the Ex Machina Zone.\n\nIn writer/director Alex Garland\u2019s thought-provoking new film\u2014out Friday\u2014we meet Ava (Alicia Vikander), an artificially-intelligent robot. Ava\u2019s creator, genius tech billionaire Nathan (Oscar Isaac), has asked his employee Caleb (Domhnall Gleeson) to determine whether Ava\u2019s thinking is indistinguishable from a human\u2019s. Until she meets Caleb, Ava has only ever met her maker and one other woman. (Hence the failing of the Bechdel Test, which stipulates that a movie must feature two female characters who talk to each other about something other than a man.) Her existence, and her ability to learn how to interact, is a fascinating study of what makes us human.\n\nIt\u2019s also a compelling, if problematic, look at the interactions between men and women\u2014or at least that\u2019s what I thought.\n\nWhile interviewing Garland for a magazine piece, I asked him about the roles of men and women in his film; his response was that Ava is \u201cnot a woman, she is literally genderless.\u201d Despite using female pronouns, he said, \u201cthe things that would define gender in a man and a woman, she lacks them, except in external terms. \u2026 I\u2019m not even sure consciousness itself has a gender.\u201d\n\nIn a way, Garland is right; pure intelligence wouldn\u2019t have a gender any more than it would have a race. But to say that and then place that consciousness into a body that it will immediately recognize its likeness as female negates that point. If Ava has truly been educated about the human race, then she knows her face and form appeal to certain segments of the population. But even thornier is the fact that Ava falls squarely into so many of the tropes of women in film. She\u2019s a femme fatale, a seductress posing as a damsel in distress, using her wiles to get Caleb to save her from Nathan and his Dr.-Frankenstein-with-tech-money quest to build a perfect woman. (Women: So much better when you can construct them out of bespoke parts and switch them off if they\u2019re not working properly, amirite?)\n\nAccording to Garland, these tropes are intentionally front-and-center. He believes his movie is a commentary on the \u201cconstructs we\u2019ve made around girls in their early 20s and the way we condition them culturally\u201d and why Caleb would feel the need to save her from her maker. \u201cYou\u2019re supposed to think it\u2019s creepy,\u201d he says. \u201cYou\u2019re not supposed to warm to [Nathan] over that stuff, you\u2019re supposed to feel unnerved, and therefore that she needs to be rescued.\u201d\n\nYet, in the pursuit of that commentary, the movie ends up re-enacting those same patterns. Ava does prove to be the smartest creature on the screen, but the message we\u2019re left with at the end of Ex Machina is still that the best way for a miraculously intelligent creature to get what she wants is to flirt manipulatively. (And why wouldn\u2019t she? All of her information about human interaction comes from her creepy creator and the Internet.) Why doesn\u2019t Chappie have to put up with this bullshit?\n\nAva\u2019s predicament really isn\u2019t that different from many female AIs who have come before her, from Metropolis\u2019 Maria to Her\u2019s Samantha to Blade Runner\u2019s Pris. She is an android in female form, and thus she simply reflects how Hollywood has been depicting women\u2014robotic or otherwise\u2014for decades. In Blade Runner, the male replicants Roy Batty and Leon are struggling to change their short lifespans, while \u201cbasic pleasure model\u201d Pris helps the cause by draping herself on J.F. Sebastian. In Prometheus, David is intellectually curious, but never sexualized. (Yet when Idris Elba\u2019s Janek accuses Charlize Theron\u2019s Meredith Vickers of being a robot, she responds with \u201cMy room. Ten minutes.\u201d Because sex is the easiest way to prove you\u2019re a real woman.) Sentient male androids want to conquer or explore or seek intellectual enlightenment; female droids may have the same goals, but they always do it with a little bit of sex appeal, or at least in a sexy package. (Still have doubts? Ex Machina\u2019s marketing campaign at South by Southwest involved Ava showing up on Tinder.)\n\nThis tendency to give female AIs the most basic and stereotypical feminine characteristics is, according to Kathleen Richardson, a senior research fellow in the ethics of robotics at De Montfort University in the UK, probably a reflection of \u201cwhat some men think about women\u2014that they\u2019re not fully human beings.\u201d To put a finer point on it, she told Live Science recently, \u201cwhat\u2019s necessary about them can be replicated, but when it comes to more sophisticated robots, they have to be male.\u201d\n\nWhen I spoke to Richardson, author of An Anthropology of Robots and AI: Annihilation Anxiety and Machines, she also noted this leads to female robot characters becoming just pieces of full people\u2014a beautiful body, a caretaking nature\u2014but not ones with full intelligence. This is largely true in Ex Machina\u2014and not just because Nathan has a lab full of body parts\u2014but also in a lot of movies where the artificial intelligence has to be packaged in a certain way if the robot is perceived to be female. (She also notes the real robotics world suffers from the same problems as a lot of AI fiction, but that \u201cmany robotic scientists are open to a conversation about this.\u201d)\n\nWomen, whatever their qualities\u2014intelligent, vulnerable, strong\u2014are always presented in an attractive form, as if the package is the only way to deliver these qualities. Kathleen Richardson\n\n\u201cSometimes the female robots have \u2018violent\u2019 characteristics (as Terminator 3\u2019s T-X character), but it\u2019s always presented in a beautiful form,\u201d Richardson says. \u201cWomen, whatever their qualities\u2014intelligent, vulnerable, strong\u2014are always presented in an attractive form, as if the package is the only way to deliver these qualities. Male intelligence, strength, vulnerabilities, etc. can be delivered in a multiple and varied kind of outer packaging.\u201d\n\nThink of it this way: Ava demonstrates her consciousness/intelligence in a form and with a sensuality that David in Prometheus never had to. Short Circuit\u2019s Number 5/Johnny Five was cute, but he never had to employ it for survival the way Pris did in Blade Runner. Even AIs with no physical form at all seem to get sexualized based simply on their voices. It\u2019s not like HAL 9000 ever sparked up a relationship with Dave in 2001: A Space Odyssey the way Samantha did in Her. \u201cHer is playing on the fact that the audience knows what [Scarlett Johansson] looks like,\u201d Richardson says. \u201cNo one really needs to know who the voice of HAL was, because HAL was an intelligent machine. We need to know about the disembodied voices of our AI avatars if they\u2019re female so that males can buy into the ideas of the sexualized person behind the representation.\u201d\n\nIf this argument about the roles women get in movies versus the roles men get is starting to sound familiar, it should. Ever since Laura Mulvey\u2019s 1975 essay \u201cVisual Pleasure and Narrative Cinema\u201d film critics and fans have been monitoring the ways that women are represented and seen onscreen. (If you\u2019ve heard the term \u201cthe male gaze,\u201d this is why.) This ongoing discourse is the reason thing like the Bechdel Test, which started out just as a comic trip referencing Alien, struck a nerve and stuck around. The thrust of Mulvey\u2019s argument is that the bulk of films are seen from a male perspective\u2014that a woman in a film is often \u201ctied to her place as bearer of meaning, not maker of meaning.\u201d Yes, Ava learns to use seduction as manipulation, and the audience might learn how screwed up that is because it\u2019s more blatant when even a robot can pull it off, but Ex Machina doesn\u2019t go any further in deconstructing the problem than that. She\u2019s a bearer, not a maker.\n\nTo be fair, not all of this is Ex Machina\u2019s fault\u2014or the fault of any AI film. Often, social constructs mandate that we gender most things, whether they\u2019re intended to be gendered or not. Interstellar\u2019s TARS looks like a Mies van der Rohe Kit-Kat bar, yet we refer to TARS as \u201chim.\u201d Is that because of the machine\u2019s deep(ish) voice or because narrative constructs lead us to believe robots with scientific intellectual aims are masculine?\n\nIt\u2019s nearly impossible to tease the two apart, and that knottiness is baked into British computer scientist Alan Turing\u2019s original test in a way that can never be removed. If the goal is for a machine can convince a human that it\u2019s human, then the machine has to assume some kind of gender because we see all humans as having a gender. Even if, in the Turing test model, a judge is just looking at the output of a chatbot, he or she would ascribe gender to the output without even thinking about it. (Note the chatbot that convinced judges that it was real last year did so by convincing them it was a 13-year-old boy named Eugene.)\n\nEx Machina sidesteps this a bit by making Eva visible; Caleb he knows he\u2019s talking to a robot, and knows what that robot looks like. Nathan just wants to \u201cshow you that she\u2019s a robot and then see if you still feel she has consciousness.\u201d What Nathan actually wants Caleb to do is something more akin to Blade Runner\u2019s Voight-Kampff test, where the subject can flirt Sean Young-style but you know she\u2019s a replicant. But if that\u2019s the case, why does so much of Ex Machina focus on her proving that consciousness through flirtatious interactions and not, say, a discussion of the horrors of war? Johnny Five discovered mortality by crushing a grasshopper leading to a fear of being switched off, and we felt his plight all the same, why not Ava? (Don\u2019t answer that.)\n\nThe thing is, Alan Turing himself actually might have wanted AI to be something akin to what Caleb wants: actual companionship. When WIRED spoke to screenwriter Graham Moore about The Imitation Game back in November he noted that much of Turing\u2019s work in AI was about \u201cbringing Christopher [Morcom, Turing\u2019s first love] back.\u201d But while Turing, if he would\u2019ve ever been able to rebuild Morcom, would\u2019ve been making someone he could talk to and share ideas with, the female representations of Turing\u2019s dream in movies often personify it through far less intellectual pursuits. Think of David in Prometheus; his primary goal was assisting on the mission, not seducing Vickers. As a \u201cmale\u201d AI in a film he was given an intellectual pursuit, not a romantic one. Is it possible Ava could\u2019ve convinced Caleb she passed the test with fewer pleading glances and more analysis of world affairs? What would Ava have done to pass if she was a he?\n\nObviously, wanting affection is part of what makes us human; by showing that, Ava is showing a highly-evolved part of herself. But by only showing that, and her highly manipulative nature, she is left as a less-than-whole character. She\u2019s almost the colder, darker side of Her\u2019s Samantha, who served as a Manic Pixie Dream Operating System. Like Her, Ex Machina is a smart, beautiful film. But when the only female lead in your movie is one whose function is to turn the male lead on while being in a position to be turned off, that says a lot about what you think of the value of women in films. Saying it\u2019s the result of your protagonist being \u201ccreepy,\u201d as Garland does, doesn\u2019t really absolve you of that."},
{"url": "http://www.lego.com/en-us/aboutus/news-room/2012/june/guiness-world-record-to-the-lego-group", "link_title": "The world's largest tyre manufacturer (not what you'd think)", "sentiment": 0.16355555555555557, "text": "It may come as a surprise that a staggering 318 million LEGO tyres are produced each year, that\u2019s over 870,000 each day!\n\n \n\n The LEGO factories produces tyres 24 hours per day, 365 days per year because nearly half of all LEGO sets include a wheel of some sort.\n\n \n\n Emma Owen, PR & Promotions Manager at LEGO UK says: \u00a0\u201cBeing awarded a Guinness World Record for largest tyre manufacture per annum is likely to be a surprise to many, but not to us and we are thrilled to take this record, 50 years after the invention of the LEGO tyre.\u00a0 Naturally we have previously broken records for creating the tallest LEGO towers, however with LEGO bricks, creativity holds no bounds and we have adopted this theory and looked outside the box with this latest world record challenge.\u201d\n\n \n\n Until the invention of the LEGO wheel in 1962, children simply simulated wheels with LEGO bricks.\u00a0 Today, with every second LEGO set featuring wheels, from police cars to safari jeeps, imaginations can go into overdrive.\n\n \n\n Wheel Facts\n\nThe LEGO Group produces 381 million wheels a year.\n\n \n\n Before the LEGO wheel was introduced, some LEGO sets included pre-moulded miniature cars, or children simulated wheels by using their LEGO brick collection.\n\n \n\n The first LEGO wheel was included in set No.400, which launched in 1962.\u00a0 It was the best selling set in 1967 selling 820,400 boxes.\n\n \n\n The smallest LEGO wheel stands 14.4mm high and belongs to a small two-seater car.\n\n \n\n The largest LEGO wheel stands at 10.7cm high and features on the Power Puller, launched in 2000."},
{"url": "https://slack.com/jobs", "link_title": "Work for Slack \u2013 but mostly (only?) in an office", "sentiment": 0.26791666666666664, "text": "Slack is a team communication tool. It brings together all of your team communications in one place, instantly searchable and available wherever you go. Launched in February 2014, it is now the fastest growing B2B application ever and is used by over 750,000 daily active users. Learn more\n\nWe're a San Francisco-based company founded by core members of the original Flickr team. Our investors include Accel Partners, Andreessen Horowitz, The Social + Capital Partnership, KPCB and Google Ventures. Our mission is to make people's working lives easier, simpler, and more enjoyable.\n\nWe want great talent and inspiring leaders in all areas: from business operations to product design and general management. If you want to join a small startup that's making an outsized impact (and that's making a lot of people happy) please get in touch. We value diversity, experience, gumption and panache. And a good vocabulary.\n\nInterested in a position but don't see it listed below? Please send your r\u00e9sum\u00e9 to work@slack.com with the desired position in the subject line and we will consider you for future openings. Please be sure to include a helpful message introducing yourself.\n\nIf you don't want to work here and you want to ask us a question about something else please open a support request."},
{"url": "https://medium.com/chrysaora-weekly/your-project-deserves-a-good-death-f345026b6e77", "link_title": "Your Project Deserves a Good Death", "sentiment": 0.063227219934537, "text": "Adapted from a talk I gave at Brio in 2013, one of the most important conferences I\u2019ve ever attended. I am still thankful to Adam for running it and to Diana for scheming to get me invited.\n\nMost of the people I know are compulsive starters. We constantly create new projects, companies, organizations, and events; sometimes, we even get roped into adopting other people\u2019s projects and entities. Some of these start as late-night jokes with friends we want to spend more time with, some are the next logical steps in our careers. Some are born out of frustration-fueled insight; some even become the thing that defines us for the rest of our lives.\n\nAlmost all of them will not be the last thing we start.\n\nWhen and if any of these projects take off\u200a\u2014\u200aif people come to your impromptu event, use your slapdashed prototype, or start participating in your movement\u200a\u2014\u200ayou\u2019ll often find an overwhelming pressure to keep it going. We have this idea that real success is measured in growth and longevity. That if you do something well once, you should be so grateful that you should do it again, and bigger, and for as long as you can. As entrepreneurs and organizers, this idea is deeply ingrained into our social norms: our projects and companies are actually evaluated on whether they have ambitious growth forecasts and plans for sustainability. But there are lots of external constraints on the growth and life of projects other than your ability to make them succeed\u200a\u2014\u200anot least your own enthusiasm, tolerance, and availability.\n\nThe unfortunate flip-side of a growth-obsessed culture is that we equate project endings with failures. We talk about project death with the same hushed tones and awkward euphemisms as we do death or broken relationships, which is to say that we try not to talk or think about it at all.\n\nAll of this means that we are generating a huge number of projects and entities that don\u2019t need to last forever without doing an adequate job of planning for these many inevitable ends. And so we avoid ending things altogether. We drag projects along well past their usefulness. We don\u2019t give them proper closure because thinking about them makes us feel guilty, and announcing them makes us feel weak. Instead they quietly disappear, unmemorialized and undocumented.\n\nThere are many types of undignified deaths for projects and entities, such as:\n\nEndings don\u2019t have to be failures, but if they almost certainly will be if you don\u2019t plan for them."},
{"url": "http://www.quora.com/How-did-Mark-Cuban-save-his-wealth-from-the-dot-com-crash?share=1", "link_title": "How did Mark Cuban avoid the crash?", "sentiment": 0.12662623539982032, "text": "[Re \"survive\", a phrase like \"preserve his net worth through\" or \"thrive during\", is more accurate and less insulting to at least a billion people on Earth actually struggling to survive every day.]For those who did not understand Mark Cuban's own answer or its technical terms, I will explain what he did \"during the lockup period\"1. Any time more than about 20% of your wealth is in one stock you are \"overexposed\" to that stock.\u00a0 For most people, not just the wealthy, a sudden loss of half of that value would be a 10% drop in net worth.\u00a0 That could cut your credit or trigger time wasting banking or asset sales that could undermine confidence of others... Start rumours... Margin calls... Cost you far more than that first 10%.2. \"Hedging\" is insurance against overexposure.\u00a0 It is mandatory in some financial fields like commodity crops (wheat, soy, corn, etc) because most farmers are overexposed to low yields due to pests or weather and also overexposed to low prices when yields are high.\u00a0 It is mandatory for all responsible portfolio management or funds in practice but it's hard to enforce in law.\u00a0 Vast Depression scale crashes happen because of overexposure to say residential real estate prices peaking due to NINJA loans.\u00a0 Its very important.Globally the Financial Standards Board is negotiating Basel 4 to force big banks to \"stress test\" holdings for overexposure to very unusual and unanticipated scenarios including sudden crashes.\u00a0 But it will remain your responsibility to hedge your own exposures.An excellent book on this is \"Seeing Tomorrow\" by Ron Dembo and Andrew Freeman.\u00a0 Its about hedging and scenarios anyone can do, using Ron's approach to minimize regret.\u00a0 Real estate examples and many others are given.3. With almost all his wealth in Yahoo stock, and unable to sell it because of agreements that such deals always contain to prevent huge stock sales immediately after an acquisition for stock, Mark had a problem.4. It is quite inexpensive or initially free to buy reliable insurance against overexposure to a single widely publicly traded stock.\u00a0 One approach combines derivatives that are cheap or worthless at the moment, but will pay off if the stock drops catastrophically.\u00a0 While you may have to pay if the stock rises, plus whatever fees or derivatives prices, you don't care, since you own that rising stock too.Unfortunately Mark's agreement with Yahoo and securities rules prohibited short dealing by an insider who suspects the stock will drop.\u00a0 Typically in an acquisition no such bets directly against the stock or sales are allowed for months.\u00a0 So he had to find another solution.5. Index funds that included more than 5% Yahoo stock were also ruled out, but Mark found one with almost all its money in Internet (which would almost certainly fall with Yahoo) but"},
{"url": "http://www.viabilify.com/ops", "link_title": "Startup Operations Are Super Fertile Grounds for Learning", "sentiment": 0.26579161579161575, "text": "It's been three full years since I walked on stage\u00a0with my cap, gown, and flip flops (yeah... that happened) to accept my college diploma.\n\nI learned a ton in the first couple years by working in investment banking and founding my own startup.\n\nThe last year has been with\u00a0an awesome startup called Life360\u00a0which doubled in headcount\u00a0since I joined. Here are some things I've learned in operations\u00a0at a company with this kind of\u00a0growth.\n\nThe fastest learning\u00a0happens\u00a0by osmosis. I learn the most when I\u00a0experience\u00a0an ecosystem first-hand. Reading or writing about it will take me down a linear learning curve and that's much, much\u00a0flatter (and slower) than the exponential one that comes with just being there or seeing it yourself.\n\nThere's only so much another person can tell you about the best way to do your\u00a0job.\u00a0I can ask a bunch of questions and read about how other people do something, but at the end of the day you join a startup\u00a0to figure out your piece of the puzzle and own it.\n\nLearning also has a risk/return tradeoff. To learn how to do something, I have to take 100% of the risk required to learn how to do it. Risk of failure is the\u00a0price of admission to true learning.\n\nOver-communicating with people you\u00a0work with ensures you're on the same page. Progress (or lack thereof)\u00a0is hardly surprising when you're\u00a0constantly communicating with precisely accurate nouns, verbs and adjectives.\n\nLike many things, zoom out in moderation. Asking\u00a0the right big picture questions the right number of times will solidify the big picture in my mind and help me think like my bosses (and their bosses)\u00a0do. Then I can zoom back into my work and figure it out.\n\nExecute with cross-functional considerations.\u00a0You can only be in one meeting at a time so\u00a0by definition you'll be excluded from every other overlapping meeting at the\u00a0company (and the\u00a0world). Finding creative ways to get the download\u00a0from other meetings makes sure you don't work in a vacuum. You can also think of this as a way to reduce \"oh shit\" moments.\n\nThinking of a better\u00a0way to do something is 1% as impressive as doing something in a better way. There's a time for planning, of course. But execution is all that matters ultimately. I've realized I\u00a0love working with people who have a bias towards action."},
{"url": "http://www.viabilify.com/support", "link_title": "A Startup Framework for Supporting Millions of Users", "sentiment": 0.19505662687480868, "text": "That's a lot of P's but it's true. Gathering information about upcoming features and changes to the existing UI is crucial in building a world-class Customer Support department. CS has to know 100% of what's going live in the product!\u00a0Otherwise\u00a0the customer may end\u00a0up knowing more than the agent does about the product. As most of us know from our experience as customers, this is the absolute worst customer experience imaginable.\n\nBy the same token, we take it as our job to anticipate customer questions in advance. If something looks confusing and we believe it might not be intuitive to the customer, our product managers listen and incorporate our feedback into the final releases.\n\nWe've taken several steps to ensure excellent\u00a0communication between CS and Product\n\nWe always have our finger on the pulse of releases and we pride ourselves on\u00a0knowing about release dates and the items in each release as well as anyone else at the company. We've grown\u00a0from being unsure and surprised to being confident and prepared. Likewise, the product team has a much healthier understanding of how our customers view our service and are better equipped to delight them.\n\nThere are a bunch of small processes we've established with the rest of the company but the\u00a0one we've seen pay the most dividends so far has been our CS Ridealongs.\n\nA lot of systems and tools have been inherited by those operating in the sixth-seventh year of a company's existence. Since most employees have been at the company less than a couple years, the\u00a0feedback we've received from dozens of ridealongs so far can be summarized as: \"holy shit, I had no idea any we had any of this but this is awesome to know.\"\n\nCS Ridealongs are very simple. We sit down with each employee (for new hires this is done in their first week at the company) for 90 minutes and give a comprehensive overview of our users and the tools we use to communicate with them. The scope of topics covered includes payment processing, our help desk software, database queries, company org chart, and their favorite -\u00a0listening to a live customer call\u00a0and consequently discussing it.\n\nGreat employees are inspired and empowered by seeing the belly of the beast; some of the most valuable\u00a0information we receive comes from employees who go the extra mile to inform\u00a0CS because of\u00a0their CS Ridealong."},
{"url": "http://www.scottaaronson.com/blog/?p=2293", "link_title": "NSA in P/poly: The Power of Precomputation", "sentiment": 0.07745582090368823, "text": "Even after the Snowden revelations, there remained\u00a0one big mystery about what the NSA was doing and how. \u00a0The NSA\u2019s classified 2013 budget request mentioned, as a priority item,\u00a0\u201cgroundbreaking cryptanalytic capabilities to defeat adversarial cryptography and exploit internet traffic.\u201d \u00a0There was a requested increase, of several hundred million dollars, for \u201ccryptanalytic IT services\u201d and \u201ccryptanalysis and exploitation services program C\u201d (whatever that was). \u00a0And a classified presentation slide showed encrypted data being passed to a high-performance computing system called \u201cTURMOIL,\u201d and decrypts coming out. \u00a0But whatever was going on inside TURMOIL\u00a0seemed to be secret even within NSA; someone at\u00a0Snowden\u2019s level wouldn\u2019t have had access\u00a0to the\u00a0details.\n\nSo, what was (or is) inside the NSA\u2019s cryptanalytic black box? \u00a0A quantum computer? \u00a0Maybe even one that they bought from D-Wave? \u00a0(Rimshot.) \u00a0A fast classical factoring algorithm? \u00a0A proof of P=NP? \u00a0Commentators on the Internet rushed to suggest each of these far-reaching possibilities. \u00a0Some of us tried to\u00a0pour cold water on these speculations\u2014pointing out that one could envision\u00a0many\u00a0scenarios\u00a0that were a little\u00a0more prosaic, a little more tied to the details of how public-key crypto is actually used in the real world. \u00a0Were we just na\u00efve?\n\nThis week, a\u00a0new bombshell 14-author paper\u00a0(see also the website) advances an exceedingly plausible hypothesis about what may have been\u00a0the NSA\u2019s greatest\u00a0cryptanalytic secret of recent years. \u00a0One of the authors is J. Alex Halderman of the University of Michigan, my\u00a0best friend since junior high school, who I\u2019ve blogged about before. \u00a0Because of that,\u00a0I had some advance knowledge\u00a0of this scoop, and found myself having to do what regular Shtetl-Optimized readers will know is the single hardest thing in the world for me: bite my tongue and not say\u00a0anything. \u00a0Until now, that is.\n\nBesides Alex, the other authors are\u00a0David Adrian, Karthikeyan Bhargavan, Zakir Durumeric, Pierrick Gaudry, Matthew Green,\u00a0Nadia Heninger, Drew Springall, Emmanuel Thom\u00e9, Luke Valenta,\u00a0Benjamin VanderSloot, Eric Wustrow, Santiago Zanella-B\u00e9guelink, and Paul Zimmermann (two of these, Green and Heninger, have previously turned up on Shtetl-Optimized).\n\nThese authors study vulnerabilities in Diffie-Hellman key exchange, the\u00a0\u201coriginal\u201d (but still widely-used) public-key cryptosystem, the one that predates even RSA. \u00a0Diffie-Hellman is the thing where Alice and Bob first agree on a huge prime number p and a number g, then Alice picks a secret a and sends Bob ga (mod p), and Bob picks a secret b and sends Alice gb (mod p), and then Alice and Bob\u00a0can both compute (ga)b=(gb)a=gab (mod p), but an eavesdropper who\u2019s listening in only knows p, g, ga (mod p), and gb (mod p), and one can plausibly conjecture that it\u2019s hard from those things alone to get gab (mod p). \u00a0So then Alice and Bob share a secret unknown to the eavesdropper, which they didn\u2019t before, and they can use that secret to start doing cryptography.\n\nAs far as anyone knows today, the best\u00a0way to break Diffie-Hellman is simply by calculating discrete logarithms: that is, solving the problem of recovering a given only g and h=ga (mod p). \u00a0At least on a classical computer, the fastest known algorithm for discrete logarithms (over fields of prime order) is the number field sieve (NFS). \u00a0Under plausible conjectures about the distribution of \u201csmooth\u201d numbers, NFS uses time that grows like exp((1.923+o(1))(log p)1/3(log log p)2/3), where the exp and logs are base e (and yes, even the lower-order stuff like (log log p)2/3\u00a0makes a big difference in practice). \u00a0Of course, once you know the running time of the best-known algorithm, you can then try to choose a key size (that is, a value of log(p)) that\u2019s out of reach for that algorithm on the computing hardware of today.\n\nBut there\u2019s one\u00a0crucial further fact, which has been understood\u00a0for at least a decade by theoretical cryptographers, but somehow was slow to\u00a0filter out to the people who deploy\u00a0practical cryptosystems. \u00a0The further fact is that in NFS, you can arrange things so that almost all the discrete-logging effort\u00a0depends only on the prime number p, and not at all on the specific numbers g and h for which you\u2019re trying to take\u00a0the discrete log. \u00a0After this initial \u201cprecomputation\u201d step, you then have a massive database\u00a0that you can use to speed up the \u201cdescent\u201d step: the step of solving of ga=h (mod p), for any\u00a0(g,h) pair that you want.\n\nIt\u2019s a little like the complexity class P/poly, where a single, hard-to-compute \u201cadvice string\u201d unlocks exponentially many inputs once you have it. \u00a0(Or\u00a0a bit more precisely, one could say that NFS reveals\u00a0that exponentiation modulo a prime number is sort of\u00a0a trapdoor one-way function, except that the trapdoor information is subexponential-size, and given the trapdoor, inverting the function is still\u00a0subexponential-time, but a milder subexponential than before.)\n\nThe kicker is that,\u00a0in practice, a large percentage of all clients and servers that\u00a0use Diffie-Hellman key exchange use the same few prime numbers p. \u00a0This means that, if you wanted to decrypt a large fraction of all the traffic encrypted with Diffie-Hellman, you wouldn\u2019t need\u00a0to do NFS over and over: you could just do it for a few p\u2019s and cache the results. \u00a0This fact can singlehandedly change the outlook for\u00a0breaking Diffie-Hellman.\n\nThe story is different depending on the key size, log(p). \u00a0In the 1990s, the US government insisted on \u201cexport-grade\u201d cryptography for products sold overseas (what a quaint concept!), which meant that the key size was restricted to 512 bits. \u00a0For 512-bit keys, Adrian et al. were able to implement NFS and use it to do the precomputation\u00a0step in about 7 days on a cluster with a few thousand cores. \u00a0After this initial precomputation step (which produced 2.5GB of data), doing the descent, to find the discrete log for a specific (g,h) pair, took only about 90 seconds on a 24-core machine.\n\nOK, but no one still uses 512-bit keys, do they? \u00a0The first part of Adrian et al.\u2019s paper demonstrates\u00a0that, because of implementation issues, even today you can force many servers to \u201cdowngrade\u201d to the 512-bit, export-grade keys\u2014and then, having done so, you can stall for time for 90 seconds as you figure out the session key, and then do a man-in-the-middle attack and take over and impersonate the server. \u00a0It\u2019s an impressive example of the sort of game computer security researchers have been playing for a long time\u2014but it\u2019s really just a warmup to the main act.\n\nAs you\u2019d expect, many servers today are configured more intelligently, and will only agree to 1024-bit keys. \u00a0But even there, Adrian et al. found that a large fraction of servers\u00a0rely on just a single 1024-bit prime (!), and many of the ones that don\u2019t rely on just a few other primes. \u00a0Adrian et al. estimate that, for a single 1024-bit prime, doing the NFS precomputation would take about 45 million years using a single core\u2014or to put it more ominously, 1 year using 45 million cores. \u00a0If you built special-purpose hardware, that could go down by almost two orders of magnitude, putting the monetary cost at a few hundred million dollars, completely within the reach of a sufficiently determined nation-state. \u00a0Once the precomputation was done, and the terabytes\u00a0of output stored in a data center somewhere, computing a particular discrete log would then take about 30 days using 1 core, or mere minutes using a supercomputer. \u00a0Once again, none of this is assuming any algorithmic advances beyond what\u2019s publicly known. \u00a0(Of course, it\u2019s possible that the NSA also has some algorithmic advances; even modest ones could obviate the need for special-purpose hardware.)\n\nWhile writing this post, I did my own back-of-the-envelope, and got that using NFS, calculating\u00a0a 1024-bit discrete log should be about\u00a07.5 million times harder than calculating\u00a0a 512-bit discrete log. \u00a0So, extrapolating from the 7 days it took Adrian et al.\u00a0to do it for 512 bits, this suggests that it might\u2019ve\u00a0taken them about\u00a0143,840 years to calculate\u00a01024-bit discrete logs with the few thousand\u00a0cores\u00a0they had, or 1 year if they had 143,840 times as many cores\u00a0(since almost all this stuff is extremely parallelizable). \u00a0Adrian et al. mention optimizations that they expect would improve this by a factor of 3,\u00a0giving us about 100 million core-years, very\u00a0similar to\u00a0Adrian et al.\u2019s\u00a0estimate of 45 million core-years (the lower-order terms in the running time of NFS might account for some of the remaining discrepancy).\n\nAdrian et al. mount a detailed argument in their paper that all\u00a0of the details about NSA\u2019s \u201cgroundbreaking cryptanalytic capabilities\u201d that we learned from the Snowden documents match what would\u00a0be true if the NSA were doing something like the above. \u00a0The way Alex put it to me is that, sure, the NSA might not have been doing this, but if not, then he would like\u00a0to\u00a0understand why not\u2014for it would\u2019ve been completely\u00a0feasible within the cryptanalytic budget they had, and the NSA would\u2019ve known that, and it would\u2019ve been a\u00a0very good\u00a0codebreaking value for the money.\n\nNow that we know about this weakness of Diffie-Hellman key exchange, what can be done?\n\nThe most obvious solution\u2014but a good one!\u2014is just to use longer keys. \u00a0For decades, when applied cryptographers would announce some attack like this, theorists like me would say with exasperation: \u201cdude, why don\u2019t you fix\u00a0all these problems in one stroke by\u00a0just, like, increasing the key sizes by a\u00a0factor of 10? \u00a0when it\u2019s an exponential against a polynomial, we all know the exponential\u00a0will win eventually, so why not just go out to where it does?\u201d \u00a0The applied cryptographers explain to us, with equal exasperation in their voices, that there are all sorts of reasons why not, from efficiency to\u00a0(maybe the biggest thing) backwards-compatibility.\u00a0 You can\u2019t unilaterally demand 2048-bit keys, if millions of your customers are using browsers that only understand\u00a01024-bit keys. \u00a0On the other hand, given the new revelations, it looks like there really will be\u00a0a big\u00a0push\u00a0to migrate to larger key sizes, as the theorists would\u2019ve suggested from their ivory towers.\n\nA second, equally-obvious solution is to stop relying so much on the same few prime numbers in Diffie-Hellman key exchange. \u00a0(Note that the reason RSA isn\u2019t vulnerable to this particular attack is that it inherently requires a different composite number N for each public key.) \u00a0In practice, generating a new huge\u00a0random prime number tends to be expensive\u2014taking, say, a few minutes\u2014which is why people so often rely on \u201cstandard\u201d primes. \u00a0At the least, we could use\u00a0libraries of millions of \u201csafe\u201d primes, from which a prime for a given session is\u00a0chosen randomly.\n\nA third\u00a0solution is to migrate to elliptic-curve cryptography (ECC), which as far as anyone knows today, is much less vulnerable to descent attacks than the original Diffie-Hellman scheme. \u00a0Alas, there\u2019s been a lot of understandable distrust of ECC after the DUAL_EC_DBRG scandal, in which it came out\u00a0that the NSA backdoored some of NIST\u2019s elliptic-curve-based\u00a0pseudorandom generators by choosing particular elliptic curves that it knew how handle. \u00a0But maybe the right lesson to draw is mod-p\u00a0groups and elliptic-curve groups\u00a0both\u00a0seem to be pretty good for cryptography, but the mod-p groups are less good if everyone is using the same few prime numbers p (and\u00a0those primes are \u201cwithin nation-state range\u201d), and the elliptic-curve groups are less good if everyone is using the same few elliptic curves. \u00a0(A lot of these things do seem pretty\u00a0predictable\u00a0with hindsight, but how many did you predict?)\n\nMany people will use this paper to ask political questions, like: hasn\u2019t the NSA\u2019s codebreaking mission once again usurped\u00a0its mission to ensure the nation\u2019s information security? \u00a0Doesn\u2019t the 512-bit vulnerability that many Diffie-Hellman implementations still face, as a holdover from the 1990s export rules, illustrate why\u00a0encryption should never\u00a0be deliberately weakened for purposes of \u201cnational security\u201d? \u00a0How can we get over the issue of backwards-compatibility, and get everyone using strong crypto? \u00a0People absolutely should be asking such\u00a0questions.\n\nBut for\u00a0readers of this blog, there\u2019s one question that probably looms even larger than those of freedom versus security, openness versus secrecy, etc.: namely, the question of theory versus practice. \u00a0Which \u201cside\u201d should be said to have \u201cwon\u201d this round? \u00a0Some will say: those useless theoretical cryptographers, they didn\u2019t even know how their coveted Diffie-Hellman system could be broken in the real world! \u00a0The theoretical cryptographers might reply: of course\u00a0we\u00a0knew about the ability to do precomputation with NFS! \u00a0This wasn\u2019t some NSA secret; it\u2019s something we discussed openly for years. \u00a0And if someone told us\u00a0how Diffie-Hellman\u00a0was actually being used (with much of the\u00a0world relying on the same few primes), we could\u2019ve immediately spotted\u00a0the potential for such an attack. \u00a0To which others might reply: then why didn\u2019t you spot it?\n\nPerhaps the right lesson to draw is how silly such debates really are. \u00a0In the end, piecing this story together\u00a0took a team that was willing to do everything from learning some fairly difficult number theory to\u00a0coding up\u00a0simulations\u00a0to\u00a0poring over the Snowden documents for clues about the NSA\u2019s budget. \u00a0Clear thought doesn\u2019t respect the boundaries between disciplines, or between theory and practice.\n\nThis entry was posted on Friday, May 22nd, 2015 at 9:41 pm and is filed under Complexity, Nerd Interest. You can follow any responses to this entry through the RSS 2.0 feed. You can leave a response, or trackback from your own site."},
{"url": "https://www.fpcomplete.com/blog/2015/05/thousand-user-haskell-survey", "link_title": "What do Haskellers want?", "sentiment": 0.14369556507322462, "text": "The Commercial Haskell SIG members want to help people adopt Haskell. What would help? Data beats speculation, so FP Complete recently emailed surveys to over 16000 people interested in Haskell. The questions were aimed at identifying needs rather than celebrating past successes, and at helping applied users rather than researchers.\n\nOver 1240 people sent detailed replies, spending over 250 person-hours to provide their answers.\n\nThis rich data set includes extensive information on Haskell user needs. We are open-sourcing the entire anonymized data set, downloadable by clicking here [.zip]. There are numeric ratings and extensive textual comments. Feel free to analyze the data -- or just read the summaries -- and share your most helpful and actionable conclusions with the community. We will too.\n\nAlthough we have completed only basic analysis, here are some of our first findings -- those so clear that they show up on even the most basic examination of the aggregate data.\n\nWe have started deeper statistical analysis of the data, and we hope that some readers of this post will perform -- and share -- even better analyses than we can. New issues may become clearer by clustering or segmenting users, or through other statistical techniques. Also, we may find more clarity about needs through deeper study of textual responses. Follow-up studies are also a possibility.\n\nWe propose that the community, given this large and detailed data set, should set some of its priorities in a data-driven manner focused on user-expressed needs. This effort should be complementary to the ongoing research on issues of historic Haskell strength such as language design and runtime architecture.\n\nWe request that useful findings or insights derived from the open-sourced data set be shared with the community, including attribution of the source of the data and the methods of analysis used.\n\nThe data collected strongly invites clustering or segmentation, so as to identify needs of different sub-populations. FP Complete has already begun one such study.\n\nThe data collected includes extensive textual remarks which should be studied by knowledgeable people for insights. Automated text analysis methods may also be applicable.\n\nCost-benefit analysis seems worthwhile: based on identified needs, what improvements would help the most people, to the greatest degree, at the least cost and/or the least delay? A method to match volunteer contributors with identified high-payoff projects also seems worthwhile.\n\nIt would be useful to merge the data from versions 0.1 and 0.2 with version 1.0 of the survey, since 1.0 includes only 71% of the total answers received. Differences between the questions and scales make this a nontrivial, but still realistic, goal.\n\nIf important new hypotheses require testing, or if further detail is needed, we intend to conduct follow-up research at some future date among users who volunteered their email addresses for follow-up questions.\n\nA future repeat survey could determine which improvement efforts are successful.\n\nThis was not a survey of the general public, but of a population motivated to provide feedback on Haskell. Invitees included 16165 non-opted-out email addresses gathered from FP Complete's website, in randomized order. Due to privacy considerations this list will not be published, but FP Complete was able to use it to contact these users since the survey was directly related to their demonstrated interest in Haskell. The high quality of the list is reflected in the extremely high response rate (7.7%), the low bounce rate (1.9%), and the low unsubscribe rate (also 1.9%).\n\nSurveys were conducted using SurveyGizmo.com, with an email inviting each participant to click a link to a four-page Web-based survey. Survey form 0.1 invitations went to 1999 users of whom 190 completed the survey. Survey form 0.2, incorporating some edits, went to 2000 users of whom 170 completed the survey. Survey form 1.0, incorporating further edits, went to 12166 users of whom 894 completed the survey.\n\nForm 0.2 incorporated edits to eliminate questions yielding little information about how to help users, either because satisfaction was very high (the language itself, the compiler, the community) or because two questions were redundant. Also, new questions inspired by textual responses to form 0.1 were included.\n\nForm 1.0 incorporated further such edits. Also, the rating scale was changed to ask about helping the user's (and team's) future choice of Haskell rather than current usefulness/difficulty. The ratings questions were displayed under the heading \"Would improvements help you and your group to choose Haskell for your future work?\"\n\nResponses were processed anonymously, but users were given the option to fill in their email address if they would accept follow-up questions, and the option to name their company/organization. Users were informed that the survey results, without these fields, would be shared with the community.\n\nWe are grateful to the many, many people who spent their valuable time and expertise completing and returning their survey forms. Thanks to Dr. Tristan Webb and Ms. Noelle McCool-Smiley, both of FP Complete, for their material help in formulating and conducting the survey. Thanks to FP Complete's corporate customers for providing the revenues that allow us to fund this and other community projects. Thanks to the Commercial Haskell SIG for providing the motivation behind this project. Thanks to the many volunteers who've spent absolutely huge amounts of time and expertise making Haskell as good as it is today, and who continue to make improvements like those requested by the survey participants. Thanks to the companies that allow some of their staff to spend company time making such contributions to the common good. Special thanks to the late Professor Paul Hudak; may we all strive to live up to his example."},
{"url": "https://blog.logentries.com/2015/05/fun-with-javascript-on-the-jvm/", "link_title": "Fun with JavaScript on the JVM", "sentiment": 0.2025735930735931, "text": "It\u2019s easy to see how JavaScript is everywhere these days. The barrier to entry is extremely low; anybody with a browser can write and evaluate it, and with advancements in runtimes like Google\u2019s V8, writing server-side JS is now a viable proposition. It\u2019s easy to forget, then, that Rhino- one of\u00a0original JavaScript interpreters was written in Java. Not only that, but Mozilla is still looking after the venerable codebase and a variant is still bundled with every Java runtime. While Rhino has been given several new leases of life, the latest & greatest version doesn\u2019t ship with modern JVMs.\n\nIn addition, Oracle has decided to jump on the server-side JS bandwagon with their own internally developed implementation,\u00a0Nashorn, a brand new interpreter built into Java 8 which was designed from the outset to take advantage of the bytecode-level improvements in recent JREs. It also comes out of the box with ECMAScript 5 support (6 is in the pipeline), CommonJS module support and other features that node.js developers take for granted. A node compatibility layer is also under heavy development.\n\nInside a Java process, you can get hold of a nashorn instance like this:\n\nIt\u2019s also possible to require external JS modules which selectively expose parts of their APIs:\n\nAnd from another JS file in the same directory:\n\nModularity is handy, but not wildly exciting. What\u2019s more interesting is the language\u00a0interoperability- i.e. your\u00a0JS code can extend and call Java objects and vice-versa, like this:\n\nTo illustrate how this can be used to good effect, let\u2019s show a real use-case: writing routing logic for Zuul in JavaScript. Zuul is an open-source load balancer written by Netflix and heavily used at the edge of their services. Its main attraction over components like ELB, HAProxy, e.t.c is the ability to decorate, filter and reroute traffic in arbitrarily complex ways without incurring any down-time. Routing logic can also theoretically be written in any language that runs on the JVM (code that Netflix has made public largely uses Groovy), so it should be easy enough for us to create a JavaScript implementation\n\nFirst thing we need to do is implement the DynamicCodeCompiler interface, taking Netflix\u2019s Groovy implementation as a reference. A DynamicCodeCompiler is the piece that gives Zuul much of its value- by taking source files it discovers on a predefined path and interpreting them on the fly:\n\nNext, let\u2019s add a filter- we\u2019ll pull the Netflix sample project which just redirects requests to the Apache organization home page and take some inspiration from the Groovy filters that come out of the box. Let\u2019s reimplement a simple pre-routing filter that adds information about the remote host to the debug log. First, we need to import a few classes that come with the Zuul core library:\n\nNext, we need to extend ZuulFilter with our implementation:\n\nFor reference, the original Groovy implementation can be found here. Notice that the Debug and RequestContext classes are only visible inside\u00a0the with() {} block.\n\nTechnical innovations like Nashorn show that you\u2019re not tied to writing Java when you have a big investment in the JVM; if a more expressive language or framework is more appropriate for the job at hand, there\u2019s nothing stopping you using it while still leveraging your existing code, something we take advantage of \u00a0here at Logentries."},
{"url": "http://finance.yahoo.com/news/why-1-4-silicon-valley-135956332.html", "link_title": "One in Four Silicon Valley Homeowners Searching Outside Silicon Valley", "sentiment": 0.1386664360229934, "text": "According to the New York Times, major food companies are turning to Hampton Creek for its powdered egg substitutes as a way to solve the current egg shortage. Insight, with founder and CEO Josh Tetrick.\n\nYour Grateful Nation is dedicated to helping Special Forces veterans enter the corporate world and Knot Standard provides complimentary suits to vets. Mad Money's Jim Cramer spoke with Rob Clapper, Your Grateful executive director; John Ballay, Knot St...\n\nChairman of the Fisher House Foundation, Ken Fisher, discusses the Hero Miles program with CNBC's Dina Gusovsky. During Military Appreciation Month, Fisher is asking every traveler to donate 1,000 of their miles to replenish the Hero Miles programs tha...\n\nAt a recent conference, the founder of one technology titan asked another if it was even possible to build a platform-technology company outside of Silicon Valley. It was a fair question, given the dominance of Google (GOOGL), Facebook (FB) and Apple (AAPL). But from where I sat, it seemed easier to build a company of that size today almost anywhere except Silicon Valley.\n\nOthers have had the same thought. A spate of start-ups and now venture funds have recently left Silicon Valley for LA ( Snapchat ), Chicago (Keepsake), Seattle (Sherbert) and even Ohio (Drive).\n\n\n\nThe company where I work, Redfin , understands this impulse better than anyone. We are real estate brokers, with technology used by 10 million-plus people each month looking to move. And the simplest trend we see in American life is that Silicon Valley is no longer just the place talented people move to; it's the place those people are moving from. (Tweet this)\n\n\n\nIn 2011, 1 in 7 people in the Bay Area searched Redfin.com for homes outside of the Bay Area. Now it's 1 in 4. As Adam Wiener, our chief growth officer, announced to other executives last month: \"The dam has broken.\"\n\nIn the past four years, the number of Bay Area people searching for Seattle homes has quadrupled; for Portland homes, that number has quintupled. For every 13 Bay Area people searching for a home, one is now searching in the Pacific Northwest alone.\n\nAnd these aren't just idle online searches. One of our Denver employees had a simple answer for where she is now meeting our clients: \"at the airport,\" with many flying in from northern California.\n\nSilicon Valley transplants have become so common that Redfin's Boston agents just this week reported resentment among locals who can't compete. \"My God, they are pouring in,\" Redfin's Boston broker, Alex Coon, wrote me this morning, \"particularly in Cambridge and Somerville.\"\n\nThe result? According to Matt Zborezny, the Redfin agent for that area, 20 percent above asking price is the new norm.\n\nFolks are leaving Silicon Valley, mostly because they can't afford to stay. For the first time ever, the median price for a Silicon Valley home just exceeded $1 million. That's about double what it is in other tech cities, like Boston or Seattle, and triple what it is in aspiring technology hubs, like Portland, Denver or Austin.\n\nThose in technology who can afford to stay in Silicon Valley all know it as one of the most beautiful places to live in the world, but a wariness has sunk in as folks from other walks of life are forced to leave: coffee shops are wall-to-wall with aspiring entrepreneurs, and restaurants buzz with talk of valuations and venture capital. It can be too much.\n\nJust today a journalist who has covered technology from San Francisco for nearly a decade told me that people here seem more focused on money than in the past. If that's true, it isn't entirely by choice: People hop from job to job and deal to deal because sometimes that's the only way they can afford to stay.\n\nAccording to compensation data company PayScale.com, Silicon Valley engineers earn nearly 50 percent more than their Boston counterparts; in Seattle that difference is smaller, but still significant, at 12 percent. Nowhere is the pay difference large enough to offset the cost of housing.\n\n\n\nFor these mostly entry-level jobs, the median level of experience is two to four years, with marketing managers at five to six years. At the most competitive companies, salaries are significantly higher.\n\nIn our own experience at Redfin employing engineers in Seattle and San Francisco, we've noticed that as Google, Facebook and Dropbox have opened Seattle offices, the differences in engineering pay, especially among recent graduates of top computer-science programs, have disappeared. But the pay of all the people responsible for the actual day-to-day operations of the business-in accounting, marketing or human resources-is more closely tied to the local cost of living. This is why, as Glassdoor reports today, the best places for jobs in America are now up-and-coming tech hubs like Raleigh and Austin, ranking ahead of San Jose or San Francisco.\n\nRead More The 10 most expensive real estate markets in the US\n\nOur board, which once encouraged us to pay whatever it takes to hire engineers in San Francisco, is now also asking if we want to explore opening engineering offices in Portland and Austin. Technologies such as Slack , SourceTree and Stash, and examples of purely virtual companies such as Automattic and GitHub , have made it easier than ever for people to contribute from anywhere. And those folks are more likely to stick around. The CEO of a publicly traded Internet company recently told me the people in his recently opened Midwest office see it \"not just as a job but as a career.\"\n\nSalaries aren't the only costs that are lower in other places. Silicon Valley commercial rents are nearly double what they would be in Denver or Portland, and 50 percent higher than Austin or Seattle. For a 100-person office, the difference is $400,000 a year, lowering operating expenses by about 2 percent; in a typical software company with 15 percent margins, this difference is significant.\n\nMany high-tech businesses are starting to worry about the rent: When we asked a CBRE broker, Owen Rice, for this data, he wrote back with a funny-that-you-should-ask email, noting that \"more and more we are creating multimarket analyses for our clients,\" including those based in a suddenly more expensive Seattle, as well as the Valley.\n\nBut what about the original question-whether it's possible to build a technology platform company outside Silicon Valley. A platform company builds technology used by other technology companies, from the iPhone that runs other applications to the Facebook login we use to access other websites, compounding each employee's leverage. This is why Facebook's market value exceeds $20 million per employee.\n\nThese companies don't have to worry about expenses much. As my first mentor in Silicon Valley, Kirill Sheynkman, once explained to me at a French restaurant, the point in an innovation economy isn't to spend less, it's to make more. And for a platform company, the value of being close to the technology companies that build on your platform is priceless.\n\nBut as our industry matures, the pressure will be on profits, not just revenues. And few high-tech companies get as much leverage as Facebook from each employee. Even a platform company like Twitter (TWTR) is worth about four times less per employee than Facebook. With less equity to burn, Twitter has had to be the pacesetter in raising San Francisco engineering salaries, which is why its stock is now under so much earnings pressure. Only the techiest of tech companies-and only their tech people-don't feel the pinch.\n\nNow of course, Silicon Valley isn't going to empty out. Its population remained constant over the last decade and will remain so again in this one. More people will come here, but more will leave, too. The result will be the Valley-fication of America, a form of gentrification more extreme than most of America has seen before, with high-tech jobs, high incomes and more expensive coffee, yoga studios and-yes-houses, too.\n\n-By Glenn Kelman, CEO of Redfin, a next-generation technology-powered real estate broker. Follow him on Twitter @glennkelman."},
{"url": "https://community.qualys.com/blogs/securitylabs/2015/05/22/ssl-labs-increased-penalty-when-tls-12-is-not-supported", "link_title": "SSL Labs: Increased Penalty When TLS 1.2 Is Not Supported", "sentiment": 0.14307295268833728, "text": "Earlier this week we released SSL Labs 1.17.10, whose main purpose was to increase the penalty when RC4 is used with modern protocols (i.e., TLS 1.1 and TLS 1.2). We had announced this change some time ago, and then put in place on May 20. The same release introduced another change, which was to increase the penalty for servers that don't support TLS 1.2 from B to C. And it seems that this second change is being somewhat controversial, with many asking us to better explain why we did that.\n\nAlthough what initially prompted us to think about changing the grading for not supporting TLS 1.2 was grade harmonisation (ensuring that a wide range of servers all get grades that make sense -- in other words, to have better-configured servers have better grades), that doesn't change the fact that the reality is that TLS 1.0 is an obsolete security protocol. TLS 1.0 came out in 1999, followed by TLS 1.1 in 2003 and TLS 1.2 in 2008. These new protocol versions were released for a reason -- to address security issues with earlier protocol versions. But, despite being obsolete, TLS 1.0 continues to be the best supported protocol version on many servers. It's not very bad, mind you -- we know from SSL Pulse that about 60% of servers already support TLS 1.2. Client-side, the situation is probably better, because modern browsers have supported TLS 1.2 since 2013. You could say that, overall server configuration is the weaker link.\n\nIn that light, we feel that the increase of the penalty for the lack of TLS 1.2 is the natural next step in the deprecation of TLS 1.0. In fact, SSL Labs is probably late in doing that. Just last month, the PCI Security Council deprecated SSL v3 and TLS 1.0 for commercial transactions. No new systems are allowed to use TLS 1.0 for credit card processing and existing systems must immediately begin to transition to better protocols. In comparison, the SSL Labs change of grading is only a mild nudge in the right direction. And, while some people are not happy that we're pushing for TLS 1.2, others are complaining that we're not doing enough. For example, the Chrome browser has been warning about lack of TLS 1.2 and authenticated (GCM) suites for some time now. Clearly, it's difficult to make everyone happy.\n\nThe bottom line is that TLS 1.0 is insecure and we must migrate away from it. In 2011, there came the BEAST attack, and, in 2013, the Lucky 13 attack. TLS 1.0 remains vulnerable to this problems, but TLS 1.2 (with authenticated suites) isn't. These attacks are serious and some organisations continue to use RC4 in combination with TLS 1.0 just to be sure that they are mitigated. We understand that many organisations face significant challenges adding support TLS 1.2, but that is unavoidable. In computer technology, and in security in particular, it is often necessary to keep running just to stay in place.\n\nWe did get one thing wrong, however -- we didn't communicate our grading changes in advance. It was not our intention to surprise anyone. In fact, we'd prefer much more if changes were smoother. To that end, in the future we'll be announcing all grading changes with at least one month notice, and hopefully more for some more significant changes."},
{"url": "http://teknixx.com/some-linux-iptables-examples/", "link_title": "Some Linux Iptables Examples", "sentiment": 0.04297258297258297, "text": "Iptables is an application program that allows a system administrator to configure the tables provided by the Linux kernel firewall and the chains and rules it stores. Today we are going to review some of the most useful examples\n\nType the following command as root:\n\n\n\nYou can use line numbers to delete or insert new rules into the firewall.\n\nIf you are using CentOS / RHEL / Fedora Linux, enter:\n\nYou can use the iptables command itself to stop the firewall and delete all rules:\n\nTo display line number along with other information for existing rules, enter:\n\nYou will get the list of IP. Look at the number on the left, then use number to delete it. For example delete line number 4, enter:\n\nOR find source IP 202.54.1.1 and delete from rule:\n\nTo insert one or more rules in the selected chain as the given rule number use the following syntax. First find out line numbers, enter:\n\nTo insert rule between 1 and 2, enter:\n\n \n\n To view updated rules, enter:\n\n \n\n Sample outputs:\n\nTo save firewall rules under CentOS / RHEL / Fedora Linux, enter:\n\n \n\n In this example, drop an IP and save firewall rules:\n\n \n\n For all other distros use the iptables-save command:\n\n\n\nTo restore firewall rules form a file called /root/my.active.firewall.rules, enter:\n\n \n\n To restore firewall rules under CentOS / RHEL / Fedora Linux, enter:\n\n\n\nTo drop all traffic:\n\n # iptables -P INPUT DROP\n\n # iptables -P OUTPUT DROP\n\n # iptables -P FORWARD DROP\n\n # iptables -L -v -n\n\n #### you will not able to connect anywhere as all traffic is dropped ###\n\n # ping\u00a0teknixx.com\n\n # wget http://www.kernel.org/pub/linux/kernel/v3.0/testing/linux-3.2-rc5.tar.bz2\n\nTo drop all incoming / forwarded packets, but allow outgoing traffic, enter:\n\n # iptables -P INPUT DROP\n\n # iptables -P FORWARD DROP\n\n # iptables -P OUTPUT ACCEPT\n\n # iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT\n\n # iptables -L -v -n\n\n ### *** now ping and wget should work *** ###\n\n # ping\u00a0teknixx.com\n\n # wget http://www.kernel.org/pub/linux/kernel/v3.0/testing/linux-3.2-rc5.tar.bz2\n\nIP spoofing is nothing but to stop the following IPv4 address ranges for private networks on your public interfaces. Packets with non-routable source addresses should be rejected using the following syntax:\n\n\n\nTo block all service requests on port 80, enter:\n\n\n\nTo block port 80 only for an ip address 1.2.3.4, enter:\n\n\n\nTo block outgoing traffic to a particular host or domain such as teknixx.com, enter:\n\n \n\n Sample outputs:\n\nNote down its ip address and type the following to block all outgoing traffic to 75.126.153.206:\n\n \n\n You can use a subnet as follows:\n\n\n\nFirst, find out all ip address of facebook.com, enter:\n\n \n\n Sample outputs:\n\nTo prevent outgoing access to www.facebook.com, enter:\n\n \n\n You can also use domain name, enter:\n\n\n\nType the following to log and block IP spoofing on public interface called eth1\n\n \n\n By default everything is logged to /var/log/messages file.\n\n\n\nThe -m limit module can limit the number of log entries created per time. This is used to prevent flooding your log file. To log and drop spoofing per 5 minutes, in bursts of at most 7 entries .\n\n\n\nUse the following syntax:\n\n # iptables -A INPUT -m mac --mac-source 00:0F:EA:91:04:08 -j DROP\n\n ## *only accept traffic for TCP port # 8080 from mac 00:0F:EA:91:04:07 * ##\n\n # iptables -A INPUT -p tcp --destination-port 22 -m mac --mac-source 00:0F:EA:91:04:07 -j ACCEPT\n\nType the following command to block ICMP ping requests:\n\n \n\n Ping responses can also be limited to certain networks or hosts:\n\n \n\n The following only accepts limited type of ICMP requests:\n\n ### ** assumed that default INPUT policy set to DROP ** #############\n\n iptables -A INPUT -p icmp --icmp-type echo-reply -j ACCEPT\n\n iptables -A INPUT -p icmp --icmp-type destination-unreachable -j ACCEPT\n\n iptables -A INPUT -p icmp --icmp-type time-exceeded -j ACCEPT\n\n ## ** all our server to respond to pings ** ##\n\n iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT\n\nUse the following syntax to open a range of ports:\n\n\n\nUse the following syntax to open a range of IP address:\n\n ## only accept connection to tcp port 80 (Apache) if ip is between 192.168.1.100 and 192.168.1.200 ##\n\n iptables -A INPUT -p tcp --destination-port 80 -m iprange --src-range 192.168.1.100-192.168.1.200 -j ACCEPT\n\nWhen you restart the iptables service it will drop established connections as it unload modules from the system under RHEL / Fedora / CentOS Linux. Edit, /etc/sysconfig/iptables-config and set IPTABLES_MODULES_UNLOAD as follows:\n\nUse the crit log level to send messages to a log file instead of console:\n\n\n\nThe following shows syntax for opening and closing common TCP and UDP ports:\n\nYou can use connlimit module to put such restrictions. To allow 3 ssh connections per client host, enter:\n\n"},
{"url": "http://www.unicode.org/reports/tr51/#Diversity", "link_title": "How the new diverse Emoji work \u2013 Unicode Standard", "sentiment": 0.07498390065547773, "text": "This document aims to improve the interoperability of emoji characters across implementations by providing guidelines and data.\n\nThis is a document which may be updated, replaced, or superseded by other documents at any time. Publication does not imply endorsement by the Unicode Consortium. This is not a stable document; it is inappropriate to cite this document as other than a work in progress.\n\nPlease submit corrigenda and other comments with the online reporting form [Feedback]. Related information that is useful in understanding this document is found in the References. For the latest version of the Unicode Standard see [Unicode]. For a list of current Unicode Technical Reports see [Reports]. For more information about versions of the Unicode Standard, see [Versions].\n\nEmoji are pictographs (pictorial symbols) that are typically presented in a colorful cartoon form and used inline in text. They represent things such as faces, weather, vehicles and buildings, food and drink, animals and plants, or icons that represent emotions, feelings, or activities.\n\nThe word emoji comes from the Japanese:\n\nEmoji on smartphones and in chat and email applications have become extremely popular worldwide. As of 2015, for example, Instagram reported that \u201cin March of this year, nearly half of text [on Instagram] contained emoji.\u201d Individual emoji also vary greatly in popularity (and even by country), as described in the SwiftKey Emoji Report. See emoji press page for details about this.\n\nEspecially in social media and short text, emoji provide flavor and emphasis to messages. They help to compensate for the lack of gestures and intonation in text.\n\nEmoji may be represented internally as graphics or they may be represented by normal glyphs encoded in fonts like other characters. These latter are called emoji characters for clarity. Some Unicode characters are normally displayed as emoji; some are normally displayed as ordinary text, and some can be displayed both ways. See also the OED: emoji, n.\n\nThere\u2019s been considerable media attention to emoji since they appeared in the Unicode Standard, with increased attention starting in late 2013. For example, there were some 6,000 articles on the emoji appearing in Unicode 7.0, according to Google News. See the emoji press page for many samples of such articles, and also the Keynote from the 38th Internationalization & Unicode Conference.\n\nEmoji became available in 1999 on Japanese mobile phones. There was an early proposal in 2000 to encode DoCoMo emoji in Unicode. At that time, it was unclear whether these characters would come into widespread use\u2014and there wasn't support from the Japanese mobile phone carriers to add them to Unicode\u2014so no action was taken.\n\nThe emoji turned out to be quite popular in Japan, but each mobile phone carrier developed different (but partially overlapping) sets, and each mobile phone vendor used their own text encoding extensions, which were incompatible with one another. The vendors developed cross-mapping tables to allow limited interchange of emoji characters with phones from other vendors, including email. Characters from other platforms that could not be displayed were represented with \u3013 (U+3013 GETA MARK), but it was all too easy for the characters to get corrupted or dropped.\n\nWhen non-Japanese email and mobile phone vendors started to support email exchange with the Japanese carriers, they ran into those problems. Moreover, there was no way to represent these characters in Unicode, which was the basis for text in all modern programs. In 2006, Google started work on converting Japanese emoji to Unicode private-use codes, leading to the development of internal mapping tables for supporting the carrier emoji via Unicode characters in 2007.\n\nThere are, however, many problems with a private-use approach, and thus a proposal was made to the Unicode Consortium to expand the scope of symbols to encompass emoji. This proposal was approved in May 2007, leading to the formation of a symbols subcommittee, and in August 2007 the technical committee agreed to support the encoding of emoji in Unicode based on a set of principles developed by the subcommittee. The following are a few of the documents tracking the progression of Unicode emoji characters.\n\nIn 2009, the first Unicode characters explicitly intended as emoji were added to Unicode 5.2 for interoperability with the ARIB (Association of Radio Industries and Businesses) set. A set of 722 characters was defined as the union of emoji characters used by Japanese mobile phone carriers: 114 of these characters were already in Unicode 5.2. In 2010, the remaining 608 emoji characters were added to Unicode 6.0, along with some other emoji characters. In 2012, a few more emoji were added to Unicode 6.1, and in 2014 a larger number were added to Unicode 7.0.\n\nHere is a summary of when some of the major sources of pictographs used as emoji were encoded in Unicode. These sources include other characters in addition to emoji.\n\nUnicode characters can correspond to multiple sources. The L column contains single-letter abbreviations for use in charts and data files. Characters that do not correspond to any of these sources can be marked with Other (x).\n\nFor a detailed view of when various source sets of emoji were added to Unicode, see emoji-versions-sources (the format is explained in Data Files). The UCD data file EmojiSources.txt shows the correspondence to the original Japanese carrier symbols.\n\nThe Selected Products table lists when Unicode emoji characters were incorporated into selected products. (The Private Use characters (PUA) were a temporary solution.)\n\nPeople often ask how many emoji are in the Unicode Standard. This question does not have a simple answer, because there is no clear line separating which pictographic characters should be displayed with a typical emoji style. For a complete picture, see Which Characters are Emoji.\n\nThe colored images used in this document and associated charts are for illustration only. They do not appear in the Unicode Standard, which has only black and white images. They are either made available by the respective vendors for use in this document, or are believed to be available for non-commercial reuse. Inquiries for permission to use vendor images should be directed to those vendors, not to the Unicode Consortium. For more information, see Rights to Emoji Images.\n\nThe term emoticon refers to a series of text characters (typically punctuation or symbols) that is meant to represent a facial expression or gesture (sometimes when viewed sideways), such as the following.\n\nEmoticons predate Unicode and emoji, but were later adapted to include Unicode characters. The following examples use not only ASCII characters, but also U+203F ( \u203f ), U+FE35 ( \ufe35 ), U+25C9 ( \u25c9 ), and U+0CA0 ( \u0ca0 ).\n\nOften implementations allow emoticons to be used to input emoji. For example, the emoticon ;-) can be mapped to in a chat window. The term emoticon is sometimes used in a broader sense, to also include the emoji for facial expressions and gestures. That broad sense is used in the Unicode block name Emoticons, covering the code points from U+1F600 to U+1F64F.\n\nUnicode is the foundation for text in all modern software: it\u2019s how all mobile phones, desktops, and other computers represent the text of every language. People are using Unicode every time they type a key on their phone or desktop computer, and every time they look at a web page or text in an application. It is very important that the standard be stable, and that every character that goes into it be scrutinized carefully. This requires a formal process with a long development cycle. For example, the dark sunglasses character was first proposed years before it was released in Unicode 7.0.\n\nCharacters considered for encoding must normally be in widespread use as elements of text. The emoji and various symbols were added to Unicode because of their use as characters for text-messaging in a number of Japanese manufacturers\u2019 corporate standards, and other places, or in long-standing use in widely distributed fonts such as Wingdings and Webdings. In many cases, the characters were added for complete round-tripping to and from a source set, not because they were inherently of more importance than other characters. For example, the clamshell phone character was included because it was in Wingdings and Webdings, not because it is more important than, say, a \u201cskunk\u201d character.\n\nIn some cases, a character was added to complete a set: for example, a rugby football character was added to Unicode 6.0 to complement the american football character (the soccer ball had been added back in Unicode 5.2). Similarly, a mechanism was added that could be used to represent all country flags (those corresponding to a two-letter unicode_region_subtag), such as the flag for Canada, even though the Japanese carrier set only had 10 country flags.\n\nThis document describes a new set of selection factors used to weigh the encoding of prospective candidates, in Annex C: Selection Factors.\n\nThat annex also points to instructions on submitting character encoding proposals. People wanting to submit emoji for consideration for encoding should see that annex. It may be helpful to review the Unicode Mail List as well.\n\nFor a list of frequently asked questions on emoji, see the Unicode Emoji FAQ.\n\nIt also provides background information about emoji, and discusses longer-term approaches to emoji.\n\nAs new Unicode characters are added or the \u201ccommon practice\u201d for emoji usage changes, the data and recommendations supplied by this document may change in accordance. Thus the recommendations and data will change across versions of this document.\n\nAdditions beyond Unicode 7.0 are being addressed by the Unicode Technical Committee: as any new characters are approved, this document will be updated as appropriate.\n\nThe following provide more formal definitions of some of the terms used in this document. Readers who are more interested in other features of the document may choose to continue from Section 2 Design Guidelines .\n\nFor more details about level 1 and level 2 emoji, see Section 3 Which Characters are Emoji .\n\nFor more details about emoji and text presentation, see 2 Design Guidelines and Section 4 Presentation Style .\n\nFor more details about emoji modifiers, see Section 2.2 Diversity.\n\nUnicode characters can have many different presentations as text. An \"a\" for example, can look quite different depending on the font. Emoji characters can have two main kinds of presentation:\n\nMore precisely, a text presentation is a simple foreground shape whose color which is determined by other information, such as setting a color on the text, while an emoji presentation determines the color(s) of the character, and is typically multicolored. In other words, when someone changes the text color in a word processor, a character with an emoji presentation will not change color.\n\nAny Unicode character can be presented with a text presentation, as in the Unicode charts. For the emoji presentation, both the name and the representative glyph in the Unicode chart should be taken into account when designing the appearance of the emoji, along with the images used by other vendors. The shape of the character can vary significantly. For example, here are just some of the possible images for U+1F36D LOLLIPOP, U+1F36E CUSTARD, U+1F36F HONEY POT, and U+1F370 SHORTCAKE:\n\nWhile the shape of the character can vary significantly, designers should maintain the same \u201ccore\u201d shape, based on the shapes used mostly commonly in industry practice. For example, a U+1F36F HONEY POT encodes for a pictorial representation of a pot of honey, not for some semantic like \"sweet\". It would be unexpected to represent U+1F36F HONEY POT as a sugar cube, for example. Deviating too far from that core shape can cause interoperability problems: see accidentally-sending-friends-a-hairy-heart-emoji. Direction (whether a person or object faces to the right or left, up or down) should also be maintained where possible, because a change in direction can change the meaning: when sending \u201ccrocodile shot by police\u201d, people expect any recipient to see the pistol pointing in the same direction as when they composed it. Similarly, the U+1F6B6 pedestrian should face to the left , not to the right.\n\nGeneral-purpose emoji for people and body parts should also not be given overly specific images: the general recommendation is to be as neutral as possible regarding race, ethnicity, and gender. Thus for the character U+1F64B happy person raising one hand, the recommendation is to use a neutral graphic like instead of an overly-specific image like . This includes the characters listed in the annotations chart under \u201chuman\u201d. The representative glyph used in the charts, or images from other vendors may be misleading: for example, the construction worker may be male or female. For more information, see the Unicode Emoji FAQ.\n\nNames of symbols such as BLACK MEDIUM SQUARE or WHITE MEDIUM SQUARE are not meant to indicate that the corresponding character must be presented in black or white, respectively; rather, the use of \u201cblack\u201d and \u201cwhite\u201d in the names is generally just to contrast filled versus outline shapes, or a darker color fill versus a lighter color fill. Similarly, in other symbols such as the hands U+261A BLACK LEFT POINTING INDEX and U+261C WHITE LEFT POINTING INDEX, the words \u201cwhite\u201d and \u201cblack\u201d also refer to outlined versus filled, and do not indicate skin color.\n\nHowever, other color words in the name, such as YELLOW, typically provide a recommendation as to the emoji presentation, which should be followed to avoid interoperability problems.\n\nEmoji characters may not always be displayed on a white background. They are often best given a faint, narrow contrasting border to keep the character visually distinct from a similarly colored background. Thus a Japanese flag would have a border so that it would be visible on a white background, and a Swiss flag have a border so that it is visible on a red background.\n\nCurrent practice is for emoji to have a square aspect ratio, deriving from their origin in Japanese. For interoperability, it is recommended that this practice be continued with current and future emoji.\n\nCombining marks may be applied to emoji, just like they can be applied to other characters. When that is done, the combination should take on an emoji presentation. For example, a is represented as the sequence \"1\" plus an emoji variation selector plus U+20E3 COMBINING ENCLOSING KEYCAP. Systems are unlikely, however, to support arbitrary combining marks with arbitrary emoji. Aside from U+20E3, the following can be used:\n\nFor example, (pedestrian crossing ahead) can be represented as + U+20E4, and (no bicycles allowed) can be represented as + U+20E0.\n\nThe following emoji have explicit gender, based on the name and explicit, intentional contrasts with other characters.\n\nU+1F466 boy\n\n U+1F467 girl\n\n U+1F468 man\n\n U+1F469 woman\n\n U+1F474 older man\n\n U+1F475 older woman\n\n U+1F46B man and woman holding hands\n\n U+1F46C two men holding hands\n\n U+1F46D two women holding hands\n\n U+1F6B9 mens symbol\n\n U+1F6BA womens symbol\n\n\n\nU+1F478 princess\n\n U+1F46F woman with bunny ears\n\n U+1F470 bride with veil\n\n U+1F472 man with gua pi mao\n\n U+1F473 man with turban\n\n U+1F574 man in business suit levitating\n\n U+1F385 father christmas\n\nAll others should be depicted in a gender-neutral way.\n\n\n\nPeople all over the world want to have emoji that reflect more human diversity, especially for skin tone. The Unicode emoji characters for people and body parts are meant to be generic, yet following the precedents set by the original Japanese carrier images, they are often shown with a light skin tone instead of a more generic (nonhuman) appearance, such as a yellow/orange color or a silhouette.\n\nFive symbol modifier characters that provide for a range of skin tones for human emoji are planned for Unicode Version 8.0 (scheduled for mid-2015). These characters are based on the six tones of the Fitzpatrick scale, a recognized standard for dermatology (there are many examples of this scale online, such as FitzpatrickSkinType.pdf). The exact shades may vary between implementations.\n\nThese characters have been designed so that even where diverse color images for human emoji are not available, readers can see what the intended meaning was.\n\nThe default representation of these modifier characters when used alone is as a color swatch. Whenever one of these characters immediately follows certain characters (such as WOMAN), then a font should show the sequence as a single glyph corresponding to the image for the person(s) or body part with the specified skin tone, such as the following:\n\nHowever, even if the font doesn\u2019t show the combined character, the user can still see that a skin tone was intended:\n\nThis may fall back to a black and white stippled or hatched image such as when colorful emoji are not supported.\n\nWhen a human emoji is not immediately followed by a emoji modifier character, it should use a generic, non-realistic skin tone, such as:\n\nFor example, the following set uses gray as the generic skin tone:\n\nAs to hair color, dark hair tends to be more neutral, because people of every skin tone can have black (or very dark brown) hair\u2014however, there is no requirement for any particular hair color. One exception is PERSON WITH BLOND HAIR, which needs to have blond hair regardless of skin tone.\n\nTo have an effect on an emoji, an emoji modifier must immediately follow that emoji. There is only one exception: there may be an emoji variation selector between them. The emoji modifier automatically implies the emoji presentation style, so the variation selector is not needed. However, if the emoji modifier is present it must come immediately after the modified emoji character, such as in:\n\nAny other intervening character causes the emoji modifier to appear as a free-standing character. Thus\n\nThe basic solution for each of these cases is to represent the multi-person grouping as a sequence of characters\u2014a separate character for each person intended to be part of the grouping, along with characters for any other symbols that are part of the grouping. Each person in the grouping could optionally be followed by an emoji modifier. For example, conveying the notion of COUPLE WITH HEART for a couple involving two women can use a sequence with WOMAN followed by an emoji-style HEAVY BLACK HEART followed by another WOMAN character; each of the WOMAN characters could have an emoji modifier if desired. This makes use of conventions already found in current emoji usage, in which certain sequences of characters are intended to be read as a single unit. For example:\n\nSome implementations may provide single glyphs that correspond to several such sequences, and may provide a palette or keyboard that generates the appropriate sequences for the glyphs shown. In that case U+200D ZERO WIDTH JOINER (ZWJ) can be used in the sequences as an indication that a single glyph (a ligature) should be used if available. If such a sequence is sent to a system that does not have a corresponding single glyph, the ZWJ characters would be ignored and a sequence of separate images would be displayed. Thus the ZWJ mechanism should only be used where the sequence of separate images would make sense to a recipient using an implementation that didn't support the combined glyph.\n\nFor example, the following are possible displays:\n\nSee also Annex E: Existing Use of ZWJ Sequences.\n\nIn a sequence of characters connected using ZWJ, it is recommended that the entire sequence have an emoji presentation if any character in the sequence has explicit or default emoji presentation.\n\nImplementations can present the emoji modifiers as separate characters in an input palette, or present the combined characters using mechanisms such as long press.\n\nThe emoji modifiers are not intended for combination with arbitrary emoji characters. Instead, they are restricted to the following characters, in two separate sets. Of these characters, it is strongly recommended that the Primary set for combination be supported. No characters outside of these two sets are to be combined with emoji modifiers. These sets may change over time, with successive versions of this document. The Images are in the recommended sort order, while the Code points and names are in code point order.\n\nThe following chart shows the shows the expected display with emoji modifiers, depending on the preceding character and the level of support for the emoji modifier. The \u201cUnsupported\u201d rows show how the character would typically appear on a system that doesn't have a font with that character in it: with a missing glyph indicator.\n\nThe interaction between variation selectors and emoji modifiers is specified as follows:\n\nA supported emoji modifier sequence should be treated as a single grapheme cluster for editing purposes (cursor moment, deletion, etc.); word break, line break, etc. For input, the composition of that cluster does not need to be apparent to the user: it appears on the screen as a single image. On a phone, for example, a long-press on a human figure can bring up a minipalette of different skin tones, without the user having to separately find the human figure and then the modifier. The following shows some possible appearances:\n\nOf course, there are many other types of diversity in human appearance besides different skin tones: Different hair styles and color, use of eyeglasses, various kinds of facial hair, different body shapes, different headwear, and so on. It is beyond the scope of Unicode to provide an encoding-based mechanism for representing every aspect of human appearance diversity that emoji users might want to indicate. The best approach for communicating very specific human images\u2014or any type of image in which preservation of specific appearance is very important\u2014is the use of embedded graphics, as described in Longer Term Solutions.\n\nThere are 722 Unicode emoji characters corresponding to the Japanese carrier sets.\n\nIn addition, most vendors support another 126 characters (from Unicode 6.0 and 6.1):\n\nReview Note: Once these are finalized, we'll replace the contents by a single image to speed up loading.\n\nThe carrier emoji plus the common additions comprise the set of level 1 emoji.\n\nThere are another 247 flags (aside from the 10 from the Japanese carrier sets) that can be optionally supported with Unicode 6.0 characters.\n\nSome of these flags use the same glyphs. For more about flags, see Annex B: Flags.\n\nOne of the goals of this document is to provide data for which Unicode characters should normally be considered to be emoji. Based on the data under development, that includes the following characters. Most, but not all, of these are new in Unicode 7.0. This gives a total of 1,245 emoji characters (or sequences) for Unicode 7.0.\n\nThus vendors that support emoji should provide a colorful appearance for each of these, such as the following:\n\nThe Unicode 8.0 candidates are listed below. For details, including sample colorful images, see Annex D: Emoji Candidates for Unicode 8.0.\n\nReview Note: For final production, the text will be modify to remove the term \"candidates\", and adjust the surrounding wording as appropriate. It may be clearer to fold the 8.0 characters into the Standard Additions.\n\nThese comprise the set of level 2 emoji.\n\nThis document provides data files, described in the section Data Files, for determining the set of characters which are expected to have an emoji presentation, either as a default or as a alternate presentation. While Unicode conformance allows any character to be given an emoji representation, characters that are not listed in the Data Files should not normally be given an emoji presentation. For example, pictographic symbols such as keyboard symbols or math symbols (like ANGLE) that should never be treated as emoji. These are current recommendations: existing symbols can be added to this list over time.\n\nThis data was derived by starting with the characters that came from the original Japanese sets, plus those that major vendors have provided emoji fonts for. Characters that are similar to those in shape or design were then added. Often these characters are in the same Unicode blocks as the original set, but sometimes not.\n\nThis document takes a functional view regarding the identification of emoji: pictographs are categorized as emoji when it is reasonable to give them an emoji presentation, and where they are sufficiently distinct from other emoji characters. Symbols with a graphical form that people may treat as pictographs, such as U+2615 HELM SYMBOL (introduced in Unicode 3.0) may be included.\n\nThis document takes a functional view as to the identification of emoji, which is that pictographs\u2014or symbols that have a graphical form that people may treat as pictographs\u2014are categorized as emoji, such as U+260E BLACK TELEPHONE (introduced in Unicode 1.1) or U+2615 HOT BEVERAGE (introduced in Unicode 4.0):\n\nThe data does not include non-pictographs, except for those in Unicode that are used to represent characters from emoji sources, such as:\n\nGame pieces, such as the dominos (\ud83c\udc30 \ud83c\udc31 \ud83c\udc32 ... \ud83c\udc91 \ud83c\udc92), are currently not included as emoji, with the exceptions of U+1F0CF ( ) PLAYING CARD BLACK JOKER and U+1F004\u00a0(\u00a0 \u00a0) MAHJONG TILE RED DRAGON. These are included because they correspond each to an emoji character from one of the carrier sets.\n\nCertain emoji have defined variation sequences, where an emoji character can be followed by one of two invisible emoji variation selectors:\n\nThis capability was added in Unicode 6.1. Some systems may also provide this distinction with higher-level markup, rather than variation sequences. For more information on these selectors, see the file StandardizedVariants.html.\n\nImplementations should support both styles of presentation for the characters with variation sequences, if possible. Most of these characters are emoji that were unified with preexisting characters. Because people are now using emoji presentation for a broader set of characters, it is anticipated that more such variation sequences will be needed.\n\nHowever, even where the variation selectors exist, it has not been clear for implementers whether the default presentation for pictographs should be emoji or text. That means that a piece of text may show up in a different style than intended when shared across platforms. While this is all a perfectly legitimate for Unicode characters\u2014presentation style is never guaranteed\u2014a shared sense among developers of when to use emoji presentation by default is important, so that there are fewer unexpected and \"jarring\" presentations. Implementations need to know what the generally expected default presentation is, to promote interoperability across platforms and applications.\n\nThere has been no clear line for implementers between three categories of Unicode characters:\n\nThe data files, described in the section Data Files, provide data to distinguish between the first two categories: see the Default column of full-emoji-list. The data assignment is based upon current usage in browsers for Unicode 6.3 characters. For other characters, especially the new 7.0 characters, the assignment is based on that of the related emoji characters. For example, the \u201cvulcan\u201d hand is marked as emoji-default because of the emoji styling currently given to other hands like . The text-only characters are all those not listed in the data files.\n\nIn general, emoji characters are marked as text-default if they were in common use and predated the use of emoji. The characters are otherwise marked as emoji-default. For example, the negative squared A and B are text-default, while the negative squared AB is emoji-default. The reason is that A and B are part of a set of negative squared letters A-Z, while the AB was a new character. The default status may change over time, however, if common usage changes.\n\nThe presentation of a given emoji character depends on the environment, whether or not there is an emoji or text variation selector, and the default presentation style (emoji vs text). In informal environments like texting and chats, it is more appropriate for most emoji characters to appear with a colorful emoji presentation, and only get a text presentation with a text variation selector. Conversely, in formal environments such as word processing, it is generally better for emoji characters to appear with a text presentation, and only get the colorful emoji presentation with the emoji variation selector.\n\nBased on those factors, here is typical presentation behavior. However, these guidelines may change with changing user expectations.\n\nNeither the Unicode code point order, nor the standard Unicode Collation ordering (DUCET), are currently well suited for emoji, since they separate conceptually-related characters. From the user's perspective, the ordering in the following selection of characters sorted by DUCET appears quite random, as illustrated by the following example:\n\nThe emoji-ordering data file shows an ordering for emoji characters that groups them together in a more natural fashion.\n\nThis ordering groups characters presents a cleaner and more expected ordering for sorted lists of characters. The groupings include: faces, people, body-parts, emotion, clothing, animals, plants, food, places, transport, and so on. The ordering also groups more naturally for the purpose of selection in input palettes. However, for sorting, each character must occur in only one position, which is not a restriction for input palettes. See Section 6 Input.\n\nEmoji are not typically typed on a keyboard. Instead, they are generally picked from a palette, or recognized via a dictionary. The mobile keyboards typically have a button to select a palette of emoji, such as in the left image below. Clicking on the button reveals a palette, as in the right image.\n\nThe palettes need to be organized in a meaningful way for users. They typically provide a small number of broad categories, such as People, Nature, and so on. These categories typically have 100-200 emoji.\n\nMany characters can be categorized in multiple ways: an orange is both a plant and a food. Unlike a sort order, an input palette can have multiple instances of a single character. It can thus extend the sort ordering to add characters in any groupings where people might reasonably be expected to look for them.\n\nMore advanced palettes will have long-press enabled, so that people can press-and-hold on an emoji and have a set of related emoji pop up. This allows for faster navigation, with less scrolling through the palette.\n\nAnnotations for emoji characters are much more finely grained keywords. They can be used for searching characters, and are often easier than palettes for entering emoji characters. For example, when someone types \u201chourglass\u201d on their mobile phone, they could see and pick from either of the matching emoji characters or . That is often much easier than scrolling through the palette and visually inspecting the screen. Input mechanisms may also map emoticons to emoji as keyboard shortcuts: typing :-) can result in .\n\nIn some input systems, a word or phrase bracketed by colons is used to explicitly pick emoji characters. Thus typing in \u201cI saw an :ambulance:\u201d is converted to \u201cI saw an \u201d. For completeness, such systems might support all of the full Unicode names, such as :first quarter moon with face: for . Spaces within the phrase may be represented by _, as in the following:\n\nHowever, in general the full Unicode names are not especially suitable for that sort of use; they were designed to be unique identifiers, and tend to be overly long or confusing.\n\nSearching includes both searching for emoji characters in queries, and finding emoji characters in the target. These are most useful when they include the annotations as synonyms or hints. For example, when someone searches for on yelp.com, they see matches for \u201cgas station\u201d. Conversely, searching for \u201cgas pump\u201d in a search engine could find pages containing . Similarly, searching for \u201cgas pump\u201d in an email program can bring up all the emails containing .\n\nThere is no requirement for uniqueness in both palette categories and annotations: an emoji should show up wherever users would expect it. A gas pump might show up under \u201cobject\u201d and \u201ctravel\u201d; a heart under \u201cheart\u201d and \u201cemotion\u201d, a under \u201canimal\u201d, \u201ccat\u201d, and \u201cheart\u201d.\n\nAnnotations are language-specific: searching on yelp.de, someone would expect a search for to result in matches for \u201cTankstelle\u201d. Thus annotations need to be in multiple languages to be useful across languages. They should also include regional annotations within a given language, like \u201cpetrol station\u201d, which people would expect search for to result in on yelp.co.uk. An English annotation cannot simply be translated into different languages, since different words may have different associations in different languages. The emoji may be associated with Mexican or Southwestern restaurants in the US, but not be associated with them in, say, Greece.\n\nThere is one further kind of annotation, called a TTS name, for text-to-speech processing. For accessibility when reading text, it is useful to have a short, descriptive name for an emoji character. A Unicode character name can often serve as a basis for this, but its requirements for name uniqueness often ends up with names that are overly long, such as black right-pointing double triangle with vertical bar for . TTS names are also outside the current scope of this document.\n\nThe longer-term goal for implementations should be to support embedded graphics, in addition to the emoji characters. Embedded graphics allow arbitrary emoji symbols, and are not be dependent on additional Unicode encoding. Here are some examples of this:\n\nHowever, to be as effective and simple to use as emoji characters, a full solution requires significant infrastructure changes to allow simple, reliable input and transport of images (stickers) in texting, chat, mobile phones, email programs, virtual and mobile keyboards, and so on. (Even so, such images will never interchange in environments that only support plain text, such as email addresses.) Until that time, many implementations will need to use Unicode emoji instead.\n\nFor example, mobile keyboards need to be enhanced. Enabling embedded graphics would involve adding an additional custom mechanism for users to add in their own graphics or purchase additional sets, such as a sign to add an image to the palette above. This would prompt the user to paste or otherwise select a graphic, and add annotations for dictionary selection.\n\nWith such an enhanced mobile keyboard, the user could then select those graphics in the same way as selecting the Unicode emoji. If users started adding many custom graphics, the mobile keyboard might even be enhanced to allow ordering or organization of those graphics so that they can be quickly accessed. The extra graphics would need to be disabled if the target of the mobile keyboard (such as an email header line) would only accept text.\n\nOther features required to make embedded graphics work well include the ability of images to scale with font size, inclusion of embedded images in more transport protocols, switching services and applications to use protocols that do permit inclusion of embedded images (eg, MMS versus SMS for text messages). There will always, however, be places where embedded graphics can\u2019t be used\u2014such as email headers, SMS messages, or file names. There are also privacy aspects to implementations of embedded graphics: if the graphic itself is not packaged with the text, but instead is just a reference to an image on a server, then that server could track usage.\n\nThe main data file is [emoji-data]. The format for that file is described in its header.\n\nSee Emoji Charts for a collection of charts that have been generated from the emoji data file that may be useful in helping to understand it and the related CLDR emoji data (annotations and ordering). These charts are not versioned, and are purely illustrative; the data to use for implementation is in [emoji-data].\n\n26 REGIONAL INDICATOR symbols are used in pairs to represent country flags. Only valid sequences should be used, where:\n\nEmoji are generally presented with a square aspect ratio, which presents a problem for flags. The flag for Qatar is over 150% wider than tall; for Switzerland it is square; for Nepal it is over 20% taller than wide. To avoid a ransom-note effect, implementations may want to use a fixed ratio across all flags, such as 150%, with a blank band on the top and bottom. (The average width for flags is between 150% and 165%.) Narrower flags, such as the Swiss flag, may also have white bands on the side.\n\nFlags should have a visible edge. One option is to use a 1 pixel gray line chosen to be contrasting with the adjacent field color.\n\nThe code point order of flags is by region code, which will not be intuitive for viewers, since that rarely matches the order of countries in the viewer's language. English speakers are surprised that the flag for Germany comes before the flag for Djibouti. An alternative is to present the sorted order according to the localized country name, using CLDR data.\n\nFor an open-source set of flag images (png and svg), see region-flags.\n\nIn the past, most emoji characters have been selected primarily on the basis of compatibility. The scope is being broadened to include other factors, as listed below.\n\nTo submit a proposal for a new emoji character, fill out the form for Submitting Character Proposals. To that form, also add an annex that lists each of the selection factors below, and for each one provides evidence as to what degree each proposed character would satisfy that factor.\n\nNone of these factors are completely determinant. For example, the word for an object may be extremely common on the internet, but the object not necessarily a good candidate due to other factors.\n\nNote that symbols used in signage or user interfaces may be encoded in Unicode for reasons unconnected with their use as emoji.\n\nAside from the new diversity characters, the Unicode Consortium has accepted 36 other emoji characters as candidates\u00a0for Unicode 8.0, scheduled for mid-2015. These are candidates\u2014not yet finalized\u2014so some may not appear in the release.\n\nReview Note: Change the text from the \"candidate\" wording, since this document is intended for release shortly after Unicode 8.0, when they will no longer be candidates.\n\nThe Emoji modifiers are discussed in Section 2.2 Diversity. The Faces, Hands, and Zodiac Symbols are for compatibility with other messaging and mail systems. There are many other possible emoji that could be added, but releases need to be restricted to a manageable number. Many other emoji characters, such as other food items and symbols of religious significance, are still being assessed, and could appear in a future release of the Unicode Standard. See also Annex C: Selection Factors.\n\nThe images in the Draft Chart Glyph column below are draft black and white versions for the Unicode charts. They are likely to change before release. Once finalized, vendors that support emoji should provide a colorful appearance for each of these. The samples in the Sample Colored Glyph column below use a variety of different styles to show some possible presentations. These are only samples; vendor images may vary.\n\nSince April 2015, Apple\u2019s system software has used the ZWJ mechanism to enable presentation of multiple variations for FAMILY, COUPLE WITH HEART, and KISS; these are available as single images in the OS X Emoji Picker and the iOS Emoji Keyboard, and display as single images on those systems. These may be included in e-mail or text message sent to other systems.\n\nMark Davis and Peter Edberg created the initial versions of this document, and maintain the text.\n\nThanks to Shervin Afshar, Julie Allen, Michele Coady, Chenjintao (\u9648\u9526\u6d9b), Chenshiwei, Craig Cummings, Norbert Lindenberg, Ken Lunde, Rick McGowan, Katsuhiko Momoi, Katrina Parrott, Michelle Perham, Addison Phillips, Roozbeh Pournader, Markus Scherer, and Ken Whistler for feedback on and contributions to this document, including earlier versions. Thanks to Michael Everson for draft candidate 8.0 images.\n\nThanks to Apple, Microsoft, Google, and iDiversicons for supplying images for illustration.\n\nThe colored images used in this document and associated charts are for illustration only. They do not appear in the Unicode Standard, which has only black and white images. They are either made available by the respective vendors for use in this document, or are believed to be available for non-commercial reuse.\n\nThe Unicode Consortium is not a designer or purveyor of emoji images, nor is it the owner of any of the color images used in the document, nor does it negotiate licenses for their use. Inquiries for permission to use vendor images should be directed to those vendors, not to the Unicode Consortium.\n\nThe following summarizes modifications from the previous revisions of this document.\n\nCopyright \u00a9 2015 Unicode, Inc. All Rights Reserved. The Unicode Consortium makes no expressed or implied warranty of any kind, and assumes no liability for errors or omissions. No liability is assumed for incidental and consequential damages in connection with or arising out of the use of the information or programs contained or accompanying this technical report. The Unicode Terms of Use apply.\n\nUnicode and the Unicode logo are trademarks of Unicode, Inc., and are registered in some jurisdictions."},
{"url": "https://www.eff.org/deeplinks/2015/05/oversight-report-fbis-use-patriot-act-highlights-need-intelligence-reform-crucial", "link_title": "Report on FBI\u2019s Use of Patriot Act Highlights Need for Intelligence Reform", "sentiment": 0.059735554300771695, "text": "The Justice Department\u2019s Office of the Inspector General (OIG) yesterday released another report on the Federal Bureau of Investigation\u2019s use of Section 215 of the Patriot Act between 2007 and 2009. The report was long delayed due to declassification and redaction issues, but the timing is appropriate considering that the Senate is spending the waning hours of its legislative session considering the impending expiration of Section 215.\n\nThat\u2019s because the OIG report heightens the case for meaningful reform of the intelligence community by undermining many of the flimsy defenses offered by defenders of the status quo. Above all, the report demonstrates that secrecy and lack of oversight in the administration of surveillance laws is perhaps as significant as outright misuse.\n\nSection 215 is most famously the authority that the National Security Agency claims allows it to conduct mass collection of Americans\u2019 telephone records. A federal appeals court recently ruled that this interpretation was \u201cunprecedented and unwarranted\u201d and that the NSA\u2019s program was illegal. However, the FBI is actually the agency that administers the law, presenting applications for the collection of information to the secretive Foreign Intelligence Surveillance Court (FISC) on behalf of NSA, as well as the FBI itself.\n\nWhen the Patriot Act was reauthorized in 2005, Congress sought to address some concerns about Section 215 by mandating review by the OIG. The new report is the third to discuss the FBI\u2019s use of Section 215, and it revisits some of the problems uncovered by the previous reports. Most egregiously, the FBI took seven years to obey a law intended to protect Americans\u2019 privacy.\u00a0 The 2005 reauthorization required the FBI to adopt particularized \u201cminimization procedures\u201d to limit the amount of private information retained and disseminated by the FBI under Section 215 by no later than March 2006. But the FBI didn\u2019t do so until March 2013. During that time, of course, the FBI was continually assisting the NSA by filing applications for ongoing mass collection of telephone records, using an illegal interpretation of Section 215.\n\nBut the new report also shows that government\u2019s unauthorized interpretation of the \u201crelevance\u201d provision in Section 215 wasn\u2019t the only strained statutory reading of the law. During its seven years of foot-dragging over Section 215 minimization procedures, the FBI instead used a set of \u201cInterim Procedures\u201d that incorporated existing FBI National Security Investigations (NSI) Guidelines and \u201cconstrued\u201d them to meet the new requirements of the 2005 reauthorization. But \u201cFBI agents were already required to comply with the NSI Guidelines in their entirety,\u201d so \u201cthe Interim Procedures did not add any new requirements.\u201d In other words, the FBI unilaterally decided it could meet a new duty imposed by Congress by declaring its preexisting duty was enough.\n\nAs bad as the FBI\u2019s years-long failure to comply with the law are the failures of the FBI\u2019s overseers. The OIG first noted the problems with the FBI\u2019s Interim Procedures in its 2008 report, but not until 2009 did the FISC, the court charged with evaluating the FBI\u2019s 215 applications, take notice and ask the FBI to explain. Given that it still took another four years for the FBI to adopt the final procedures and that the FISC uniformly continued approving 215 applications all the while, it\u2019s not clear the FISC\u2019s involvement mattered much.\n\nNor does the ultimate outcome of this story instill much confidence in the process. Even the final Section 215 minimization procedures from 2013 contain key language allowing retention of information \u201cnecessary to understand foreign intelligence.\u201d As the report notes, this standard is undefined and is subject to open-ended interpretation by government lawyers, risking undermining the minimizations procedures\u2019 privacy protections\n\nAs we\u2019ve described at length, the FISC is hamstrung by its secrecy and the one-sided nature of its proceedings, something this report illustrates in detail. Moreover, the OIG itself had repeated difficulty obtaining certain information for its reports because the FBI claimed that it was not allowed to disclose this information for \u201coversight purposes.\u201d\n\nThe new OIG report also has large swaths of information redacted from the public version, and some of these redactions are troubling for anyone who favors robust oversight. For example, the report discusses the use of Section 215 for bulk surveillance\u2014a fact the government only officially acknowledged after Edward Snowden disclosed proof\u2014but the only unredacted information in this section of the report concerns the NSA\u2019s phone records program. Despite the redactions, it is clear that Section 215 is used for bulk collection of other records, something that has been previously reported. Other examples abound: the report notes that the FBI employs a \u201cclassified directive\u201d to define the term \u201cU.S. Person\u201d and that the agency cannot definitively say what information counts as \u201cmetadata.\u201d The withholding of this information from public scrutiny confounds true oversight, including any attempt to do an accounting of how many Americans are subject to surveillance under Section 215.\n\nPublic debate about surveillance reform and Section 215 has understandably focused on the NSA and the phone records program. The OIG report, however, is an excellent reminder of several key points as we continue to fight: 1) Our concerns about the NSA should not cause us to ignore the FBI\u2019s role in illegal surveillance; 2) Section 215 is about much more than bulk collection of phone records; and 3) so long as the intelligence and law enforcement communities can easily hide behind \u201cit\u2019s classified,\u201d true reform will be a long way off."},
{"url": "http://blog.cryptographyengineering.com/2015/05/attack-of-week-logjam.html", "link_title": "Attack of the week: Logjam", "sentiment": 0.02196831147918106, "text": "The good news here is that weak Diffie-Hellman parameters are almost never used purposely on the Internet. Only a trivial fraction of the SSL/TLS servers out there today will organically negotiate 512-bit Diffie-Hellman. For the most part these are crappy embedded devices such as routers and video-conferencing gateways.\n\n\n\n \n\n This all sound simple enough. However, one of the early,\u00a0at all. SSLv3 and TLS tacked on an authentication process -- but one that takes place only at the end of the handshake.*\u00a0 You see, before SSL/TLS peers can start engaging in all this fancy cryptography, they first need to decide which ciphers they're going to use. This is done through a negotiation process in which the client proposes some options (e.g., RSA, DHE, DHE-EXPORT), and the server picks one.This all sound simple enough. However, one of the early, well known flaws in SSL/TLS is the protocol's failure to properly authenticate these 'negotiation' messages. In very early versions of SSL they were not authenticated. SSLv3 and TLS tacked on an authentication process -- but one that takes place only at theof the handshake.*\n\nServerKeyExchange (shown at right). The signed portion of this message covers the parameters, but neglects to include any information about\u00a0which\u00a0ciphersuite the server thinks it's negotiating. If you remember that the only difference between DHE and DHE-EXPORT is the size\u00a0of the parameters the server sends down, you might start to see the problem.\n\n \n\n Here it is in a nutshell: if the server supports DHE-EXPORT, the attacker can 'edit' the negotiation messages sent from the a client -- even if the client doesn't support export DHE -- replacing the client's list of supported ciphers with\u00a0only export DHE. The server will in turn send back a signed 512-bit export-grade Diffie-Hellman tuple, which the client will blindly accept -- because it doesn't realize that\u00a0the server is negotiating the export version of the ciphersuite. From its perspective this message looks just like 'standard' Diffie-Hellman with really crappy parameters.\u00a0 This is particularly unfortunate given that TLS servers often have the ability to authenticate their messages using digital signatures , but don't really take advantage of this. For example, when two parties negotiate Diffie-Hellman, the parameters sent by the server are transmitted within a signed message called the(shown at right). The signed portion of this message covers the parameters, but neglects to include any information aboutciphersuite the server thinks it's negotiating. If you remember that the only difference between DHE and DHE-EXPORT is theof the parameters the server sends down, you might start to see the problem.Here it is in a nutshell: if the server supports DHE-EXPORT, the attacker can 'edit' the negotiation messages sent from the a client -- even if the client doesn't support export DHE -- replacing the client's list of supported ciphers withexport DHE. The server will in turn send back a signed 512-bit export-grade Diffie-Hellman tuple, which the client will blindly accept -- because itFrom its perspective this message looks just like 'standard' Diffie-Hellman with really crappy parameters.\n\n\n\n All this tampering should run into a huge snag at the end of the handshake, when he client and server exchange Finished messages embedding include a MAC of the transcript. At this point the client should learn that something funny is going on, i.e., that what it sent no longer matches what the server is seeing. However, the loophole is this:\u00a0if the attacker can recover the Diffie-Hellman secret\u00a0quickly\u00a0-- before the handshake ends -- she can forge her own\u00a0Finished\u00a0messages. In that case the client and server will be none the wiser. All this tampering should run into a huge snag at the end of the handshake, when he client and server exchangemessages embedding include a MAC of the transcript. At this point the client should learn that something funny is going on, i.e., that what it sent no longer matches what the server is seeing. However, the loophole is this:\u00a0if the attacker can recover the Diffie-Hellman secret-- before the handshake ends -- she can forge her ownmessages. In that case the client and server will be none the wiser.\n\nseem to rule out solving discrete logs in real time.\n\n \n\n However, there is a complication. In practice, NFS can actually be broken up into two different steps:\n\n Pre-computation (for a given prime\u00a0p).\u00a0This includes the process of polynomial selection, sieving, and linear algebra, all of which depend only on\u00a0p. The output of this stage is a table for use in the second stage. Solving to find\u00a0a\u00a0(for a given\u00a0ga\u00a0mod p).\u00a0The final stage, called the descent, uses the table from the precomputation. This is the only part of the algorithm that actually involves a specific g and\u00a0ga. The important thing to know is that the first stage of the attack consumes the vast majority of the time, up to a full week on a large-scale compute cluster. The descent stage, on the other hand, requires only a few core-minutes. Thus the attack cost depends primarily on where the server gets its Diffie-Hellman parameters from. The best case for an attacker is when\u00a0p\u00a0is hard-coded into the server software and used across millions of machines. The worst case is when p\u00a0is re-generated routinely by the server.\n\n \n\n I'll let you guess what real TLS servers actually do. export Diffie-Hellman is particularly awful, with only two (!) primes used across up 92% of enabled Apache/mod_ssl sites.\n\n Number of seconds to solve a\n\n 512-bit discrete log\u00a0paper \n\n The upshot of all of this is that about two weeks of pre-computation is sufficient to build a table that allows you to perform the downgrade against most export-enabled servers\u00a0in just a few minutes (see the chart at right).\u00a0This is fast enough that it can be done before the TLS connection timeout. Moreover, even if this is not fast enough, the connection can often be held open longer by using clever protocol tricks, such as sending TLS warning messages to reset the timeout clock.\n\n \n\n Keep in mind that none of this shared prime craziness matters when you're using sufficiently large prime numbers (on the order of 2048 bits or more). It's only a practical issue you're using small primes, like 512-bit, 768-bit or -- and here's a sticky one I'll come back to in a minute -- 1024 bit.\n\n How do you fix the downgrade to export DHE? The best and most obvious fix for this problem is to exterminate export ciphersuites from the Internet. Unfortunately, these awful configurations are the default in a number of server software packages (looking at you e.g., ).\n\n \n\n A simpler fix is to upgrade the major web browsers to resist the attack. The easy way to do this is to enforce a larger minimum size for received DHE keys. The problem here is that the fix itself causes some collateral damage -- it will break a small but significant fraction of lousy servers that organically negotiate (non-export) DHE with 512 bit keys.\n\n \n\n The good news here is that the major browsers have decided to break the Internet (a little) rather than allow it to break them. Each has agreed to raise the minimum size limit to at least 768 bits, and some to a \n\n What does this mean for larger parameter sizes? In fact, large-scale Internet scans by the team at University of Michigan show that most popular web servers software tends to re-use a small number of primes across thousands of server instances. This is done because generating prime numbers is scary, so implementers default to using a hard-coded value or a config file supplied by your Linux distribution. The situation forDiffie-Hellman is particularly awful, with only two (!) primes used across up 92% of enabled Apache/mod_ssl sites.The upshot of all of this is that about two weeks of pre-computation is sufficient to build a table that allows you to perform the downgrade against most export-enabled servers(see the chart at right)This is fast enough that it can be done before the TLS connection timeout. Moreover, even if this is not fast enough, the connection can often be held open longer by using clever protocol tricks, such as sending TLS warning messages to reset the timeout clock.Keep in mind that none of this shared prime craziness matters when you're using sufficiently large prime numbers (on the order of 2048 bits or more). It's only a practical issue you're using small primes, like 512-bit, 768-bit or -- and here's a sticky one I'll come back to in a minute -- 1024 bit.The best and most obvious fix for this problem is to exterminate export ciphersuites from the Internet. Unfortunately, these awful configurations are the default in a number of server software packages (looking at you Postfix ), and getting people to update their configurations is surprisingly difficult (see FREAK ).A simpler fix is to upgrade the major web browsers to resist the attack. The easy way to do this is to enforce a larger minimum size for received DHE keys. The problem here is that the fix itself causes some collateral damage -- it will break a small but significant fraction of lousy servers that organically negotiate (non-export) DHE with 512 bit keys.The good news here is that the major browsers have decided to break the Internet (a little) rather than allow it to break them. Each has agreed to raise the minimum size limit to at least 768 bits, and some to a minimum of 1024 bits . It's still not perfect, since 1024-bit DHE may not be cryptographically sound against powerful attackers, but it does address the immediate export attack. In the longer term the question is whether to use larger negotiated DHE groups , or abandon DHE altogether and move to elliptic curves. In practice, the fastest route to solving the discrete logarithm in finite fields is via an algorithm called the Number Field Sieve (NFS). Using NFS to solve a single 512-bit discrete logarithm instance requires several core-years -- or about week of wall-clock time given a few thousand cores -- which wouldto rule out solving discrete logs in real time.However, there is a complication. In practice, NFS can actually be broken up into two different steps:\n\neven when you account for active downgrade attacks. The vast majority of servers use Diffie-Hellman moduli of length at least 1024 bits. (The widespread use of 1024 is largely due to a hard-cap in older Java clients. Go away Java.)\n\n \n\n While 2048-bit moduli are generally believed to be outside of anyone's reach, 1024-bit DHE has long been considered to be at least within groping range of nation-state attackers. We've known this for years, of course, but the practical implications haven't been quite clear. This paper tries to shine some light on that, using Internet-wide measurements and software/hardware estimates.\n\n \n\n If you recall from above, the most critical aspect of the NFS attack is the need to perform large amounts of pre-computation on a given Diffie-Hellman prime p, followed by a relatively short calculation to break any given connection that uses p. At the 512-bit size the pre-computation only requires about a week. The question then is, how much does it cost for a 1024-bit prime, and how common are shared primes?\n\n \n\n While there's no exact way to know how much the 1024-bit attack would cost, the paper attempts to provide some extrapolations based on current knowledge. With software, the cost of the pre-computation seems quite high -- on the order of 35 million core-years. Making this happen for a given prime within a reasonable amount of time (say, one year) would appear to require billions of dollars of computing equipment if we assume no algorithmic improvements. Even if we rule out such\u00a0improvements, it's conceivable that this cost might be brought down to a few hundred million dollars using hardware. This doesn't seem out of bounds when you consider\u00a0\n\n \n\n What's interesting is that the descent stage, required to break a given Diffie-Hellman connection, is much faster. Based on some implementation experiments by the \n\n Is the NSA actually doing this? So far all we've noted is that NFS pre-computation is at least potentially feasible when 1024-bit primes are re-used. That doesn't mean the NSA is actually doing any of it.\n\n \n\n There is some evidence, however, that suggests the NSA has decryption capability that's at least consistent with such a break. This evidence comes from a series of Snowden documents\u00a0\n\n \n\n While the architecture described by the documents mentions attacks against many protocols, the bulk of the energy seems to be around the\u00a0\n\n \n\n The nature of the NSA's exploit is never made clear in the documents, but diagram at right gives a lot of the architectural details. The system involves collecting Internet Key Exchange (IKE) handshakes, transmitting them to the NSA's Cryptanalysis and Exploitation Services (CES) enclave, and feeding them into a decryption system that controls substantial high performance computing resources to process the intercepted exchanges. This is at least circumstantially consistent\u00a0with Diffie-Hellman cryptanalysis.\n\n \n\n Of course it's entirely possible that the attack is based on a bad random number generator, weak symmetric encryption, or any number of engineered backdoors. There are a few pieces of evidence that militate towards a Diffie-Hellman break, however:\n\n \n\n IPSec (or rather, the IKE key exchange) uses Diffie-Hellman for every single connection, meaning that it can't be broken without some kind of exploit, although this doesn't rule out the other explanations. The IKE exchange is particularly vulnerable to pre-computation, since IKE uses a small number of standardized\u00a0prime numbers called the\u00a0Oakley groups, which are going on 17 years old now. Large-scale Internet scanning by the Michigan team shows that a majority of responding IPSec endpoints will gladly negotiate using Oakley Group 1 (768 bit) or Group 2 (1024 bit),\u00a0even when the initiator offers better options. The NSA's exploit appears to require the entire IKE handshake as well as any pre-shared key (PSK). These inputs would be necessary for recovery of IKEv1 session keys, but are not required in a break that involves only symmetric cryptography. The documents explicitly rule out the use of malware, or rather, they show that such malware ('TAO implants') is in use -- but that malware allows the NSA to bypass the IKE handshake altogether. The good news so far is that 512-bit Diffie-Hellman is only used by a fraction of the Internet,when you account for active downgrade attacks. The vast majority of servers use Diffie-Hellman moduli of length at least 1024 bits. (The widespread use of 1024 is largely due to a hard-cap in older Java clients. Go away Java.)While 2048-bit moduli are generally believed to be outside of anyone's reach, 1024-bit DHE has long been considered to be at least within groping range of nation-state attackers. We've known this for years, of course, but the practical implications haven't been quite clear. This paper tries to shine some light on that, using Internet-wide measurements and software/hardware estimates.If you recall from above, the most critical aspect of the NFS attack is the need to perform large amounts of pre-computation on a given Diffie-Hellman prime, followed by a relatively short calculation to break any given connection that usesAt the 512-bit size the pre-computation only requires about a week. The question then is, how much does it cost for a 1024-bit prime, and how common are shared primes?While there's no exact way to know how much the 1024-bit attack would cost, the paper attempts to provide some extrapolations based on current knowledge. With software, the cost of the pre-computation seems quite high -- on the order of 35 million core-years. Making this happen for a given prime within a reasonable amount of time (say, one year) would appear to require billions of dollars of computing equipmentEven if we rule out such\u00a0improvements, it's conceivable that this cost might be brought down to a few hundred million dollars using hardware. This doesn't seem out of bounds when you consider leaked NSA cryptanalysis budgets. What's interesting is that the descent stage, required to break a given Diffie-Hellman connection, isfaster. Based on some implementation experiments by the CADO-NFS team, it may be possible to break a Diffie-Hellman connection in as little as 30 core-days, with parallelization hugely reducing the wall-clock time. This might even make near-real-time decryption of Diffie-Hellman connections practical.So far all we've noted is that NFS pre-computation is at leastfeasible when 1024-bit primes are re-used. That doesn't mean the NSA is actually doing any of it.There is some evidence, however, that suggests the NSA has decryption capability that's at least consistent with such a break. This evidence comes from a series of Snowden documents published last winter in Der Spiegel . Together they\u00a0describe a large-scale effort at NSA and GCHQ, capable of decrypting 'vast' amounts of Internet traffic, including IPSec, SSH and HTTPS connections.While the architecture described by the documents mentions attacks against many protocols, the bulk of the energy seems to be around the IPSec and IKE protocols , which are used to establish Virtual Private Networks (VPNs) between individuals and corporate networks such as financial institutions.The nature of the NSA's exploit is never made clear in the documents, but diagram at right gives a lot of the architectural details. The system involves collecting Internet Key Exchange (IKE) handshakes, transmitting them to the NSA's Cryptanalysis and Exploitation Services (CES) enclave, and feeding them into a decryption system that controls substantial high performance computing resources to process the intercepted exchanges. This is at least circumstantially consistent\u00a0with Diffie-Hellman cryptanalysis.Of course it's entirely possible that the attack is based on a bad random number generator, weak symmetric encryption, or any number of engineered backdoors. There are a few pieces of evidence that militate towards a Diffie-Hellman break, however:"},
{"url": "http://blog.siftscience.com/blog/2015/making-d3-react-the-best-of-friends", "link_title": "d-Threeact: Making d3 and React the Best of Friends", "sentiment": 0.28898809523809527, "text": "A little less than a year ago, the Sift Science Console Team decided to migrate its Backbone and Marionette Views to ReactJS (see also our post on specific React tips we learned along the way). Among the many facets of a piece-by-piece migration like this was figuring out how best to manage (err...'reactify') our d3 charts. There were already a couple good reads/listens on this topic\u2014with very different views on the responsibilities of each library\u2014which we found quite helpful in establishing our own approach.\n\nUltimately, we decided that our d3 code should be isolated from our React code and only be available to React via a simple React Component interface. This way, we could take advantage of React's one-way data flow into the component but still let the powerful d3 take care of all things svg-related (layout, data-binding, transitions, etc). So our ideal component would look like:"},
{"url": "https://medium.com/@AustinHay/unlocking-the-secrets-of-mobile-growth-881322e71ad0", "link_title": "Unlocking the Secrets of Mobile Growth with the Lead Growth Engineer Pinterest", "sentiment": 0.20707998350855486, "text": "Building and growing your own app is the modern crucible that every developer faces. The formula to get started often looks the same:\n\nBut the journey of building such apps has a darker side most people will never experience. There are over 1.2 million apps in the iOS store to date and as many, if not more, in the Android Play Store. Between navigating the competition, figuring out the right monetization model, or getting users on board, most apps\u200a\u2014\u200aand the very real people behind them\u200a\u2014\u200ahardly have a fighting chance. Today, less than 2% of the Top 250 publishers for iPhone apps in the U.S. App Store are newcomers.\n\nSo if you\u2019re a developer and you\u2019ve spent hours and hours pouring yourself into building a great app that can provide real value to others around the world, what\u2019s your best chance at success? The answer is simple: focus on growth.\n\nIn fact, leaders in mobile think that growth is the only problem that they face in building their app or a minimum viable product that actually matters. But what if you could sit down with some of these leaders and dig deeper. What if you could ask them how to build your growth engine?\n\nThat\u2019s exactly what I did. And you can listen in on all of it.\n\nIn this short conversation with John Egan, Lead Growth Engineer at Pinterest, and Alex Austin, CEO and Co-Founder of Branch Metrics, we dive into the most crucial elements of growth that every app developer and mobile expert should care about. As discussed in this conversation, here are four high-level principles that will help you succeed in driving growth.\n\nActivations are an essential growth metric, but often people don\u2019t know what they mean to their app, or they don\u2019t know where to start in acquiring them. Luckily, John Egan shed some light on activations and explained, \u201cAt Pinterest, we view activations\u200a\u2014\u200athe process of getting someone who just signed up for your service to stick around and become a regular user\u200a\u2014\u200aas being one of the most important pieces of growth that you have to absolutely nail and continually improve.\u201d\n\nSo, how does John deliver on that mantra? By focusing on NUX, or New User Experience. NUX is something that is often overlooked, but is insanely important in driving higher activations. Based on his experience building Kindred, a photobook sharing app, Alex added that from the top of their funnel to install and registration, they experienced more than a 50% dropoff. In his words, \u201cYou did all this work to try to get somebody to get into your app and become a real, loyal user, and half of them never come back again.\u201d\n\nThe importance of monitoring and improving activations really hit home with Alex\u2019s following point: As hard as you work just to get people into the app, you have to work twice as hard to get that same number of users through the activation process. It\u2019s for this reason that a great activation experience is so important; it is the ultimate gateway to engagement and in-app success.\n\nSo really then, what\u2019s the deal with activations? As John put it, \u201cUsers won\u2019t stick around for a variety of reasons, but the biggest reason is that they don\u2019t understand the value your app provides to them.\u201d The key to activations is communicating your value prop effectively. As an app developer, the single best thing you can do to increase activations is to really understand and demonstrate your value prop as soon as you can. Communicate different use cases to users and find the one that really resonates with them.\n\nIt\u2019s a simple question, and a struggle that apps face when trying to grow their user base in a frictionless way. Just because Pinterest has forced registration (you can\u2019t use Pinterest without signing up) doesn\u2019t mean everyone should. While forced registration is a mixed bag of costs and benefits for every app, the only sure way to know is to do deep analysis around your signup wall. Test and iterate to see what works. If 2\u20133 months down the line you see that loss reflected in the funnel, then you know that a forced registration strategy may not be the best for your app.\n\nBut beyond just knowing if you need a signup wall is how to drive the success of that signup wall. As John noted, this is where gathering user information before and during signup is so important in improving the user experience. There are a few key approaches to doing this:\n\n- Personalize the experience through context\n\n- Gather user interests during signup to customize the home-feed\n\n- Re-engage users through emails and notifications\n\n- Create a habit of visitation\n\nThis last point from John was particularly clarifying. Pinterest tries to find and send interesting content by email in an attempt to form a habit of visiting the app for great content. Building the habit of visiting the app is important, and app devs can do it by continually showing current users the value their service is providing current users. It\u2019s no surprise that this is so inherently tied with activations, which require you to demonstrate your value proposition early, often, and very clearly.\n\nI think John put it best when he mentioned that, \u201ckeeping users engaged is really all about continuing to deliver on your value proposition.\u201d\n\nIt\u2019s no surprise, but one of the best ways to drive growth is by implementing sharing and referral features inside of your app that enable organic, word of mouth drivers. In highlighting this thought, John revisited his experience at Shopkick where a majority of their app signups came through invites.\n\nWhat were some of the things John did to drive high conversion to invite and responses to invites?\n\n1.) Deeplink to the app store further and further upstream. John made an active decision to move away from mobile web-splash because they experienced too much dropoff with this additional step. But Alex qualifies this point, reminding us that a mobile splash screen before the app store can serve a very amazing purpose, acting as the app\u2019s gateway to the desktop web. In cases like these, it\u2019s all about optimizing for conversions versus dropoffs. If a mobile splash page creates too much dropoff from the web integrated version of your site, then consider not using it. Otherwise, as Alex states succinctly, \u201c[If you] provide the carrot before the app store install process, [you can] expect good returns for mobile web splash.\u201d\n\n2.) Referrals are a huge part of a high growth app engine. For John, Shopkick was all about referrals. To really do referrals right, you need to identify your biggest strengths as an app and leverage those. For Pinterest, that strength is great content. Pinterest has over 50 billion pins, which act as a sharing network in themselves. What is your app equivalent to pins?\n\n3.) Incentives structures make a huge difference. The more interesting and appealing you can make the incentive the more people will invite others. At Shopkick it looked like this: you got 50 kicks for every person that joined, and so does the person that signs up. However, the incentive that worked best went bigger: offering 2000 kicks for inviting 3 or more friends worked really well because having the bigger, more appealing incentive increased people\u2019s willingness to invite. And what\u2019s the cost to you, as a developer? Well 2000 kicks was less than $8, meaning for solid, loyal, reliable new users the CPC is less than $3 and highly scaleable. As John mentioned, the best part was that the better prize (that required 3 or more invites) not only increased people\u2019s willingness to invite, but the number of invites they sent. There\u2019s something for growth developers to care about: not only more people inviting, but more people being invited per person.\n\n4.) Enable your users to share. It sounds like following all of John\u2019s advice would lead to this naturally, but how can app developers create a system to keep people inviting once they\u2019ve done it once? Prompting once might not be enough. For John and Shopkick, they had a series of in-app prompts. Specifically, one in the NUX, one in the profile page, one in the rewards mall (if you clicked on a reward, Shopkick would tell you the price in the number of people you had to invite). Beyond this, the points balance for every user would have a prompt for invite.\n\nThough John and Alex\u2019s advice on building an incredible sharing and referral program should hit home for developers looking to grow their app, it\u2019s important to take it all with a grain of salt. Don\u2019t just go through the motions to check the box for \u201cinvites and sharing.\u201d Do it because it matters, do it right, and make your sharing program fun, unique, and easy to participate in. Don\u2019t do it unless it means something to your service.\n\nIn our final moments, John and Alex also shared their growth advice for mobile devs just starting the arduous journey of app development. At the core of it all, you have to figure out product-market fit. It may sound obvious and straight out of HBO\u2019s Silicon Valley, but John articulated on what Product Market fit means specifically in terms of growth:\n\nEarly Stage Apps\n\nProduct-market fit is all about really knowing your value proposition. There\u2019s no point spinning your wheels creating growth features and building a growth engine if nobody is going to get value out of it.\n\nLater Stage Apps with Traction\n\nAt this stage, product-market fit should already be established. Now, focus on figuring out which channels really allow your users to discover, love, and share your app.\n\nWhat not to do? Don\u2019t blast your message all over the internet\u200a\u2014\u200afigure out your specific users and where they spend their time.\n\nSimilarly, Alex went on to explain that its not only important to figure out the product-market fit initially, but to continue it all the way through the funnel. For example, while building Kindred, Alex found that conversion from activation to sharing was incredibly low. Put lightly, \u201cThe real problem is that nobody was buying anything with our app. The product wasn\u2019t something that could have massive appeal. It made us realize our business model was fundamentally flawed.\u201d\n\nIf you have yet to be convinced of the importance of product-market fit, perhaps John\u2019s closing remarks on Sean Ellis will do the trick. This alternative approach, especially relevant for consumer products, requires you to survey your users and ask, \u201cHow disappointed would you be if the product went away?\u201d Asking this question forces you to pay attention to the users that matter most: the people who would be very disappointed if your product was gone.\n\nAs an app developer looking to grow your product and prove product-market fit, you should focus on people who care enough, who would go spread your product through word of mouth and tell their friends about it. To really assess product market fit, you want to see that 20\u201340% of your users would be \u201cvery disappointed\u201d if your product went away."},
{"url": "https://www.b2bsphere.com", "link_title": "Show HN: Business Collaboration for Small Business", "sentiment": 0.0, "text": "Search for qualified suppliers from verified source and post RFQ"},
{"url": "http://www.linuxveda.com/2015/05/22/steam-os-developers-got-netflix-working-linux-wrap/", "link_title": "Developer Brings Netflix to Steam OS: Linux Wrap", "sentiment": 0.06354166666666665, "text": "A developer that goes by the handle ProfessorKaos64 posted on reddit:\n\nWe are hiring! We are looking for tutorials, tips & tricks, reviews, and news analysis. If you are interested, fill this form and contact us now!\n\nBe advised this is a work in progress. Netflix will launch via google-chrome-stable and connect to netflix in \u2013kiosk mode. A default users config directory for Google chrome is copied over, so that the \u201cunexpected shutdown\u201d message is avoided (since for now, you must use the center Xbox 360 controller / PS3 controller to exit.). I am currently working on creating some snap in script code to add Hulu, Youtube, and so forth. I need to gather the images and test those sites first though.\n\n \n\n He has posted the script on GitHub for others to try. It\u2019s in really early stage of development but it\u2019s surprising and at the same time disappointing to see that Valve made no efforts whatsoever to bring steaming services like Netflix to their platform that they plan to dominate the living room.\n\nStreaming services like Netflix, Hulu or HBO are encouraging people to cut cords and move online. Only thing holding Valve back could be the fact that SteamMachine have yet to hit the mass market to give and incentive to the \u2018streaming\u2019 services to consider it a viable platform. That said, services like Netflix do support Linux natively through Google Chrome browser so what is stopping Valve from integrating the service to their \u2018Big Picture\u2019?"},
{"url": "http://www.linuxveda.com/2015/05/22/fedora-22-will-arrive-time/", "link_title": "Fedora 22 will arrive on time", "sentiment": 0.027575757575757573, "text": "The scheduled meeting for today happened and Fedora 22 is set to launch on May 26, next Tuesday. In the Fedora mailing lists Jaroslav Reznik said \u201cAt the Fedora 22 Final Go/No-Go Meeting #2 that just occurred, it was agreed to Go with the Fedora 22 Final by Fedora QA, Release Engineering and Development.\u201d\n\nThere\u2019s one certainty in life, that is: Fedora will never arrive on time. In a post to the Fedora devel-announce mailing list the results of the \u2018Fedora 22 Final Go/No-Go\u2019 meeting were announced, it was a No-Go. Thankfully there was another meeting later today (May 22) to determine whether the release can be signed off and that\u2019s where it was decided to \u2018Go\u2019.\n\nFedora acts as a test-bed for new features that eventually get introduced in RHEL. In Fedora 22 we get Python 3 by default, Python 2 is not even going to be installed but shall be available in the Fedora repositories should applications still need it.\n\nThe other major change is the switch from the YUM package manager to the DNF package manager. DNF is the natural successor to YUM, DNF is an abbreviation of \u2018Dandified Yum\u2019. DNF addresses the major issues present in YUM such as poor performance, high memory usage, and the slowness of its iterative dependency resolution.\n\nFedora\u2019s \u2018blocker bugs\u2019 website suggests that four blockers are present in the release casting doubt onto whether the final release will be signed off tomorrow. One of the listed bugs includes the filesystem corruption issue in the 4.0 kernel, which has now been fixed but distros are yet to deploy the fix."},
{"url": "http://blog.edward-kim.com/work", "link_title": "Work", "sentiment": 0.26459235209235205, "text": "After graduating from college, my first job was as an engineer in the Volkswagen Group Electronics Research Lab. The mission of this group was to build and integrate web services that would improve the driving experience for our customers. I joined Volkswagen because I was excited about this mission, and I worked hard with like-minded people to help usher in the age of internet-connected vehicles.\n\nI loved every day of work at Volkswagen because of the mission that I was on with my team. But 2 years later, I decided to leave to start my own company. On my last day of work, a feeling of sadness overwhelmed me as I was packing my desk. I probably even cried a little. But it wasn\u2019t just because I would not be there to see the mission I signed up for through to completion. It was more because my colleagues had become my friends and I had to say goodbye to all the wonderful people I had created so many memories with.\n\nLooking back, I realized a very interesting thing happened: I joined a company to be a part of its mission. But over time, the people I worked with became a bigger and bigger part of what kept me there. By the end of it, I came to the realization that while I still very much loved the mission we were on, the people that I interacted with every day were even bigger factors of why I loved coming into work.\n\nIf the environment is just right, the lines between work and life can sometimes blur over time. We join companies and initially get put in teams with complete strangers. We solve problems and celebrate our wins together. We chit-chat in the hall, have a drink together, laugh, fight, and become friends. We sometimes even fall in and out of love with each other.\n\nThough we may still call it \u201cwork\u201d, the people we see every day gradually seeps into our lives. Of course, a lot of life happens outside of work as well, but the boundaries of work and life are not as clear as the hours on our time sheets.\n\nI\u2019m excited to come into work every day. It\u2019s because of the mission I\u2019m on, but it\u2019s more because I love the people I get to be around.\n\nMuch of life happens at work. Much of life happens outside of work."},
{"url": "http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial", "link_title": "Deep Learning for NLP (without Magic)", "sentiment": 0.0923955296404276, "text": "Machine learning is everywhere in today's NLP, but by and large machine learning amounts to numerical optimization of weights for human designed representations and features. The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appropriate for complex interpretation tasks. This tutorial aims to cover the basic motivation, ideas, models and learning algorithms in deep learning for natural language processing. Recently, these methods have been shown to perform very well on various NLP tasks such as language modeling, POS tagging, named entity recognition, sentiment analysis and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed resources or time-intensive feature engineering. Despite these advantages, many researchers in NLP are not familiar with these methods. Our focus is on insight and understanding, using graphical illustrations and simple, intuitive derivations. The goal of the tutorial is to make the inner workings of these techniques transparent, intuitive and their results interpretable, rather than black boxes labeled \"magic here\". The first part of the tutorial presents the basics of neural networks, neural word vectors, several simple models based on local windows and the math and algorithms of training via backpropagation. In this section applications include language modeling and POS tagging. In the second section we present recursive neural networks which can learn structured tree outputs as well as vector representations for phrases and sentences. We cover both equations as well as applications. We show how training can be achieved by a modified version of the backpropagation algorithm introduced before. These modifications allow the algorithm to work on tree structures. Applications include sentiment analysis and paraphrase detection. We also draw connections to recent work in semantic compositionality in vector spaces. The principle goal, again, is to make these methods appear intuitive and interpretable rather than mathematically confusing. By this point in the tutorial, the audience members should have a clear understanding of how to build a deep learning system for word-, sentence- and document-level tasks. The last part of the tutorial gives a general overview of the different applications of deep learning in NLP, including bag of words models. We will provide a discussion of NLP-oriented issues in modeling, interpretation, representational power, and optimization.\n\nSave your text first, then fill out captcha, then save again.\n\nHi Richard, I see that you use MATLAB and Java. It is better than use, for instance, Theano (That I see you also use)? I'm ML scientist (NLP), various on ML concepts are clear to me (specially on regularized machines and MLP) although there is a huge to learn. However implementations and large experiments are missing in mi phd thesis (on learning semantic features) so I'd like to get started. I know MATLAB, C and Python (little less), however the latter seems to be better for several NLP tasks. Any way for possible cases you consider according to your expertise, what do you recommend me?"},
{"url": "http://xtensio.com", "link_title": "Xtensio", "sentiment": 0.0, "text": "Who are your customers? What are their aspirations and frustrations? Every decision you make, from product features\u2019 design to the tone of your taglines, needs to answer to\u00a0your customers. This User (or Buyer) Persona Creator helps you come up with your user \u201ctypes\u201d. See Tool"},
{"url": "http://popcorntimefree.info/?ref=hackernews", "link_title": "Show HN: Popcorn Time in your browser", "sentiment": 0.0, "text": "Oops. It looks like we don't have that movie. Try another one."},
{"url": "http://www.theglobeandmail.com/report-on-business/excerpt-how-blackberrys-bid-to-one-up-the-iphone-failed/article24555365/", "link_title": "How BlackBerry\u2019s bid to one-up the iPhone failed", "sentiment": 0.09524355472216431, "text": "Mike Lazaridis, ever the boy electrician, liked to relax by tearing apart small machines in his spare time. Just as he once opened radios in his basement lab for fun, Lazaridis lifted hoods on competitors\u2019 phones. Staff visiting his third-floor office in a building called RIM 4 grew accustomed to disemboweled phones with chipsets, antennas, and wires strewn across his desk. Usually the desktop autopsies confirmed Lazaridis\u2019s faith that BlackBerry was the smartest phone on the market.\n\nIn the summer of 2007, however, Lazaridis cracked open a phone that gave him pause. \u201cThey\u2019ve put a Mac in this thing,\u201d he marvelled after peering inside one of the new iPhones. Ever since Apple\u2019s phone went on sale in June, critics and consumers were effusive about the sleek phone\u2019s playful touch screen, elegant graphics, and high-resolution images. Lazaridis saw much more. This was no ordinary smartphone. It was a small mobile Apple computer whose operating system used 700 megabytes of memory \u2013 more than twenty-two times the computing power of the BlackBerry. The iPhone had a full Safari browser that traveled everywhere on the Internet. With AT&T\u2019s backing, he could see, Apple was changing the direction of the industry.\n\nLazaridis shared the revelation with his handset engineers, who had been pushing to expand BlackBerry\u2019s Internet reach for years. Before, Lazaridis had waved them off. Carriers wouldn\u2019t allow RIM to include more than a simple browser because it would crash their networks. After his iPhone autopsy, however, he realized the smartphone race was in danger of shifting. If consumers and carriers continued to embrace the iPhone, BlackBerry would need more than its efficient e-mail and battery to lead the market. \u201cIf this thing catches on, we\u2019re competing with a Mac, not a Nokia,\u201d he said. The new battleground was mobile computing. Lazaridis figured RIM\u2019s core corporate market was safe because the iPhone couldn\u2019t match BlackBerry\u2019s reliable keyboard and in-house network delivery of secure e-mails. But in the consumer market, where the Pearl phone was competing, RIM needed a full Web browser. BlackBerry was a sensation because it put e-mail in people\u2019s pockets. Now, iPhone was offering the full Internet. If BlackBerry was to prevail, he told RIM\u2019s engineers, \u201cWe have to fix everything that\u2019s wrong with the iPhone.\u201d\n\nWhile Lazaridis pushed internally for a response to the iPhone, publicly he and Balsillie dismissed their new rival. Companies often ignore competitors\u2019 triumphs, but by downplaying a consumer sensation, RIM suddenly seemed out of touch. \u201cI haven\u2019t seen one,\u201d RIM co-CEO Jim Balsillie told the Toronto Star after the iPhone went on sale in June. Months later, when the iPhone grabbed a fifth of the U.S. smartphone market, Lazaridis complained to the New York Times about its keyboard: \u201cI couldn\u2019t type on it and I still can\u2019t type on it, and a lot of my friends can\u2019t type on it. . . . It\u2019s hard to type on a piece of glass.\u201d\n\nWith every click of his PowerPoint presentation, Lazaridis felt his audience grow slack and bored. It was late August 2007 and RIM\u2019s boss was making a pitch in a Manhattan hotel meeting room to a team of senior executives from Verizon and its British affiliate Vodafone. Lazaridis and chief operating officer Larry Conlee had been invited to New York by the carriers to propose new phone ideas. Although the iPhone wasn\u2019t mentioned, there was no doubt Verizon and Vodafone were looking for a device that might supplant what was now America\u2019s fastest-selling smartphone.\n\nJudging by the drooping faces of John Stratton, Verizon\u2019s chief marketing officer, his colleagues, and the executives from Vodafone, Lazaridis was losing the room.\n\nRIM\u2019s co-CEO had started with a pitch for BlackBerry Bold, due to launch in 2008. Clicking from slide to slide, Lazaridis extolled Bold\u2019s improved keyboard, with an innovative track pad to replace the trackball, and large screen. This, he told the room, was the best phone RIM ever designed. But to Stratton and company, Bold failed to live up to its name. Up against AT&T and its exclusive multiyear deal to sell the iPhone, Verizon had little interest in another keyboard phone, nor did Vodafone. \u201cThe whole atmosphere was, AT&T has the iPhone and we don\u2019t, so what do we do?\u201d remembers Conlee. \u201cNeither of those carriers likes to lose. It\u2019s a religious war.\u201d\n\nVerizon had been caught off guard by iPhone\u2019s ascendency. Two years earlier, Verizon rejected an overture from Steve Jobs to partner with Apple on its plans for a new phone. A stickler for bandwidth reliability, the New York-based carrier wouldn\u2019t relinquish control of its network to an unseen phone Jobs wanted complete authority to design. Like Lazaridis, Verizon executives correctly predicted iPhone traffic would create gridlock on AT&T\u2019s network. What they didn\u2019t anticipate was that consumers didn\u2019t care. A multibillion-dollar market in carrier revenue was opening up and AT&T had a lock on the hottest device. RIM\u2019s Bold was no match for the iPhone.\n\nSensing the audience\u2019s mood, Lazaridis hurried to plan B. Up on the screen, surrounded by lightning, shone an ebony glass-covered phone. That, Lazaridis explained, was Storm. Phone and computer companies had experimented with touch-screen devices for years. None, he said, could match the magic touch of Storm. Pulling out a prototype, Lazaridis pressed a finger on the glass screen. There would be no sweeping fingers, no clumsy iPhone typos on this device. To make the point, his finger hovered like a computer mouse over a digital version of BlackBerry\u2019s signature keyboard on the phone\u2019s touch screen. When he pressed on a digital key, the entire screen clicked down like a giant button, replicating the tactile feel of tapping a BlackBerry keyboard. RIM had combined the navigation feel of a computer mouse with the secure handling of a BlackBerry keyboard. Under the hood, the ingenious floating Storm screen was designed to activate existing BlackBerry software every time it was clicked. This was how RIM would outsmart Apple, by combining the best of BlackBerry with the seductive lure of a touch screen. His old swagger returning, Lazaridis hailed the next smartphone wave. No one disagreed. Superlatives followed as Verizon and Vodaphone executives passed around the prototype. \u201cThey were over the moon,\u201d Lazaridis would remember. \u201cThey loved the prototype. They called it revolutionary.\u201d\n\nRIM had its own reasons for backing the kind of touch phone that Lazaridis had initially and so publicly disdained. Verizon and Vodafone were two of the world\u2019s biggest carriers with deep ties into the U.S. and European consumer phone market. Their endorsement of Storm came with an estimated $100-million marketing budget and thousands of retail stores to promote the phone. If Storm took off, the two carriers could potentially sell millions of phones. RIM could stand toe-to-toe with Apple. This was the biggest break in RIM\u2019s history. When Lazaridis and Conlee returned to Waterloo, Balsillie had only one reservation about the Verizon contract. RIM had to make the transformative phone in nine months. Was it possible for RIM to deliver in such a short time frame? The answer, Lazaridis and Conlee agreed, was yes.\n\nConlee broke the news about RIM\u2019s ambitious deal to a select group of engineering executives shortly after the Manhattan meeting. In a room located adjacent to his office in RIM 4, Conlee outlined the secret project for the company\u2019s first touch phone. The code name for the product was Project Storm, a nod to the disruptive impact RIM hoped the phone would have on the market. But that day the name captured a blizzard of objections from the company\u2019s engineers.\n\nRIM was racing to roll out Bold phones for 2008; now it wanted to shift gears and create a new phone in nine months! It took eighteen months to create a new BlackBerry. A touch phone was something else. Although Storm would use BlackBerry\u2019s existing operating system, it would need new hardware, radio and antenna configurations, and additional software. RIM products were reliable, never this rushed. There would be no time for proper \u201csoak-testing\u201d\u2013 engineering talk for working bugs out of software. Waving off protests, Conlee, RIM\u2019s product enforcer, asked each engineer to explain what he or she needed to make the touch phone happen. The room of problem solvers reluctantly itemized the parts, software, and staff they would need, immediately. Conlee then turned to Perry Jarmuszewski, a soft-spoken radio engineer who had been with RIM for more than a decade. \u201cPerry I guess you\u2019re good to go. You haven\u2019t said anything,\u201d Conlee offered.\n\nJarmuszewski, who preferred solving problems to making them, had deliberately held his tongue. Prodded by Conlee, he pushed back. \u201cOn a scale of 0 to 10, if 10 means no way, then this project is an 11,\u201d he said. \u201cIt\u2019s impossible. It\u2019s something I would not be able to deliver.\u201d Conlee shrugged and gave his marching orders: \u201cWell, you guys are the heads of our engineering groups. You are paid accordingly. I expect you to get it done. Verizon wants an answer to the iPhone. We have to do it.\u201d\n\nAs engineers filed out, they looked anxiously at RIM\u2019s chief technology officer, David Yach. He\u2019d just returned from a short holiday ready to devote the next months to fine-tuning the Bold phone. Now this. Yach would later say he and his colleagues understood the importance of the touch-phone contract. \u201cBut the importance didn\u2019t mean we could get it done any faster than we\u2019d ever built a phone before,\u201d says Yach.\n\n\u201cDid we push the teams too hard?\u201d says Lazaridis. \u201cProbably. Can you show me a company that doesn\u2019t? I\u2019d be hard-pressed to believe you. The pressure Jobs put his iPhone team through was worse than anything I ever put on my team. The fact is, that\u2019s how business runs.\u201d\n\nAfter years of flying below the radar, RIM\u2019s chiefs were in the limelight as Lazaridis and Balsillie won awards and mainstream media attention. In March 2008, Wall Street\u2019s weekly financial bible, Barron\u2019s, called RIM\u2019s co-CEOs \u201cunder-appreciated northern lights,\u201d adding both to its annual list of the world\u2019s best CEOs. Also on the list was Steve Jobs. After Mac computers, iTunes, and the iPhone, Jobs was Silicon Valley\u2019s undisputed king of cool. By comparison, Lazaridis and Balsillie were bright but awkward public speakers. When Jobs spoke, his fans cheered. When Lazaridis and Balsillie stepped onstage, people sometimes scratched their heads.\n\nFor all his confidence, Balsillie could be a surprisingly baffling public speaker. The executive who carefully rehearsed scripts for customer presentations preferred a let\u2019s-see-what-happens approach to interviews. He once explained his speaking strategy to university students: \u201cThe great thing is, when I talk, nobody knows what I\u2019m going to say, including me.\u201d On April Fools Day, 2008, Balsillie gave the kind of spontaneous interview that gives publicists coronaries. Wearing a tan jacket and a blue T-shirt, Balsillie sat down with George Stroumboulopoulos, host of a popular Canadian Broadcasting Corporation TV show. Referencing the popular iPhone, Stroumboulopoulos asked if it was time to add to RIM\u2019s lineup: \u201cDo you ever look at it and go, \u2018What are we going to do if this isn\u2019t our primary business, growing RIM beyond . . . a BlackBerry?\u2019 \u201d\n\n\u201cUm, no,\u201d Balsillie laughed, \u201cwe\u2019re a very poorly diversified portfolio.\u201d \u201cYou\u2019re just going to focus on one thing!\u201d said Stroumboulopoulos.\n\n\u201cIt either goes to the moon or it crashes to Earth,\u201d Balsillie replied.\n\nIn the spring of 2008, no one believed RIM would flame out. Its stock market value was more than $70-billion, quarterly revenues were up 100 per cent from the previous year, and the company sold sixty thousand BlackBerrys daily. Still, the company couldn\u2019t afford to be arrogant. The iPhone had grabbed a 17 per cent share of the U.S. smartphone market, while RIM\u2019s share slipped from 45 to 40 per cent. This was more than a battle of duelling devices. Apple and RIM were competing to capture consumer imagination. When Jobs promoted the iPhone he talked about tangible pleasures \u2013 the ability to search Paris maps, listen to Bob Dylan, play video games, and tap cameras that captured the world. When Lazaridis talked about RIM\u2019s phones, you needed an engineering degree to parse his words. Unveiling RIM\u2019s Bold phone at a conference in Orlando, Florida, in May 2008, he began with a spiel ripped from a product manual: \u201c3G tri-band HSDPA. Quad band Edge. Wi-Fi A, B, and G. GPS. 624 megahertz strong-armed with MMX. Powerhouse processing. Bold. Brilliant, strong colour display. The best keyboard we\u2019ve ever made.\u201d\n\nTranslation? RIM was launching a third-generation phone that came with Wi-Fi, GPS, and a more powerful processor. To technology wonks in the theatre, corporate IT managers, and CIOs, Lazaridis made perfect sense. He was announcing the smartest new smartphone for business customers. But to investors, journalists, and non-engineers, Lazaridis might as well have been reciting algorithms.\n\nIn other public appearances that spring, Lazaridis fretted about the very thing iPhone users considered irrelevant: network capacity. One of the great strengths of RIM\u2019s internal network system was its ability to compress large amounts of data, a service that reduced bandwidth use and data charges for big customers. When he was presented with a global leadership award by the Computerworld honors program, Lazaridis, the practical engineer, lectured an interviewer about the wireless industry\u2019s long history of preserving limited bandwidth: \u201cWe have to keep that same way of reasoning, that same conservative conserving mind-set going forward as we apply more and more applications to the wireless spectrum.\u201d\n\nThree months later, in July 2008, Apple smashed the networks Lazaridis wanted to conserve by launching the App Store. The online outlet was stocked with software applications that iPhone users, then numbering 6 million, could download. A finger swipe could race cars through video games, book hotel rooms, and order food. Apple sold more than 10 million apps in three days. The number rose to 60 million in a month. By 2011, the App store was stocked with half a million apps and had registered more than 15 billion downloads.\n\nBandwidth conservation was yesterday\u2019s priority. AT&T\u2019s networks were so clogged that customers began suing Apple and the carrier for dropped calls and other transmission headaches. The message was clear: wireless data traffic was only going to get bigger. The answer was not conservation, rather, it was bigger, faster wireless highways.\n\nWhile Lazaridis chased a new path with Project Storm, Balsillie pursued a once unthinkable opportunity in a northwest Chicago suburb. Schaumburg, Illinois, had been Motorola\u2019s home base since 1976, to accommodate its expansion into communications equipment, pagers, semiconductors, and Quasar televisions. Its greatest success came in the 1980s and 1990s when it parlayed its pioneering cellphone technology into a global powerhouse, earning dominant market shares in North America, Asia, and Europe. By 2006 cellphones accounted for two-thirds of Motorola\u2019s $43-billion in revenues. The glory days, however, didn\u2019t last. Preoccupied with internal turf battles, Motorola\u2019s \u201cwarring tribes\u201d had missed threats posed by BlackBerry and iPhone. It had focused most of its attention on sleek phone designs, such as the ultra\u2013thin Razr, failing to grasp that software-powered smartphones were the future. Motorola \u201cdidn\u2019t have the DNA or the people\u201d to understand the software, former CEO Ed Zander told Chicago Magazine. By the time Balsillie came calling in the spring of 2008, Motorola was under seige.\n\nMotorola\u2019s core mobile device sales were rapidly shrinking, tumbling nearly 40 per cent in 2008 from the year before to $12.1-billion. Operating losses in the group, nearly doubled to $2.2-billion in the year, a humbling decline that prompted cantankerous shareholder activist Carl Icahn to wage a noisy battle to dismantle the company. Motorola\u2019s misfortunes, Icahn complained in a public letter to shareholders, were the legacy of \u201cblunders\u201d by management. Under pressure, CEO Zander left the company, and its new chief, Greg Brown, was directed by Motorola\u2019s board to explore a sale of its mobility business. When RIM got an overture from the U.S. company\u2019s investment bankers, Balsillie and RIM\u2019s senior executives jumped at the chance to acquire the struggling mobile division. Despite its market woes, Motorola had a treasure chest of assets. Its cellphone operations and global market channels could help RIM keep pace with runaway BlackBerry demand. Motorola phone hardware could be repurposed with BlackBerry operating systems and software. If a deal could be worked out, RIM would have more clout to face Silicon Valley rivals.\n\nWhat Balsillie prized most of all was Motorola\u2019s vast arsenal of intellectual property rights. There were an estimated seventeen thousand issued mobile patents, most of them for dated cellphone technology that was more valuable in the legal arena, where RIM and its competitors faced a constant onslaught of patent lawsuits. RIM\u2019s deal maker was obsessed with intellectual property after the emotionally scarring NTP war. He lobbied governments on both sides of the border and spoke to business groups to push for reforms that might prevent the legal brinksmanship that nearly flattened RIM. Washington and Ottawa were receptive, but progress was slow. In this vacuum, tech companies strengthened their legal rights by acquiring patent collections from struggling rivals. With Motorola\u2019s patent chest, RIM would hold a much stronger hand against patent trolls and competitors alike.\n\nBalsillie and senior RIM executives and bankers met frequently in Schaumburg with Brown and his team during the spring and summer of 2008. Balsillie initially believed the takeover would happen. Motorola was under enormous pressure from investors to sell its mobility business. Motorola\u2019s weakened condition did not prevent it from slapping a rich price tag on its mobility unit. Brown, according to people familiar with the talks, wanted $10-billion for the money-losing division. The asking price, billions more than RIM was willing to pay, didn\u2019t include patents. All Motorola put on the table were dated factories, weak phone products, and a bloated staff. After studying Motorola\u2019s books, RIM concluded the division could only survive if thousands of jobs were eliminated, a mass execution that would be left to RIM. \u201cWe didn\u2019t want all these people, but they said, \u2018That\u2019s your problem,\u2019 \u201d remembers Balsillie, who, in the end, opposed shouldering what he considered \u201ca hell of a burden.\u201d\n\nAfter six months of negotiations, talks ended in August. For Motorola, the missed opportunity was devastating. With no buyers and a deepening global financial crisis, Motorola went into a tailspin. Mobility sales tumbled further and the company cut another 10,000 employees, reducing Motorola\u2019s total employee count to 60,000 from a peak of 150,000 in the late 1990s. Balsillie walked away believing Motorola Mobility was worth little more than its patents. He would later regret underestimating RIM\u2019s long-standing adversary.\n\nThere is a small white building on Columbia Street, close to the University of Waterloo, where BlackBerrys were sent to be tortured. Beatings took place in a concrete-floored lab with a white, corrugated-steel ceiling, from which pipes, wires, and row after row of high-voltage lights hung. This was where RIM\u2019s quality assurance team tested the limits of new BlackBerry models. Phones were thrown in swirling industrial tumblers, shaken by robotic arms, dropped on cement, and subjected to extreme temperatures. Afterward, a confidential report on phone flaws was circulated to product managers and executives. The meticulous attention to quality resulted in a low phone return rate, just 3 per cent.\n\nIn the summer of 2008, the quality assurance team was itself a target of abuse. RIM had to ship hundreds of thousands of Storm phones to Verizon and Vodafone. That was a problem because the new phone kept getting failing test grades. The floor of the quality assurance lab offered grim proof of Storm\u2019s fragility: shards of glass and parts everywhere. The phone\u2019s hardware engineers rejected the test findings, however. The problem, they insisted, was the quality assurance team. The pneumatic pistons that repeatedly poked Storm touch screens were too rough. These phones were not traditional BlackBerrys encased in hardy metal and plastic \u2013 they were glass-covered. Storm had to be tested by humans, the engineers insisted. So they pulled in University of Waterloo students to test the phones in the quality lab. The dazed students sat in chairs repeatedly poking the glass screens of test phones for hours. The screens survived the rhythm of human touch, but other problems soon became evident.\n\nStorms frequently crashed. The touch screen, which hovered over a hidden dome to allow digital menus, icons, and a keyboard to be poked and clicked, was stiff, cumbersome, and unreliable. Storm was specifically designed to overcome the iPhone keyboard\u2019s biggest flaw. Millions of Americans dispatched botched, often comical messages because of iPhone\u2019s unreliable, seemingly subversive auto-correct function. Type the word \u201cpens\u201d and \u201cpenis\u201d might appear. \u201cYour dad and I are going to Disney\u201d might turn into \u201cYour dad and I are going to Divorce.\u201d Storm was designed to eliminate this annoyance with a glass screen that activated BlackBerry software only when a user clicked down on the screen. The floating screen, however, became less reliable the farther a user\u2019s finger moved from the center. Poke the letters a or l, letters at the opposite end of the keyboard\u2019s home row, and the activation signal grew so weak that the phone\u2019s software sometimes misunderstood the command and responded with the wrong letter. \u201cIt needed work,\u201d Craig McLennan, a RIM sales vice president who oversaw the Verizon relationship, recalls thinking when he got his hands on the device for the first time in summer 2008. \u201cIt wasn\u2019t ready for game time.\u201d\n\nRIM insiders weren\u2019t the only ones to find fault with Storm. RIM hand-picked a few loyal customers to give the company feedback after testing early versions of the phone. One was Alexander Trewby, a vice-president of mobile development with one of RIM\u2019s largest clients, Morgan Stanley. The Storm phone Trewby received in the spring of 2008 lasted an hour. \u201cIt just turned off and then we could never turn it on again,\u201d he says. When he finally got a Storm that worked he was surprised how much he disliked it. \u201cFrom a hardware perspective, it was an automatic fail,\u201d says Trewby. \u201cWith the iPhone, where you tapped the screen, it was just a much more elegant solution, as opposed to physically pressing something and waiting for the click. The Storm felt a lot more machine-like, more mechanical, as if it was less electronic, less done by sensors. It was very much about hardware. It wasn\u2019t about software. We knew straight away it felt it was a wannabe. It was not as visionary, modern, as fun to use as an iPhone was. The device was dead on arrival.\u201d\n\nMore shocking to Trewby was Lazaridis\u2019s reaction when Trewby raised concerns about Storm at a private event for corporate customers during a BlackBerry conference in Orlando in May 2008. Lazaridis glared at Trewby, whom he knew, and then turned to an aide and complained: \u201cI thought there weren\u2019t going to be any press in here.\u201d The Wall Street Journal that day had broken the story of Storm\u2019s impending arrival, enraging Lazaridis, who hated seeing BlackBerry product details routinely leaked to tech blog sites. But his response was as troubling to Trewby as Storm\u2019s faults. Lazaridis had turned to Trewby in the past for his thoughts on new BlackBerry features, and now he was treating him like a stranger. Lazaridis seemed to be in such denial about Storm\u2019s flaws that he was shooting the messenger. A RIM executive wrote Trewby to apologize for the CEO\u2019s reaction, but the damage was done. A huge BlackBerry fan, the London executive began doubting the Canadian company. \u201cIt gets you scared because you realize they\u2019re slipping,\u201d he says. \u201cHis back was obviously up.\u201d\n\nRIM\u2019s founder worked closely with software and hardware teams to oversee the integration of the new touch screen with the company\u2019s existing software. It had been a problematic marriage from the beginning because the poke-and-click touch screen was so unreliable that it activated the wrong responses unless its software was in good working order and various components were assembled with care. While Conlee pushed far-flung foreign and domestic parts suppliers and contracted manufacturing companies to stay on schedule, Lazaridis fended off complaints from company veterans begging for more time. Quality shortcomings forced RIM to delay delivery four months past the initial June deadline. Problems persisted and soon the October ship date looked impossible.\n\nThe first phones rolling off production lines suffered from what RIM engineers called \u201chigh infant mortality rates,\u201d a greater chance of failing in early life. The problems persisted as the Christmas selling season approached. If the phones were not shipped before late November, there was a good chance Verizon would walk from the contract. RIM senior executives agreed it was better to ship a flawed product than no product at all. To engineers, Lazaridis repeated the same mantra: \u201cWe\u2019ve bet the company on this. It\u2019s critically important. We have to get this done.\u201d When problems persisted, RIM\u2019s chief technology officer, David Yach says Lazaridis grew frustrated. \u201cHe was almost incredulous that it couldn\u2019t be done,\u201d he says.\n\nLazaridis\u2019s conviction that RIM could deliver a new phone within a year came down to faith, a deep abiding confidence in himself and his company. A follower of the Christian Science movement and Emmet Fox\u2019s sermons on the transformative power of human will, Lazaridis believed people could, if sufficiently determined and talented, shape their own destiny. The fabulous success of BlackBerry only cemented that belief. Where would RIM be if not for his and Balsillie\u2019s persistence in the face of countless near-fatal reversals and product challenges? BlackBerry lived because Lazaridis and Balsillie never gave up. Ever. \u201cMike believes that the mind can will things to happen,\u201d Balsillie says. And Lazaridis always aimed high. He didn\u2019t just want to catch up to Apple. Storm had to be better than the iPhone. \u201cMike thought of himself as Canada\u2019s Bill Gates. He wanted to beat Steve Jobs,\u201d Balsillie says.\n\nFor his part, Lazaridis says the rush with Storm was unavoidable: RIM couldn\u2019t afford to say no to the biggest contract in its history. Besides, RIM\u2019s engineers had done the impossible before, pushing out the Pearl in less than a year while juggling other phone launches. He was confident the magic would work again with Storm. \u201cThis is a team that prides itself on pulling off miracles, pulling all-nighters, working hard, solving the most complex problems, getting things done on time, getting things done under the wire. This is a team with a can-do spirit,\u201d he says. At first Lazaridis\u2019s faith appeared to pay off. RIM had assigned a team to hand-assemble Storm phones before they moved to mass production, and the results were impressive. Under expert hands, the movable screen had been carefully calibrated to react to user clicks. The phones worked flawlessly and Verizon\u2019s executives, according to RIM officials, loved the early samples. It was a different story when the phones were mass-produced by RIM\u2019s manufacturing partners in Mexico and Europe.\n\nRacing by what Balsillie calls \u201cthe seat of our pants,\u201d the company pushed Storm out the door in time for Black Friday, November 28, the busiest shopping day of the year in the United States. Verizon and Vodafone had tested and approved the phone, but RIM knew they were shipping an unfinished product. Balsillie remembers the weeks before the launch as a nervous time. After years of high-speed typing on BlackBerry keys, Balsillie was dismayed at Storm\u2019s delayed response to touch-screen clicks. \u201cIt was slow,\u201d he says, like someone walking with ten-pound bags tied to each leg. When he took his concerns to Lazaridis, Balsillie says: \u201cMike said he\u2019ll fix it. You trust him.\u201d Other employees were not so optimistic, some privately referring to their new product as a \u201cshit storm.\u201d\n\nCritics were merciless about RIM\u2019s new offering. \u201cHead-bangingly frustrating,\u201d said New York Times columnist David Pogue in a scathing critique two days after Storm went on sale. \u201cStorm had more bugs than a summer picnic,\u201d he wrote, going on to list a litany of complaints: \u201cFreezes, abrupt re-boots, non-responsive controls, cosmetic glitches.\u201d Raising a question quietly asked by numerous RIM employees, the Times reviewer concluded, \u201cHow did this thing ever reach the market? Was everyone involved just too terrified to pull the emergency brake on this train?\u201d\n\nBritish actor and gadget reviewer Stephen Fry was similarly caustic. A fan of Apple products and BlackBerry\u2019s Bold, Fry complained about the absence of wi-fi,a free local network application available on iPhone. As for Storm\u2019s touch screen, he described the \u201cjudder, lag and jerk\u201d of the click-screen keyboard as \u201ca painful horror.\u201d He compared typing an e-mail to \u201can antelope trying to open a packet of cigarettes.\u201d Storm, Fry concluded, \u201cis the Edsel of smartphones, an absolute smeller from top to bottom.\u201d\n\nSlamming any technology device backed by Silicon Valley\u2019s forceful public relations armies was rare in 2007. Twitter, today\u2019s social media venting platform of choice, was just a year old and technology bloggers sometimes pulled their punches out of fear of being cut off from future interviews and product events. When the British celebrity took a sledgehammer to the admired BlackBerry brand, his comments went viral. Was he trying to sabotage BlackBerry, a BBC reporter asked? Unrepentant, Fry replied, \u201cHonestly: play with the Storm for two days as I have and you will admire my patience at not throwing it out the window.\u201d\n\nAlthough reviews were devastating, BlackBerry\u2019s fans had faith in RIM\u2019s record of producing reliable phones. Borrowing a page from AT&T\u2019s Apple promotions, Verizon lubricated sales by heavily subsidizing Storm phone purchases for any customer signing up for a two-year phone contract. After rebates, the touch BlackBerry sold for $200. The low price, combined with BlackBerry\u2019s reputation for quality and innovation, attracted hundreds of thousands of customers early on. While sales soared, RIM\u2019s engineers worked feverishly to repair software glitches with upgrades. The more phones sold, the more time RIM had to clean up after its Storm. By the end of January 2009 hope grew within the company that Storm might lift off. The company\u2019s chief promoter, Balsillie, told the Wall Street Journal RIM was producing 250,000 phones a week to keep up with demand. In a bullish forecast he pronounced that Storm was \u201can overwhelming success.\u201d"},
{"url": "https://www.ssl2buy.com/alphassl-wildcard.php", "link_title": "$100 for 3-year wildcard SSL cert with code RENEW3YR", "sentiment": 0.04737815947493366, "text": "I have completed SSL configuration. Where do I access the download from now?\n\nYou should receive SSL certificate code via email to registered account email address. This email contains SSL certificate code and how to install help guide.\n\nBefore paying I\u2019d like to know when and how I setup the details for the certificate.\n\nIt\u2019s very simple, quick and easy process and whole process takes 5 minutes or less. Step1: Signup account and purchase wildcard ssl certificate Step2: Receive SSL configuration link email. Submit CSR and domain information to complete configuration process. Step3: Claim domain ownership (called domain control verification) and receive ssl certificate.\n\nWhat domain I must to add into \u201cDomain Name(s)\u201d Field?\n\nWhile you generate CSR key for wildcard ssl, add Domain Name *.yourdomain.com\n\nNo, wildcard SSL does not support SAN. Wildcard SSL certificates are designed to support unlimited/multiple sub-domains. Our wildcard ssl works for Exchange Server \u2013 all versions.\n\nI\u2019m adding CSR to generate SSL, but get error: -6001 \u2013 We are unable to decode this CSR.\n\nIt seems the CSR contains invalid domain name format. You should generate new CSR key for *.yourdomain.com common name.\n\nI just ordered my first ssl with you but can\u2019t find where to put my CSR.\n\nCheck for new received emails from SSL2BUY.com. There should be SSL configuration link email received.\n\nI am new to the SSL cert world and accidentally deleted my key and CSR from the server while trying to install the SSL certificate. How can I fix this?\n\nWe offer free cost reissue. Generate new key set and start support ticket in your account with new key set. We\u2019ll reissue it in few minutes.\n\nIf I purchase wildcard ssl for *.mydomain.com, It will automatically also cover the root domain (mydomain.com) or not?\n\nYes. Wildcard SSL issued for *.mydomain.com will automatically cover mydomain.com\n\nI\u2019ve several servers hosting for multiple sub-domains. How I can install single wildcard ssl on all these servers?\n\noption 1: We offer free cost ssl reissue. You should generate CSR key from all servers and contact us for ssl reissue. option 2: You should export wildcard ssl from the first server you installed it and then import on other servers to install it. Contact our technical support for how to guide.\n\nDo I need to install AlphaSSL root and intermediate CA certs?\n\nHow long does it take for you guys to issue a wildcard SSL cert after purchase?\n\nIt takes 5 minutes or less for wildcard SSL issuance process. In case you don\u2019t receive it in meantime, contact our online chat support.\n\nCan you give me the command for OpenSSL to generate the AlphaSSL CSR for a wildcard?"},
{"url": "https://www.patreon.com/creation?hid=2480951&rf=379549", "link_title": "Interview with Charles Nutter (Co-Lead JRuby Project)  GOTO Chicago 2015", "sentiment": 0.25674715909090906, "text": "First video published for the GOTO Chicago 2015 conference. This also includes the new format and look for UGtastic interviews. Charles Nutter Co-lead JRuby Project GOTO Chicago 2015 Presentation: Beyond JVM: How the platform is evolving for new languages and features The JVM has traditionally hosted many languages, but only those closest to Java itself got the full optimization treatment. Those days are changing rapidly with new JVM features like invokedynamic and lambda, and with the rise of new frameworks like Truffle and Nashorn. In this talk we'll explore what it takes to optimize more exotic languages like JavaScript and Ruby and see how the JVM is evolving to do a better job. We'll see how projects like JRuby and Nashorn are trying to work around gaps in the JVM's optimizer. We'll see how JRuby+Truffle routes around the JVM to achieve blistering fast performance. And we'll look at upcoming projects on the JVM that will make language-building even easier. http://gotocon.com/chicago-2015/presentation/Beyond%20JVM:%20How%20the%20platform%20is%20evolving%20for%20new%20languages%20and%20features Also please look into our Patreon campaign at https://ugtastic.com/patreon and consider supporting UGtastic. Thanks for watching and supporting UGtastic!"},
{"url": "http://www.reuters.com/article/2015/05/22/facebook-oculus-lawsuit-idUSL1N0YD1ZU20150522", "link_title": "Founder of Facebook's Oculus hit with lawsuit", "sentiment": 0.06934523809523808, "text": "SAN FRANCISCO The founder of virtual reality glasses maker Oculus VR Inc, acquired by Facebook Inc for $2 billion, has been accused of taking confidential information he learned while working with another company and passing it off as his own, according to a lawsuit filed this week.\n\nThe plaintiff, Hawaii-based company Total Recall Technologies, said it hired Oculus founder Palmer Luckey in 2011 to build a prototype head mounted display. Luckey signed a confidentiality agreement, according to the lawsuit filed on Wednesday.\n\nThroughout the latter half of 2011 and into 2012, Luckey received feedback and information to improve the design of the display. However, Luckey used information he learned from his partnership when he launched a Kickstarter campaign for his own head mounted display called the Oculus Rift, according to the lawsuit.\n\nThe lawsuit seeks compensatory and punitive damages but does not specify and amount.\n\nFacebook's $2 billion acquisition of Oculus last year was its first-ever hardware deal, as the company sought a way into the fast-growing wearable devices arena.\n\nOculus is listed as a defendant along with Luckey, and a Facebook representative on Friday declined to comment. The lawsuit was reported earlier by The Recorder, a California legal newspaper.\n\nFor its lawsuit Total Recall Technologies enlisted global law firm Quinn Emanuel Urquhart & Sullivan, which also counts Google and Samsung Electronics Co Ltd as major clients. Luckey is accused of breach of contract and fraud, among other claims.\n\nThe case in U.S. District Court, Northern District of California is Total Recall Technologies vs. Palmer Luckey and Oculus VR, Inc., 15-2281."},
{"url": "https://www.flickr.com/photos/silvery/2414538926/", "link_title": "Where's Waldo in Google Maps?", "sentiment": 0.05476190476190477, "text": "Canadian artist Melanie Coles built a large image of the iconic \"Waldo\" onto a rooftop at an undisclosed location in Vancouver, British Columbia, Canada.\n\nThe image was intended to be later searched out through interactive mapping applications like Google Earth and Google Maps.\n\nCheck out my blog article on this subject, Where's Waldo in Google Earth.\n\nI got this photo from Melanie Coles' sister's Flickr set, Where's Waldo in Vancouver? Check out the additional shots she has on the construction and such!\n\nFYI - a key relevance of this is that Where's Waldo is known as Where's Wally outside the US.\n\nSPOILER: I have placed this image onto a map here in Flickr, pinpointing the building's location in Vancouver where the building with the image is located. While you can view the satellite image of the building now, it will be some months before Yahoo! Maps has updated the images -- so, it will be a while before Waldo is visible in the online map. To view the map, click on the \"(map)\" link found to the righthand side of this description, under the \"Additional Information\" list in the sidebar next to the text \"Taken in Vancouver, British Columbia\". -->"},
{"url": "http://byob.berkeley.edu/", "link_title": "Snap: A visual drag-and-drop programming language", "sentiment": 0.19761904761904764, "text": "Snap! (formerly BYOB) is a visual, drag-and-drop programming language. It is an extended reimplementation of Scratch (a project of the Lifelong Kindergarten Group at the MIT Media Lab) that allows you to Build Your Own Blocks. It also features first class lists, first class procedures, and continuations. These added capabilities make it suitable for a serious introduction to computer science for high school or college students.\n\nSnap! runs in your browser. It is implemented using Javascript, which is designed to limit the ability of browser-based software to affect your computer, so it's safe to run even other people's projects, even if you don't trust our competence or good intentions.\n\nSnap! is presented by the University of California at Berkeley. It was developed by Jens M\u00f6nig at MioSoft Corporation (now at SAP), with design input and documentation by Brian Harvey at Berkeley, and contributions by students at Berkeley and elsewhere.\n\nThis material is based partly upon work supported by the National Science Foundation under Grant No. 1138596. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."},
{"url": "http://www.datasciencecentral.com/forum/topics/are-you-smart-enough-to-work-at-google", "link_title": "Are you smart enough to work at Google?", "sentiment": 0.2611111111111111, "text": "Very interesting book despite its provocative title, and the fact that - a priori - I would not want to work for Google. It reveals many of the questions Google interviewers ask to candidates. These are actually very interesting questions that do not have a bad or good answer, but are aimed at judging your capability to resolve conflicts (capacity to work in a team), scale, and favor a robust, practical solution over fragile/costly perfection. These questions are by no means the typical riddle like what's next in the sequence 1 1 2 3 5 8. I was particularly intrigued by these discussions\n\nDon't get me wrong, I love Microsoft, and I think it is an insult to write such things in this book. But it makes a point: people who get hired at Google (or at Microsoft for that matter) are those who provide useful solutions, not those obsessed by perfect, exact, beautiful or elegant solutions.\n\nHere's a link\u00a0where you can get more info about the book."},
{"url": "http://perispomeni.club/~tsagi/", "link_title": "Just created perispomeni.club", "sentiment": 0.19402958152958152, "text": "It is an awesome and simple experiment that lives today!\n\nWhat I immediately thought about was that I could help friends get to know UNIX a little better and I could learn something too by managing a server.\n\nMany of my friends have to dive in linux for work and they feel lost. What if we could dive together?\n\nThen I thought that some more advanced users could jump in and we could do stuff together. Maybe something valuable could span out of this.\n\nIn the ideal universe I have in mind, a user could start as a newbie and end up creating something useful with other users.\n\nI could live with that happening. So what the heck?!\n\nI am Greek by the way and in greek, the tilde symbol (~) is called perispomeni. I hope this solves the name mystery.\n\nIf you don\u2019t know yet what this thing is let me tell you\u2026\n\nIf you remember in an older internet era, there were websites like this one, where the user had a tilde in front of his username. e.g.: http://perispomeni.club/~tsagi\n\nThe tilde (or perispomeni) indicates the personal space of the user on the machine.\n\nThis machine is very small. Raspberry Pi small. And we can have it together if you like.\n\nYou will have a personal space (yes you can create a website/blog/whatever) which will be your room for free. And the machine is our home.\n\nWe can talk with each other in this home.\n\nSo if you ask me you will get a shell account.\n\nFill this form and I will email you when you are ready to go!"},
{"url": "http://www.slate.com/blogs/business_insider/2015/05/22/elon_musk_entrepreneur_starts_small_secretive_school_for_spacex_employees.html", "link_title": "Elon Musk Didn't Like His Kids' School\u2013So He Opened His Own", "sentiment": 0.062155248133509006, "text": "Elon Musk didn't like his kids' school, so he started his own, the inventor and entrepreneur said in an interview on Beijing Television. The school is called Ad Astra\u2014which means \"To the stars\"\u2014and is small and relatively secretive. It doesn't have its own website or a social media presence.\n\nChristina Simon, who writes about private elementary schools in Los Angeles, has done some digging around Ad Astra. She says she's been in contact with a mother whose child attends Musk's school. The mother told Simon that the relatively new Ad Astra School is \"very small and experimental,\" and caters to a small group of children whose parents are primarily SpaceX employees.\n\nMusk says in the interview that Ad Astra, which is a year old, currently has 14 kids and will increase to 20 in September. His grand vision for the school involves removing grade levels, so there's no distinction between students in 1st grade and 3rd. Musk is \"making all the children go through the same grade at the same time, like an assembly line,\" he says in the interview.\n\n\"Some people love English or languages. Some people love math. Some people love music. Different abilities, different times,\" he says. \"It makes more sense to cater the education to match their aptitudes and abilities.\"\n\nMusk pulled his kids out of their school and even hired one of their teachers away to start Ad Astra. \"I didn't see the regular schools doing the things I thought should be done,\" he says.\n\nMusk sees a fundamental flaw in how schools teach problem solving. \"It's important to teach problem solving, or teach to the problem and not the tools,\" Musk says. \"Let's say you're trying to teach people about how engines work. A more traditional approach would be saying, 'we're going to teach all about screwdrivers and wrenches.' This is a very difficult way to do it.\"\n\nInstead, Musk says it makes more sense to give students an engine and then work to disassemble it. \"How are we going to take it apart? You need a screwdriver. That's what the screwdriver is for,\" Musk explains. \"And then a very important thing happens: The relevance of the tools becomes apparent.\"\n\nSo far, Ad Astra \"seems to be going pretty well,\" according to Musk. \"The kids really love going to school.\"\n\n\"I hated going to school when I was a kid,\" Musk told his interviewer. \"It was torture.\" When Musk was a child living in Pretoria, South Africa, he was viciously bullied as a student. His classmates pushed him down a concrete stairwell. In one instance, he was beaten so badly that he needed to go to the hospital.\n\n\u201cThey got my best [expletive] friend to lure me out of hiding so they could beat me up. And that [expletive] hurt. For some reason they decided that I was it, and they were going to go after me nonstop. That\u2019s what made growing up difficult. For a number of years there was no respite. You get chased around by gangs at school who tried to beat the [expletive] out of me, and then I\u2019d come home, and it would just be awful there as well.\u201d\n\nHis difficult experiences both at home\u2014where he had a strained relationship with his father\u2014and at school would eventually lead Musk to leave South Africa for the United States. You can watch Musk's full video interview here."},
{"url": "https://www.poppy-project.org/creatures/poppy-humanoid/", "link_title": "Poppy Humanoid 3D-Printed Open-Source Robot Platform", "sentiment": -0.012727272727272728, "text": "Poppy Humanoid v1.0 robot is 85cm high and is particularly lightweight (3.5kg). This robot has 25 degrees of freedom with a multi-articulated trunk (5 DoFs). It is currently based on Robotis servomotors, allowing compliant reaction to external forces, but the use of alternative servomotors is currently explored by the community.\n\nIt has a ODROID U3 board in the head, to run programs and communicate through WiFi and Ethernet. As an alternative, it is possible to replace this card with a\u00a0Raspberry Pi 2, although for the moment, the use of the Raspberry Pi 2 with Poppy Humanoid robot is less documented.\n\nSensors include a\u00a0wide angle USB camera (170\u00b0 FOV) located in the head can be used for artificial vision, as well as all sensors embedded on Robotis motors (capable to detect positions, position errors, load, temperature).\n\nIts whole morphology can be reconfigured to modify, replace or remove any parts of its body and any of these features."},
{"url": "https://codepicnic.com/posts/codepicnic-3-slack-run-your-code-with-your-team-f7e6c85504ce6e82442c770f7c8606f0", "link_title": "Show HN: Run code through Slack with CodePicnic", "sentiment": 0.16213474025974026, "text": "At this time, almost every developer team in the world uses Slack. Slack is an revolutionary communication tool that brings the concept of chat into a new level. They made it interconnected with a full set of third-party services, and even you can create your own and integrate it with your team's chat. So we did it.\n\nOur motto is \"Your code to go\", because we believe the code wants to be run, and\u00a0we are betting on this.\n\nWe believe the code, at its mere essence is text. No just plain text. Living one. One that can and must be run, shared and embedded, without boundaries. This is why we built CodePicnic, to turn plain text in living, running code, in seconds.\n\nImagine you can send your code to all the members of #devs so they can run your code and understand what you try to explain. Don't just imagine, do.\n\nGo to the Integrations page at your Slack account and add an Outgoing Webhook:\n\nFill the form that appears with the following values. Our endpoint will be listening to the #run line followed to the kind of languages you\u2019d like to run:\n\nWe will handle every message you send in Slack with the following format:\n\nWe support JavaScript (with Node.js), PHP, Python and Ruby with the following options:\n\nOur Slack integration will process your message, create a new console with your code and return the result of your code in the message, along the URL of the created console containing your code.\u00a0The result will look like this:\n\nWe hope you enjoy this feature as much as we did. Don't forget you can create more consoles by signing up in our platform (we provide you 50 free sessions per month)."},
{"url": "https://www.youtube.com/watch?v=rYkI0_ixRDc", "link_title": "An Evening at Erlang Factory: Joe Armstrong, Mike Williams, Robert Virding", "sentiment": 0.1875, "text": "We were so excited to get a moment to chat with Francesco Cesarini, the founder and technical director of Erlang Solutions. \n\n\n\nHackers and Founders\n\nwww.hackersandfounders.com"},
{"url": "https://theconversation.com/how-alcohol-makes-you-friendlier-but-only-to-certain-people-41730", "link_title": "How alcohol makes you friendlier \u2013 but only to certain people", "sentiment": 0.029830341227400047, "text": "Drinking alcohol is associated with aggressive behaviour, accidents and ill health. Yet many of us choose to drink socially. This may reflect alcohol\u2019s actions on specific brain circuits which make us feel euphoric and less anxious. Alcohol may also make us more empathic and cause us to see other people as more attractive. But why do these reactions occur and are the positive effects of alcohol expressed towards everybody we interact with?\n\nAlcohol is a drug, one of the three most commonly used in the world, along with nicotine and caffeine. When we drink, the alcohol binds to a specific type of receptor in the brain and boosts the activity of a natural brain chemical called GABA. The effect the alcohol has on us depends in large part on the dose, and the location of these GABA receptors within the brain.\n\nEarly on in a drinking session, the alcohol acts on GABA systems to boost the levels of dopamine, the brain\u2019s reward chemical. This gives a sense of well-being and a sense of mild euphoria. Alcohol also acts on GABA receptors to impair the activity of the brain circuits that make us feel anxious and, at higher doses, alcohol inactivates a second set of brain circuits that control fear. So threatening stimuli no longer seem quite so scary. Alcohol also compromises our ability to compute risk so that situations we would normally shy away from may now seem quite inviting.\n\nAll of this points to alcohol as a facilitator of social interactions. As well as making us more empathic, laboratory studies have also shown that drinking alcohol can make us trust others more and make us temporarily more generous.\n\nOn the other hand, heavy drinking is associated with violent behaviour. This situation, however, is complex. Laboratory studies have shown that alcohol increases aggression. For example, it increases the willingness with which individuals will administer electric shocks to others. However, this effect seems to be largely restricted to those who are intrinsically aggressive in the first instance.\n\nDon\u2019t try this at home.\n\nEqually, alcohol can corrupt our ability to understand the intentions of others. The brain contains specific circuits, which connect parts of the prefrontal cortex, amygdala and temporal parietal junction, that handle our social cognitive abilities. So our ability to understand somebody else\u2019s mental perspective and their motivations for acting in a certain way become unreliable.\n\nVery big doses of alcohol can leave the functioning of these circuits so compromised that individuals can appear to be as impaired as patients with some forms of dementia. This is quite a disturbing thought given the number of people who end up in this state in city centres at the end of a good night out.\n\nAlcohol also impedes our ability to accurately interpret emotional expressions in faces. As we drink, we have a tendency to erroneously assume that some facial expressions of negative emotions are happy, and we find it particularly difficult to identify sad and angry faces. This leaves us prone to making embarrassing social errors.\n\nOne important, but often overlooked, aspect of alcohol\u2019s effect on social functioning relates to how we perceive members of our in and out-groups. Alcohol appears to encourage us to bond to members of our in-groups. However, this may come at the cost of the way we treat people outside of these groups. Similarly, alcohol makes members of our ethnic in-group appear more attractive but this effect does not extend to members of other ethnic groups.\n\nIt must be emphasised that the effects described so far are potentially reversible once the drinker has sobered up. However, chronic heavy drinking can lead to brain damage and irreversible cognitive impairments, especially poor memory function, and psychiatric problems including depression, psychoses, anxiety and suicide.\n\nSo overall, alcohol may be a friend, and indeed make us friendlier, but only to a select group of people \u2013 and they may not always reciprocate.\n\nAlcohol Friend or Foe? is part of the Pint of Science festival where academic experts talk about the latest in scientific research \u2013 at the pub."},
{"url": "http://uploadvr.com/a-growing-rift/", "link_title": "A Growing Rift", "sentiment": 0.15700538939175301, "text": "This is an opinion piece written by Tony Parisi as a guest author and reprinted with his permission. The opinions in this article are Tony\u2019s.\n\nWhile the tech press was busy fondling itself over porn as the week\u2019s big VR story, a more significant development went largely unreported. In a recent blog post, Oculus Chief Architect Atman Binstock published the lavish min hardware specs for the Oculus Rift. Binstock also announced the company\u2019s decision to suspend all OS X and Linux development indefinitely. The news undoubtedly came as a gut-punch to the VR faithful. The lack of universal platform support means that any dreams people might have had about VR for the masses will have to be put on hold\u200a\u2014\u200aeither that or it\u2019s time to look elsewhere for salvation.\n\nTony Parisi is one of the godfathers of the VR and WebGL scene. Tony has spent over 25 years working in the virtual reality industry and was one the co-creators\u00a0of VRML. Tony has written many books on the subject of web graphics, and his upcoming book \u201cLearning Virtual Reality\u201d will be available on September 25th. You can learn more about Tony in the excellent Flashback piece he did with Matt Terndrup.\n\nAt least we can stop deluding ourselves about one thing. The Oculus Rift is for games\u200a\u2014\u200aperiod; full stop. The announcement makes this crystal clear, but in hindsight it shouldn\u2019t come as a surprise. We saw early hints of the direction at the first Oculus Connect developer event, where it was evident that our little clubhouse of VR believers had been invaded by refugees from console and mobile gaming. The escalating hardware specs; the omnipresence of shoot-em-up content in the demo salon; even the ass-grabbing that foreshadowed GamerGate\u200a\u2014\u200ayes, in the Year of Our Lord 2014 a woman actually got physically groped by another attendee while waiting in line for the keynote\u200a\u2014\u200ait all felt more like a GDC than a first-ever conference devoted to building a shared virtual future.\n\nIn the months since Connect, the Oculus team had done a respectable job supporting the SDK for other operating systems. And Oculus reps at least smiled graciously whenever asked about applications that are obviously out of their gaming-first comfort zone. So it seems as if the company was reallytrying for a while there. But in the end it looks like they\u2019ve decided to hunker down. I understand the strategy, and I actually think it\u2019s the right choice for company. Developing for one platform makes the job easier. Focusing on a well-understood, lucrative product category reduces the business risk. Competition from the Vive and Project Morpheus has raised the stakes\u200a\u2014\u200awe may have a real dogfight on our hands next year. Last but not least, Oculus ison the hook to ship something, anything, and I\u2019m sure Facebook management\u2019s patience isn\u2019t infinite. I suppose it\u2019s better for the Rift to be a success at something than not at all, so: godspeed, Oculus. But where does this leave the rest of us?\n\nThere\u2019s hope coming from a couple of quarters. For desktops we have the Vive and OSVR. Valve has a good track record with supporting Mac and Linux, and HTC is committed to supporting all platforms, so it\u2019s reasonable to expect we\u2019ll get some love there. But\u200a\u2014\u200ahello\u200a\u2014\u200anobody has a Vive in hand just yet. They ship over the next few months. OSVR is fully open, so I don\u2019t think it\u2019s out of the question that we are going to have solid cross-platform capability on those devices. Last time I looked, not that many people were using OSVR, but the move by Oculus just might open new inroads for it.\n\nWhat about WebVR, you may be wondering? Oculus is the only desktop device that browsers support right now. Sign of the times: Josh Carpenter, my pal on the Mozilla VR team, told me they \u201cjust bought a bunch of PCs\u201d and he\u2019s got one on his desk next to his Mac Pro. Sigh.\n\nOn the mobile side, things are brighter, but still murky. Gear VR is the top choice, but it\u2019s far from ubiquitous, and definitely not cross-platform. Cardboard looks to be the ultimate winner, but we\u2019ll need more high-res phones and faster tracking. I hear the Cardboard team has been staffing up with high-profile talent, so maybe these are on the way soon.\n\nLong story short\u2026 there\u2019s no short story. Platforms are proliferating, and each of us is going to have to pick a battle. Oculus has made a choice which ultimately will benefit the industry\u200a\u2014\u200aby all means go forth and make VR gaming a mainstream category!\u200a\u2014\u200abut in the short term they have broadened the gap between game developers and everyone else."},
{"url": "http://fitduel.herokuapp.com", "link_title": "Show HN: FitDuel \u2013 a fitness competition between you and a friend", "sentiment": 0.4, "text": "FitDuel lets you and a friend battle it out in the gym, on the running tracks or on the scales.\n\nYou get points for working out and for losing weight or bodyfat.\n\nIt's free during the beta period."},
{"url": "http://www.producthunt.com/posts/tipo", "link_title": "New way to improve your spelling", "sentiment": 0.5681818181818181, "text": "Get a summary of the best new products, every day."},
{"url": "https://www.eff.org/deeplinks/2015/05/why-mitch-mcconnell-cannot-be-allowed-decide-fate-patriot-act", "link_title": "Why Mitch McConnell Cannot Be Allowed to Decide the Fate of the Patriot Act", "sentiment": 0.10536163695061998, "text": "Senate Majority Leader Mitch McConnell has made it clear this week that, while the Senate is rapidly approaching recess, the Senate \u201cwill stay in [session] until a deal is struck to extend\u201d the Patriot Act. McConnell has also introduced legislation for both long-term and short-term reauthorization of the Patriot Act\u2019s expiring provisions. It seems that McConnell is trying to bully the entire Senate into passing short-term reauthorization, giving him more time to further weaken reform efforts.\n\n A look at McConnell\u2019s history makes this unblinking support of unconstitutional surveillance less surprising. But what is impressive is his commitment to supporting untenable positions. He acts as if the Snowden leaks, which helped expose just how out of control NSA spying is, as well as the recent Second Circuit decision holding that the NSA\u2019s telephone records program was unlawful, never happened.\n\nThis was especially apparent today, when McConnell stood on the Senate floor and rattled off a litany of the exaggerated threats that NSA defenders have been relying on to scare Americans into submission: ISIL, Al Qaeda, Al Shabaab, safe havens in Syria for extremists, and safehouses in Yemen for terrorists. He wrapped it up with an all time greatest hits of NSA defenders, explaining why he thinks we need bulk collection (more on that below).\n\nSo what was McConnell saying pre-Snowden? An op-ed he wrote in support of Patriot Act reauthorization in 2007 shows that he hasn\u2019t changed his tune at all in eight years. In fact, he practically could have been reading sections of the article as he stood on the Senate floor today to defend NSA spying. And earlier this month, he said, \u201cThis has been a very important part of our effort to defend the homeland since 9/11.\u201d In 2011, after President Obama signed the most recent Patriot Act extension, he claimed that the Patriot Act has \u201c kept us safe for nearly a decade and Americans today should be relieved and reassured to know that these programs will continue.\u201d That\u2019s been his position all along, without regard to the what the rest of Americans (or the world, for that matter) think, after all they\u2019ve learned about NSA overreach .\n\nHere\u2019s why, in his own words (which apparently are evergreen, no matter how much we find out about NSA spying abuses), McConnell is clearly the wrong person to be calling the shots when it comes to NSA reform:\n\nWhile intelligence officials have certainly maintained that surveillance under the Patriot Act is valuable, we (and Mitch McConnell) know those claims are overblown, and sometimes simply false. Although surveillance under the Patriot Act has been shrouded in secrecy, because of Snowden\u2019s leaks, claims about the efficacy of these programs have come under scrutiny. The specific contention that \u201c54 attacks have been stopped\u201d by bulk phone records collection has been thoroughly debunked. In fact, Sen. Patrick Leahy forced former NSA director Keith Alexander to admit that the \u201c54 attacks\u201d claim was inaccurate in front of Congress.\n\nIndependent studies from the President\u2019s Review Group, the Privacy and Civil Liberties Oversight Board, and the New America Foundation have all made it clear: we don\u2019t need bulk phone records collection.\n\nWhile this may have been accurate in 2007, it is, unfortunately, not true now. The Boston Marathon bombing was a tragic example of a terrorist attack on U.S. soil. After the attack, there were questions about whether the FBI could have done more, but the FBI made it clear that there was nothing else it could have done. And while some have claimed that\u2019s because of the legal limits on what the FBI can do (more on that below), Rep. Jim Sensenbrenner very sensibly pointed out that mass records collection can actually be detrimental: \u201cIt didn't stop the Boston Marathon Massacre. Sometimes too much information means that what you are looking for is actually a very small needle in a very large hay stack. You can be drowned in too much information.\u201d\n\nAnd while the Boston Marathon bombing got a justifiably significant amount of media attention, there\u2019s also an epidemic of domestic terrorism against American Muslims that\u2019s getting little attention. In particular, the arson of mosques has become more and more widespread. This kind of terrorism isn't what Section 215 of the Patriot Act was aimed at addressing. But domestic terrorism was a significant focus of other provisions of the law. Yet these crimes are never talked about in the same breath as the Patriot Act\u2014and that\u2019s not only because terrorism against Muslims is low on the media and government\u2019s list of concerns. It\u2019s also because hate crimes against Muslims have dramatically increased since the passage of the Patriot Act, not decreased. There\u2019s no official government estimate of how many of these qualify as terrorism. But under the definition from Section 802 of the Patriot Act itself, nearly any hate crime would qualify.[1] This epidemic of terrorism simply doesn\u2019t fit into the contrived narrative of why we need the Patriot Act. But since these are terrorist attacks against Americans, McConnell might want to focus on this blatant homeland security failure, instead of trying to reauthorize a program of limited usefulness.\n\nThe oft-repeated claim that had the Patriot Act been in place before 9/11, the NSA would have been able to stop the attack is simply false. According to a 2004 report from the 9/11 Commission, authored by Senior Counsel Barbara Grewe, which the government did not declassify until five years after it was written:\n\nThese problems still persist. According to the Government Accountability Office website, as of 2015 \u201cThe sharing of terrorism-related information has been designated as high risk [for fraud, waste, and abuse, or mismanagement] because the government faces formidable challenges in analyzing and disseminating this information in a timely, accurate, and useful manner.\u201d\n\nMcConnell was certainly right when he said the Patriot Act is one of the most important pieces of legislation in a generation. But that\u2019s not because of the reasons he may have thought. It\u2019s one of the most important pieces of legislation in a generation because it is under the guise of the Patriot Act that the NSA has committed some of the most blatantly unconstitutional surveillance this country has ever grappled with.\n\nAnd he\u2019s also wrong about how Americans feel about surveillance. A recent study by Global Strategy Group (commissioned by ACLU) found:\n\nWhat\u2019s more, the 2004 Grewe report made it clear that it was actually a severe lack of understanding of what McConnell is calling \u201carcane laws,\u201d\u2014in particular Foreign Intelligence Surveillance Act [FISA]\u2014that led to intelligence failures. Agents missed information-sharing opportunities because of an \u201coverabundance of caution\u201d on the sharing of information by the NSA, as well as a failure to use all the tools available. For instance, the FBI knew it had reason to be concerned about would-be 9/11 hijacker Zacarias Moussaoui. But because they had considered starting an investigation that might require a FISA warrant, when it came to using a criminal warrant to get the information they needed \u201cthey just did not think about that option at the time.\u201d\n\nWhen it came to sharing information about the movements of hijacker Khalid al-Mihdhar, the report explains: \u201ceveryone was confused about the rules governing the sharing and use of information gathered in intelligence channels.\u201d What\u2019s more, in the years, months, and days before 9/11, the NSA already had access to a massive database of Americans\u2019 call records. Analysts\u2014at NSA or CIA\u2014could have easily searched the database for calls made from the U.S. to the safehouse in Yemen. They simply didn't.\n\nWhat should be most alarming to Americans is the level of incompetence that these failures, which could have possibly prevented the tragedy of 9/11, demonstrate. Yet these same intelligence agencies continue to conduct invasive, overbroad surveillance and ask for ever more authority\u2014without ever really addressing the problems that led to their massive failure in 2001.\n\nSenator McConnell\u2019s bald-faced use of the tragedy of 9/11 as an excuse for surveillance programs that trample on the Constitution is as repugnant now as it was then. EFF has always thought that bulk collection was unconstitutional\u2014that\u2019s why we\u2019ve been suing the NSA since 2008, years before the first Snowden leak. And now, unlike then, the truth has been very clearly demonstrated. Of course, it\u2019s not because the government decided to come clean. We know the truth because of those leaks and because of all the transparency work being done by privacy advocates. Quality over quantity must be the rule for surveillance tools. The people who protect us from terrorism should have tools that actually work, while maintaining the privacy and civil rights of all Americans. If you agree, call Congress now and let them know."},
{"url": "http://yosefk.com/blog/do-company-names-actually-matter.html", "link_title": "Do company names actually matter? (2014)", "sentiment": 0.24999999999999997, "text": "This is a bit of a trite thought, but: Can it be that company names actually matter? Consider some examples:\n\nSome of my Google/Wikipedia-based \"research\" might be off. And I doubt that founding a company called \"Huge Profits\" would necessarily net me huge profits. However:\n\nSo if you're the rare person\u00a0capable of starting a successful company, and you insist on it being a huge success\u00a0and not just a big one, make the vision as unconstrained as you can imagine."},
{"url": "http://arstechnica.com/tech-policy/2015/05/how-one-us-embassy-staffer-allegedly-sextorted-hundreds-from-his-desk/", "link_title": "How one US Embassy staffer allegedly sextorted hundreds from his desk", "sentiment": 0.05717893217893217, "text": "\"Was total luck that I got her infected because I suck at social engineering.\" An American staffer at the United States Embassy in London has been accused of running a sextortion scheme\u2014amazingly, primarily from his heavily monitored, government-owned work computer. Despite this, the embassy\u2019s network security protocol apparently failed to flag the man\u2019s behavior.\n\nThe suspect, Michael C. Ford, was arrested at Hartsfield-Jackson Atlanta International Airport on May 17 as he was flying back to London with his wife and son. The criminal complaint against him was unsealed the following day.\n\nOn Thursday, a federal judge in Atlanta set Ford\u2019s bond at $50,000, despite the impassioned pleas by prosecutors for no bond at all, and ordered him to remain under house arrest in his own home in nearby Dunwoody, Georgia.\n\nFord faces federal charges of stalking, extortion, and computer fraud, among others. To be clear, he is not accused of extorting money. During his detention hearing, according to the Atlanta Constitution-Journal, the native Georgian is suspected of harassing\u00a0hundreds of women, primarily university students.\n\nAccording to the affidavit written by Eric Kasik, a special agent of the Department of State Diplomatic Security Service, Ford would write to women, saying he had obtained nude photographs of that victim. This was the case for one woman, dubbed \u201cJane Doe One,\u201d an 18-year-old woman in Kentucky.\n\n[Ford, masquerading as \u201ctalent scout David Anderson\u201d] then demanded that Jane Doe One take videos of \"other girls\" and \"sexy girls\" who were undressing in changing rooms at pools, gyms, and clothing stores, and then give the videos to him. The target threatened Jane Doe One that, if she did not send him the demanded videos, he would post the sexually explicit photographs of Jane Doe One widely online, along with Jane Doe One's actual name and address, which he listed in the e-mail message. He also threatened to e-mail the photographs to several of Jane Doe One's acquaintances, which he listed by first name and last name and, in one case, included a phone number.\n\nThe affidavit also describes how Ford would allegedly use phishing techniques common to spammers as a way to convince Gmail users into giving up their passwords.\n\nFor example, on or about March 20, 2015, the target, using the Google e-mail account Accounts Deletion \"YYYYYYY3@gmail.com,\" sent Jane Doe Two an e-mail message to her Google e-mail account, with the subject line in the header reading: \"Goodbye, Your Email Account is Scheduled to Be Deleted.\" The body of the e-mail stated that \"We have received your request to delete your Google account. The request details are as follows: March 20, 2015 2:33 AM PDT<> IP address [Listing numbers]. The deletion process may take up to 96 hours to complete.\n\nWhen Jane Doe Two fell for this and provided the password, Ford seemingly easily found compromising photos in her e-mail and threatened to send them to her parents, one of whom is described as \u201ca well-known executive of a large, multinational company headquartered in Chicago.\u201d\n\nAmazingly, Ford seems to have conducted at least some of his creeptastic actions from his own desk and his own State Department computer inside the embassy in London.\n\nIn an attempt to identify which particular computer was assigned to the State Department IP addresses, I have spoken with DSS computer specialists, as well as State Department security personnel and Information Technology personnel. Those individuals have informed me that, by performing keyword searches using the suspect \"talent scout\" e-mail account, they identified the specific State Department computer that is located at a workstation cubicle located in the U.S. Embassy in London. Personnel from the U.S. Embassy in London told me that the only person who sits at that workstation cubicle and uses that computer is Michael C. Ford. FORD is a U.S. citizen who has worked as an Embassy employee in London since 2009. They confirmed that FORD has sat at that cubicle and used that computer since well before January 2015, when the target sent the e-mails to Jane Doe One.\n\nAccording to the Journal-Constitution, Mona Sedky, a senior trial attorney who flew down from Washington, DC, for the hearing, told the court that a search of Ford\u2019s computer \u201cturned up a spreadsheet of 262 e-mail addresses,\u201d presumed to be his victims. She also told the court that Ford confessed to the agents that arrested him, saying that he was \u201cunable\u201d to stop his behavior.\n\nHe previously was arrested in the United States for \"peeping tom\" behavior, and British authorities received a complaint about his \"e-mail stalking\" behavior, according to the affidavit.\n\nFord\u2019s attorneys did not respond to Ars\u2019 request for comment."},
{"url": "https://fortune.com/2015/05/21/google-teddy-bear/", "link_title": "Google wants to patent the creepiest idea ever", "sentiment": -0.039941077441077444, "text": "Google is working towards a patent for a sweet-looking toy with eyes that can track your movement and ears that can perk up when you speak, according to a new patent filing spotted by SmartUp Thursday.\n\nThe submission to the United States Patent and Trademark Office shows diagrams of an ordinary toy rabbit or teddy bear equipped with cameras behind its eyes and microphones in its ears.\n\n\u201cUpon reception or a detection of a social cue,\u201dthe form reads, \u201csuch as movement and/or a spoken word or phrase, the anthropomorphic device may aim its gaze at the source of the social cue.\u201d\n\nThe theoretical toy could take verbal commands and send them to \u201cmedia devices\u201d like TVs. Of course, just because Google is seeking a patent doesn\u2019t mean the product will come to light."},
{"url": "https://www.anfractuosity.com/projects/timeshifter/", "link_title": "Show HN: Transmissions over time-based side channels across a network", "sentiment": 0.10556427699284843, "text": "We created a system in order to transmit and receive data by modifying the time intervals between packets.\n\nSteganography provides a way to hide data in plain sight, in this case as time intervals of packets. As the data isn\u2019t immediately obvious it is less likely to arouse suspicion.\n\nThere are two variables which can be adjusted as parameters of the program.\n\nThe first being the threshold, that intervals between packets below or equal to, will be classed as a zero bit.\n\nAnother variable represents the ammount of time delay that is added between two packets, to generate a one bit.\n\nBy making these value lower, transmission speed is increased and the noticability of data transmission will be lower, however the potential for errors will increase.\n\nBy measuring the time intervals between many packets, before first attempting to manipulate them, we can attempt to minimise the delay we add \u2013 as we would be able to work out the minimum time interval we can add, to result in a noticable output at the receiver end.\n\nWe could make use of hamming codes to aid error correction in case we miss-detect bits.\n\nAdditionally we could add stop bits to the raw binary data in order to aid better synchronisation in case one byte is lost.\n\nWe could also make use of a block mode such as CTR/OFB using a cipher such as AES, which will allow part of the plaintext to still be recovered in case some bytes are damaged.\n\nIn order to manipulate the timing delays for packets we first need to ensure that we have the iptables NFQUEUE module loaded or compiled into our Linux kernel.\n\nTo start with we first flush the iptables rules.\n\nN.B. once you\u2019ve activated these iptables rules, then packets will not be sent out from the computer until you run the program below.\n\nYou need the libnetfilter_queue library which can be obtained from your Linux distribution\u2019s repository or http://www.netfilter.org/projects/libnetfilter_queue/\n\nThis library allows you to manipulate queued packets, we use it to manipulate the time delays between them.\n\nAs we are performing this test on a single computer, we are going to use the intervals between ping replies as the message we are going to decode.\n\nRun the following as root:\n\n Run the transmitter using: echo \u201chelloworld\u201d | ./timeshifter 0 2000 3000\n\n Run the reciever using: ./timeshifter 1 2000 3000\n\nThen on the same computer use: ping google.com\n\nIt is important to ensure in this case, that the only ICMP packets that are being sent, are those sent by the ping command we have just run."},
{"url": "http://jroweboy.github.io/c/asm/2015/01/26/when-is-main-not-a-function.html", "link_title": "Main is usually a function. So then when is it not?", "sentiment": 0.0753004886527614, "text": "It began when my coworker, despite already knowing how to program, was forced to take the intro level Computer Science course at my university. We joked with him about how he needs to make a program that works, but the grading TAs wouldn\u2019t be able to figure out how it works. So that is the requirement, to make a functioning program that completes an assignment while obfuscating it such that the graders think that it shouldn\u2019t work. With this in mind, I started to think through the arsenal of tricks in C that I\u2019ve seen used before and one thing in particular stood out. The idea for this trick I will explain how to accomplish came from a blog with the name main is usually a function which got me thinking about when would main not be a function? Let\u2019s find out then!\n\nMy problem solving process is typically the same thing I imagine most programmers do. Step 1: Google search about the problem. Step 2: Click every link on the first page that seems relevent. If not solved, try a different query and repeat. Thankfully, the answer to this question came on the very first search on this Stackoverflow answer. Apparently in 1984, a strange program won the IOCCC where main was declared as a and somehow this did stuff and printed to the screen! Too bad it was written for a whole different architecture and compiler so there is really no easy way for me to find out what it did, but judging from the fact that it is just a bunch of numbers, I can surmise that the numbers there are just the compiled binary of some short function and the linker when looking for the main function just throws this in the place of it.\n\nWith our hypothesis in place, that the code for the program is just the compiled assembly of main function represented as an array, lets see if we can replicate this by making a small program and seeing if we can do that.\n\nAlright! It worked! Kinda\u2026 So our next goal is we want it to actually print something to the screen. Thinking back to my limited ASM experience, I recalled that there are different sections of the compiled which determine where different things go. The two sections that are most relevent to us are the section and the section. contains all the executable code and it it readonly whereas contains readable and writable code, but its not executable. In our case, we can only fill in code for the main function, so anything that gets placed in the data section is a no go. We need to find a way to get the string inside the main function and reference it.\n\nI began by looking into how to print something with as little code as possible. Since I knew the target system is going to be 64bit Linux, I found that I can call the system write call and it would write out to the screen. Looking back at this now that I\u2019m writing the code, I don\u2019t think that I needed to use Assembly for this, but at the same time, I\u2019m really glad I got to learn what I did. Getting started writing inline GCC asm was the hardest part, but once I got the hang of it, it started to become easier.\n\nGetting started wasn\u2019t easy though. It turns out that most of the ASM knowledge I could find through google is all of the following: really old, Intel syntax, and for 32 bit systems. Remember in our scenario, we need the file to compile with a gcc on a 64 bit system, without any special modifcations to the compiler flags, so that means there is no special compile flags, nor can we include any custom linking steps and we want to use GCC inline AT&T syntax. Most of my time was spent trying to find information about modern assembly for 64 bit systems! Maybe my google foo is lacking :) This part was almost all trial and error. My goal was just use the write syscall to print \u201cHello world!\u201d to the screen using gcc inline asm, so why was it so hard? For the people that want to learn how to do this, I recommend the following sites: Linux syscall list, Intro to Inline Asm, and Differences between Intel and AT&T Syntax\n\nEventually my ASM code started to form and I had some code that seemed to work! Remember, my goal is to produce a main that is an array of the asm that prints Hello World.\n\nHurray! It prints! Lets take a look at the compiled code in hex now, and it should match up 1 to 1 with the asm code we wrote. I went ahead and broke down whats going on in the comments to the side.\n\nThat looks like a functioning main to me! Now lets go and grab the hex contents of it, and dump it in as a string and see if that works. We can get the hex from main by using gdb again. I\u2019m willing to guess that there must be a better way, so maybe someone can post a comment on this and let me know :) The way I did it was to load gdb and print the hex at main like so. Last time we disassembled main, we saw that it was 49 bytes long, so can use the command to save the hex to a file\n\nNow we have the hex dump, we can convert them all to integers the easiest way that I know of, and that is using python. In python 2.6 and 2.7 you can just use the following to convert it to a convenient array of ints for us to use.\n\nI figure if my bash foo and unix knowledge was greater I could find an easier way to do this, but googling things like \u201chex dump of compiled function\u201d returns several questions about how to print hex in various languages. Regardless, we now have a comma seperated array of our function so lets put that in a new file and see if it works! I went ahead and commented what each of the different values mean.\n\nSegfault! What am I doing wrong? Time to fire up gdb again and try to see what the error is. Since main is no longer a function, we can\u2019t simply just use to set a break point there. Instead, we can use to get a breakpoint at the method that calls the libc runtime startup (which in turn calls main) and we can see what address we pass to\n\nFrom testing, I found that the value pushed on is the location of main, but something seems off this time. Hang on, it put main in the section! Earlier I mentioned how is where readonly executable code goes and is where non executable read/write values go! The code is trying to run memory that is marked as non executable which is the cause of the segfault. How am I supposed to convince the compiler that my \u201cmain\u201d belongs in ?! Well, my searches turned up empty, and I was convinced that was the end of the road. Time to call it a night and my adventure a failure.\n\nBut I couldn\u2019t sleep that night without finding a solution. I continued to search and search some more until I found a very obvious and simple solution on a stack overflow post that I\u2019ve lost the url to sadly. All I have to do is declare the main function as Changing it to was all I needed to do to get it in the right section, so lets try compiling again.\n\nAck! What is it doing now! Time to gdb again and see whats happening\n\nSo looking at the code we can see the address for main is in the ASM for _start in the instruction that looks like this on my machine We can use this to set a break point on main by doing and then continue execution with\n\nI snipped some of the assembly that wasn\u2019t important. If you didn\u2019t notice what went wrong, the address pushed to print at (0x400510) is not the address we stored the string \u201cHello world!\n\n\u201d at (0x4005c3)! Its actually still pointing to the computed location in the original compiled executable and not using relative addressing to print it. That means we need to modify the assembly code in order to load the address of the string relative to the current address. As it stands, its fairly difficult to accomplish in 32 bit code, but thankfully we are using 64bit asm, so we can ues the instruction to make it easier.\n\nThe changed code is commented so you can see it. Compiling the code and checking to see if it works\n\nAnd now we can use the same techniques discussed earlier to extract the hex values as an integer array. But this time, I want to make it a little bit more disguised and tricky by using the full 4 bytes that ints give me instead. We can do that by printing the information out in gdb as an int instead of dumping the hex to a file and then copy pasting it into the program.\n\nI chose the number 13 since main was 49 bytes long and 49 / 4 rounds up to 13 just to be safe. Since we exit from the function early it shouldn\u2019t make a difference. Now all that\u2019s left is to copy and paste this back into our and run it.\n\nAnd all this time we\u2019ve been ignoring the warning message about main not being a function :)\n\nI\u2019m guessing all that will happen when my coworker turns in an assignment looking like this is they will take off points for bad coding style and say nothing else about it."},
{"url": "http://www.theverge.com/2015/5/22/8646683/elon-musk-school-spacex-children", "link_title": "Elon Musk founds a school for his 5 children", "sentiment": 0.18554421768707485, "text": "Evidently relying on the old adage \"If you want something done right, do it yourself,\" Elon Musk is now a school founder.\u00a0In a recent interview on Beijing Television, the Tesla founder revealed that he opened a school, called Ad Astra, last year for his five children. In his view, regular school wasn't educating his kids the way he thought it should.\n\n\"They weren't doing the things I thought should be done,\" Musk said during the interview. \"I thought, well, let's see what we can do. Maybe creating a school would be better.\"\n\nMusk hired one of the teachers from the boys' school to help found Ad Astra, and the school now teaches 14 elementary-school-aged kids from mostly SpaceX employees' families. The CEO wanted his school to teach according to students' individual aptitudes, so he did away with the grade structure entirely. Most importantly, he says learning should be about problem solving.\n\n\"It's important to teach problem solving, or teach to the problem and not the tools.\"\n\n\"It's important to teach problem solving, or teach to the problem and not the tools,\" Musk said. \"Let's say you're trying to teach people about how engines work. A more traditional approach would be saying, 'We're going to teach all about screwdrivers and wrenches.' This is a very difficult way to do it. A much better way would be, like, 'Here's the engine. Now let's take it apart. How are we gonna take it apart? Oh you need a screwdriver!'\"\n\nRight now, the school is small and experimental, and Musk is unsure about how long his children will be enrolled. But the kids seem to like it, and the class size will grow to 20 by the fall."},
{"url": "http://www.pcworld.com/article/2925372/us-proposes-tighter-export-rules-for-computer-security-tools.html", "link_title": "US proposes tighter export rules for computer security tools", "sentiment": 0.06428856231487812, "text": "The U.S. Commerce Department has proposed tighter export rules for computer security tools, a potentially controversial revision to an international agreement aimed at controlling weapons technology.\n\nOn Wednesday, the department published a proposal in the Federal Register and opened a two-month comment period.\n\nThe changes are proposed to the Wassenaar Arrangement, an international agreement reached in 1995, aimed at limiting the spread of \u201cdual use\u201d technologies that could be used for harm.\n\nForty-one countries participate in the Wassenaar Arrangement, and lists of controlled items are revised annually.\n\nThe Commerce Department\u2019s Bureau of Industry and Security (BIS) is proposing requiring a license in order to export certain cybersecurity tools used for penetrating systems and analyzing network communications.\n\nIf asked by the BIS, those applying for a license \u201cmust include a copy of the sections of source code and other software (e.g.,\u00a0libraries and header files) that implement or invoke the controlled cybersecurity functionality.\u201d\n\nItems destined for export to government users in Australia, Canada, New Zealand or the U.K.\u2014the so-called \u201cFive Eyes\u201d nations which the U.S. belongs to\u2014would be subject to looser restrictions. Those nations\u2019 intelligence agencies collaborate closely.\n\nThe proposal would modify rules added to the Wassenaar Arrangement in 2013 that limit the export of technologies related to intrusion and traffic inspection.\n\nThe definition of intrusion software would also encompass \u201cproprietary research on the vulnerabilities and exploitation of computers and network-capable devices,\u201d the proposal said.\n\nTools that would not be considered intrusion software include hypervisors, debuggers and ones used for reverse engineering software.\n\nThere has long been concern that software tools in the wrong hands could cause harm. But security professionals who conduct security tests of organizations often employ the same software tools as those used by attackers.\n\nThomas Rid, a professor in\u00a0the Department of War Studies\u00a0at King\u2019s College London, wrote on Twitter that the proposed export regulations \u201cseem too broad; could even damage cybersecurity.\u201d\n\nMany private computer security companies sell information on software vulnerabilities for commercial purposes, a practice that has been criticized.\n\nThose companies have defended their sales models, arguing that without a financial incentive, the software vulnerabilities may not have been found, which ultimately protects users. Many have policies that forbid selling sensitive information to unvetted parties.\n\nThe proposal said there is a \u201cpolicy of presumptive denial for items that have or support rootkit or zero-day exploit capabilities.\u201d\n\nRootkits are hard-to-detect programs used for electronically spying on a computer, and a zero-day exploit is attack code that can take advantage of a software flaw.\n\nChanges to the list of controlled items covered by the Wassenaar Agreement are decided by consensus at its annual plenary meeting in December."},
{"url": "http://motherboard.vice.com/read/everything-sounds-the-same-when-youre-depressed", "link_title": "Everything Sounds the Same When You\u2019re Depressed", "sentiment": 0.0452715934858792, "text": "A group of friends invites you along to a bar. As you order a drink, one friend leans over to ask you a question. You try to focus on his speech, but all you can hear is the bartender speaking to another customer, the music playing in the background, the din of conversations surrounding you.\n\nExperiences like this are common for many people with depression, according to new research that shows people with depression have trouble understanding the speech of others in environments with a lot of background chatter. The tendency can make these individuals feel more isolated in social settings and perpetuate the condition.\n\nThe study, which will be presented at the 169th meeting of the Acoustical Society of America this week, surveyed undergraduate students at the University of Texas at Austin with low and elevated symptoms of depression. Researchers had students listen to a recording of a sentence mixed with noise, and later type out what they heard to determine how accurately they perceived the speech\n\n\u201cWe have found when you are depressed, you are more distractible and that prevents you from focusing or listening in on speech.\u201d Bharath Chandrasekaran, assistant professor of communication sciences and disorders at the University of Texas-Austin and an author on the study, told Motherboard.\n\nAlthough the field of psychoacoustics recognizes five basic types of emotional speech (angry, fearful, happy, sad and neutral), Chandrasekaran said most speech perception studies thus far have focused neutral speech. Because of this, researchers used samples with all five kinds of speech in the study to see if it affected participants\u2019 perception. Each student underwent 50 individual tests, with 10 sentences of all five types of emotional speech. Contrary to what researchers anticipated, Chandrasekaran said depressed individuals did not have a better perception of negative emotional speech.\n\n\u201cWe know depressed people have an increased focus on negativity, but the speech issue found in this study is not emotion-specific,\u201d he said. \u201cWe found it doesn\u2019t matter if it is emotive sentences or neutral speech, it depended on the setting.\u201d\n\nBackground noise that affects a listener\u2019s ability to understand someone else\u2019s speech is broken down into two categories in psychoacoustics: energetic masking and informational masking. Energetic masking includes sound from outside, non-linguistic sources, such as construction noise, whereas informational masking comes from human conversation and other cognitive sources.\n\nResearchers found depressed students performed more poorly in informational masking environments than energetic masking environments, meaning a depressed person who has trouble deciphering the words of a friend at a cocktail party wouldn\u2019t have the problem to the same degree in a construction zone.\n\nChandrasekaran said although it is widely acknowledged that depression, which affects approximately 121 million individuals globally, causes deficits in interpersonal communication, this particular area of research is relatively unexplored.\n\n\u201cThere\u2019s a lot of work on the output from people who are depressed\u2013\u2013research shows when you have major depressive disorder your cadence and emotional speech is reduced,\u201d he said. \u201cBut we don\u2019t know as much about speech perception in depressed individuals.\u201d\n\nIn the future, Chandrasekaran says the researchers want to study similar effects on individuals with a wide range of diagnosed depressive disorders. \n\n"},
{"url": "https://github.com/auth0/jwt-as-api-keys", "link_title": "Example of Using JWTs as API Keys", "sentiment": 0.27070707070707073, "text": "This is a sample that shows how you can use JWTs as API Keys. You can read more about this in this amazing article.\n\nI want to be able to call the API of MyApp.com. For that, first I must login to MyApp.com with my user credentials. Once I'm logged in, I can then create the API Key to MyApp.com which I can then use to call its API.\n\nYou must send the as the payload when calling this method. This will Login that user and return a Token for that user authenticated.\n\nYou can create a new API token by calling this endpoint. In order to call it, the user must be first logged in. You can login by calling the endpoint and then sending the created JWT in the header when calling this endpoint. It'll return a JWT which is the API token with the requested scopes (By default this are 2 now)\n\nThis endpoint mimis calling the Follow action. In order to call this endpoint, you need an API token that contains the scope.\n\nThis endpoint mimis calling the get user names action. In order to call this endpoint, you need an API token that contains the and scopes.\n\nJust download, and then ."},
{"url": "http://www.wsj.com/articles/behind-the-rise-and-fall-of-blackberry-1432311912?mod=LS1", "link_title": "The Inside Story of How the iPhone Crippled BlackBerry", "sentiment": -0.25, "text": "Mike Lazaridis was home on his treadmill when he saw the televised report about Apple Inc.\u2019s newest product. Research In Motion\u2019s founder soon forgot about exercise that day in January 2007. There was Steve Jobs on a San Francisco stage waving a small glass object, downloading music, videos and maps from the Internet onto a device he called the iPhone.\n\n\u201cHow did they do that?\u201d Mr. Lazaridis wondered. His curiosity turned to disbelief when Stanley Sigman, the chief executive of Cingular Wireless joined Mr. Jobs to announce..."},
{"url": "http://blog.netizencorp.com/2015/05/16/a-disturbing-trend-in-cyber-security-scams/", "link_title": "A Disturbing Trend in Cyber Security Scams", "sentiment": 0.014616402116402114, "text": "Everyone has potentially heard of the annoying \u201cransomware\u201d floating about which is malware designed to\u00a0hold computers and sensitive data hostage in exchange for a \u201cfee\u201d that seems to increase ever more as it is paid. There appears to be a trend of late, within the burgeoning cyber security field, of so-called \u201cexperts\u201d and solution providers crossing the thin line between White Hat and Black Hat (good and bad hacker) to actually break into systems or steal data and then sell a solution to the desperate business owner whose website has been hacked or hijacked. Either that, or some companies are inflating or outright fabricating a d ata breach by \u201cfinding\u201d your private records in the nefarious underworld of the Internet.\n\nThis has been in the news, with even some large companies being accused of this practice, and lately with small and medium businesses reporting their belief that they have been a victim of this \u201ccyber solution\u201d extortion.\n\nSo, what are we seeing? Let\u2019s use a couple hypothetical companies. Business \u201cA\u201d sells widgets online, through their self-hosted content management or e-commerce system. Patching this years-old system is secondary to operating their business, so staying on top of every security update is not only difficult, but often impossible without a certain level of technical expertise at their disposal.\n\nOne day, Business \u201cA\u201d gets an email, out of the blue, from a company we\u2019ll call the \u201ccyber provider,\u201d maybe from overseas, that \u201cjust so happens\u201d to notice that their site was exploited through an obscure vulnerability to serve up pharmacy ads and they found this through what they say was a chance manual search. It also \u201cjust so happens\u201d that this email comes within a day, maybe two, of this obscure vulnerability being exploited, as a check of the backup record\u00a0reveals. The \u201ccyber provider\u201d then offers a solution to fix the site \u201cimmediately\u201d by just happening to know what the exact exploit used was the company\u00a0website an ocean or so away. Incredible coincidence? Maybe. Probably\u00a0not.\n\nThis scenario is akin to\u00a0returning to returning to your car at a large shopping center parking lot and seeing a row of vehicles with smashed tailights and, just coincidentally, a flyer on each window from a company specializing in \u201cquick, easy taillight repair.\u201d Just good timing? Maybe. Probably not.\n\nPeople and companies are looking to take advantage of the increasing prominence of cyber threats by serving up their own brand of fear. The lesson is this:\n\nHas your website been hacked or have you been the victim of a similar scheme from a purported cyber security company? If so, let us know\u00a0immediately so we can improve our threat database and give you a no-strings-attached evaluation of your security situation."},
{"url": "http://pando.com/2015/05/21/masschallenge-is-quietly-growing-into-one-of-the-most-influential-forces-for-global-entrepreneurship/?sf38270663=1", "link_title": "MassChallenge growing into influential force for global entrepreneurship", "sentiment": 0.10966255966255968, "text": "Through a maze of hallways, each indistinguishable from the next, in the still very under-construction and hard to find HULT International School of Business on the Boston-Cambridge line, the world\u2019s largest startup accelerator, MassChallenge, held the official welcoming ceremony yesterday afternoon for the 128 startups that will be joining its four-month bootcamp for early stage companies.\n\nIt was an appropriate setting for the organization that may be Boston\u2019s greatest contribution to the U.S. startup ecosystem, and yet gets overlooked in the discussion of the East Coast\u2019s most innovative tech-focused organizations, among them Codecademy, Flatiron School, the Boston and New York Techstars, and Betaworks.\n\nIt\u2019s even overshadowed in the Boston-area by a handful of recognizable tech companies, including TripAdvisor, Kayak, Wayfair, Akamai, Care.com, DraftKings, and HubSpot. But MassChallenge, one of the city\u2019s non-profits, is\u00a0on the cusp of becoming an internationally recognized brand with the\u00a0chance to make an impact on a massive global scale.\n\nWhile yesterday officially marked the start of this year\u2019s program in Boston, the accelerator also unveiled the 90 companies that will be participating in MassChallenge UK \u2014 the second global expansion for the accelerator, with Israel being the first. Next week, many of the same members of the MassChallenge team that welcomed the broad group of startups to its original program will be in London to do the same for its first European outpost.\n\nBut while it\u2019s impressive that the accelerator\u00a0currently helps more than 200 early stage companies gain traction in the U.S. and abroad, it\u2019s the organization\u2019s next moves that may make it a literal 500 startups.\n\nMassChallenge is unique among startup programs and competitions in that it doesn\u2019t take any equity in the companies that participate in its programs. For that reason, among others, MassChallenge has appealed to government programs in places like Russia, Brazil, Columbia, and\u00a0many others that are trying to promote entrepreneurship in their home nations. MassChallenge has claimed to be the largest startup accelerator in the world just due to the number of companies that take part in its program each year; it may also be able to stake a claim to that title due to its massive global presence.\n\nJust this week, in addition to selecting a couple hundred companies from the more than 2,000 that applied to the program, MassChallenge also revealed that Israel Ganot, the co-founder of device return company Gazelle, will be taking over leadership of MassChallenge Israel as its managing director.\n\nA large number of MassChallenge\u2019s sponsors \u2014 which subsidize the program and its financial\u00a0grants that range from $10,000 to $100,000 \u2014 are either multi-national corporations or the business development arms of nations in Europe, Asia, and South America. Chatting with John Harthorne, a co-founder and the chief executive of MassChallenge, the organization is being approached by so many countries and businesses looking to partner with the program, it is having to turn people away.\n\nHarthorne himself is the central figure to MassChallenge\u2019s international growth, and yet he often stays in the wings and lets the young crop of MassChallenge\u2019s emerging leadership team be the public face of the program. I haven\u2019t seen his travel itinerary from the past year, but judging from what folks within the organization have said as well as his attendance at public events over the past years, it wouldn\u2019t be hard to estimate that Harthorne has spent at least half the year traveling to potential new MassChallenge sites abroad. Although nothing is official, MassChallenge already has official agreements to partner with the governments of Germany, Russia, France, and a host of others, in the near future. Rumors of MassChallenge programs in South America and Africa are as constant as Harthorne\u2019s trips to the international terminal at Logan Airport.\n\nAnd while its growth is one of the main reasons to watch MassChallenge going forward, the role it plays in helping early stage companies, even beyond tech, is why it has such an appeal to entrepreneurs.\n\nWhile companies coming out of the Techstars Boston program have often gained more public attention, quite a few got their start at MassChallenge, including fast-growing Localytics. But, because of the breadth of startups in its program, MassChallenge companies are making a bigger impact. Just take two companies that took part in the program two years ago. One, Silverside Detectors, created a sensor system, which can be setup at tollbooths around major metropolitan areas, that can detect if someone is trying to bring nuclear materials into a city. The other, the MIT-built UPower, created a nuclear reactor that can power a city, which is the size of a dorm room refrigerator.\n\nThe companies that were accepted into this year\u2019s MassChallenge program range from the usual group of payments and transportation tech companies, to outliers like Six Foods, which makes edible cricket chips, to Unshrinkit, a developer of a product that can fix clothes accidentally shrunken, to one company trying to develop a way to fuel cars while they are driving.\n\nTalking to a few of the companies yesterday, there was a general excitement at having been accepted into the program. And, while its international growth is a metric to point to for the program\u2019s potential success, the full impact of MassChallenge won\u2019t be known for a number of years, quite possibly when the companies that have gone through the program start to become the massive multi-national corporations that subsidize the program."},
{"url": "http://blog.gem.co/post/119547746215/making-the-case-for-haskell", "link_title": "Making the Case for Haskell", "sentiment": 0.10331990341891335, "text": "This post is a recap of a recent Gem HQ afternoon tech talk by our own James Larisch, software enthusiast and intrepid autodidact.\n\nAt Gem, we are passionate about creating the world\u2019s most reliable and secure API for developers who want to integrate Bitcoin wallets into their applications. Ruby is a fantastic and elegant language for getting this done, and paired with the actor-based Celluloid library it makes for a concurrent, rock-solid system that does exactly what we need it to. That being said, we also love exploring other ways of developing and thinking about software.\n\nHaskell is a wonderful example of how different a language can be, and I\u2019d like to examine some of the differences between Ruby, a language I enjoy using every day, and Haskell, a language I\u2019ve enjoyed exploring.\n\nBack in elementary school, you ran into things like . Let\u2019s create a Ruby method with the same name: .\n\nWe\u2019ll call the mathematical expression , and the Ruby .\n\nIn math, does not evaluate the function with the parameter . is equal to , and vice-versa. Really think about that. There is no execution in mathematics. There is only equality and restating. You can restate as that function\u2019s body, and you can restate as .\n\nThis concept is called referential transparency. It\u2019s the idea that at any point you can replace with , and no matter where you do it, the represented value will be unchanged \u2013 a true statement before the replacement will remain true afterwards. This is obviously the case in math.. but what about programs?\n\nIn , we certainly do not have referential transparency. This is mainly because calling is not equality, it is execution. You are executing the computer code that the Ruby interpreter stored when it parsed , then writing to the memory at location the value returned by .\n\nNot to mention in the execution order we are mutating an instance variable. This is essentially telling the computer to change a block of memory somewhere.\n\nThis is fine. But it means every time you send the command to the computer, something different occurs. It changes the state of the universe (in this case, the amount of things animal has eaten)!\n\nYou cannot replace every occurrence of with , because the execution is vital to the correctness and expected behavior of our program. As a Ruby programmer (or programmer used to imperative), this seems normal.\n\nIf you feed the animal to over 100 and it tries to , it will puke (raise an error). So if you are feeding the animal with the trainer in some area of your program, then you try to make the animal run, it could puke! What if there are other ways to feed the animal? How would you debug this: the animal is puking, and you don\u2019t know at what point you have overfed it?\n\nYou might print out animal\u2019s at different places in your program. You might drop a into your program and inspect to see when it was incremented over 100. (This is obviously a contrived example, but humor me.)\n\nThese debugging techniques rely on inspecting some global, mutable state. Things can have different values at different times in the program. Debugging often means stopping at specific times in the order of execution and looking around at the state of the universe. We\u2019re used to that, but victims also get used to Stockholm Syndrome.\n\nI\u2019ll respond to that with a counter-question.\n\nAs you can probably tell - this problem doesn\u2019t even make sense in mathematical terms. Because remember, math is not an ordered execution of statements, it\u2019s equality. You are not describing what to do, you\u2019re describing what is.\n\nSo you can\u2019t change the state on some global object and then later on read that value back to yourself.\n\nHaskell attempts to bring this mathematical is-ness to programming.\n\nHaskell obviously must evaluate expressions, because Haskell exists in the Real World. Math is not bound by this constraint because it is only concerned with The Ultimate Truth.\n\nBut we can break away from this concept of execution / commands (imperative programming) and move into more functional / declarative programming. In Haskell, there is no giving the computer commands. No telling the computer to mutate some globally held state. Functions take an input and return an output. As dictated by the language, there will be no side effects (no ).\n\nWhat does this mean? It\u2019s means Haskell is referentially transparent, because you the compiler guarantees that if , any can be replaced with .\n\nEveryone probably wants me to translate the above Ruby code into Haskell. This doesn\u2019t really make sense, because Haskell simply does not have (encourage) the ability to create that kind of stateful object and provide commands for mutating it.\n\nLet\u2019s break down what the Ruby is doing. It\u2019s mutating the current , then returning . In Haskell, without mutation, there is only input & output.\n\nSo you could do:\n\nWhich takes an and adds one to it. Notice this has no notion of an - you could try to fit one in to another function but what would it do? It would end up returning a new , because Haskell does not allow (encourage) mutating objects, it simply returns new ones. This is a symptom of a lack of memory mutation. It is a simply of equality instead of assignment. I hope now you understand why modeling the above problem simply doesn\u2019t make sense in the Haskell paradigm.\n\nBut it looks very similar to mathematics, doesn\u2019t it? , and the compiler can replace all instances of with , by only evaluating it once. This is not assignment, it\u2019s equality. It\u2019s is-ness.\n\nIf this seems too impractical for common usage, think about something we all encounter every day: HTTP requests. What is an HTTP request but a request & a response? An input and an output? In Haskell you could construct a function:\n\nNo global state to deal with, just input and output.\n\nAs an aside, the more astute readers will no doubt be screaming: \u201cthe world has global state! databases! IO! WTF?\u201d\u2026 this is a topic for a future post: How Haskell Handles IO and Ordered Execution (MONADS)\n\nSo why do all of this? Why go through the trouble of shifting your brain in order to comprehend this stuff? Why write programs this way?\n\nThey are much easier to debug.\n\nRemember, the issues we had in Ruby were mostly caused by the state of the universe changing based on the order of execution. To debug, we breakpoint and inspect the world. In Haskell, there is none of this.\n\nThere is only what went into the function and what comes out. Testing becomes simply a matter of ensuring you get the output you desire on a certain set of inputs. It removes the reliance on context from function execution. Functions will always evaluate to the same thing, no matter what the context, which is very powerful for reasoning.\n\nIn this context, Haskell presents a simpler way of thinking about your programs. Keeping track of software you write is difficult, and adding state to the mix can make life tougher. When using libraries, often you have to understand the library as a whole in order to understand a single utility function. With Haskell, there is no global state, so a function can be used quite modularly.\n\nThinking about your programs as one or a few isms could alleviate the much of the cognitive burden of mentally tracking your program.\n\nNo. But that doesn\u2019t mean we don\u2019t love taking the time to explore new paradigms, languages, and ideas. At Gem we are passionate software engineers, but we are also software enthusiasts.\n\nExploring new ways of thinking allows us to examine and appreciate the idiosyncrasies inherent in current and new technologies.\n\nComparing Ruby to Haskell is a beautiful example of how different programming languages can be, and I think examining the differences is what makes computer science fun."},
{"url": "http://www.wsj.com/articles/amazon-targets-etsy-with-handmade-marketplace-1432332301?mod=LS1", "link_title": "Amazon Targets Etsy with \u2018Handmade\u2019 Marketplace", "sentiment": -0.044318181818181826, "text": "Having vanquished scores of brick-and-mortar retailers, Amazon.com Inc. has a new target: Etsy Inc.\n\nThe Seattle Web retailer is prepping a marketplace for artisan goods it is calling Handmade. Etsy sellers received invites reviewed by The Wall Street Journal over the past few days to sign up for the new section of the Amazon site."},
{"url": "https://pennypledge.co", "link_title": "Ask HN: Struggling to get users. What is turning visitors away?", "sentiment": 0.16666666666666666, "text": "There is a better way to support online content than by tolerating annoying ads.\n\nPenny Pledge allows you to support great content anywhere on the web."},
{"url": "http://hackaday.com/2015/05/15/smarter-than-wood-saw-blade-makes-perfect-foldable-joints/", "link_title": "Hackaday: Saw blade makes perfectly foldable joints", "sentiment": 0.2354380268853953, "text": "[Andrew Klein] knows the pain of building drawers from plywood. It can be a pain to get all of the pieces measured and cut just right. Then you have to line them up, glue them together,\u00a0and clamp them perfectly. It\u2019s time-consuming and frustrating. Then one day it hit him that he might be able to make the whole process much easier using a custom saw blade.\n\nThe the video below, [Andrew] does a great job explaining how the concept works using a piece of paper. The trick is that the plywood must be cut in a very specific shape. This shape results in the plywood just barely being held together, almost as if it\u2019s hinged. The resulting groove can then be filled with\u00a0wood glue, and the plywood is folded over on itself. This folding process leaves no gaps in the wood and results in a strong joint. Luckily this special shape can be cut with a specialized saw blade.\n\nThis new process removes the requirement of having five separate pieces for a drawer. Instead, only four cuts are needed on a single piece of square plywood. The corners are then removed with a razor blade and all four sides are folded up and into place. [Andrew] shows that his prototype blade needs a little bit of work, but he\u2019s so hopeful that this new invention will be useful to others."},
{"url": "https://www.eff.org/deeplinks/2015/05/unexpected-policy-laundering-implications-garcia-v-google-dissent-reverse-policy", "link_title": "The Unexpected Policy Laundering Implications of the Garcia V. Google Dissent", "sentiment": 0.1617457469431154, "text": "The Ninth Circuit Court of Appeals this week sensibly, if belatedly, reversed its mistaken order requiring Google to take down a controversial video based on a specious copyright claim. But there\u2019s more to this story than the free speech win. Unfortunately, Judge Alex Kozinski's dissent points to an alarming policy laundering trend in its reliance on the Beijing Treaty for Audiovisual Performances, a deeply problematic international agreement that the United States has signed but not yet ratified.\n\nTwo major factors make Judge Kozinski's dependence on the Beijing Treaty a stretch. For one, the Treaty doesn't even go into effect until 24 more countries ratify it. For another, he cites specifically to the Patent and Trademark Office Fact Sheet\u2014and as the majority notes, that agency \"lacks legal authority to interpret and administer the Copyright Act.\"\n\nAll of that said, the really troubling part about Judge Kozinski's Beijing Treaty citation is that it may herald a new and particularly pernicious form of policy laundering.\n\nThe scholar Margot Kaminski explained how that might be the case earlier this year, after Judge Kozinski mentioned the Beijing Treaty during Garcia oral arguments. The concerns she outlined then are even more resonant now:\n\nSupporters of secretive agreements like the Trans-Pacific Partnership (TPP) argue that they don't require a change to U.S. law. Because new policies won't end up getting laundered in, they claim, transparency is less important.\n\nJudge Kozinski's argument demonstrates the problem with that reasoning. Even if you take negotiators' word that they're hewing close to U.S. law, they're still engaging in what Kaminski has elsewhere dubbed \u201cregulatory paraphrasing\u201d: because they're not transcribing U.S. law verbatim, they're necessarily making interpretations. Those interpretations can make non-obvious but important changes, like transforming a standard into a rule, or vice-versa.\n\nAmbiguities in the law, which should properly be interpreted by judges, end up getting settled by negotiators and the corporate lobbyists that influence them. When judges cite these interpretations, it can close the policy laundering loop, crystallizing policy made through an inappropriate (and often secretive) legal process. We saw hints of it from the Solicitor General in the Aereo Supreme Court argument, and we see it too with legislators who bring up possible conflicts with international agreements as an argument against domestic reform.\n\nJudge Kozinski has written compelling and impressive dissents before\u2014the 1993 White v. Samsung is a classic in the genre. Unfortunately, in Garcia he missed the mark. We\u2019re glad the majority rejected arguments and chose instead to support traditional copyright principles."},
{"url": "https://www.youtube.com/watch?v=Nn_afSOwmqo", "link_title": "The First Hoverboard Flight (1'02\")", "sentiment": 0.12142857142857143, "text": "Rating is available when the video has been rented.\n\nThis feature is not available right now. Please try again later."},
{"url": "https://dennisforbes.ca/index.php/2015/05/21/selling-solutions-the-ugly-side-of-consulting/", "link_title": "Selling Solutions: The Ugly Side of Consulting \u2190 Dennis Forbes", "sentiment": 0.12216989331945345, "text": "Over my career I\u2019ve experienced a wide variety of working situations as geography, life situation (e.g. children), or the desire for something different encouraged change.\n\nI worked in a small firm of professional engineers building solutions for the power generation industry, sharing a small office in a one-story building at the edge of a mid-sized city. I often rode a bicycle to work, the day veering to miserable when an unexpected rain rolled in.\n\nMy most vivid memory of that office was a coworker who ate oranges regularly: I loved the fragrance from the peels that filled the air. Strange how so many memories are scent based.\n\nIt was a motivated, fundamentally agile and fast moving workplace, and there was a good amount of passion for what we were doing.\n\nWhen my wife got a great job offer in another city, and realistically I had exhausted the challenges of the role, I moved on.\n\nI worked in a skyscraper in a mega city for a large telecommunications company, commuting in by train. I walked from the station to the office everyday (underground when the weather was bad), taking a slightly different, circuitous route for variety, then finding my way to my tiny cube in a forest of cubes. Every lunch I would wander the area and buy food from a street vendor, which given Toronto\u2019s street food bureaucracy meant a hot dog most of the time.\n\nI had no idea what the people in cubes near me even did (though when I thought back to reminisce, it was the workplace depicted in the movie 1984 \u2014 the John Hurt version \u2014 that came to mind), and can\u2019t recall their names. The people I actually worked with were in other cities and sites.\n\nThen I was recruited by a bank, enticed by financial products and benefits that drew me given the birth of my first child. Canada is a destructively reserved, uncompetitive nation in general, so being recruited was pretty unique to start with, and a bit of an ego boost, even if the banking conglomerate had to completely upend their pay scale in the process. Through the years described I turned down a variety of offers from Bay Area and Seattle companies: Canada is my home and I rather like living here, even though in practice the people and organizations I end up working with are almost always in other nations.\n\nI worked for a mega financial company in a suburban office, again in a series of cubes. Most of my coworkers were careerist who by and large despised their job and despised the company, but nonetheless planned as if it were their indefinite future. Which was at odds with the goals of the executive who were doing their best to ensure that most of them had a short future with the company. The commute got torturous, and it was incredibly hard to have any feeling of accomplishment with the work: Change was tightly restricted, the process so limiting that the lifecycle of every project was a burst of unsanctioned activity, usually by a rogue team, which then became a sanctioned solution that was largely static, growing ever more obsolete until it was eventually replaced by the next emergent upstart unsanctioned project, or an external solution that could be developed without any of those internal controls or restrictions. Progress, uh, finds a way.\n\nThat model, seen at so many firms, is like stocking your kitchen with only quinoa and a\u00e7a\u00ed berries, talking about the importance of good eating, and then buying a Big Mac and a large pizza whenever your hunger overwhelms you. Tight controls at a financial organization are obviously required, but when they restrict action to the point that the only progress is by taking solutions from outside purveyors or rogue teams with no controls, the whole exercise is rendered absurd.\n\nThe work was becoming soul destroying. It also risked being career destroying. I moved on.\n\nI worked for a small financial technology company building innovative solutions using the latest patterns and practices and technologies to provide a competitive edge for the operations of the firm. The ability to architect things from the beginning, to use the best, most appropriate technologies, and to have a good amount of financial backing, made it interesting.\n\nIt was a good company with good people, but when my mentor and a partner in the firm passed away, it was time to move on.\n\nSince, and at varied times in between, I\u2019ve worked as an independent consultant through my company yafla Inc. Always working with customers directly, never through a body shopper.\n\nMany of those engagements have been hugely rewarding experiences, where I\u2019m doing challenging work that makes a huge difference with smart clients that optimize the benefits.\n\nThe financial rewards can be good, though when you consider the countless hours of research, learning, investigating, building trial solutions of emergent technologies, etc, and then add the personal costs like office space and the accouterments of doing business\u2026it isn\u2019t as amazing. They are quite a contrast to some of the wild tales that are making the rounds: Every couple of weeks I see the community percolating someone\u2019s claims about charging tens of thousands per week with ease for some vague, undefined service, choosing and picking their clients at will. The motives for their disclosure is often a bit dubious (if someone is claiming incredible things and using that exposure to pitch something else, or to establish an air of authority, be wary. It\u2019s the \u201cfinancial guru\u201d who makes \u201cso much money\u201d on the markets that he\u2019s mostly interested in getting the pittance you\u2019ll pay for his how to secrets pamphlet \u2014 the facts don\u2019t correspond), the situation so unlikely that it\u2019s one of those \u201cextraordinary claims require extraordinary evidence\u201d situations, yet is often presented more as a girlfriend-in-Canada situation.\n\nThe claims of enormous riches and lines of clients might be true, but I always read them with a hefty dose of suspicion. As should you.\n\nI mention this because I always worry that loads of smart developers read those stories and quit their jobs in a rush to start making their tens of thousands per week, choosing among countless offers to find the perfect role as desired from week to week, jetting off to Europe on a whim.\n\nIf you don\u2019t already have the reputation and the network, you\u2019re probably going to hit a complete brick wall with an empty sales funnel and a sense of desperation that soon has you making WordPress templates for sub-minimum wage on some freelance site. Even if you have a great reputation and an extensive network of well-heeled clients, with some unique niche skills, the industry is, in my experience, much more difficult than a lot of these stories would have you believe.\n\nI can only speak to my own experiences, and maybe I\u2019m just speaking from a limited perspective, but with clients among financial and technology firms in New York City, Los Angeles, Toronto, Europe and the Bay Area, it\u2019s a very different world to the one often presented.\n\nMost destructively, doing consulting is a world where a lot of people just want to waste your time, and it\u2019s hard to know until it\u2019s too late.\n\nThere\u2019s an episode in the enjoyable series \u201cSilicon Valley\u201d where a \u201cprospective investor\u201d calls the team in to talk about their technology, the ostensible goal being to discover if it\u2019s worth investing in. Midway through, members of the Pied Piper team realize that they are being \u201cbrain raped\u201d, the whole ruse really just a way to try to get some free solutions.\n\nThis, I have found, is the reality of a good percentage of consulting engagement \u201csales\u201d processes, and I suspect is the reason many firms engage a sales staff that know little enough, talking to middle-management on the other side that also knows so little about the specifics, that their engagement can\u2019t really, by itself, bring much value to anyone (even though it\u2019s very costly for both sides), getting the client to the pay stage without actually telling them what for, on the backs of a selection of well cooked steaks and bottles of wine.\n\nI could, of course, simply refuse to talk specifics (the decisions that set the foundation for success are themselves very valuable, and are often the greatest contributors to project success, performance, and lifespan) until the clock has started ticking and a contract is signed, or demand a fee for the initial sales process, and I\u2019m sure such is the solution for those people purportedly charging $40,000 per week, picking among the long line of potential customers throwing money at them, but that doesn\u2019t really work in practice for me: It\u2019s hard to give a client a sense of the value that you\u2019re bringing, or the scope and scale of the commitment to get to the delicious deliverables, without talking through their problem and how you\u2019re going to solve it, making timelines and roadmaps and technology stack diagrams.\n\nThe good clients realize the value and continue the process through to the next stage, but there are a seemingly limitless number of bad clients who\u2019ll immediately start browsing freelancer sites to find the lowest hourly they can with some matching keywords (high concurrency vectorization via in-memory data representations\u2026well this guy says he used the MySQL database so that\u2019s about the same), demanding that you match it, which I of course decline. Of course that will always come back to haunt them, but that doesn\u2019t undo the fact that it wasted a lot of my time, and there is no reward or satisfaction seeing their march to failure.\n\nRepeat this several dozen times, sometimes with the same non-clients shamelessly attempting a round two. I won\u2019t even get to the issue of clients who suddenly have grievances with rates after the solution is delivered to specification and on-time, or simply don\u2019t have the capacity to pay at all (clients that have demonstrated traits of being profoundly cheap \u2014 if they\u2019re calling you to fix a disaster project that has been failing for years with their far flung, offshore team that they chose purely based upon cut-rate prices [there are incredibly talented and capable \u201coffshore\u201d teams, and I am never one to denigrate fearfully based on geography \u2014 there are brilliant minds across the globe. But there are other teams that are put together on the basis of price alone, the goal being to have the largest headcount at the lowest cost, the results always an absolute disaster] \u2014 are usually circling the drain and are almost always a client you don\u2019t want).\n\nThese are all time sucking distractions. The solution, of course, is to grow to the point where you have sales staff, and an accounts receivable staff, and various intermediaries and so on, but pretty soon the overhead is so great that the next solution is to ride on the backs of low cost talent, hiring new graduates who are mostly trained in how to increase billable hours. See: Most medium to large consulting companies.\n\nIt\u2019s a descent to mediocrity, which is not something that interests me. I need the challenge, and the reward of doing great work. That\u2019s what drives me.\n\nMost prospective clients, good and bad, will compare your rate with that of a hypothetical, generalist full-time employee, and this can be an ongoing source of friction. Even if you hit the ground running (usually having done an enormous amount of work to be ready for this), with a very specialized, proven set of unique skills and a long track record of success, quickly pounding out a solution using every best practice on your own hardware in your own office, over a short engagement that is just a rapid fire of high value deliverables, someone is going to divide some numbers and compare your rate with the unloaded contrived hourly of that guy who made the Excel spreadsheet calculations a few years back (so, like, a programmer or something) and currently bides his day browsing Reddit.\n\nIt happens in almost every engagement. It\u2019s why the common claim of enormous rates rings so hollow \u2014 if you say that you\u2019re going to save an organization $500,000, and do the work to make that actually happen \u2014 say by doing an analysis and modifications to a process that required 10x the hardware it should, not even counting the customer and reputation loss through miserable response times \u2014 if that took you an hour, many clients will quickly run an \u201cequivalent full time employee\u201d calculation and figure that your work was worth $88,000 / 52 / 5 / 8\u2026$42. And thirty cents\u2026what the heck, make it thirty five cents, kid, but don\u2019t spend it all at one place.\n\nThe compensate-based-upon-value-derived client is more of a unicorn than a common case, most instead concerned only with the amount of time you spent doing the visible aspects of the job. That same messed up evaluation is the curse of traditional employees as well, most employers concerning themselves most with how many hours you warm a seat than the fundamental value you bring to the organization.\n\nThat process performance example is a very real one that I\u2019ve endured a number of times. If clients have a long running process or query, I have a bit of a humored internal standard that I can usually reduce the runtime 98%, which you could also say makes it run in 1/50th the time. More often than not it is 99%+ (to 99.9%+. If someone mentions the words big data, it\u2019s usually on the very high end of the savings estimates), only attacking the largest and most egregious culprits. These clients can task their entire team to try to do this for months on end and see no success, yet when I provide that solution, it immediately becomes the normal baseline, and is judged for the time spent (which usually isn\u2019t much time at all), not for the enormous value derived.\n\nThis story has played out hundreds of times, and each time I always think I should provide an A:B demonstration, with the B being black-boxed and only available for some large, ransom-like fee.\n\nSuch \u201cvalue proposition\u201d arguments are a complete non-starter in most engagements. Moreso if the people you interact with at the firm don\u2019t particularly care if the company saves or makes $500,000\u2026because that doesn\u2019t directly benefit them (in that 98% scenario, if your intermediary is the one who pushed the organization to scale-out their \u201cbig data\u201d 500MB database, your improvements probably won\u2019t see much enthusiasm), which is often the case. As the client firm grows larger, the circle of concern of many of the people you interact with collapses, until their net concern is \u201cwhat can you do to make me look good\u201d (see \u2014 the clients who wastes your time. Once you\u2019ve delivered the exact action plan, technology stack, and approach, they can be the seer that seems to have a handle on the project\u2026for a while at least).\n\nIt\u2019s a serious question of ethics when the person you answer to at a client has motives that are fundamentally at odds with the organization.\n\nIt\u2019s a tough business. It\u2019s tough finding the right clients, tougher still convincing them to use your services, and tougher yet again to get a decent rate. It\u2019s tough to do what\u2019s right for a client without simply pandering to the personal ego and agenda of the company representative that controls your engagement. And now you need to get them to actually pay.\n\nDon\u2019t take this as sour grapes, bitter, or anything else with a negative motive. In most people\u2019s judgment I have done pretty well, however that\u2019s under some of the most perfect conditions possible (unique skills and a pretty good reputation and established network in a pretty rich industry), but even still I\u2019ve dealt with an enormous amount of nonsense, and dealt with the same demotivating distractions day in and day out.\n\nOkay I am bitter about the fake clients: I really despise the time wasting aspect, and that almost feels like I was robbed of some of my lifespan.\n\nWhich is why I mostly disengaged from doing consulting. Without compromising the service, becoming the mediocrity that defines the scene, there is just too much wasted time and effort for sporadic reward. Even among long-time clients who have benefited enormously from my services, the false economics (the \u201cequivalent to a full time\u201d calculation, and boring discussions around it) just get too exhausting to argue with repeatedly as various different people rotate into the discussion. Organizations will dump tens of millions achieving effectively nothing year in and year out, but a relatively small engagement with an \u201coutsider\u201d, fundamentally improving and rocketing the organization forward, becomes an enormous point of political strife. Most prospects are either too small to realistically afford an engagement, or large enough that individuals have established fiefdoms and motivations aren\u2019t entirely honest.\n\nMaybe I\u2019m just doing it wrong. Perhaps. But it just isn\u2019t for me. I remain skeptical of the too-good-to-be-true tales that are too common.\n\nI\u2019m doing something quite a bit different now, still working for myself, in my comfortable and absurdly productive home office, but with much more focus and selectivity of the people I work with. That is not particularly relevant to this piece (though I am selling a white paper that describes the approach), mentioning it only because it seems contextual.\n\nA lot of consulting is bullshit. For many people, I suspect, it would be much less rewarding than just getting a normal job. Be skeptical of overly optimistic stories to the contrary."},
{"url": "http://qz.com/410027/apple-watch-orders-fell-sharply-after-the-first-day-and-havent-grown-since-a-shopping-data-firm-says/", "link_title": "Apple Watch orders fell sharply after the first day and haven\u2019t grown since", "sentiment": 0.0962719298245614, "text": "Apple Watch pre-orders started with a bang\u00a0and Apple\u2019s initial inventory quickly\u00a0sold out. Then\u00a0US orders immediately fell, and have remained mostly\u00a0flat since, according to analysis\u00a0from Slice Intelligence, a company that\u00a0tracks US consumer spending through e-commerce\u00a0email receipts.\n\nApple has taken orders for almost 2.5 million watches in the US through Monday, May 18, according to Slice\u2019s projections, which are based on more than 14,000 online shoppers. More than half of those orders were placed on April 10, the first day Apple accepted watch pre-orders in the US and eight other countries, according to Slice.\n\nSince the first day\u2014which we\u2019ve excluded\u00a0from this\u00a0next chart\u00a0to\u00a0focus on detail\u2014US orders\u00a0have generally remained under 30,000 per day, according to Slice\u2019s projections. Note the spike\u00a0on April 24, the day US pre-orders started arriving\u2014and when people started posting their initial Apple Watch\u00a0experiences\u00a0and real-life photos.\n\nTo be sure, it\u2019s still early days for the Apple Watch and the broader wearable-gadgets market. It took Apple more than two years to sell 2 million iPods, and several months to sell that many iPhones. And,\u00a0to reiterate: This\u00a0is\u00a0a projection based on one company\u2019s data in one country. Apple did not immediately respond to a request for comment on its accuracy.\n\nBigger picture,\u00a0the Watch is still figuring out its purpose. Most early adopters\u00a0seem to like it\u00a0so far, but it\u2019s hardly a must-have device yet. And it seems the sort of gadget that will be most popular around the holidays, much like Apple\u2019s other consumer devices.\n\nOne Wall Street analyst, Morgan Stanley\u2019s Katy Huberty, recently increased her projection of first-year global Apple Watch shipments to 36 million, based on survey results\u00a0showing increased purchase intentions among US consumers. A second firm, however, just reportedly\u00a0decreased its estimates\u00a0to less\u00a0than 15 million watches, based on weak demand. To reach 36 million shipments, Apple would need to average\u00a0almost 100,000 per day worldwide.\n\nOne big question is how Apple Watch sales\u00a0will change once people are able to simply walk into an Apple Store, try one on, and buy it. That hasn\u2019t happened yet. Apple\u2019s online store is still showing\u00a0several-week backorders for common configurations, and its retail stores do not yet have inventory."},
{"url": "http://www.godotengine.org/wp/godot-1-1-out/", "link_title": "Godot 1.1 Out", "sentiment": 0.35901515151515156, "text": "After many months of hard work (and many more of bug fixing), Godot 1.1 is out!! This release brings a completely new 2D engine and more features (feature list below). At this point Godot is one of the most advanced 2D engines out there. \u00a0Check out (And share!) this video with the new feature showcase!\n\nGodot 1.1 can be obtained at the Downloads section.\n\nWith this release, Godot becomes one of the best options out there to develop modern 2D games.\u00a0While the community is eagerly waiting for the same work on modernizing the 3D side of engine, the developers are waiting for more news on the newly announced Vulkan API, as it would be ideal to adopt it instead of OpenGL3/4 or OpenGL ES3. Meanwhile, the focus for 1.2 will be on modernizing the editor UI for better usability. There is a long list the community has put together on this \u00a0and we are still welcoming\u00a0feedback. As always, Godot is\u00a0developed with the community and for the community.\n\nThere will also soon be:\n\n1) A new website, more community oriented, developed by Theo Hallenius, where everyone will be able to create documentation and content.\n\n2)\u00a0An asset sharing section, so the community can better share the content it creates and help each other.\n\n3) A diffusion section, where you will be able to find material, talks, etc that you can use to help spread the world and teach others about Godot.\n\nGodot does not have millions of dollars in investment for PR, so it relies on you in order for the rest of the world to find out about it. \u00a0Talk about it, tweet about it, write articles, organize talks, and share this news!!"},
{"url": "http://www.united.com/web/en-US/content/Contact/bugbounty.aspx", "link_title": "United Airlines Bug Bounty Program", "sentiment": 0.09543756896698072, "text": "At United, we take your safety, security and privacy seriously. We utilize best practices and are confident that our systems are secure. We are committed to protecting our customers' privacy and the personal data we receive from them, which is why we are offering a bug bounty program \u2014 the first of its kind within the airline industry. We believe that this program will further bolster our security and allow us to continue to provide excellent service. If you think you have discovered a potential bug that affects our websites, apps and/or online portals, please let us know. If the submission meets our requirements, we\u2019ll gladly reward you for your time and effort. Before reporting an issue, please review the \"United Terms.\" By participating in the bug bounty program, you agree to comply with these terms. A bug bounty program permits independent researchers to discover and report issues that affect the confidentiality, integrity and/or availability of customer or company information and rewards them for being the first to discover a bug. To ensure that submissions and payouts are fair and impactful, the following eligibility requirements and guidelines apply to all researchers submitting bug reports: All bugs must be new discoveries. Award miles will be provided only to the first researcher who submits a particular bug. The researcher must be a MileagePlus member in good standing. If you\u2019re not yet a member, join the MileagePlus program now. The researcher must not reside in a country currently on a United States sanctions list. The researcher submitting the bug must not be an employee of United Airlines, any Star Alliance\u2122 member airline or any other partner airline, or a family member or household member of an employee of United Airlines or any partner airline. The researcher submitting the bug must not be the author of the vulnerable code. Bugs that are eligible for submission: Bugs on customer-facing websites such as: Bugs in third-party programs loaded by united.com or its other online properties Timing attacks that prove the existence of a private repository, user or reservation The ability to brute-force reservations, MileagePlus numbers, PINs or passwords Bugs that are not eligible for submission: Bugs that only affect legacy or unsupported browsers, plugins or operating systems Bugs on internal sites for United employees or agents (not customer-facing) Bugs on partner or third-party websites or apps Attempting any of the following will result in permanent disqualification from the bug bounty program and possible criminal and/or legal investigation. The compromise or testing of MileagePlus accounts that are not your own Any testing on aircraft or aircraft systems such as inflight entertainment or inflight Wi-Fi Any threats, attempts at coercion or extortion of United employees, Star Alliance member airline employees, other partner airline employees, or customers Physical attacks against United employees, Star Alliance member airline employees, other partner airline employees, or customers If you have discovered a bug that meets the requirements, and you\u2019re the first eligible researcher to report it, we will gladly reward you for your efforts. Below is our bounty payout structure, which is based on the severity and impact of bugs. If you think you have discovered an eligible bug, we would love to work with you to resolve the issue. Please email us at bugbounty@united.com and include \"Bug Bounty Submission\" in the subject line. Within the body of the email, please describe the nature of the bug along with any steps required to replicate it, as well as pertinent applications, programs or tools used to discover the bug. Include your legal name, MileagePlus number and phone number with your submission. Please feel free to reach out to us at bugbounty@united.com with any questions regarding the bug bounty program. We will be sure to respond to you soon as possible. We look forward to hearing from you. The United Terms govern your participation in the Program and it is your responsibility to read and understand all of them. The Program is offered at the discretion of United Airlines and its affiliates, and United has the right to terminate or modify the Program, program rules, procedures, benefits or conditions of participation, in whole or in part, at any time, with or without notice (\"Program Rules\"). The Program Rules supplement the united.com Terms and Conditions and Legal Notices, the United Privacy Policy and the MileagePlus Program Rules (collectively with the Program Rules, the \"United Terms\"). By participating, you agree to comply with the United Terms. The Program is not a game or competition, but rather an experimental and discretionary reward program. Offer is valid for qualified \"Bugs\" submitted on or after May 11, 2015. We may cancel the Program at any time and the decision as to whether or not to pay award miles is entirely within United's discretion. The United \"Bug Bounty\" offer is open only to United MileagePlus members who are 14 years of age or older at time of submission. Offer is void where prohibited and subject to all laws. Employees, officers and directors (and their respective immediate family members (spouse, parents, siblings, children) or household members (whether or not related)) of United Airlines, Inc. or its parent(s), subsidiaries, affiliated companies, agents, or contractors, and anyone who participates in the administration of the Bug Bounty program are not eligible. Bugs must be submitted to bugbounty@united.com and include the researcher's legal name, MileagePlus number and phone number as well as a thorough description of the Bug and supporting evidence. Bugs must be new discoveries. Award miles will be provided only to the first eligible researcher to submit a particular Bug. In event of disclosure of PII other than your own test account, please cease the affecting activity and document steps to replicate as soon as possible. The researcher submitting the Bug must not be the author of the vulnerable code. Bugs or potential Bugs you discover may not at any time be disclosed publicly or to a third-party. Doing so will disqualify you from receiving award miles. You must not knowingly or intentionally access or acquire the personal information of any United customer or member. In the event it is determined you knowingly or intentionally accessed the personal information of any United customer or member, you will become immediately ineligible to participate in this Program. In the event you inadvertently access or acquire the personal information of any United customer or member, you must immediately cease all activity. Award miles may be earned once for each qualifying Bug submitted. You can earn award miles an unlimited number of times in accordance with these terms and conditions. You are responsible for any tax implications that apply based on your country of residency and citizenship. United will provide a payout for each qualifying Bug once the issue has been remediated. Our desired timeframe to remediate each issue is within 90 days following the confirmation of each qualifying Bug. Neither your Participation in the Program nor anything contained in the United Terms shall be construed as creating or implying a joint venture, partnership, agency or employment relationship between you and United or its affiliates. Information you receive or collect about United or its affiliates or members through the Program, whether in oral, visual, written or electronic format, may be deemed proprietary and confidential (\"Confidential Information\"). For purposes of the Program, information and/or material shall be deemed \"Confidential Information\" if such information and/or material is otherwise not generally available to the public, or given the nature of the information or material, a reasonable person would consider such information and/or material \"confidential\" or \"proprietary.\" Confidential Information must be kept confidential and only used in connection with the Program. You may not use, disclose or distribute any such Confidential Information without United's prior written consent. You agree to defend, indemnify and hold harmless United and its affiliates and the officers, directors, agents, employees and vendors of United and its affiliates from any claim or demand (including attorneys' fees) made or incurred by any third party due to or arising out of your participation in the Program, your breach of the United Terms or your improper use of the Program. Award miles offered under this Program are not Premier\u00ae qualifying miles. Offer is subject to change without notice. Other restrictions may apply. Miles accrued, awards, and benefits issued are subject to change and are subject to the rules of the United MileagePlus program, including without limitation the Premier\u00ae program, which are expressly incorporated herein. United may change the MileagePlus Program including, but not limited to, rules, regulations, travel awards and special offers or terminate the MileagePlus Program at any time and without notice. United and its subsidiaries, affiliates and agents are not responsible for any products or services of other participating companies and partners. Taxes and fees related to award travel are the responsibility of the member. Bonus award miles, award miles and any other miles earned through non-flight activity do not count toward qualification for Premier status unless expressly stated otherwise. The accumulation of mileage or Premier status or any other status does not entitle members to any vested rights with respect to the MileagePlus Program. All calculations made in connection with the United MileagePlus Program and/or the Premier Program, including without limitation the accumulation of mileage and the satisfaction of the qualification requirements of the Premier Program, and/or the revisions of calculations (including any estimates), will be made by United Airlines and MileagePlus in their discretion and such calculations will be considered final. Information in this communication that relates to the MileagePlus Program does not purport to be complete or comprehensive and may not include all of the information that a member may believe is important, and is qualified in its entirety by reference to all of the information on the united.com website and the MileagePlus Program rules. United and MileagePlus are registered service marks. For complete details about the MileagePlus Program, go to united.com/MileagePlus"},
{"url": "https://github.com/quii/mockingjay-server", "link_title": "Show HN: Mockingjay \u2013 Fake servers and consumer-driven contracts", "sentiment": 0.04404761904761904, "text": "Mockingjay lets you define the contract between a consumer and producer and with just a configuration file you get:\n\nMockingjay makes it really easy to check your integration points. It's fast, requires no coding and is better than other solutions because it will ensure your mock servers and real integration points are consistent\n\n# define as many as you need...\n\n$ mockingjay-server -config=example.yaml -realURL=http://some-real-api.com 2015/04/13 21:06:06 Test endpoint (GET /hello) is incompatible with http://some-real-api - Couldn t reach real server 2015/04/13 21:06:06 Failing endpoint (POST /card) is incompatible with http://some-real-api - Couldn 2015/04/13 21:06:06 At least one endpoint was incompatible with the real URL supplied\n\nCalling this will return you a JSON list of requests\n\nMockingjay has an annoying friend, a monkey. Given a monkey configuration you can make your fake service misbehave. This can be useful for performance tests where you want to simulate a more realistic scenario (i.e all integration points are painful).\n\n# Writes a different body 50% of the time # Delays initial writing of response by a second 20% of the time"},
{"url": "http://www.latimes.com/entertainment/arts/miranda/la-et-cam-five-reasons-why-palmyra-matters-as-archaeological-ruins-site-20150520-column.html", "link_title": "Why Palmyra's ruins are so important", "sentiment": 0.18314905814905816, "text": "The fate of one of the world's most important archaeological\u00a0treasures hangs in the balance after\u00a0the Islamist\u00a0militant group ISIS overwhelmed\u00a0the historic city of Palmyra, also known as Tadmur.\n\nThe city contains the ruins of what, according to UNESCO, \"was\u00a0one of the most important cultural centers of the ancient world\" \u2014 an important Silk Road hub where East met West more 2,000 years ago. A World Heritage Site, Palmyra is heralded by experts as having\u00a0some of the finest Roman-era ruins in existence.\n\n\"It makes Rome blush,\" says Stephennie Mulder, an\u00a0archaeologist and professor of Middle Eastern Studies at the University of Texas at Austin. \"When you approach the site, it rises out of the desert like some sort of a mirage out of a fairy tale.\"\n\nThe ancient city, which\u00a0flourished in the 1st and 2nd centuries, has numerous historic\u00a0structures. Among them: a 3,600-foot long colonnade, an agora (or marketplace), an amphitheater, an urban quarter, a series of tombs, a hilltop castle\u00a0and the Temple of\u00a0Bel\u00a0(also spelled Ba'al), an important, cross-cultural religious site pagan site.\n\nIn addition to executing 17 people in the area, according to some accounts \u2014 including, quite possibly, children\u00a0\u2014 the fear now is that ISIS, which has wreaked destruction on other important ruins sites,\u00a0will now get around to\u00a0tumbling\u00a0a crown jewel.\n\nPalmyra is a\u00a0monument to antiquity. Here is what made it \u2014 and what still makes it \u2014 important:\n\n1. It is among the\u00a0world's oldest settlements\n\n \"We as Americans can't appreciate the history that is there,\" says James Gelvin, a professor of Middle Eastern history at UCLA, who last visited Palmyra in the 1990s. \"When we talk about the places that vie to the be oldest inhabited place on Earth, Palmyra is the sort of location you think about.\"\n\nThough the city is often associated with the Roman Empire, under which it attained prominence, its history extends well beyond that. In fact, the city is mentioned in tablets that date as far back as the 19th century BCE. It grew in importance as\u00a0a\u00a0caravan stop in the third century BCE, an\u00a0important desert pit-stop between the Mediterranean and the Euphrates River in Iraq.\n\n\"When you approach Palmyra, it's really about the journey through the desert and this green oasis emerges and you find yourself in this amazing place,\" says Andrew Smith, author of \"Roman Palmyra: Identity, Community, and State Formation,\" as well as a\u00a0professor\u00a0of Classics at George Washington University.\u00a0\"In the first century, [the Roman author and naturalist] Pliny the Elder described it as an island in a sea of sand.\"\n\n2. It is one of the most exquisite intact ancient ruins sites in the world\n\n \"Visually and architecturally, there are very few sites in the Roman world that have this much architecture in tact,\" says Mulder, who lived in Syria for 12 years. \"Palmyra has been out there in the middle of the desert and hasn't been subjected to intense urbanization. It puts Rome to shame ... That's what makes it so amazing. You can essentially walk into a 2,000-year-old city.\"\n\n\"The Romans really built on a scale in the\u00a0Eastern provinces that was unprecedented anywhere in the Empire,\" she adds. \"In archeology, there is only one other place that takes my\u00a0breath away in the approach, and that's probably [the ancient\u00a0city of]\u00a0Baalbek, in Lebanon.\"\n\n\"How do you radiate authority when you don't have the same communications that we have today?\" he asks. \"You do that by building these massive structures. So when you arrive, you feel overwhelmed. It is just extraordinary.\"\n\nThe ruins have survived as long as they have because they have generally been respected.\n\n\"Muslims have long lived quite comfortably with sites like this for over 1,400 years,\" says Mulder. \"There's nothing inherent to Islam about the way these sites are viewed. All of this is a very modern interpretation of Islam \u2014 embracing the notion that any kind of intercessor between human beings and god is a form of idolatry. ISIS has destroyed Islamic shrines, too. It's a heritage terror or a form of genocide, erasing the past in order to create a purified ideal.\"\n\n3. Palmyra was an important hub, where\u00a0East met West\n\n \"Palmyra was a nexus of interaction,\" says Smith. \"You might see one brother wearing a Roman\u00a0toga and another wearing Persian trousers. That's what's interesting about it.\"\n\n\"It looks Greco-Roman, but if you look closely you see Persian and Semitic styles, as well as influences from India and China,\" explains Mulder. \"Some of the funerary sculptures display facial features from Indian sculpture.\"\n\nIn fact, one of its most remarkable structures \u2014 the Temple of Bel \u2014 is emblematic\u00a0of the ways in which different cultures worked together. During its apex, 2,000 years ago, previously nomadic ethnicities settled in the city, and all of these contributed to the construction of monuments.\n\n\"They each contributed what they could to these buildings,\" Smith says. \"And we know this by the inscriptions. All over the place, you see inscribed something along the lines of 'here's the wealthy patron who contributed this column.' The city is covered in these inscriptions. And what they show is different people coming together to build.\"\n\n\"Syria has always been a multicultural, multifaith, multiethnic place,\" adds Mulder, \"and people have found a way to live together. Over its history, it's had Christians, Muslims, Jews and so many others that have co-existed over so many millennia. That's the history that Isis would like to eliminate with its history of purified Islam.\"\n\n4. It was an important pagan site"},
{"url": "http://www.techworld.com/news/security/androids-reset-function-fails-delete-data-from-samsung-htc-smartphones-3612861/", "link_title": "Android's reset function fails to delete data from Samsung and HTC smartphones", "sentiment": -0.015555555555555555, "text": "The built-in factory reset on Android smartphones is so poorly implemented on popular models it should never be relied on to erase sensitive data, the Asset Disposal & Information Security Alliance (ADISA) has warned after performing a forensic analysis of a range of current end-of-life handsets.\n\nAuthored by ADISA founder Steve Mellings and Professor Andrew Blyth of the University of South Wales, the pair lined up 24 smartphones representative of those commonly disposed of in the channel, made up of 11 BlackBerrys, seven from Apple\u2019s iPhone range and six Androids models from Samsung and HTC.\n\nThese included from the BlackBerry Bold 9900, BlackBerry Torch 9810 and BlackBerry Curve 9320, iPhone 3GS, iPhone 4, and iPhone 5, plus Samsung\u2019s Android-based i8160, i890, i9100 Galaxy, and HTC\u2019s Android Wildfire S and Cha Cha.\n\nThe reset function in the BlackBerrys and iPhones performed well and the researchers found it impossible to recover any data. However, the Android smartphones offered a very different story, with even multiple resets under test conditions unable to properly erase multiple categories of test data contained on them such as phone contacts, SMS messages, Calendar events, call logs, images, videos, and even apps and data.\n\nThe methodology was a simple matter of placing the same data set on each handset, executing a reset and then attempting to recover data using a common forensic tool under test conditions.\n\nIt\u2019s not clear why the Android smartphones retained so much data but ADISA\u2019s researchers suggest that the makers are simply one stage removed from the underlying operating system and don\u2019t implement all the hardware features correctly. There is no indication that the results would be different on more recent handets from other vendors.\n\n\u201cIt can be speculated that the poor performance of the Android devices is that as Android runs on multiple hardware platforms, the developers are unable to integrate their software platform into the hardware such that specific hardware features cannot be utilised,\u201d the researchers said in a white paper covering the teams findings.\n\n\u201c[By contrast] both BlackBerry and Apple control both the hardware and software platforms for their devices and as such, the software platform is will be integrated into the hardware platform so as to support specific functions.\u201d\n\nIs this Google\u2019s fault for implementing reset security without communicating its subtleties to vendors or the vendors making Android smartphones without testing this function themselves?\n\nADISA\u2019s recommendations for asset managers is that low volume Android smartphone disposal/wiping be carried out using third-party tools or services although the organisation notes that deciding which to use is not easy given that lack of independent testing.\n\nHigher volume disposal would require the use of specialist companies to avoid falling foul of regulatory requirements.\n\n\u201cThe research findings highlight some of the wider implications associated with asset management and asset disposal. An asset\u2019s end-of-life process is both technically and operationally complex, and businesses want to know with confidence that their sensitive data will not turn up unexpectedly in the possession of an unauthorised party,\" ADISA's Steve Mellings told Techworld by email.\n\n\"As the white paper outlines, smartphones can present a significant risk if carelessly disposed of. Businesses should consider applying the best practice recommendations explored in the paper, especially as changes in the workplace continue to drive mobile adoption across the business landscape.\u201d"},
{"url": "http://www.washingtonpost.com/news/morning-mix/wp/2015/05/21/could-a-trip-to-mars-fry-astronauts-brains/", "link_title": "Could a trip to Mars fry astronauts\u2019 brains?", "sentiment": 0.11799715909090908, "text": "Brain damage. Memory deterioration. Intelligence loss.\u00a0This could\u00a0be your brain on a trip to Mars.\n\nIt\u2019s all thanks to the onslaught of\u00a0galactic cosmic rays, the remnants of supernova explosions. These highly charged particles travel near the speed of light and\u00a0can zoom\u00a0through the hull of a spacecraft and human skin without a problem. Even though astronauts aren\u2019t exposed to\u00a0the\u00a0rays\u00a0at levels high enough to kill them, consistent bombardment\u00a0with\u00a0low levels of radiation can cause enough brain damage to impair their cognitive function. Astronauts would find themselves suddenly forgetful or unable to focus on a problem. Which, in the context of a complex\u00a0and difficult\u00a0space mission, has the potential to\u00a0be deadly in\u00a0its own way.\n\nThe finding, published in the open-source journal Science Advances this month, could\u00a0throw a wrench in the many ambitious missions aimed at sending humans to Earth\u2019s nearest neighboring planet: the controversial \u201cone-way trip\u201d proposed by Dutch nonprofit Mars One\u00a0and\u00a0NASA\u2019s \u201cJourney to Mars\u201d program.\n\n[Would you leave your family behind to be the first human to set foot on Mars?]\n\n\u201cThe exquisite susceptibility of neuronal architecture\u201d \u2014 in other words, brain structure \u2014 \u201cto the effects of charged particles reported here has important implications for human exploration in space,\u201d the authors wrote.\n\nThe first part of the study took place at the NASA Space Radiation Laboratory in New York, where the mice were exposed\u00a0to the kind of radiation that astronauts experience in space. Once back in their own labs, the researchers\u00a0then ran the mice through a series of cognitive tests to see\u00a0how they responded to new situations.\n\n[That one way flight to Mars could keep you waiting for decades]\n\nThe results were not encouraging. The exposed mice were much less able to\u00a0cope with new experiences or differentiate between new and familiar objects, suggesting that the rays had damaged\u00a0the parts of their brains that deal with memory and learning. When they took a look at the mice\u2019s brains, they found that\u00a0branches of cells in their brain had weakened and become less dense \u2014 a sign that those cells were less able to transmit signals.\n\nPossible brain impairment is hardly the only health issue astronauts\u00a0face. NASA is still working out ways to combat the effects of low gravity, which causes\u00a0muscles to weaken and bones to decay. Not to mention the inevitable impact of sleeplessness, cramped quarters and freeze-dried food.\n\nBut a\u00a0decline in brain function\u00a0is especially problematic for\u00a0anyone traveling to Mars, since the 100 million or so miles a message must travel to reach Earth causes a 20-minute delay\u00a0in communication. Astronauts need to be able to figure things out quickly and on their own, the researchers argue \u2014 they can\u2019t afford to suddenly forget how to deal with new experiences, as the mice did.\n\nThis problem is hard to handle, too, because any attempt at shielding the\u00a0spacecraft from cosmic rays would make\u00a0it bulkier and heavier, which would in turn require more funds and fuel. Instead, the study\u2019s authors argue, NASA needs to figure out ways to counteract the effects of exposure. For example, a drug treatment that reverses radiation\u2019s effects, co-author Charles Limoli, a neuroscientist at the University of California at Irvine, suggested to the Los Angeles Times.\n\nBut aerospace engineer\u00a0Robert Zubrin, president of the Mars Society and an adviser to Mars One, says that the study\u2019s findings have\u00a0been blown way out of proportion. Rather than administer small amounts of radiation slowly over the course of many months, the researchers in the mouse study bombarded their subjects with 30 months\u2019 worth of radiation in the course of about 30 seconds.\n\nAnd although Limoli compared\u00a0the mice\u2019s exposure level\u00a0to what an astronaut might experience during a 10- to 30-day mission when speaking with Pacific\u00a0Standard, Zubrin said that the cumulative dose the mice received is 50 percent above what Mars astronauts would endure during\u00a0a\u00a02\u00bd-year\u00a0mission.\n\n\u201cThe irradiation doses inflicted on the researchers\u2019 unfortunate subjects has no relationship to what would be experienced by astronauts on their way to Mars,\u201d he wrote on the Mars Society Web site, adding\u00a0\u201cgalactic cosmic radiation is not a show stopper for human Mars exploration, and should not be used as an excuse for delay.\u201d\n\nTo be fair,\u00a0Limoli\u00a0isn\u2019t calling his\u00a0finding a \u201cshow stopper\u201d either, he told Science magazine. It\u2019s just \u201csomething NASA needs to consider,\u201d he said."},
{"url": "http://www.cdrf.co/", "link_title": "Classy Django REST Framework \u2013 Alternative Documentation", "sentiment": 0.12222222222222223, "text": "Django REST framework is a powerful and flexible toolkit that makes it easy to build Web APIs. It provides class based generic API views and serializers. We've taken all the attributes and methods that every view/serializer defines or inherits, and flattened all that information onto one comprehensive page per class. This project is heavily based on Classy Class-Based Views and was developed by Vinta Software Studio."},
{"url": "http://www.businessinsider.com/a-temple-university-professor-faces-80-years-in-jail-for-allegedly-planning-to-pass-tech-secrets-to-china-2015-5", "link_title": "Temple Univ Prof faces 80 yrs for charges that he passed tech secrets to China", "sentiment": 0.08863636363636364, "text": "\n\nThe chairman of Temple University's physics department schemed to provide U.S. technology secrets to China in exchange for prestigious appointments for himself, federal authorities said in charging him with four counts of wire fraud.\n\nXi Xiaoxing, a naturalized U.S. citizen who was born in China, appeared in federal court Thursday in Philadelphia and was released on $100,000 bond. A person answering the phone Friday at his home in Penn Valley said he wasn't available to comment.\n\nHe faces up to 80 years in prison and a $1 million fine if convicted.\n\nProsecutors said the 47-year-old Xi had participated in a Chinese government program involving technology innovation before he took a sabbatical in 2002 to work with a U.S. company that developed a thin-film superconducting device containing magnesium diboride.\n\nSuperconductivity is the ability to conduct electricity without resistance. A superconducting thin film could be key to making computer circuits that work faster. Films of magnesium diboride are particularly promising for this use, and Xi helped developed a way to make them.\n\nThe name of the U.S. firm where Xi worked isn't included in the indictment.\n\nMichele Mucellin, a spokeswoman for the U.S. attorney's office in Philadelphia, said she couldn't comment on what positions prosecutors say Xi sought out, whether he received them or what exactly the device is.\n\nRay Betzner, a spokesman for Temple University in Philadelphia, said Friday that Xi was being replaced as chairman of the physics department.\n\n\"In light of Dr. Xi's needs to focus on the matter at hand, an acting chair has been appointed to the Physics Department,\" he said in a statement. \"Dr. Xi remains a member of the faculty.\"\n\nBetzner said earlier that the university was aware of the charges and looked forward to talking to Xi about them.\n\nXi was awarded a grant in 2004 from the U.S. Department of Defense to purchase the device to use for research, but prosecutors say he then \"exploited it for the benefit of third parties in China, including government entities,\" by sharing it with the help of his post-doctoral students from China.\n\nXi also offered to build a world-class thin film laboratory there, according to emails detailed by prosecutors.\n\nNo one else has been charged in the case.\n\nXi joined Temple in 2009 and previously was a professor at Penn State University, according to his online faculty profile. He received his doctorate in physics from China's Peking University in 1987.\n\nThe charges come two days after three Chinese citizens who earned advanced degrees from the University of Southern California and three others were charged in San Francisco with stealing wireless technology from a pair of U.S. companies. They were charged with economic espionage and theft of trade secrets, offenses that Xi was not charged with.\n\nMucellin said the two cases aren't connected."},
{"url": "http://adzerk.com/blog/2015/05/boot-2.0.0-released/", "link_title": "Boot 2.0.0 Released [Clojure]", "sentiment": 0.17602129671095187, "text": "Hey everybody, we just released Boot 2.0.0!\n\nBoot is a build tool for Clojure that we use extensively here at Adzerk for everything from command-line scripts to web applications. For a great overview of Boot and why it's cool, check out Daniel Higginbotham's blog post, Boot, the Fancy New Clojure Build Framework.\n\nSince RC1 back in December we've released 13 more release candidates, and are grateful for the feedback and assistance on them all we received from the thriving community of Boot users and contributors.\n\nWe're really confident in the utility and stability of Boot, and we encourage you to give 2.0.0 a shot.\n\nWe didn't anticipate renaming any of Boot's public functions post-RC1, but Dom Kiva-Meyer convinced us that the functions related to Filesets API really needed to be reconsidered. Thanks Dom, you were totally right!\n\nSo, with Dom's feedback, we came up with a better set of more consistently-named fileset functions and added them to . The old functions will remain forever \u2014 and tasks using them will continue to work \u2014 but they are marked and their use will print a warning message.\n\nThe number of Community Tasks has roughly doubled since RC1 and there are now over 30 community tasks listed. Go us!\n\nThe CLJSJS project has also really taken off since RC1. CLJSJS is an effort to package various JavaScript libraries for ClojureScript use, and as of today is home to 45 -based projects. Each project in CLJSJS uses the boot-cljsjs task to cut down on boilerplate and perform common tasks.\n\nBoot 2.0.0 is only a reality because of the active participation of a huge number of people over the past couple years. Beyond the 28 contributors recorded by git, many more people have contributed bug reports, documentation, and blog posts. To all who have contributed to or otherwise helped with Boot: thank you very, very much!"},
{"url": "https://github.com/swapagarwal/SaveURLtoDropbox", "link_title": "Show HN: Download files from the web, directly in your Dropbox, from any device", "sentiment": 0.05, "text": "Saving files in Dropbox just got easier!\n\nDownload files from the web, directly in your Dropbox, from any device.\n\nEnter the URL of the file, and it will be instantly transferred from their host to your Dropbox."},
{"url": "http://www.theguardian.com/technology/2015/may/22/malware-viruses-companies-preinstall", "link_title": "Malware is not only about viruses \u2013 companies preinstall it all the time", "sentiment": 0.04033882783882784, "text": "In 1983, when I started the free software movement, malware was so rare that each case was shocking and scandalous. Now it\u2019s normal.\n\nTo be sure, I am not talking about viruses. Malware is the name for a program designed to mistreat its users. Viruses typically are malicious, but software products and software preinstalled in products can also be malicious \u2013 and often are, when not free/libre.\n\nIn 1983, the software field had become dominated by proprietary (ie nonfree) programs, and users were forbidden to change or redistribute them. I developed the GNU operating system, which is often called Linux, to escape and end that injustice. But proprietary developers in the 1980s still had some ethical standards: they sincerely tried to make programs serve their users, even while denying users control over how they would be served.\n\nHow far things have sunk. Developers today shamelessly mistreat users; when caught, they claim that fine print in EULAs (end user licence agreements) makes it ethical. (That might, at most, make it lawful, which is different.) So many cases of proprietary malware have been reported, that we must consider any proprietary program suspect and dangerous. In the 21st century, proprietary software is computing for suckers.\n\nWhat sorts of wrongs are found in malware? Some programs are designed to snoop on the user. Some are designed to shackle users, such as Digital Rights Management (DRM). Some have back doors for doing remote mischief. Some even impose censorship. Some developers explicitly sabotage their users.\n\nWhat kinds of programs constitute malware? Operating systems, first of all. Windows snoops on users, shackles users and, on mobiles, censors apps; it also has a universal back door that allows Microsoft to remotely impose software changes. Microsoft sabotages Windows users by showing security holes to the NSA before fixing them.\n\nApple systems are malware too: MacOS snoops and shackles; iOS snoops, shackles, censors apps and has a back door. Even Android contains malware in a nonfree component: a back door for remote forcible installation or deinstallation of any app.\n\nWhat about nonfree apps? Plenty of malware there. Even humble flashlight apps for phones were found to be reporting data to companies. A recent study found that QR code scanner apps also snoop.\n\nApps for streaming services tend to be the worst, since they are designed to shackle users against saving a copy of the data that they receive, as well as making users identify themselves so their viewing and listening habits can be tracked.\n\nWhat about other digital products? We know about the smart TV and the Barbie doll that transmit conversations remotely. Proprietary software in cars that stops those we used to call \u201ccar owners\u201d from fixing \u201ctheir\u201d cars. If the car itself does not report everywhere you drive, an insurance company may charge you extra to go without a separate tracker. Meanwhile, some GPS navigators save up where you have gone in order to report back when connected to update the maps.\n\nAmazon\u2019s Kindle e-reader reports what page of what book is being read, plus all notes and underlining the user enters; it shackles the user against sharing or even freely giving away or lending the book, and has an Orwellian back door for erasing books.\n\nShould you trust an internet of proprietary software things?\n\nDon\u2019t be an ass.\n\nThe companies that sell malware are skilled at spinning the malfunctionalities as services to the consumer but they could offer most of these services with freedom and anonymity if they wanted to.\n\nIt is fashionable to recognise the viciousness of today\u2019s computing only to declare resistance unthinkable. Many claim that no one could resist gratification for mere freedom and privacy. But it\u2019s not as hard as they say. We can resist:\n\nIndividually, by rejecting proprietary software and web services\n\n that snoop or track.\n\nCollectively, by organising to develop free/libre replacement systems and web services that don\u2019t track who uses them.\n\nDemocratically, by legislation to criminalise various sorts of malware practices. This presupposes democracy, and democracy requires defeating treaties such as the TPP and TTIP that give companies the power to suppress democracy.\n\n"},
{"url": "http://www.ni.com/white-paper/3782/en/", "link_title": "PID Theory Explained (2011)", "sentiment": 0.09410217881292261, "text": "Proportional-Integral-Derivative (PID) control is the most common control algorithm used in industry and has been universally accepted in industrial control. The popularity of PID controllers can be attributed partly to their robust performance in a wide range of operating conditions and partly to their functional simplicity, which allows engineers to operate them in a simple, straightforward manner. \n\n \n\n As the name suggests, PID algorithm consists of three basic coefficients; proportional, integral and derivative which are varied to get optimal response. Closed loop systems, the theory of classical PID and the effects of tuning a closed loop control system are discussed in this paper. The PID toolset in LabVIEW and the ease of use of these VIs is also discussed. \n\n \n\n\n\nThe basic idea behind a PID controller is to read a sensor, then compute the desired actuator output by calculating proportional, integral, and derivative responses and summing those three components to compute the output. Before we start to define the parameters of a PID controller, we shall see what a closed loop system is and some of the terminologies associated with it.\n\n \n\n Closed Loop System\n\n In a typical control system, the process variable is the system parameter that needs to be controlled, such as temperature (\u00baC), pressure (psi), or flow rate (liters/minute). A sensor is used to measure the process variable and provide feedback to the control system. The set point is the desired or command value for the process variable, such as 100 degrees Celsius in the case of a temperature control system. At any given moment, the difference between the process variable and the set point is used by the control system algorithm (compensator), to determine the desired actuator output to drive the system (plant). For instance, if the measured temperature process variable is 100 \u00baC and the desired temperature set point is 120 \u00baC, then the actuator output specified by the control algorithm might be to drive a heater. Driving an actuator to turn on a heater causes the system to become warmer, and results in an increase in the temperature process variable. This is called a closed loop control system, because the process of reading sensors to provide constant feedback and calculating the desired actuator output is repeated continuously and at a fixed loop rate as illustrated in figure 1.\n\n \n\n In many cases, the actuator output is not the only signal that has an effect on the system. For instance, in a temperature chamber there might be a source of cool air that sometimes blows into the chamber and disturbs the temperature.Such a term is referred to as disturbance. We usually try to design the control system to minimize the effect of disturbances on the process variable.\n\n \n\n \n\n\n\n\n\n Defintion of Terminlogies\n\n The control design process begins by defining the performance requirements. Control system performance is often measured by applying a step function as the set point command variable, and then measuring the response of the process variable. Commonly, the response is quantified by measuring defined waveform characteristics. Rise Time is the amount of time the system takes to go from 10% to 90% of the steady-state, or final, value. Percent Overshoot is the amount that the process variable overshoots the final value, expressed as a percentage of the final value. Settling time is the time required for the process variable to settle to within a certain percentage (commonly 5%) of the final value. Steady-State Error is the final difference between the process variable and set point. Note that the exact definition of these quantities will vary in industry and academia.\n\n \n\n\n\n\n\n After using one or all of these quantities to define the performance requirements for a control system, it is useful to define the worst case conditions in which the control system will be expected to meet these design requirements. Often times, there is a disturbance in the system that affects the process variable or the measurement of the process variable. It is important to design a control system that performs satisfactorily during worst case conditions. The measure of how well the control system is able to overcome the effects of disturbances is referred to as the disturbance rejection of the control system.\n\n \n\n In some cases, the response of the system to a given control output may change over time or in relation to some variable. A nonlinear system is a system in which the control parameters that produce a desired response at one operating point might not produce a satisfactory response at another operating point. For instance, a chamber partially filled with fluid will exhibit a much faster response to heater output when nearly empty than it will when nearly full of fluid. The measure of how well the control system will tolerate disturbances and nonlinearities is referred to as the robustness of the control system.\n\n \n\n Some systems exhibit an undesirable behavior called deadtime. Deadtime is a delay between when a process variable changes, and when that change can be observed. For instance, if a temperature sensor is placed far away from a cold water fluid inlet valve, it will not measure a change in temperature immediately if the valve is opened or closed. Deadtime can also be caused by a system or output actuator that is slow to respond to the control command, for instance, a valve that is slow to open or close. A common source of deadtime in chemical plants is the delay caused by the flow of fluid through pipes.\n\n \n\n Loop cycle is also an important parameter of a closed loop system. The interval of time between calls to a control algorithm is the loop cycle time. Systems that change quickly or have complex behavior require faster control loop rates.\n\n \n\n \n\n\n\n\n\n Once the performance requirements have been specified, it is time to examine the system and select an appropriate control scheme. In the vast majority of applications, a PID control will provide the required results\n\n\n\n \n\n Proportional Response\n\n The proportional component depends only on the difference between the set point and the process variable. This difference is referred to as the Error term. The proportional gain (K ) determines the ratio of output response to the error signal. For instance, if the error term has a magnitude of 10, a proportional gain of 5 would produce a proportional response of 50. In general, increasing the proportional gain will increase the speed of the control system response. However, if the proportional gain is too large, the process variable will begin to oscillate. If K is increased further, the oscillations will become larger and the system will become unstable and may even oscillate out of control.\n\n\n\n\n\n \n\n Integral Response\n\n The integral component sums the error term over time. The result is that even a small error term will cause the integral component to increase slowly. The integral response will continually increase over time unless the error is zero, so the effect is to drive the Steady-State error to zero. Steady-State error is the final difference between the process variable and set point. A phenomenon called integral windup results when integral action saturates a controller without the controller driving the error signal toward zero.\n\n \n\n Derivative Response\n\n The derivative component causes the output to decrease if the process variable is increasing rapidly. The derivative response is proportional to the rate of change of the process variable. Increasing the derivative time (T ) parameter will cause the control system to react more strongly to changes in the error term and will increase the speed of the overall control system response. Most practical control systems use very small derivative time (T ), because the Derivative Response is highly sensitive to noise in the process variable signal. If the sensor feedback signal is noisy or if the control loop rate is too slow, the derivative response can make the control system unstable\n\n\n\n The process of setting the optimal gains for P, I and D to get an ideal response from a control system is called tuning. There are different methods of tuning of which the \u201cguess and check\u201d method and the Ziegler Nichols method will be discussed.\n\n \n\n The gains of a PID controller can be obtained by trial and error method. Once an engineer understands the significance of each gain parameter, this method becomes relatively easy. In this method, the I and D terms are set to zero first and the proportional gain is increased until the output of the loop oscillates. As one increases the proportional gain, the system becomes faster, but care must be taken not make the system unstable. Once P has been set to obtain a desired fast response, the integral term is increased to stop the oscillations. The integral term reduces the steady state error, but increases overshoot. Some amount of overshoot is always necessary for a fast system so that it could respond to changes immediately. The integral term is tweaked to achieve a minimal steady state error. Once the P and I have been set to get the desired fast control system with minimal steady state error, the derivative term is increased until the loop is acceptably quick to its set point. Increasing derivative term decreases overshoot and yields higher gain with stability but would cause the system to be highly sensitive to noise. Often times, engineers need to tradeoff one characteristic of a control system for another to better meet their requirements.\n\n \n\n The Ziegler-Nichols method is another popular method of tuning a PID controller. It is very similar to the trial and error method wherein I and D are set to zero and P is increased until the loop starts to oscillate. Once oscillation starts, the critical gain K and the period of oscillations P are noted. The P, I and D are then adjusted as per the tabular column shown below.\n\n\n\nLabVIEW PID toolset features a wide array of VIs that greatly help in the design of a PID based control system. Control output range limiting, integrator anti-windup and bumpless controller output for PID gain changes are some of the salient features of the PID VI. The PID Advanced VI includes all the features of the PID VI along with non-linear integral action, two degree of freedom control and error-squared control.\n\n \n\n\n\n\n\n PID palette also features some advanced VIs like the PID Autotuning VI and the PID Gain Schedule VI. The PID Autotuning VI helps in refining the PID parameters of a control system. Once an educated guess about the values of P, I and D have been made, the PID Autotuning VI helps in refining the PID parameters to obtain better response from the control system.\n\n\n\n\n\n The reliability of the controls system is greatly improved by using the LabVIEW Real Time module running on a real time target. National Instruments provides the new M Series Data Acquisition boards which provide higher accuracy and better performance than an average control system.\n\n \n\n\n\n\n\n The tight integration of these M Series boards with LabVIEW minimizes the development time involved and greatly increases the productivity of any engineer. Figure 7 shows a typical VI in LabVIEW showing PID control using NI-DAQmx API of M series devices.\n\n\n\n The PID control algorithm is a robust and simple algorithm that is widely used in the industry. The algorithm has sufficient flexibility to yield excellent results in a wide variety of applications and has been one of the main reasons for the continued use over the years. NI LabVIEW and NI plug-in data acquisition devices offer higher accuracy and better performance to make an excellent PID control system."},
{"url": "http://www.buzzfeed.com/lipmanb/10-awesome-pitch-decks-from-the-worldas-fastest-17bhn", "link_title": "10 Awesome Pitch Decks from the World\u2019s Fastest Growing Startups", "sentiment": 0.16928374655647382, "text": "BuzzFeed is arguably becoming the modern day magazine. Anyone can write listicles much like this and publish them to the site. The short and visual format shows just how short our attention spans are getting. This is BuzzFeed\u2019s investor deck from 2008.\n\nAirbnb is quickly becoming the go to site for travelers. A recent study looked at the economic impact of Airbnb on cities all over the world. 550,000 homes are shared by hosts in cities all over the world. 47% of hosts say that hosting helped them stay in their homes.\n\nFoursquare, the social location check-in app founded in 2009 has gone through many pivots. Most recently they released Swarm, a new app which focuses more on meeting with friends than checking into different locations. According to AngelList they have raised a total of $162.35M. Many argue that Foursquare\u2019s many pivots shows lack of growth. According to Foursquare\u2019s website, they have over 2 million businesses on the platform and a total of 7 billion check ins. The following is the first pitch deck that Foursquare used in 2009.\n\nBuffer, the social media management app allows users to schedule posts across social media sites and track engagement metrics. They are also known for their radical company transparency. Buffer releases profit margins employee salaries and monthly investor updates. This is the deck they used to raise a $500k seed round.\n\nLinkedIn is almost synonymous with career. Business cards are becoming an after thought at networking events. Now you\u2019ll hear, \u201cLet\u2019s connect on LinkedIn\u201d. LinkedIn\u2019s CEO, Jeff Warner, was recently interviewed by Jason Calacanis at the LAUNCH Festival where he discussed the vision for the company. Link to Interview. Now LinkedIn is a publicly traded company. Recently they acquired the online training company Lynda.com for a reported $1.5 billion. Here\u2019s a link to LinkedIn\u2019s full slide deck they used when raising Series B.\n\nSEOmoz now just MOZ is the trusted source for all things SEO. Their simple to use software helps businesses of all sizes improve how they are found by search engines.\n\nThis one is definitely one of my favorite companies. They launched at TechCrunch50 in 2008 which is comparable to TechCrunch Disrupt."},
{"url": "http://blog.differential.com/this-week-in-meteor-15/", "link_title": "This Week in Meteor #15", "sentiment": 0.22064285714285709, "text": "Welcome to issue #15 of TWiM!\n\nIf you would like updates like this emailed to you, subscribe at thisweekinmeteor.com\n\nThis is a fantastic announcement! MDG now has $20M more to build out Galaxy and the Meteor platform! It mentions that part of the funds will be to create official solutions for integrating other popular front end frameworks like Angular and React. I think that is a great move and will definitely help push more people to adopt Meteor as their platform. This makes a total of $31.2M in funding for Meteor so far and there is a very bright future ahead.\n\nThis is a great blog post that explains a bit about Galaxy (MDG's paid hosting solution) and how it will work with containers and, more specifically, Google's Kubernetes. It gives a great overview of Galaxy and I recommend reading it.\n\nFirst off, congratulations to Uri Goldshtein for joining MDG. He has been working hard on integrating Angular and Meteor with his Angular-Meteor package. So, he goes through and explains in this blog post as to why it is a better decision to use Meteor as your backend for a typical MEAN stack application. For those of you that don't know, MEAN stands for Mongo, Express, Angular, and Node. You can can use Meteor to make up for 3 out of the 4 (all but Angular). And this integration with Angular fills that gap and allows you to build MEAN applications in significantly less time and code. To quote the blog post directly:\n\nThat is a significant amount of code that is not needed to write. Thinkster, for those of you that don't know, is a very popular educational site that gained a lot of popularity for its Angular courses.\n\nThe library was updated to fix a bug in iOS safari. Just something to be aware about for those building mobile apps.\n\nMeteor core has quite a bit of Cordova-based updates coming in. Important highlights:\n\nThere are more updates, so if you are interested, read the commit.\n\nThis is a branch created by Sashko Stubailo to deprecate the use of . The use of the attribute in Meteor applications has actually been a bit of a problem. It has various security implications and other issues that developers should take into serious consideration. There is a hackpad proposal for this very issue. I recommend taking a look at it if you haven't already.\n\nThis is a fantastic blog post by our very own David Woody about scaling a Meteor app that was worked on recently. It goes into detail about various tips to help you scale and things to look out for. I highly recommend reading it and sending it to any developers asking you about scaling a Meteor application.\n\nThis package, which I believe is called , is a slightly different approach to an user accounts system than the packages. It is a bit more minimal and doesn't do much styling for you. It is another package to consider especially when you are choosing which user accounts wrapper to choose.\n\nUsually, I don't post personal products on TWiM, but this is an edge case. The author of the app does a great job of explaining why Meteor was a great choice for him and various tips for people building mobile apps in Meteor. I highly recommend reading his long description on Crater. It is basically a mini blog post within the submission.\n\nFirst, there was Telescope, now there are more open source Meteor-based projects spawning. This application is meant to be kind of like an open source alternative to Slack. I'm excited to see the development of this project further.\n\nThis is a package that helps you easily add in a form to send invites for your Slack team. All you have to do is add in your Slack Token and then insert the template to your template.\n\nThis is an awesome tutorial by David Weldon on how you can use Meteor's API to build out a model layer and add methods to your collections. I highly recommend reading it as it can really help you with building modular Meteor apps.\n\nThis package makes it easier to manage private packages. Currently, that is a bit of a problem in Meteor, but it isn't that big of an issue (in my opinion). This package allows you to point to a private GitHub tarball or even local packages. You should note that it wants you to use the NPM package instead to manage your private packages though due to issues in the build process.\n\nThis is another great open source Meteor app. It is a chat system and it has some interesting features like the Giphy integration and a meme generator.\n\nThis is a very short and simple tutorial on getting developers started on some of the Meteor basics. It gives code examples and shows benefits for using Meteor.\n\nThis is a very interesting discussion on spacebars, Meteor, and also Evan You talks about his branch that integrates spacebars with React. It is a very long thread, but definitely worth the read. I recommend that everyone should read this.\n\nThis is a special section this week to highlight where Meteor has been featured due to it's big $20M announcement.\n\nThat's all for this week! Tune in next week for more updates in the Meteor universe.\n\nIf you would like updates like this emailed to you, subscribe at thisweekinmeteor.com"},
{"url": "http://aboutshadow.com/", "link_title": "An entire privacy-based platform which aims to establish an anonymous economy", "sentiment": -0.03333333333333333, "text": "ShadowMarket is a Anonymous P2P Marketplace that enables individuals to buy and sell items, trade currency and communicate leaving only a Shadow.\n\nUpon its completion, the marketplace will provide cash liquidity, private commerce and end-to-end decentralized stability to the Shadow Economy.\n\nThe Shadow team is still actively developing the ShadowMarket core infrastructure\u2014ShadowMarket is scheduled for release in the second quarter of 2015."},
{"url": "http://danluu.com/datacenter-power", "link_title": "Nowadays we build datacentes near cheap sources of power instead of steel mills", "sentiment": 0.15037136066547832, "text": "Why are people so concerned with hardware power consumption nowadays? Some common answers to this question are that power is critically important for phones, tablets, and laptops and that we can put more silicon on a modern chip than we can effectively use. In 2001 Patrick Gelsinger observed that if scaling continued at then-current rates, chips would have the power density of a nuclear reactor by 2005, a rocket nozzle by 2010, and the surface of the sun by 2015. Needless to say, that didn\u2019t happen. The importance of portables and scaling limits are both valid and important reasons, but since they\u2019re widely discussed, I\u2019m going to talk about an underrated reason.\n\nPeople often focus on the portable market because it\u2019s cannibalizing desktop market, but that\u2019s not the only growth market \u2013 servers are also becoming more important than desktops, and power is really important for servers. To see why power is important for servers, let\u2019s look at some calculations about how what it costs to run a datacenter from Hennessy & Patterson.\n\nOne of the issues is that you pay for power multiple times. Some power is lost at the substation, although we might not have to pay for that directly. Then we lose more storing energy in a UPS. This figure below states 6%, but smaller scale datacenters can easily lose twice that. After that, we lose more power stepping down the power to a voltage that a server can accept. That\u2019s over a 10% loss for a setup that\u2019s pretty efficient.\n\nAfter that, we lose more power in the server\u2019s power supply, stepping down the voltage to levels that are useful inside a computer, which is often about another 10% loss (not pictured in the figure below).\n\nAnd then once we get the power into servers, it gets turned into waste heat. To keep the servers from melting, we have to pay for power to cool them. Barroso and Holzle estimated that 30%-50% of the power drawn by a datacenter is used for chillers, and that an additional 10%-20% is for the CRAC (air circulation). That means for every watt of power used in the server, we pay for another 1-2 watts of support power.\n\nAnd to actually get all this power, we have to pay for the infrastructure required to get the power into and throughout the datacenter. Hennessy & Patterson estimate that of the $90M cost of an example datacenter (just the facilities \u2013 not the servers), 82% is associated with power and cooling. The servers in the datacenter are estimated to only cost $70M. It\u2019s not fair to compare those numbers directly since servers need to get replaced more often than datacenters; once you take into account the cost over the entire lifetime of the datacenter, the amortized cost of power and cooling comes out to be 33% of the total cost, when servers have a 3 year lifetime and infrastructure has a 10-15 year lifetime.\n\nGoing back to the Barroso and Holzle book, processors are responsible for about a third of the compute-related power draw in a datacenter (including networking), which means that just powering processors and their associated cooling and power distribution is about 11% of the total cost of operating a datacenter. By comparison, the cost of all networking equipment is 8%, and the cost of the employees that run the datacenter is 2%.\n\nComputation uses a lot of power. We used to build steel mills near cheap sources of power, but now that\u2019s where we build datacenters. As companies start considering the full cost of applications, we\u2019re going to see a lot more power optimized solutions. Unfortunately, this is really hard. On the software side, with the exceptions of toy microbechmark examples, best practices for writing power efficient code still aren\u2019t well understood. On the hardware side, Intel recently released a new generation of chips with significantly improved performance per watt that doesn\u2019t have much better absolute performance than the previous generation. On the hardware/software co-design front, Microsoft has had some luck with building custom hardware for deep learning, achieving 2x the power efficiency of GPU-based solutions. They also cite a nearly 2x TCO improvement by applying FPGAs to to bing search. But with existing tools, hardware accelerators are costly enough that dedicated hardware only makes sense for the largest companies. There isn\u2019t an easy answer to this problem.\n\nIf you liked this post, you\u2019d probably like chapter 6 of Hennessy & Patterson, which walks through not only the cost of power, but a number of related back of the envolope caclulations relating to datacenter performance and cost.\n\nApologies for the quickly scribbled down post. I jotted this down shortly before signing an NDA for an interview where I expected to learn some related information and I wanted to make sure I had my thoughts written down before there was any possibility of being contaminated with information that\u2019s under NDA."},
{"url": "http://techcrunch.com/2015/05/18/api-ai/", "link_title": "Magic: Without the People", "sentiment": 0.17659197012138184, "text": "Want to add a Siri-like \u201cconversational interface\u201d to your mobile app or device? Then api.ai, the Palo Alto-headquartered startup and Russian team behind Speaktoit Assistant, a Siri alternative for Android, iOS and Windows Phone, has had you covered for a while.\n\nBut now the company has refined its offering to make it a lot easier for developers in the Internet-of-Things (IoT) space, such as the smart home and wearables, to be able to use its technology to enhance their offerings.\n\nOriginally launched last September, api.ai effectively opened up the AI and natural language tech that powered Speaktoit so that developers could add conversational interfaces to their apps. And although 5,000 or so developers signed up to the platform, the feedback the api.ai team received informed them of two things.\n\nFirstly there was a lot of interest shown, not just by mobile app developers, but also from the IoT space, namely the smart home, and wearables, such as smart watches \u2014 areas the company had always planned on targeting.\n\nSecondly, for many developers the platform required too much work upfront; despite the huge amount of heavily lifting the company\u2019s machine learning-based tech already does, developers were craving more out-of-the-box examples they could easily plug into.\n\nAs a result, the api.ai team have gone to work to make its conversational UI a lot more context-aware by adding what the startup calls \u201cpre-defined domains\u201d, including those for various IoT categories. This means the platform knows ahead of time what domain any defined entities and intent applies to.\n\nSo for example, if a developer wanted to add voice recognition to control a smart lighting system, api.ai would already know they are working within the smart home domain and is able to tap into its existing AI library related to that domain.\n\n\u201cDevelopers can now start from something right out of the box,\u201d co-founder and CEO Ilya Gelfenbeyn tells me. \u201cThey can use domains like news, weather or smart home, and so on.\u201d\n\nDevelopers can also describe their own interactions and scenarios by simply providing a few examples based on the device\u2019s capability, and the api.ai platform will use these to seed a more fully developed conversational UI.\n\n\u201cWhat our system will do is train itself based on these examples, by finding some common semantic units, to enable it to understand further examples that were not covered by the developer,\u201d he explains.\n\nHere\u2019s an example provided by api.ai of how that might pan out in practice:\n\nPerson: It\u2019s very dark here.\n\n Smart Home: Let\u2019s turn on the light then.\n\n Person: Turn it to romantic mode.\n\n Smart Home: Ooh, I see. Here it is.\n\n Person: Still too bright.\n\n Smart Home: Taking it to the minimum.\n\n Person: Same in the kitchen.\n\n Smart Home: Lights in the kitchen are on.\n\n Person: Turn on the heating there as well.\n\n Smart Home: Thermostat is on for the kitchen only.\n\nAnd here\u2019s what a smartwatch app for Magic might look like if it were powered by api.ai:\n\nMeanwhile, the company\u2019s tech supports 13 languages including Chinese, English French, Korean, Portuguese and Spanish. On the developer side it supports an array of platforms and coding languages including iOS, Android, HTML, Cordova, Python, C#, Xamarin, and Unity with native SDKs."},
{"url": "http://realm.io/", "link_title": "Realm \u2013 a mobile database replacement for SQLite and Core Data", "sentiment": 0.43333333333333335, "text": "// but can be easily fetched from any thread // You can also write to a Realm from any thread\n\n// but can be easily fetched from any thread // You can also write to a Realm from any thread\n\n// but can be easily fetched from any thread // You can also write to a Realm from any thread,"},
{"url": "http://view.samurajdata.se/psview.php?id=6fbf619d", "link_title": "An introduction to Core Erlang (2001)", "sentiment": 0.1130952380952381, "text": "Click on page to switch between small and large images. Original document."},
{"url": "https://firstlook.org/theintercept/2015/05/22/apple-google-spy-summit-cia-gchq-ditchley-surveillance", "link_title": "Apple and Google Just Attended a Confidential Spy Summit in an English Mansion", "sentiment": 0.042927455774677985, "text": "At an 18th-century mansion in England\u2019s countryside last week, current and former spy chiefs from seven\u00a0countries faced off with representatives from tech giants Apple and Google to discuss government surveillance in the aftermath of Edward Snowden\u2019s leaks.\n\nThe three-day conference, which took place behind closed doors and under strict rules about confidentiality, was aimed at debating the line between privacy and security.\n\nAmong an extraordinary list of attendees were a host of current or former heads from spy agencies such as the CIA and British electronic surveillance agency Government Communications Headquarters, or GCHQ. Other current or former top spooks from Australia, Canada, France, Germany and Sweden were also in attendance. Google, Apple, and telecommunications company Vodafone sent some of their senior policy and legal staff to the discussions. And a handful of academics and journalists were also present.\n\nAccording to an event program obtained by The Intercept, questions on the agenda included: \u201cAre we being misled by the term \u2018mass surveillance\u2019?\u201d \u201cIs spying on allies/friends/potential adversaries inevitable if there is a perceived national security interest?\u201d \u201cWho should authorize intrusive intelligence operations such as interception?\u201d \u201cWhat should be the nature of the security relationship between intelligence agencies and private sector providers, especially when they may in any case be cooperating against cyber threats in general?\u201d And, \u201cHow much should the press disclose about intelligence activity?\u201d\n\nRichard Salgado, Google\u2019s legal director for law enforcement and information security; Verity Harding, Google\u2019s U.K. public policy manager and head of security and privacy policy; Jane Horvath, Apple\u2019s senior director of global privacy; Erik Neuenschwander, Apple\u2019s product security and privacy manager; Matthew Kirk, Vodafone Group\u2019s external affairs director; and Phillipa McCrostie, global vice chair of transaction advisory services, Ernst & Young. From the U.S.: John McLaughlin, the CIA\u2019s former acting director and deputy director; Jami Miscik, the CIA\u2019s former director of intelligence; Mona Sutphen, member of President Obama\u2019s Intelligence Advisory Board and former White House deputy chief of staff; Rachel Brand, member of the Privacy and Civil Liberties Oversight Board; George Newcombe, board of visitors, Columbia Law School; David Ignatius, Washington Post columnist and associate editor; and Sue Halpern, New York Review of Books contributor. Robert Hannigan, current chief of British surveillance agency GCHQ; Sir David Omand, former GCHQ chief; Sir Malcolm Rifkind, former head of the British parliament\u2019s Intelligence and Security Committee; Lord Butler of Brockwell, member of the Intelligence and Security Committee; Dr. Jamie Saunders, director of the National Cybercrime Unit at the National Crime Agency; Sir Mark Waller, Intelligence Services Commissioner; Peter Clarke, former head of Counter Terrorism Command at London\u2019s Metropolitan Police; Baroness Neville-Jones, House of Lords special representative to business on cyber security and member of the joint parliamentary committee on national security strategy; John Spellar, member of parliament; Duncan Campbell, investigative journalist; Gordon Corera, BBC security correspondent; and Professor Timothy Garton Ash, historian and author. Ernst Uhrlau, former head of the German federal intelligence service, the BND; Christophe Bigot, director of strategy for French surveillance agency Directorate General for External Security; Ingvar Akesson, former director general of Sweden\u2019s surveillance agency, the FRA; Gilles de Kerchove, the European Union\u2019s counterterrorism coordinator; Isabelle Falque-Pierrotin, chair of the EU\u2019s Article 29 Working Party, which deals with data protection issues; Dr Giuseppe Busia, secretary general of the Italian data protection authority; and Jacob Kohnstamm, chairman of the Dutch data protection authority. David Irvine, former chief of the Australian Security Intelligence Organisation; Richard Fadden, Canadian government national security adviser and deputy minister at the Department of National Defense, former director of the Canadian Security Intelligence Service; Kent Roach, professor of law at the University of Toronto; and Jacques Fremont, president, Quebec Human Rights and Youth Rights Commission.\n\nThe event was chaired by the former British MI6 spy chief Sir John Scarlett and organized by the Ditchley Foundation, which holds several behind-closed-doors conferences every year at its mansion in Oxfordshire (pictured above) in an effort to address \u201ccomplex issues of international concern.\u201d The discussions are held under what is called the Chatham House Rule, meaning what is said by each attendee during the meetings cannot be publicly revealed, a setup intended to encourage open and frank discussion. The program outlining the conference on surveillance told participants they could \u201cdraw afterwards on the substance of what has been said\u201d but warned them \u201cnot under any circumstances to reveal to any person not present at the conference\u201d details exposing what particular named individuals talked about.\n\nInvestigative reporter Duncan Campbell, who attended the event, told The Intercept that it was a \u201cremarkable\u201d gathering that \u201cwould have been inconceivable without Snowden,\u201d the National Security Agency whistleblower.\n\n\u201cAway from the fetid heat of political posturing and populist headlines, I heard some unexpected and surprising comments from senior intelligence voices, including that \u2018cold winds of transparency\u2019 had arrived and were here to stay,\u201d said Campbell, who has been reporting on British spy agencies over a career spanning four decades.\n\nHe added: \u201cPerhaps to many participants\u2019 surprise, there was general agreement across broad divides of opinion that Snowden \u2013 love him or hate him \u2013 had changed the landscape; and that change towards transparency, or at least \u2018translucency\u2019 and providing more information about intelligence activities affecting privacy, was both overdue and necessary.\u201d\n\nOne particularly notable attendee was GCHQ chief Hannigan, who stayed only for the first day of the discussions. Hannigan recently took over the top British eavesdropping job, and one of the first things he did in the post was to publicly accuse U.S. tech companies of being \u201ccommand-and-control networks of choice for terrorists and criminals,\u201d which is not likely to have gone down well with the likes of Google and Apple. (Neither Google nor\u00a0Apple had responded to requests for comment on this story at time of publication.)\n\nHannigan may have viewed the event as an opportunity to rein in his rhetoric and attempt to gain the trust of the tech giants. The British spy chief has said U.S. tech companies should provide \u201cgreater support\u201d to surveillance agencies and that he wants to see \u201cbetter arrangements for facilitating lawful investigation by security and law enforcement agencies than we have now.\u201d In the U.S., similar pressure has been exerted on the companies, with federal agencies pushing for greater cooperation on surveillance amid an increased adoption of encryption technology that protects the privacy of communications.\n\nIn the aftermath of Snowden revelations showing extensive Internet surveillance perpetrated by British and American spies and their allies, Google and other companies have reportedly become more resistant to government data requests. Google engineers were outraged by some of the disclosures and openly sent a \u201cfuck you\u201d to the surveillance agencies while hardening Google\u2019s security. Meanwhile, Apple has expanded the range of data that\u2019s encrypted by default on iPhones, iPads, and Mac computers, and CEO Tim Cook has vowed never to give the government access to Apple servers, stating \u201cwe all have a right to privacy.\u201d But the Ditchley event is a sign that, behind the scenes at least, a dialogue is beginning to open up between the tech giants and the spy agencies post-Snowden, and relations may be thawing."},
{"url": "https://www.sherbit.io/what-does-your-iphone-know-about-you/", "link_title": "What Does Your iPhone Know About You?", "sentiment": 0.04755892255892255, "text": "Sherbit works as a \u2018hub\u2019 for your data by utilizing your apps\u2019 \u2018application programming interfaces,\u2019 or \u201cAPIs.\u201d An API is an interface that allow multiple pieces of software to interact with each other. For example: when you try to buy something online, the website needs to verify that your credit card information is correct. The website\u00a0uses an API to send your financial data to another application that processes the payment; after that application confirms the transaction, the website notifies you that the order has been placed. The user only sees the site\u2019s\u00a0interface, but in the background, many applications are communicating with each other using APIs.\n\nApple\u2019s Core Motion and Core Location APIs provide access to data collected by the iPhone\u2019s GPS and motion sensors. The iPhone 6 uses a co-processor called \u201cM8\u201d to continuously measure data from the motion sensors without drawing power from the phone\u2019s main processor. Apple\u2019s goal is to make it easier for fitness and health applications to track your activity; whether you\u2019re walking or running, biking or driving, at work or at home, etc. Sherbit lets you see this information in a new context by analyzing it with data provided by your other apps\u2019 APIs."},
{"url": "https://codegolf.stackexchange.com/q/50521/31414", "link_title": "Shortest code to generate ASCII fractals", "sentiment": 0.1568181818181818, "text": "I came across this really nice snow-flake like ASCII fractal by Michael Naylor [citation needed but not found]. Just like a fractal, it grows exponentially in size with each order/iteration. Below are the details about the construction and a few examples for various orders:\n\nConsider the order 1 Flow Snake to be built of a path containing 7 edges and 8 vertices (labelled below. Enlarged for feasibility):\n\nNow for each next order, you simply replace the edges with a rotated version of this original order 1 pattern. Use the following 3 rules for replacing the edges:\n\n1 For a horizontal edge, replace it with the original shape as is:\n\n2 For a edge ( in the above construction), replace it with the following rotated version:\n\n3 For a edge ( and above), replace it with the following rotated version:\n\nSo for example, order 2 with vertices from order 1 labelled will look like\n\nNow for any higher order, you simply break up the current level into edges of lengths 1 , 1 or 2 and repeat the process. Do note that even after replacing, the common vertices between any two consecutive edges are still coinciding.\n\nThis is code-golf so shortest code in bytes win!\n\nThe first post of the series generates a leaderboard.\n\nTo make sure that your answers show up, please start every answer with a headline, using the following Markdown template:\n\nwhere is the size of your submission. If you improve your score, you can keep old scores in the headline, by striking them through. For instance:"},
{"url": "https://www.sherbit.io/sherbit-and-smart-thermostats/", "link_title": "Sherbit and \u201cSmart\u201d Thermostats", "sentiment": 0.11256212922879591, "text": "In the past few years, manufacturers have developed \u2018programmable\u2019 thermostats to allow users to automatically set certain temperatures for certain times of the day \u2013 for example, lowering the temperature when you know you\u2019ll be away from home. \u201cSmart\u201d thermostats, like the Nest Learning Thermostat, take this process a step further \u2013 these devices \u201clearn\u201d the temperatures you like by analyzing your thermostat use over time, and then program themselves\u00a0accordingly in order to save energy. Nest Labs (owned by Google) claims that over the past two years Learning Thermostat users have saved more than 10 percent on heating bills and 15 percent on cooling bills; enough savings for the product\u00a0to pay for itself (at $250) in less than two years. Though there are only a few million \u201csmart\u201d thermostats installed in homes today, a market research firm recently predicted that there will be 32 million of these devices in use by 2020.\n\nNest thermostats collect large amounts of data in order to determine\u00a0their optimal\u00a0programming settings\u2014the information stream from home automation devices presents a lot of lucrative opportunities for Google, though it raises a number of serious privacy and security concerns (You can stay\u00a0informed about these issues with Sherbit\u2019s PrivacyMe tool, launching later this year). On top of this \u2013 although savvy users can access a lot of this data, there aren\u2019t many software tools available that can\u00a0analyze and interpret that data outside of the first-party software provided by Nest. That\u2019s where Sherbit comes in:\n\nYou can combine your Nest thermostat with an activity tracker like Fitbit or UP (by Jawbone) to learn how your room temperature affects your quality of sleep. Jawbone\u2019s UP app has a new sleep tracking interface which detects\u00a0how much REM, deep, and light sleep a user is getting, and during what times of night. You can use Sherbit to graph these data points against your room\u2019s temperature during the hours you are asleep to gain greater insight into your sleep behavior.\n\nYou can visualize how your thermostat use varies with changes in weather by analyzing data from your iOS device\u2019s Weather app with data from your Nest thermostat. Sherbit\u2019s graphing interface allows you to compare these data points\u2014when you\u2019re cooling, when you\u2019re heating, and outside temperature during the hours you were at home\u2014so you can make better decisions about your home energy. Try using automation tools like IFFTT to put these insights into action!\n\nNest\u2019s Auto-Away feature uses a combination of sensors to detect when you leave your house and when you return home, so the thermostat can turn itself down automatically.\u00a0Nest just\u00a0uses\u00a0this information in order to make energy-saving decisions\u2014but you\u00a0can use this data\u00a0to analyze all kinds of aspects of your life. Look at different classes of contacts in your address book to see \u201cWho do I send e-mails to when I\u2019m at home?\u201d or at your phone usage time to ask \u201cDo I spend more time talking on the phone at home or at work?\u201d\n\nThe data is there, the question is: what will you do with it? Can you think of other creative ways to use your data?"},
{"url": "http://www.windowscentral.com/bill-gatess-personal-agent-project-microsoft-might-be-called-office-now", "link_title": "Bill Gates' project at Microsoft rumored to be persalsonal assistant app", "sentiment": 0.0, "text": "But Cortana is there na"},
{"url": "https://www.archlinux.org/news/data-corruption-on-software-raid-0-when-discard-is-used/", "link_title": "Data corruption on software RAID 0 when discard is used", "sentiment": 0.08425925925925926, "text": "Recent Linux kernels (4.0.2+, LTS 3.14.41+), pushed to the [core] repository in the past couple of weeks, were affected by a bug that can cause data corruption on file systems mounted with the option and residing on software RAID 0 arrays. (If you do not use software RAID 0 or the option, then this issue does not affect you.)\n\nThe issue has been addressed in the and updates. Due to the nature of the bug, however, it is likely that data corruption has already occurred on systems running the aforementioned kernels. It is strongly advised to verify the integrity of affected file systems using and/or restore their data from known good backups.\n\nFor further information please read the LKML post by Holger Kiehl, the related article on Phoronix, as well as the proposed fix that was backported to the Arch kernels."},
{"url": "http://en.wikipedia.org/wiki/Atari_video_game_burial", "link_title": "Atari video game burial", "sentiment": -0.014229902042402036, "text": "The Atari video game burial was a mass burial of unsold video game cartridges, consoles, and computers in a New Mexico landfill site, undertaken by American video game and home computer company Atari, Inc. in 1983. The goods buried were believed to be unsold copies of E.T. the Extra-Terrestrial, one of the biggest commercial failures in video gaming and often cited as one of the worst video games ever released, and the Atari 2600 port of Pac-Man, which was commercially successful but critically maligned.\n\nSince the burial was first reported in the press, there have been doubts as to its veracity and scope, leading to a minority considering it an urban legend. However, the event has become a cultural icon and a reminder of the North American video game crash of 1983; it was the end result of a disastrous fiscal year which saw Atari, Inc. sold off by its parent company. Though it was believed that millions of copies of E.T. were disposed of in the landfill, Atari officials later verified the numbers to be around 700,000 cartridges of various titles, including E.T.\n\nIn 2014, Fuel Industries, Microsoft, and others worked with the New Mexico government to excavate the site to validate the contents of the landfill as part of a documentary called \"Atari: Game Over\". On April 26, 2014, the excavation revealed discarded games and hardware. Only a small fraction, about 1300 games, were recovered during the excavation period, with a portion given for curation and the rest auctioned to raise money for a museum to commemorate the burial.\n\nAtari, Inc. had been purchased by Warner Communications in 1976 for $28 million, and had seen its net worth grow to $2 billion by 1982.[2] By this time, the company accounted for 80% of the video gaming market;[2] and was responsible for over half of its parent company's revenues,[3] earning some 65\u201370% of their operating profits.[2][3] By the last quarter of 1982, its growth in the following year was expected to be in the region of 50%.[2] However, on December 7, 1982, the company reported that its earnings had only increased by 10\u201315%, rather than the predicted figure.[2] The next day saw Warner Communications' share prices fall by a third, and the quarter ended with Warner's profits falling by 56%.[2] In addition, Atari's CEO, Ray Kassar, was later investigated for possible insider trading charges as a result of selling some five thousand shares in Warner less than half an hour before reporting Atari's lower-than-expected earnings. Kassar was later cleared of any wrongdoing, although he was forced to resign his position the following July.[4] Atari, Inc. would go on to lose $536 million in 1983, and was sold off by Warner Communications the following year.[2]\n\nAtari's tendency to port arcade games for its home console had led to some of its most commercially successful games, including the port of its own coin-op Asteroids, and the licensed versions of Taito's Space Invaders and Namco's Pac-Man. When the latter game received its official port to the Atari 2600, Atari was confident that sales figures would be high, and manufactured 12 million cartridges\u2014despite having sold only around 10 million Atari 2600 consoles.[2] It was believed that the game would be successful enough not only to earn an estimated $500 million, but also to boost sales of the console itself by several million as gamers sought to play the home conversion.[5][6] However, the finished product, released in March 1982, was critically panned for its poor gameplay,[2] and although it became the console's best-selling title after shipping 7 million units, it still left Atari with over 5 million unsold cartridges\u2014a problem compounded by the high rate of customers returning the game for refunds.[5][7]\n\nFurther to the problems caused by Pac-Man\u200d\u200a'\u200bs underwhelming sales, Atari also faced great difficulty as a result of its video game adaptation of the film E.T. the Extra-Terrestrial. The game, also titled E.T. the Extra-Terrestrial, was a result of a deal between Warner Communications and the film's director Steven Spielberg. The concept of a video game based on a film, instead of porting an arcade coin-op or building on an established franchise, was unheard of at the time.[2] It was later reported that Warner had paid $20\u201325 million for the rights, which was at the time quite a high figure for video game licensing.[5] Atari manufactured 5 million cartridges for the game;[2] however, upon its release in December 1982, only 1.5 million copies were sold, leaving Atari still holding onto over half of the game cartridges.[8] The game was critically panned, and is now seen as one of the worst ever made.[9][10] Billboard magazine's Earl Paige reported that the large number of unsold E.T the Extra-Terrestrial games, along with an increase in competition, prompted retailers to demand official return programs from video game manufacturers.[11]\n\nThe failures of these titles were further compounded by Atari's business dealings from 1981. Confident in strong sales, the company had told its distributors to place their 1982 orders all at once. However, video game sales in 1982 had slowed, and distributors who had ordered en masse in expectation of high turnover were left to simply return large quantities of unsold stock to Atari. As a result, the company soon found itself in possession of several million essentially useless video game cartridges, which it would be entirely unable to sell.[2]\n\nIn September 1983, the Alamogordo Daily News of Alamogordo, New Mexico reported in a series of articles, that between 10 and 20[12] semi-trailer truckloads of Atari boxes, cartridges, and systems from an Atari storehouse in El Paso, Texas, were crushed and buried at the landfill within the city. It was Atari's first dealings with the landfill, which was chosen because no scavenging was allowed and its garbage was crushed and buried nightly. Atari's stated reason for the burial was that it was changing from Atari 2600 to Atari 5200 games,[13] but this was later contradicted by a worker who claimed that this was not the case.[14] Atari official Bruce Enten stated that Atari was mostly sending broken and returned material to the Alamogordo dump and that it was \"by-and-large inoperable stuff.\"[12]\n\nOn September 27, 1983, the news service UPI reported that \"people watching the operation said it included cassettes of the popular video games E.T., Pac-Man, Ms. Pac-Man, the consoles used to convey the games to television screens and high-priced personal computers.\"[15] The news service Knight-Ridder further reported on the looting of the dump on September 28 by local kids, stating \"kids in this town of 25,000 began robbing the Atari grave, coming up with cartridges of such games as E.T., Raiders of the Lost Ark, Defender, and Berzerk.\"[16]\n\nOn September 28, 1983, The New York Times reported on the story of Atari's dumping in New Mexico. An Atari representative confirmed the story for the newspaper, stating that the discarded inventory came from Atari's plant in El Paso, which was being closed and converted to a recycling facility.[17] The reports noted that the site was guarded to prevent reporters and the public from affirming the contents. The Times article never suggested any of the specific game titles being destroyed, but subsequent reports have generally linked the story of the dumping to the well-known failure of E.T.[2] Additionally, the headline \"City to Atari: 'E.T.' trash go home\" in one edition of the Alamogordo News seems to imply some of the cartridges were E.T., but then follows with a humorous interpretation of E.T. meaning \"Extra-territorial\" and never specifically mentions the game.[12]\n\nStarting on September 29, 1983, a layer of concrete was poured on top of the crushed materials, a rare occurrence in waste disposal. An anonymous workman's stated reason for the concrete was: \"There are dead animals down there. We wouldn't want any children to get hurt digging in the dump.\"[14] Eventually, the city began to protest the large amount of dumping Atari was doing, with one commissioner stating that the area did not want to become \"an industrial waste dump for El Paso.\"[12] The local manager ordered the dumping to be ended shortly afterwards. Due to Atari's unpopular dumping, Alamogordo later passed an Emergency Management Act and created the Emergency Management Task Force to limit the future flexibility of the garbage contractor to secure outside business for the landfill for monetary purposes. Alamogordo's then mayor, Henry Pacelli, commented that, \"We do not want to see something like this happen again.\"[14]\n\nAll of these factors have led to wide speculation that most of the 3.5\u00a0million unsold copies of E.T. the Extra-Terrestrial ultimately wound up in this landfill, crushed and encased in concrete.[18] It has also been reported that prototypes for the proposed Atari Mindlink controller system were disposed of at the site,[19] which only further fuels speculation, since Atari Museum owner Curt Vendel actually currently owns the Mindlink prototypes.[20]\n\nThe conflicting information surrounding the burial has led to the claim of it being an \"E.T. Dump\" being referred to as an urban legend,[21] which in turn has led to a degree of skepticism and doubt over the veracity of the dumping story itself, and the relevance of conflating the event with the later industry downturn.[22][23] As recently as October 2004, Howard Scott Warshaw, the programmer responsible for the E.T. the Extra-Terrestrial game expressed doubts that the destruction of millions of copies of the game ever took place. Warshaw also believes that Atari's downfall was more a result of their business practices\u2014including alleged block booking of poorly selling games with successful ones when dealing with distributors\u2014than any specific failed games.[24] This latter view has been echoed by Travis Fahs of IGN, who believes that Atari's problems, including their huge surplus of unsold stock, arose from the company's overestimation of the sustainability of Atari 2600 sales, rather than being due to the individual quality of games being released.[25] Writing for the Pacific Historical Review, John Wills has also described the burial as an urban legend, calling it \"widely acknowledged but rarely substantiated\". Wills believes that the location's place in the public psyche\u2014its proximity to the sites of both the Trinity nuclear test and Roswell UFO incident\u2014has aided the popularity of the story.[26]\n\nThe incident has also become something of a cultural symbol representative of the North American video game crash of 1983, often cited as a cautionary tale about the hubris of poor business practices,[27][28][29] despite suggestions that the burial allowed the company to write off the disposed-of material for tax relief purposes.[28] The legacy of the burial has led it to be referenced in popular culture. The music video for the song \"When I Wake Up\" by Wintergreen depicts the band traveling to the landfill site and proceeding to dig up the abandoned cartridges;[30] the video's director Keith Schofield had worked with video game-based music videos before.[31] The novel Lucky Wander Boy by D.B. Weiss features a scene which takes place outside of Alamogordo, in which two of the characters discuss a parking lot which has been built over the site of the burial.[32] The 2014 film Angry Video Game Nerd: The Movie features a plot centered on the burial.[33]\n\nOn May 28, 2013, the Alamogordo City Commission granted Fuel Industries, a Canadian entertainment company, six months of access to the landfill to film a documentary about the burial and to excavate the dump site.[34] Xbox Entertainment Studios planned to air this documentary series as an exclusive to the Xbox One and Xbox 360 in 2014 as part of a multi-part documentary series being produced by Lightbox, a US/UK production company.[35] Though the excavation was momentarily stalled due to a complaint by the New Mexico Environmental Protection Division Solid Waste Bureau citing potential hazards, the issues were resolved in early April 2014 to allow the excavation to proceed.[36]\n\nExcavation started on April 26, 2014 as an open event to the public.[37] E. T. the Extra-Terrestrial designer Howard Scott Warshaw and director Zak Penn attended the event as part of a documentary about the burial,[38] as did local residents such as Armando Ortega, a city official who is reportedly one of the original children to raid the dump in 1983. Ortega stated that although he and his friends found dozens of quality games, they gave the E.T. cartridges away because the \"game sucked ... you couldn't finish it\".[39][40] James Heller, the former Atari manager in charge of the original burial, was also on hand at the excavation. Heller revealed that he had originally ordered the site to be covered in concrete. Contrary to the urban legend that claims millions of cartridges were buried there, Heller stated that only 728,000 cartridges were buried.[41]\n\nRemnants of E.T. and other Atari games were discovered in the early hours of the excavation, as reported by Microsoft's Larry Hyrb.[42][43] A team of archaeologists was present to examine and document the Atari material unearthed by excavation machinery: Andrew Reinhard (American School of Classical Studies at Athens), Richard Rothaus (Trefoil Cultural and Environmental), Bill Caraher (University of North Dakota), with support from video game historian Raiford Guins (SUNY - Stony Brook) and historian Bret Weber (University of North Dakota). [44] Only about 1300 cartridges of the estimated 700,000 were removed from the burial, as the remaining materials were deeper than expected and made them more difficult to access, according to Alamogordo mayor Susie Galea. The burial was refilled following this event.[45]\n\nThe documentary Atari: Game Over, which features the burial site and its excavation, was released on November 20, 2014.[46][47]\n\nOf the recovered materials, a fraction has been given to the New Mexico Museum of Space History for display, and another 100 to the documentary producers Lightbox and Fuel Entertainment. Galea believes the remaining 700 cartridges can be sold by the city of Alamogordo through the Museum of Space History. She hopes that the sale of these games can help fund recognition of the burial site as a tourist attraction in the future.[45] The City of Alamogordo approved the auction of the games in September 2014, to be sold through eBay and the Alamogordo Council website. The sale had brought $37,000 dollars as of mid-November 2014[48] and will be completed by the end of the year.[49][50]\n\nOne of the E.T. cartridges that had been dug up was taken by the Smithsonian Institution for its records, calling the cartridge both representative of the burial site but also in terms of video games, how the cartridge represents \"the ongoing challenge of making a good film to a video game adaptation, the decline of Atari, the end of an era for video game manufacturing, and the video game cartridge life cycle\".[51]"},
{"url": "http://www.newgeography.com/content/004926-california-2060", "link_title": "California in 2060?", "sentiment": 0.014350649350649352, "text": "The California Department of Finance (DOF) has issued population projections for the state\u2019s counties to 2060.\u00a0 Forecasts are provided for every decade, from a 2010 base. The DOF projects that the the state will grow from 37.3 million residents in 2010 to 51.7 million in 2060. This is a 0.7 percent annual growth rate over the next 50 years. By contrast, California's growth rate was 1.7 percent annually over the last 50 years (1960-2010), and a much higher 3.0 percent in the growth heyday of 1940 to 1990. However, even with this slower rate, California is expected to grow slightly more quickly than the nation (0.6 percent annually).\n\nThe current projections are considerably more conservative than those made by DOF less than a decade ago. In 2007, DOF forecast that California would have 60 million residents in 2050. The current population project for 2050 is substantially smaller, at 49.8 million.\n\nTo understand where this growth is projected to take place --- and not --- we look at CSA's (consolidated statistical areas).\u00a0 CSA's are economically connected, adjacent metropolitan areas. CSA's require a 15 percent employment interchange between the metropolitan areas. Metropolitan areas themselves are defined by a 25 percent commuting interchange between outlying counties and central counties, each of which must have at least one-half of its population in the core urban area.\n\nAs Michael Barone pointed out in his analysis of the 2014 population estimates, sometimes it is not obvious when one metropolitan area changes into another, as in the cases of San Francisco/San Jose and Los Angeles/Riverside-San Bernardino, which are CSA's. Another example is New York and the southwestern Connecticut suburbs in Fairfield and New Haven counties. This is because there is no break in the continuous urbanization.\n\nIf the DOF has it right, in a half century, California will be home to eight major metropolitan complexes. which I am defining as combined statistical areas (CSA's) or\u00a0 \"stand alone\" metropolitan areas with more than 1,000,000 population (Figure 1).\n\nThe Los Angeles metropolitan complex (Los Angeles-Riverside, including Los Angeles, Orange, Riverside, San Bernardino and Ventura counties) would remain by far the largest, growing from 17.9 million to 22.8 million. One-third of the growth would be in Los Angeles County, and two-thirds outside. Riverside and San Bernardino counties would receive most of the growth (53 percent). Riverside County would grow the fastest, adding 68 percent to its population (Figure 2). Overall, the Los Angeles metropolitan complex would grow 27.3 percent, well below the projected state rate of 38.4 percent. This is quite a turnaround for a metropolitan complex that was once among the fastest growing in human history.\n\nThe San Francisco Bay metropolitan complex, including the San Francisco, San Jose, Santa Cruz, Vallejo, Santa Rosa and Stockton metropolitan areas would grow a much faster 45.6 percent, from 8.1 million in 2010 to 11.9 million in 2060. The core city of San Francisco would add nearly 300,000, growing 36.3 percent to 1.1 million, (nearly the state rate). However, only 8 percent of the Bay Area growth would be in San Francisco, and 92 percent outside (Figure 3).\u00a0 Four counties would add more than 500,000 residents, including Santa Clara (800,000), Alameda (680,000), Contra Costa (519,000), and newly added San Joaquin county, which is defined as the Stockton metropolitan area (620,000). San Joaquin County would also grow the fastest, at 90 percent, reaching 1.3 million. This growth is to be expected, since San Joaquin is one of the more peripheral counties, and where the metropolitan fringe (which includes the commuting shed) has been expanding the most.\n\nThe San Diego metropolitan complex, a \"stand alone\" metropolitan area, would grow nearly as slowly as Los Angeles. San Diego's population of 3.1 million in 2010 would rise to 4.1 million in 2060, an increase of 30.8 percent.\n\nSacramento's metropolitan complex includes the Sacramento, Truckee-Grass Valley and Yuba City metropolitan areas. Sacramento is projected to grow 52.8 percent, from 2.4 million in 2010 to 3.7 million in 2060.\n\nFour additional metropolitan complexes with more than 1 million population are projected, all in the San Joaquin Valley.\n\nFresno, which includes Fresno County and Madera County, would grow from 1.1 million to 1.9 million, for a nearly 75 percent growth rate.\n\nBakersfield (Kern County) would be the fastest growing among major metropolitan complexes. Bakersfield would grow from 840,000 in 2010 to 1.8 million in 2060, for a growth rate of 111 percent.\n\nModesto (Stanislaus and Merced counties) would be the seventh largest metropolitan complex. From a 2010 population of 770,000, Modesto would grow 74 percent to 1,340,000. However, it is possible that by 2060 the commuting shed will reach the San Francisco Bay metropolitan complex, causing it to consume Modesto, as it already has Stockton.\n\nIn 2060, California would get its eighth major metropolitan area, with Visalia-Hanford reaching 1,040,000, up 74 percent from 2010 (Tulare and Kings Counties).\n\nOutside of these areas, the largest metropolitan complex would be Salinas, which is projected to have 530,000 residents by 2060. However, Salinas is close enough to the San Francisco Bay Area that it could be added to that area's commuting shed by 2040. The next largest metropolitan area would be El Centro (Imperial County), with a population projected to reach 340,000 by 2060. El Centro, however, could be included in the San Diego commuter shed by that time, making it a part of the San Diego metropolitan complex. The next largest metropolitan complexes would be in the northern Sacramento Valley, Redding and Chico, both approximately 300,000.\n\nOnly 2.4 million Californians lived outside the 8 major metropolitan complexes, or 7 percent of the population. Growth in these areas is expected to be slow, with only a 27 percent increase to 2060.\n\nOf course, it is virtually impossible to accurately predict demographic trends 50 years into the future. California\u2019s slower than expected growth in recent decades reflected general economic weakness since 1990, and the impact of ultra-high housing prices, particularly on the coast. However, the 2060 California projections provide an interesting view of the future from today's perspective.\n\nPhoto: Bakersfield: Fastest Growth Projected 2010 to 2060. \"Bakersfield CA - sign\" by nickchapman - originally posted to Flickr as P1000493. Licensed under CC BY 2.0 via Wikimedia Commons.\n\nWendell Cox is principal of Demographia, an international public policy and demographics firm. He is co-author of the \"Demographia International Housing Affordability Survey\" and author of \"Demographia World Urban Areas\" and \"War on the Dream: How Anti-Sprawl Policy Threatens the Quality of Life.\" He was appointed to three terms on the Los Angeles County Transportation Commission, where he served with the leading city and county leadership as the only non-elected member. He served as a visiting professor at the\u00a0Conservatoire National des Arts et Metiers,\u00a0a national university in Paris.\u00a0Wendell Cox is Chair, Housing Affordability and Municipal Policy for the\u00a0Frontier Centre for Public Policy\u00a0(Canada), is a Senior Fellow of the\u00a0Center for Opportunity Urbanism\u00a0and is a member of the Board of Advisors of the\u00a0Center for Demographics and Policy\u00a0at Chapman University."},
{"url": "http://www.nber.org/papers/w18901", "link_title": "The Great Reversal in the Demand for Skill and Cognitive Tasks", "sentiment": 0.1487908496732026, "text": "What explains the current low rate of employment in the US? While there has been substantial debate over this question in recent years, we believe that considerable added insight can be derived by focusing on changes in the labor market at the turn of the century. In particular, we argue that in about the year 2000, the demand for skill (or, more specifically, for cognitive tasks often associated with high educational skill) underwent a reversal. Many researchers have documented a strong, ongoing increase in the demand for skills in the decades leading up to 2000. In this paper, we document a decline in that demand in the years since 2000, even as the supply of high education workers continues to grow. We go on to show that, in response to this demand reversal, high-skilled workers have moved down the occupational ladder and have begun to perform jobs traditionally performed by lower-skilled workers. This de-skilling process, in turn, results in high-skilled workers pushing low-skilled workers even further down the occupational ladder and, to some degree, out of the labor force all together. In order to understand these patterns, we offer a simple extension to the standard skill biased technical change model that views cognitive tasks as a stock rather than a flow. We show how such a model can explain the trends in the data that we present, and offers a novel interpretation of the current employment situation in the US.\n\nYou may purchase this paper on-line in .pdf format from SSRN.com ($5) for electronic delivery.\n\nYou should expect a free download if you are a subscriber, a corporate associate of the NBER, a journalist, an employee of the U.S. federal government with a \".GOV\" domain name, or a resident of nearly any developing country or transition economy. If you usually get free papers at work/university but do not at home, you can either connect to your work VPN or proxy (if any) or elect to have a link to the paper emailed to your work email address below. The email address must be connected to a subscribing college, university, or other subscribing institution. Gmail and other free email addresses will not have access. E-mail:\n\nForthcoming: The Great Reversal in the Demand for Skill and Cognitive Tasks, Paul Beaudry, David A. Green, Ben Sand. in The Labor Market in the Aftermath of the Great Recession, Mas and Card. 2014"},
{"url": "http://www.theverge.com/2015/5/22/8645983/firefox-gives-up-on-25-dollar-smartphone", "link_title": "Mozilla gives up on producing $25 smartphones", "sentiment": 0.21759217793700553, "text": "Mozilla is learning that making smartphones dirt cheap doesn't guarantee success when you're running up against Google's Android operating system.\u00a0CNET reports that in an email to employees sent out on Thursday, CEO\u00a0Chris Beard made it clear that the company will soon be changing its mobile strategy. \"We have not seen sufficient traction for a $25 phone,\" Beard wrote. He went on to say, \"We will focus on efforts that provide a better user experience, rather than focusing on cost alone.\"\n\nThat's not to say that Mozilla will exclusively be targeting the high-end iPhone and Android flagship market. The company seems determined to produce enticing options across a broad range of prices, and more Mozilla employees will likely be asked to help gauge just how Firefox OS phones stack up against an endless sea of Android competition. \"While we won't be able to live and breathe on each and every target device for our core product and technology, we can on phones that are powerful enough for each of us to make our primary phone,\" Beard wrote. His email even mentions that Mozilla will continue to explore developing feature phones \u2014 presumably employees won't be forced to carry those around.\n\nMozilla is thinking about adding support for Android apps as another way of giving Firefox OS a boost. Beard hints this would be limited to \"key apps\" rather than allowing any and all Android apps onto the platform.\n\nSo Firefox OS phones are still very much alive. \"Firefox OS is critical to ensure the Web remains the single greatest public resource the world has ever known,\" Beard wrote. It just sounds as though Mozilla is backing away from the idea that it's feasible to make any kind of decent smartphone for $25. That's probably for the best since our hands-on time with its proof-of-concept $25 device at Mobile World Congress\u00a0produced plenty of frustration and little else. That simply won't do when lined up next to \"sophisticated competition from the most aggressive and largest technology companies in the world\" as Beard put it."},
{"url": "http://www.cnet.com/news/google-has-a-near-perfect-universal-translator-for-portuguese-at-least/", "link_title": "Google has a 'near perfect' universal translator \u2013 for Portuguese (2013)", "sentiment": 0.10102343308865046, "text": "Google continues its efforts to bring us the world of \" Star Trek \" and life on the U.S.S. Enterprise four centuries ahead of schedule -- minus the really hard stuff like the warp drive. The company's latest effort along these lines, according to Android product guru Hugo Barra, is a real-time universal translator.\n\nBarra told the U.K. Times that \"several years\" from now, he envisions devices (likely Android phones or something similar) that allow people to travel around the globe without having to be concerned about language barriers. Barra also spoke of the ability for calls to be translated from one language to another in real time, so that a person on one end of the call might speak in English, and that speech would then be instantly translated into Portuguese for the person listening on the other end in Sao Paulo.\n\nIn fact, English and Portuguese was one language pairing that Barra specifically cited as already providing \"near perfect\" translations on Google's prototype devices. Translation from Mandarin to the recently extinct Eyak language of southeast Alaska? Yea, that might be a little trickier.\n\nBefore we go praising Google for another forward-thinking humanitarian initiative, it's worth noting that the ability to listen to and translate countless conversations across the world amounts to a brand new mountain of data for the company to parse for ad targeting and other revenue-generating possibilities.\n\nAlso, Google might not be the first to master instant translation. I recently was given a demonstration of a similar instant real-time translation service for phone calls from an Israel-based startup called Lexifone that's not only available right now, it's also pretty cheap and accurate, if a little jarring (the translation essentially adds two loudmouth digital voices to a phone call).\n\nNonetheless, I'd certainly welcome a universal translator feature integrated into Android at some point in the future, particularly if it does a better job than the current crop of third-party translation apps , most of which suffer from subpar speech recognition. Perfecting speech recognition is one area where Google already has a significant investment with its all-in approach to Google Now.\n\nPerhaps by the end of the several years that Barra mentions, we'll not only see perfected universal translators, but the full-blown \"Star Trek\" computer that people at Google seem to be so obsessed with ."},
{"url": "http://pindexis.github.io/marker/", "link_title": "Show HN: Bookmark your terminal commands with Marker", "sentiment": 0.11739631336405532, "text": "Do you heavily use Ctrl+R(search through history) to search for commands that you frequently use?\n\n Marker lets you easily bookmark these commands and quickly retrieve them without going through Shell Aliases/Functions/Multiple Ctrl-R...\n\n It offers the following features:\n\nSimplicity is key, Three keyboard shortcuts let you take most of Marker:\n\nMarker is a composed of shell script, and a python tool:\n\n The shell code acts as a wrapper around that python tool, it's responsible for managing the user input in the command-line(adding/removing text, moving the cursor around etc...).\n\n The python utility in the other hand(called marker) contains the app logic. It manages the bookmarked data, do the matching, and present a UI selector if it's called in interactive mode. It doesn't depend on the Shell script, so it can be called separately as a command line utility( )\n\nThe communication between the shell script and the python commandline tool is done via a temporary file. For example, here's how things work when Ctrl+space is pressed:\n\nYou can take a look at for more details. Most magic happens there.\n\nBash uses an external library(Readline) to process the user input in the command-line(including keyboard bindings). This separation makes it hard to script and extend the command-line when certain keys are pressed. For example, It's not possible to invoke shell functions intuitively when a user press a keyboard shortcut and manipulate the command-line from those functions(in contrast with zshell where the input processor zle is integrated within the shell).\n\nA couple of hacks were made to make Marker work with Bash, notably triggering shell-expand-line to evaluate a shell function with the current written string as an argument. This shell function will then executes some logic and dynamically bind a certain sequence of characters to a temporary keyboard shortcut which will be executed finally by the original shortcut(ie )( contains more details).\n\nSadly, hacks come with a cost: It's not possible to use the keyboard shortcuts and with commands that contain single quotes ( ) because single quotes are used to enclose the user input. So you probably should use double quotes with escaping instead(see here for difference between single and double quotes)"},
{"url": "http://www.polygon.com/features/2014/7/16/5884227/cd-projekt-the-witcher-3", "link_title": "Inside the company that made the Witcher", "sentiment": 0.0695593256508749, "text": "Inside the company that made The Witcher He swings the sword over his head, then down quickly. A little grunt escapes his lips. His form, as near as I can tell, is perfect. Then he holds the pose, sword down, tip near the floor, until the woman manning the computer console tells him to stop. Then he does it again. We're in the motion-capture room at Polish game developer CD Projekt Red. The man on the stage, wearing black spandex speckled with white dots, is creating animations for the characters in the upcoming mobile game Crimson Trail. Before doing that, he performed all the swordplay in the upcoming game The Witcher 3: Wild Hunt. He and the woman running the computer are both speaking Polish, but it's easy enough to understand what they're saying. It's the universal language of motion capture. They're running down a list of moves and characters. We're asked to not photograph this list, or the computer screen bolted to the wall, because the characters, monsters and weapons are not yet public knowledge. The swordsman switches weapons. He's now doing work with a staff, swinging it over his head in graceful arcs, then bringing it down, or to the side. He does a running swing, stopping right on the X taped to the floor, with a grunt. I heft the sword he's just discarded. It's real. Heavy. Worked steel with a dull blade. Ten pounds, easy. Maybe more. The swordsman has been at this for almost an hour. He will continue all day. This is his job. Swinging a sword, being the Witcher (and other characters). The games that incorporate his sword-swinging are the most popular and successful video games made in Poland, and the story of the company that made them is \u2014 perhaps more than any other company \u2014 the story of Polish game development. As we file out of the motion-capture room, back into the Warsaw rain, we begin to unravel the story of CD Projekt. How this massive game company and publishing house started from nothing to conquer Poland, and in some ways the world. How it went from a shed in a muddy field in Warsaw to this sprawling complex along a highway. How it developed the game that even Barack Obama owns.\n\nThe tour starts in the cafeteria. It's fully staffed and vegetarian only (with fish). CD Projekt's co-founder, Marcin Iwinski, is a vegetarian and so his company is, too. If you want meat or grease, you can go down the road, but Iwinski would rather provide something healthier for his employees. \"In a two-kilometer radius there's pretty much nothing [to eat] that won't kill you,\" he says. \"People spend a big chunk of their lives here, so we'd like it to be a nice place to live for part of the time.\" The space is large and bright, with brick walls and IKEA furniture. There's a stage where the studio's resident musicians occasionally play. A concert is scheduled for the next day, in fact. A group of CD Projekt developers are in a metal band. The cafeteria backs onto the glass-fronted lobby. Pull a large, black curtain around the caf\u00e9 entrance, and the sounds of eating and chatting inside are almost inaudible. The place fills up at lunchtime. Most CDPR employees seem to be fine with eating vegetarian. Or else disinclined to walk in the rain. We're in the lobby to meet CD Projekt's business development manager, Rafal Jaki, and hear about how The Witcher world is expanding to include not just video games, but comic books, a board game and Crimson Trail, just announced at this year's E3. Outside of Poland, The Witcher is known as a role-playing video game that is based on a little-known book. But inside of Poland, it's a different story. The Witcher is part of a rich literary history that, for Poles, is as important as J.R.R. Tolkien's. The Witcher books and stories by Andrzej Sapkowski feature a half-mutant monster hunter named Geralt. A dark fantasy tale full of mature themes and violence, it's the type of story that has recently become popular in the West. Sapkowski's first Witcher story was published in Poland in 1986. Without The Witcher and its sequel The Witcher 2, there would be no CD Projekt Red. The video games just happen to share some of the characters. And the comic books, Jaki tells me, sitting in a white pod-like chair that would not look out of place in a space station, are part of a plan to expand the offerings of the Witcher universe into places where video games can't reach. And hopefully bring some of that rich history to Western audiences. \"[Sapkowski] writes about things that are relevant to people,\" says Jaki. \"It's not just, 'OK, there's an evil wizard you have to slay, and then you save the princess and everything is happy.' It's more dark, realistic and gritty in a way. \"For example, you have racism in the Witcher world. The elves and the dwarves are discriminated against because of their race. This is a topic that's very up-to-date here and now. Even though it's elves and dwarves, you can relate to the problems they're having.\" For CD Projekt Red, the game-development side of CD Projekt, that relatability has made The Witcher an engine that's driven the entire company. The studio is currently at work on The Witcher 3 in many of the cavernous rooms inside the CDPR complex. (The board game, mobile game and comic books are largely created elsewhere.) And there's a small section of the complex devoted to GOG, or Good Old Games, the online distribution service devoted to PC and Mac classics and new indie titles. CDPR has partnered with publisher Dark Horse for the comic books, which introduce characters and monsters that gamers may encounter in The Witcher 3. The board game is also part of the plan to make the Witcher world accessible to people who may not be into RPGs or comic books. It's just another avenue for CDPR to exploit its devastatingly successful franchise. The lobby we're sitting in is lined with awards and photographs of CDPR founders shaking hands with luminaries. The prime minister of Poland here. Barack Obama there. The awards cover two entire walls, from floor to ceiling. Another wall is nothing but magazine covers, all featuring The Witcher. The Witcher did this. All of it. The awards, the photographs, the massive complex of buildings and the hundreds of employees. Without The Witcher and its sequel The Witcher 2, there would be no CD Projekt Red. And without CD Projekt Red there might arguably be no Polish video game industry. At least not one so successful. According to CD Projekt Red, without The Witcher game, there wouldn't even be The Witcher at all. The stories were popular in Poland, but the video game turned main character Geralt and his universe into a worldwide sensation. \"We weren't buying The Witcher,\" says Iwinski, referring to CDPR's purchase of The Witcher rights from author Andrzej Sapkowski. \"We were buying a [story] and then we turned it into The Witcher, which became known all around the world.\"\n\nCD Projekt Red was founded in 1994 by Iwinski and his then business partner Michal Kicinski, although Poland at that time was such a different place that the creation of the company wasn't so much a founding as a gradual evolution. One day Iwinski and Kicinski were selling cracked and localized Western games on CDs in a Warsaw marketplace; the next they were a business. \"When you say founding CD Projekt, it sounds so serious,\" Iwinski says. We're sitting with him in the company's \"medieval room,\" where they do Witcher interviews. It's wallpapered in a faux brick, with wrought-iron sconces holding low-wattage bulbs. On the ceiling is a map of The Witcher's world, as depicted in the video games. \"We were just fans of games. We wanted to legalize our business.\" \"A regular game in Poland was selling at the time maybe 1,000 or 2,000 units. On day one we sold 18,000 units of Baldur's Gate.\" Like so many other companies in Poland, CD Projekt started as a direct result of the collapse of communism. Poles were suddenly left to fend for themselves, and many like Iwinski saw the opportunity to build businesses, something that was denied to ordinary people under communist rule. He calls the years following the 1990 change in government an \"incubator of entrepreneurship.\" CD Projekt's version of entrepreneurship involved capitalizing on Poland's lax copyright laws and Western games companies' general disinterest in the Polish market. When the copyright laws changed, CD Projekt changed, shifting into licensed localization, distribution and, eventually, game development. But it all started simply as an excuse to play new games before anyone else. \"We had access to new games,\" Iwinski says. \"How silly does that sound as a reason for founding a business? But I think it was quite important. ... We were doing the first localizations in Polish. We did PR and marketing campaigns. In the beginning, the market was quite wild. No retail chains selling games. In the first few years we were just selling to mom-and-pop shops. [Customers] were coming up with a Volkswagen and filling it up with games and selling them in small stores all around Poland.\" The success of CDPR's early efforts convinced Iwinski that he wasn't the only one who wanted to play games, and that most Polish people wanted to play them in Polish. CDPR became the first company in Poland working directly with Western companies to localize games in Polish, hiring famous Polish actors to redo the spoken dialogue. Suddenly the world of game development broke wide open. CDPR could offer Western companies something no one else was offering. Iwinski developed a relationship with game publisher Interplay and began networking with game companies and attending events around the world. In one case, he met BioWare founders Ray Muzyka and Greg Zeschuk and pitched them on localizing the popular RPG Baldur's Gate for Polish audiences. BioWare agreed, with one caveat \u2014 CDPR would have to do all the work. \"We said we'd take the risk,\" says Iwinski. \"Of course, the decision was very hard for us. We worked six months on the localization. I was working on it. My father was helping, because he was a producer on movies at the studios that employed famous Polish actors. This was a game that established totally new standards. \"A regular game in Poland was selling at the time maybe 1,000 or 2,000 units. On day one we sold 18,000 units of Baldur's Gate.\" CDPR had to rent a separate warehouse just to hold copies of Baldur's Gate before it shipped. Ultimately the team would sell tens of thousands of units of the game, a blockbuster by the standards of the time. The project was so successful for CDPR that the team immediately began another: A PC port of the Baldur's Gate sequel, Baldur's Gate: Dark Alliance for Interplay. The game was still in development, but planned only for consoles. CDPR would have the lock on PC development for Western audiences and for Poland. There was only one catch: Interplay was falling apart. \"Our friends at Interplay called us ... and said, 'Hey, it doesn't look too good,'\" Iwinski says. \"'The company is having financial problems. Don't do this port. Nobody will pay you for it. It'll be tough for you.' \"It was like, 'OK? We started the development, so what should we do now?'\" CDPR began discussing what it could do with the code it had developed as part of the Dark Alliance port. With the content of the game itself now completely up to the Polish developers, it turned immediately to Sapkowski's The Witcher. \"The Witcher was on the top of the list,\" Iwinski says. \"We started researching whether we could get the rights. It happened that we could, and we actually acquired the rights and started working on The Witcher. So it all started, in a way, with Baldur's Gate. \"It's a story of dreams, persistence and luck, or good karma, or whatever you call it.\"\n\nThe tour continues. We're shown two floors of where The Witcher 3 is being made, all furnished in reds and chrome. Porthole-shaped windows and glass walls round out the decor. We're asked to not take pictures of any of it (to save from leaks about the game), and we're not taken anywhere remotely close to where CD Projekt is making its other big game, the dark science fiction tale Cyberpunk 2077. The studio is immense and well-appointed, but in almost every respect it looks essentially like your average large, successful video game studio. Except for one thing: CD Projekt is Poland's ultimate successful video game studio. It is, in multiple respects, the pinnacle of the industry in Poland, where most people working in games hope to eventually land. Most of the company's hundreds of workers grew up wanting to work here because of the company's resemblance to the flashy, successful video game companies in the West. Not just \"in games,\" as in the U.S., but specifically at CD Projekt, and for many of those working on The Witcher 3, specifically on that game. CD Projekt is one of the most diverse workplaces in Poland, with employees from all over Europe and elsewhere. The company has adopted English as its official language, just so teams can function. We move on to the large, back section of the main building, where CD Projekt still runs a robust game-distribution business, moving games from companies like Disney Interactive, Blizzard and Konami into Poland, alongside those from smaller European studios like Larian and Astragon. And then it's back to the lobby to talk with Lead Quest Designer Mateusz Tomaszkiewicz. He's working on The Witcher 3. Tomaszkiewicz says there are two sides to building a game from such a deep, established world as The Witcher. \"There's a good side, where we have a base on which to build, a whole universe that was created by Sapkowski,\" he says. \"We have a lot of characters that we can use, source materials we can use. We have rules by which things like magic work. We have political connections, a whole scene we can work on. That's good, because we have this large library we can use. We can add to it, but we don't have to build things from scratch.\" \"The books, I know they have many fans, and we don't want to upset them by doing something improperly.\" The bad side: Some of those well-established characters can't change. \"Geralt can't do some things that players might like to do. Geralt won't ever kill the innocent. The options for being the bad guy in this game have to be pretty limited, in ways that would fit the character. All the quests we do have to take that into consideration.\" It's a symptom of the company's greatest success coming on the back of opportunistic accomplishments. In a way, just like the company's name: \"CD Projekt.\" It harkens back to the earliest days, when the business was selling CDs loaded with other people's games. Then it began distributing other people's games. Then, localizing and porting them. That The Witcher itself started out as someone else's world is just one more piece of that same puzzle. Successful game, but someone else's world. \"The books, I know they have many fans, and we don't want to upset them by doing something improperly, so to speak,\" says Tomaszkiewicz. \"We try to be very careful in translating this whole experience into a game. We try to be as close to what Sapkowski established in the books.\" You get the sense that the company is poised to take the next step and start making some of that success from its own ideas. That in spite of all that it has achieved, there's still more waiting over the horizon \u2014 whether it's the new IP Cyberpunk 2077 or something yet to be created. \"I can't stop smiling when I'm sitting at a table in the [cafeteria],\" says Iwinski, \"and I talk with people who are smarter than me and who build this really extreme stuff, which I then see in both GOG and the builds of The Witcher 3. I'm expecting a third baby, but actually I tell my wife it's the fourth, because CD Projekt was my first one.\" Yet in spite of CD Projekt's massive success and near single-handed vitalization of Polish game development, Iwinski avoids describing CD Projekt as an ambassador for Poland. He prefers to see the company's success and that of The Witcher represent a form of love letter to the rest of the world, from deep in the Slavic heart of Poland. \"Poland's history is tied up mostly in wars between its huge neighbors,\" he says. \"That is reflected in The Witcher, because it was reflected in the creation of Sapkowski. But I think where we went really deep, ... [was with] the visual representation. ... Yesterday we had a meeting session outside of Warsaw on a lakeside. We're driving back through forests. The managing director, Guillaume \u2014 he's French \u2014 he says, 'Wow, it looks like The Witcher.' And I said, 'Yeah, that's The Witcher.\" \"All that surrounds us \u2014 the castles, the forests, the countryside \u2014 that's The Witcher. It's very Slavic. ... For me, this is where I'm coming from. I hope that for foreign players, the majority of players, this is something interesting, but at the same time, it's real. Because it is real.\""},
{"url": "http://www.motherjones.com/environment/2015/05/cia-closing-its-main-climate-research-program", "link_title": "The CIA Is Shuttering a Secretive Climate Research Program", "sentiment": 0.07410785486443382, "text": "Scientists used the Medea program to study how global warming could worsen conflict. Now that project has come to an end.\n\nOn Wednesday, when President Barack Obama spoke at the US Coast Guard Academy's commencement ceremony, he called climate change \"an immediate risk to our national security.\" In recent months, the Obama administration has repeatedly highlighted the international threats posed by global warming and has emphasized the need for the country's national security agencies to study and confront the issue.\n\nSo some national security experts were surprised to learn that an important component of that effort has been ended. A CIA spokesperson confirmed to Climate Desk that the agency is shuttering its main climate research program. Under the program, known as Medea, the CIA had allowed civilian scientists to access classified data\u2014such as ocean temperature and tidal readings gathered by Navy submarines and topography data collected by spy satellites\u2014in an effort to glean insights about how global warming could create security threats around the world. In theory, the program benefited both sides: Scientists could study environmental data that was much higher-resolution than they would normally have access to, and the CIA received research insights about climate-related threats.\n\nBut now, the program has come to a close.\n\n\"Under the Medea program to examine the implications of climate change, CIA participated in various projects,\" a CIA spokesperson explained in a statement. \"These projects have been completed and CIA will employ these research results and engage external experts as it continues to evaluate the national security implications of climate change.\"\n\n\"There's a growing gap between what we can currently get our hands on, and what we need to respond better,\" said security expert Marc Levy.\n\nThe program was originally launched in 1992 during the George H.W. Bush administration and was later shut down during President George W. Bush's term. It was re-launched under the Obama administration in 2010, with the aim of providing security clearances to roughly 60 climate scientists. Those scientists were given access to classified information that could be useful for researching global warming and tracking environmental changes that could have national security implications. Data gathered by the military and intelligence agencies is often of much higher quality than what civilian scientists normally work with.\n\nIn some cases, that data could then be declassified and published, although Francesco Femia, co-director of the Center for Climate and Security, said it is usually impossible to know whether any particular study includes data from Medea. \"You wouldn't see [Medea] referenced anywhere\" in a peer-reviewed paper, he said. But he pointed to the CIA's annual Worldwide Threat Assessment, which includes multiple references to climate change, as a probable Medea product, where the CIA likely partnered with civilian scientists to analyze classified data.\n\nWith the closure of the program, it remains unclear how much of this sort of data will remain off-limits to climate scientists. The CIA did not respond to questions about what is currently being done with the data that would have been available under the program.\n\nMarc Levy, a Columbia University political scientist, said he was surprised to learn that Medea had been shut down. \"The climate problems are getting worse in a way that our data systems are not equipped to handle,\" said Levy, who was not a participant in the CIA program but has worked closely with the US intelligence community on climate issues since the 1990s. \"There's a growing gap between what we can currently get our hands on, and what we need to respond better. So that's inconsistent with the idea that Medea has run out of useful things to do.\"\n\nThe program had some notable successes. During the Clinton administration, Levy said, it gave researchers access to classified data on sea ice measurements taken by submarines, an invaluable resource for scientists studying climate change at the poles. And last fall, NASA released a trove of high-resolution satellite elevation maps that can be used to project the impacts of flooding. But Levy said the Defense Department possesses even higher-quality\u00a0satellite maps that have not been released.\n\nStill, it's possible Medea had outlived its useful life, said Rolf Mowatt-Larssen, a 23-year veteran of the CIA who\u00a0had first-hand knowledge of the program before leaving the agency in 2009. He said he was not surprised to see Medea close down.\n\n\"In my judgment, the CIA is not the best lead agency for the issue; the agency's 'in-box' is already overflowing with today's threats and challenges,\" he said via email. \"CIA has little strategic planning reserves, relatively speaking, and its overseas presence is heavily action-oriented.\"\n\nSen. John Barrasso said the CIA \"should be focused on monitoring terrorists in caves, not polar bears on icebergs.\"\n\nOver the past several years, climate change has gained prominence among defense experts, many of whom see it as a \"threat multiplier\" that can exacerbate crises such as infectious disease and terrorism. Medea had been part of a larger network of climate-related initiatives across the national security community. Medea's closure notwithstanding, that network appears to be growing. Last fall, Obama issued an executive order calling on federal agencies to collaborate on developing and sharing climate data and making it accessible to the public.\n\nBut the CIA's work on climate change has drawn heavy fire from a group of congressional Republicans led by Sen. John Barrasso (Wyo.). Barrasso said last year that he believes that \"the climate is constantly changing\" and that \"the role human activity plays is not known.\" He recently authored an op-ed for the Wall Street Journal in which he listed the conflicts in Iraq, Syria, and elsewhere as \"greater challenges\" than climate change. (The Syrian civil war, however, was likely worsened by climate change.)\n\nAround the time Medea was re-instated by the Obama administration, the CIA formed a new office to oversee climate efforts called the Center for Climate Change. At the time, Barrasso said the spy agency \"should be focused on monitoring terrorists in caves, not polar bears on icebergs.\" That office was closed in 2012 (the agency wouldn't say why), leaving Medea as the CIA's main climate research program.\n\nSo does the conclusion of Medea signal that the CIA is throwing in the towel on climate altogether? Unlikely, according to Femia. At this point, he said, US security agencies, including the CIA, are still sorting out what resources they can best offer in the effort to adapt to climate change. Regardless of whether the CIA is facilitating civilian research, he said, \"continuing to integrate climate change information into its assessments of both unstable and stable regions of the world will be critical.\"\n\n\"Otherwise,\" added Femia, \"we will have a blind spot that prevents us from adequately protecting the United States.\""},
{"url": "http://experiments.oskarth.com/nand-to-tetris-1/", "link_title": "Nand to Tetris 1, with Dan Luu", "sentiment": 0.15213822268566785, "text": "In the last few weeks I\u2019ve been working my way through the excellent book Elements of Computing Systems - building a modern computer from first principles as part of the equally excellent Nand to Tetris MOOC.\n\nI started reading the book a few years ago when I attended Recurse Center, but instead of completing it I ended up writing a domain specific language for the first few chapters. A useful exercise, no doubt, but this time around I intend to finish the whole book.\n\nThe main reason I want to go through the book is because I want to have a better sense of how a computer works, and get a rough idea how one could build one. In a sense its value is proportional to how well it serves as a mental model for the real world.\n\nSo how does it stack up? This is the first of a multi part series, starting with the first three chapters of the Elements of Computer Systems book, on boolean logic, boolean arithmetic, and sequential logic. I wrote down a collection of assertions that I wanted to diff with what\u2019s out there in the real world. Dan Luu was gracious enough to give me some great answers based on his expertise in the field. His answers are in italics.\n\nWhile I highly recommended that you\u2019ve taken the equivalent of a Nand to Tetris course, this is not strictly necessary. With some luck this series will convince you to embark on a similar project on your own.\n\n1. Any boolean function can and usually is built from NAND gates.\n\nWhile this is conceptually accurate, this usually isn\u2019t done in practice. There are multiple reasons for this, but it mostly comes down to cost, power, and performance. The performance aspect is that you can directly implement functions with transistors more efficiently than you can with NAND gates. You can, very loosely, think of this as similar to how compilers sometimes inline functions and then do optimizations across inlined functions.\n\nThis property, that any boolean function (i.e. any truth table) can be built using just NAND gates, is called functional completeness, and the proof is quite neat. Consider a truth table for some function and some variables. Each row where the function evaluates to true can be represented by ANDing together the variables, which are represented either as true or NOT true. We then OR together all rows to get a complete representation of that function\u2019s truth table. For example, Xor(a,b) evaluates to true when either a or b is true. We can represent this as follows: OR(AND(a, NOT(b)) , AND(NOT(a), b)). We can thus express any boolean function using just AND, NOT and OR. It then turns out, using De Morgan\u2019s laws and similar logical relationships, that we can express AND, NOT, OR in terms of NAND.\n\nAnother cool thing Dan taught me is why NAND gates are usually prefered over NOR gates, despite both of them being functionally complete. If you are interested in that, you can read more here. However, we did manage to get to the moon in the 60s using just NOR gates.\n\n2. Logical functions are built up from more elementary ones.\n\nAgain, this is conceptually correct, but for performance reasons people sometimes build logical functions directly from transistors. For much more detail on this, Weste & Harris is great. For a quick explanation, see this. That explanation isn\u2019t self contained. Some things you want to know are that the funny symbol near the bottom of those diagrams is ground (0), the funny line/symbol at the top is the on voltage (1). Then you have the transistors. If there\u2019s a bubble on the gate (input), that\u2019s a PMOS transistor. It turns on (conducts) when the input is 0, and it\u2019s good at passing \u201c1\u201ds. Otherwise, it\u2019s an NMOS, and has the opposite properties.\n\nSimilar to the answer above, the interface is correct but the implementation is naive to the point of being misleading.\n\nYes, 1\u2019s complement is rarely used, although there are some applications where it\u2019s superior. You might also be interested in logarithmic and residue number systems, which make some operations easier (faster) at the cost of making other operations slower. For more on that, Koren has a really nice text.\n\nAlso, in contrast to the address you build in nand2tetris, adders are commonly built using some kind of parallel prefix tree to reduce the delay (i.e., increase the performance). Carry-lookahead adders are probably the simplest form of this, but they\u2019re not usually the fastest thing you can do. The Weste&Harris book mentioned above has a lot more information on different types of prefix trees.\n\nThis one was funny, as I think of a carry-look-ahead as a neat optimization, whereas in the real world it\u2019s too slow to use by itself.\n\n4. When adding integers in a real-world Adder, overflows are ignored.\n\nIt depends! It\u2019s not an error, but there\u2019s often an output from the ALU that signals an overflow.\n\n5. Our ALU is essentially the same as a real one.\n\n(Our ALU has two 16-bit inputs, six control bits, two output flags, and one 16-bit output).\n\nIt\u2019s missing pipelining, forwarding, and other performance optimizations, but, fundamentally, it does the same stuff as a \u201creal\u201d ALU. Real is in quotes since it\u2019s no less real than any other ALU, although \u201creal\u201d ALUs usually implement many more functions, have more control bits, etc. :-). Also, some \u201creal\u201d ALU operations can also take several clock cycles, unlike the one in your design.\n\n6. There\u2019s one master clock that keeps track of computer time.\n\nThis is correct for designs that are made to be simple. However, in \u201creal\u201d designs there is often more than one clock for a multitude of reasons, such as dealing with I/O devices that run at different speeds. For more on how to deal with that, see this.\n\nI would say this makes my mental model incorrect, in that I would expect one clock cycle to be the unit that everything in hardware uses, but at second thought I see why that wouldn\u2019t make sense with I/O-bound hardware parts.\n\n7. Our Register, RAM and Program Counter are essentially realistic.\n\n(In addition to 16-bit input and outputs, we have the following: Our Register has a load bit, our RAM a load bit and an n-bit address, and our Program Counter has a load, inc, and reset bit).\n\nThis is true conceptually/logically. However, in \u201creal\u201d systems register files are often custom circuits built at the transistor level, and RAMs are also custom. On chip RAMs are usually SRAMs, which are built out of transistors like other chip logic (although they typically have an analog component to them, unlike the logic you\u2019ve built). Off chip DRAM is a totally different beast. There\u2019s also normally multiple read/write ports, as opposed to just one combined read/write address.\n\nThis also makes sense, but the shared memory bit sounds scary. Yet another rabbit hole to go into, for a rainy day.\n\nIn general, most of my assertions were right on an interface level, but wrong on an implementation level. Is this a good mental model? I think so. Unless you are building a real computer it\u2019s good enough, conceptually. You could also, theoretically at least, build a computer using the tools given to you in Nand to Tetris that would be similar to an Intel machine from the early 80s, which isn\u2019t that bad.\n\nIn the next part we\u2019ll move higher up the stack, looking at machine language, computer architecture and an assembler."},
{"url": "https://www.indiegogo.com/projects/whoop-de-doo-vibrator-like-no-other#/story", "link_title": "Whoop.de.doo \u2013 Vibrator like no other", "sentiment": 0.1549226116349404, "text": "Whoop.de.doo has generated a lot of positive responses among the general public and potential customers as well as in the media.\n\nIn recent years, Anna has worked hard to build the Whoop.de.doo team. She used her personal savings and a student loan to get the first item into production \u2013 the Venus Balls, which have gone on to become highly popular among women. Following several months of research and development financed from the sales of the Venus Balls, the Whoop.de.doo Vibrator has entered its final prototype phase, and in the middle of June, ten selected women will conduct the last round of testing on ten fully functional prototypes. Then, at the start of July, we\u2019ll be in position to launch production of the molds and electronic components in order to get the first Whoop.de.doo Vibrators to customers by\u00a0December 2015.\n\nYou can join us also at\u00a0www.whoopdedoo.me\u00a0and www.annamaresova.com\n\nWhat\u2019s the difference between the classic and light versions of the Whoop.de.doo Venus Balls?\n\n\n\nSimply put, the Whoop.de.doo Venus Balls are similar to exercise weights \u2013 the heavier they are, the harder the vaginal wall has to work. However, the intense sensation of the Venus Balls sliding out of the vagina, which is actually a good thing, can be unpleasant for some women. That\u2019s why we\u2019ve developed the light version, which is primarily intended for women who are new to using the Venus Balls, or have recently given birth naturally. The heavier version provides a more intense workout. Women who do sports need not be concerned about using the heavier version.\n\nHow do the Venus Balls work?\n\n\n\nThe Whoop.de.doo Venus Balls work on the time-tested principle of a ball within a ball. Each movement of the inner ball creates a stimulating vibration that gently massages the vaginal wall and improves blood circulation. The massaging effect can be altered through either spontaneous or regular muscle contractions, which in turn strengthen the pelvic floor. Thanks to the silicone lining, the vibrations are very quiet but very intense.\n\nWe recommend thoroughly cleaning your Whoop.de.doo product after each use. For optimum maintenance, use an antibacterial soap and wash under running water. Store the product in its soft, breathable cloth pouch (SWISS+COTTON). The Whoop.de.doo love toys should be kept in a cool, dark, dry place. The products are made using medical-grade, hypoallergenic silicone. Bacteria do not cling to medical-grade silicone the way they do to the gel or latex surfaces of other erotic toys, and silicone also quickly adjusts to body temperature. Do not use any product if its surface is damaged or torn!\n\nDevelopment of both the Venus Balls and the Vibrator was conducted in close consultation with a gynecologist. The products are made of medical-grade silicone, which is manufactured in Germany and then subjected to thorough testing at the Czech National Institute of Health in Prague.\n\nThe Whoop.de.doo Vibrator has a pair of magnetic contacts at the control end. Simply place the ring of the supplied charging cable onto the magnetic contacts and use a standard USB port to recharge the battery. The internal charging circuit requires a DC 5V/500mA power source, which is provided by the majority of mobile phone chargers and computer USB ports available today. The charging cable is fitted with a standard USB type-A connector.\n\nAre the Whoop.de.doo products backed by a warranty?\n\nAbsolutely. Whoop.de.doo takes great pride in the workmanship and quality of its products. In our relationships with our customers, we apply the very openness and transparency that lies at the heart of the Whoop.de.doo mission. Thus, we kindly ask you to be patient. Our legal team is currently working to iron out the details of our warranty policy. Rest assured that our priority is ensuring that our customers, both current and future, are always happy. You\u2019ve got our back here; we\u2019ve got your back the minute you purchase one of our products, period.\n\nWill Whoop.de.doo products be shipped discreetly?\n\nAbsolutely \u2013 the packaging is completely discreet.\n\nHow do I pay for shipping?\n\nDon\u2019t worry about a thing. Each of the prices listed in our campaign includes all shipping costs to your selected destination.\n\nFor additional questions concerning international shipping, please feel free to send us a message. We\u2019ll get back to you right away.\n\nCan I support this project anonymously?\u00a0Yes. Indiegogo gives you the option to donate money to a project anonymously. Simply check the \u201cDon\u2019t display my name on the campaign page\u201d box when you go to donate!"},
{"url": "http://www.washingtonpost.com/goingoutguide/the-20-diner-takes-a-smoke-break-to-find-washingtons-best-barbecue/2015/05/21/085f35f8-fb42-11e4-a13c-193b1241d51a_story.html", "link_title": "Why big-city barbecue is suddenly better than ever", "sentiment": 0.2132501967729241, "text": "For as long as pitmasters have cooked meats, low and slow, over smoldering coals, the mantra has remained the same: The best barbecue belongs to the country or in small towns, far removed from potentially punishing city regulations or reproving urbanites who enjoy their brisket only when the smoke exhaust floats into someone else\u2019s neighborhood.\n\nBut in recent years, pitmasters in Brooklyn, Phoenix and other metropolitan areas, including Washington, have taken that old mantra and ripped it to shreds, tossing the torn pieces into their wood-fired smokers. The man leading the charge for new traditionalist barbecue is Aaron Franklin, who was once just another Austin-based rocker looking for his break in the music business.\n\n[Ranking the best barbecue joints in the D.C. area]\n\nBut during the 2000s, Franklin gradually rechanneled his obsessive tendencies from music to urban barbecue. In late 2009, he opened Franklin Barbecue in Austin, initially in a trailer, and four years later, he earned the top spot on Texas Monthly\u2019s list of the state\u2019s 50 best barbecue joints (or, as the magazine notes in its wry, hyperbolic, Texas-centric view, the \u201c50 Best BBQ Joints in the World!\u201d). Earlier this month, Franklin also became the first pitmaster to win a James Beard Award in a regional chef category.\n\n\u201cIt\u2019s undeniable what he did in Austin,\u201d says Daniel Vaughn, barbecue editor for Texas Monthly. \u201cIt made all other cities wake up and realize all their excuses about making good barbecue [are] moot.\u201d\n\nBarbecue has always been available in and around the concrete-and-glass towers of American cities, some of it even legendary. Think Charles Vergos\u2019 Rendezvous in Memphis or Arthur Bryant\u2019s in Kansas City, Mo., places with charcoal- or wood-burning pits that pre-date the high-tech commercial smokers that now dominate our city meat markets. The modern set-it-and-forget it smoker has taken much of the stress out of producing barbecue, depending how much a pitmaster relies on the supplemental gas or electric heat.\n\nFranklin, by contrast, avoids pits connected to a utility line. He custom-builds smokers from used 1,000-gallon propane tanks. He cooks with only wood, relying on his ability to source well-seasoned post oak and burn those logs evenly over the many hours required to smoke brisket, pork and other large cuts of meat. He has, over time, developed his own style. Take his brisket, the pride and joy of Franklin Barbecue: He buys only prime meat, aggressively trims it and wraps it in butcher paper, not foil, at some point during the smoking process. His is an artisanal method, not interested in concessions to either time or labor.\n\nCustomers eat it up, standing in line for hours to get a taste of Franklin\u2019s juicy, salt-and-pepper briskets. The place sells out every day it opens for business. It has also inspired pitmasters, both in Austin and in cities far removed from Central Texas, to ditch the gas-assisted smokers and adopt a more traditional approach to barbecue. You can feel the Franklin influence at places such as Delaney Barbecue/BrisketTown in Brooklyn, Little Miss BBQ in Phoenix or Killen\u2019s Barbecue outside Houston. In fact, Ronnie Killen, pitmaster at Killen\u2019s, called out Franklin in an interview with the Houston Chronicle.\n\n\u201cI\u2019m coming after you,\u201d Killen promised, threatening one day to better Franklin as the king of Texas barbecue.\n\nPitmasters at the District\u2019s two leading barbecue joints have made pilgrimages to Franklin Barbecue. Robert Sonderman at DCity Smokehouse on Florida Avenue NW and Brendan Woody at Fat Pete\u2019s in Cleveland Park made separate visits to the Austin establishment as part of research trips when they tended the smokers at Hill Country, that Penn Quarter homage to Central Texas barbecue, which opened in 2011.\n\n\u201cIt was pretty damn awesome,\u201d Sonderman recalls of his meal at Franklin.\n\nAlthough neither Sonderman nor Woody calls Aaron Franklin a direct influence, both pitmasters have adopted a similar traditionalist\u2019s approach to barbecue. They haven\u2019t custom-welded their own smokers yet, as Franklin does by outfitting used 1,000-gallon propane tanks with fireboxes, smoke stacks and cooking grates. But Sonderman and Woody, like Franklin, rely on machines that burn wood only. Both use J&R smokers, and both, not coincidentally, produce meats with an exceptional hardwood perfume.\n\n\u201cYou get a better smoke ring and better smoke development\u201d with an all-wood smoker, Woody says.\n\nUnsurprisingly, the brisket at DCity and Fat Pete\u2019s put all others to shame: Their slices are moist and rich with rendered fat, and they pull apart with the slightest tug; they\u2019re also encrusted in bark, an explosive fusion of fat, moisture, smoke, seasoning and spices. You don\u2019t need a drop of sauce to savor these briskets.\n\nOperating all-wood smokers, of course, requires more babysitting than machines that can kick on gas or electrical heat when the internal cooking temperature dips below the desired one. With gas-assisted smokers, pitmasters can load their machines with meat (likely briskets and pork shoulders) at the end of service, add a few logs, and sleep soundly without fear the fire will die during the night. This high-tech approach typically eats up less wood and requires fewer employees to watch over the smoker, two significant pluses in a competitive urban marketplace where skilled pitmasters can be hard to find. Certainly harder to find than in Texas.\n\n[How Fat Pete\u2019s keeps its smoker going overnight]\n\nBut a wood smoker is still only a tool, not a guarantee of quality barbecue. The greater Washington region is packed with smokers of all shapes and sizes. Electric smokers outfitted with wood chip boxes. Gas smokers that burn logs, too. Smokers fueled by wood pellets. Pure wood smokers. Each can, at least theoretically, produce good barbecue.\n\nIn many ways, it\u2019s never been easier to find barbecue in the DMV, even if many of us are still mourning the loss of Mr. P\u2019s Ribs and Fish, the legendary smokehouse caravan that went dormant when its founder, Fate Pittman Jr., died in February.\n\nWashingtonians can now devour barbecue while sitting in a beer garden, standing next to a food truck, sipping barrel-aged cocktails, even while contributing to injured veterans. Barbecue is also expanding in scope and reach: The owners of Urban Bar-B-Que Company sold a minority stake to Ledo Pizza to expand their smoked meats concept beyond its suburban D.C. base, and Jim Foss, former director of D.C. operations for Hill Country, expects to open Smokehouse Live, a 16,000-square-foot barbecue and music joint in Leesburg, sometime after Memorial Day.\n\n[Smokehouse Live will become the region\u2019s largest BBQ joint]\n\nBut with so many options, how can you be sure you\u2019re not wasting cash on inferior barbecue? In short: You can\u2019t. Good barbecue remains a fickle tyrant, demanding attention, patience and pitmasters willing to give themselves over to the higher calling of smoked meats. A true enthusiast asks questions wherever he or she eats: What kind of smoker does the pitmaster use, and how does he or she use it? Just because a pitmaster can burn logs doesn\u2019t mean he will rely on wood. He may, in fact, cook mostly with gas, using hardwoods as essentially an aromatic.\n\n\u201cThe machine can only do so much for you,\u201d says DCity\u2019s Sonderman. \u201cYou still got to have some know-how.\u201d\n\nKnow-how comes with time and practice. The pitmaster\u2019s skill set doesn\u2019t begin and end with the ability to build and maintain a strong, clean fire. Pitmasters also develop rubs and marinades that accent the different meats. They learn to trim and cook everything from brisket to spare ribs. They know when meats are finished, often by touch only. And perhaps most important of all, they understand how to maintain the succulence of meats pulled fresh from the smoker. Nothing can kill good barbecue faster than meats allowed to dry out in a warming unit, the wood smoke diminishing and the bark disintegrating with each passing hour.\n\nAs such, you\u2019ll want to know how a joint keeps its smoked meats warm, which leads to a related question: How often do they refresh the smoker with new meat? A barbecue joint that constantly sells out may be discouraging to late afternoon diners, but it indicates a place that smokes meats daily. Also: Beware the pitmaster or barbecue employee who deflects too many questions. They may be protecting proprietary information, but just as likely, they\u2019re hiding inferior craft.\n\nOne recent afternoon in Woodbridge, as I pulled into Dixie Bones BBQ for lunch, I asked the host if the Southern Pride smoker sitting in the parking lot cooked with only wood, or with a wood-and-gas combination. He said the smoker was just one of several on premise, none of which burned only logs.\n\n\u201cWe cheat,\u201d he said with a hearty laugh and smile. The comment was disarming. It was also honest and gave me a good idea of what to expect.\n\n\u2022 Ranking the best barbecue joints in the D.C. area"},
{"url": "https://www.youtube.com/watch?v=mJd4v2XQx_M", "link_title": "How to generate personalized videos for your Unbounce leads", "sentiment": 0.5238095238095237, "text": "Register Now!\u00a0http://unbouncepages.com/how-to-engag....\u00a0Landing pages are lead generators. As marketers, we spend a lot of time trying our best to get our audience attention, but sometimes we forget how important it is to engage with them. Personalized videos are about making meaningful connections and driving ROI.\n\n\n\nIn this Sezion Webinar you will learn how to automatically create a personalized video for every lead that comes thought your landing pages built with Unbounce. We'll also learn how Sezion works and the main concepts to create powerful personalized videos for your leads.\n\n\n\nYou'll Learn:\n\n1) Why Personalized Videos are a powerful tool for today's Marketers?\n\n2) Sezion's Personalized Videos solutions and how a Personalized Video Template works.\n\n3) Best practices and tips to engange with your leads.\n\n\n\nWebinar Resources:\n\n- Slideshare: http://www.slideshare.net/sezion/webi...."},
{"url": "http://www.washingtonpost.com/wp-dyn/content/article/2007/05/04/AR2007050402555.html", "link_title": "Free Trade's Great, but Offshoring Rattles Me (2007)", "sentiment": 0.149621421859794, "text": "I'm a free trader down to my toes. Always have been. Yet lately, I'm being treated as a heretic by many of my fellow economists. Why? Because I have stuck my neck out and predicted that the offshoring of service jobs from rich countries such as the United States to poor countries such as India may pose major problems for tens of millions of American workers over the coming decades. In fact, I think offshoring may be the biggest political issue in economics for a generation.\n\nWhen I say this, many of my fellow free-traders react with a mixture of disbelief, pity and hostility. Blinder, have you lost your mind? (Answer: I think not.) Have you forgotten about the basic economic gains from international trade? (Answer: No.) Are you advocating some form of protectionism? (Answer: No !) Aren't you giving aid and comfort to the enemies of free trade? (Answer: No, I'm trying to save free trade from itself.)\n\nThe reason for my alleged apostasy is that the nature of international trade is changing before our eyes. We used to think, roughly, that an item was tradable only if it could be put in a box and shipped. That's no longer true. Nowadays, a growing list of services can be zapped across international borders electronically. It's electrons that move, not boxes. We're all familiar with call centers, but electronic service delivery has already extended to computer programming, a variety of engineering services, accounting, security analysis and a lot else. And much more is on the way.\n\nWhy do I say much more? Because two powerful, historical forces are driving these changes, and both are virtually certain to grow stronger over time.\n\nThe first is technology, especially information and communications technology, which has been improving at an astonishing pace in recent decades. As the technology advances, the quality of now-familiar modes of communication (such as telephones, videoconferencing and the Internet) will improve, and entirely new forms of communication may be invented. One clear implication of the upward march of technology is that a widening array of services will become deliverable electronically from afar. And it's not just low-skill services such as key punching, transcription and telemarketing. It's also high-skill services such as radiology, architecture and engineering -- maybe even college teaching.\n\nThe second driver is the entry of about 1.5 billion \"new\" workers into the world economy. These folks aren't new to the world, of course. But they live in places such as China, India and the former Soviet bloc -- countries that used to stand outside the world economy. For those who say, \"Sure, but most of them are low-skilled workers,\" I have two answers. First, even a small percentage of 1.5 billion people is a lot of folks. And second, India and China will certainly educate hundreds of millions more in the coming decades. So there will be a lot of willing and able people available to do the jobs that technology will move offshore.\n\nLooking at these two historic forces from the perspective of the world as a whole, one can only get a warm feeling. Improvements in technology will raise living standards, just as they have since the dawn of the Industrial Revolution. And the availability of millions of new electronically deliverable service jobs in, say, India and China will help alleviate poverty on a mass scale. Offshoring will also reduce costs and boost productivity in the United States. So repeat after me: Globalization is good for the world. Which is where economists usually stop.\n\nAnd where my alleged apostasy starts.\n\nFor these same forces don't look so benign from the viewpoint of an American computer programmer or accountant. They've done what they were told to do: They went to college and prepared for well-paid careers with bountiful employment opportunities. But now their bosses are eyeing legions of well-qualified, English-speaking programmers and accountants in India, for example, who will happily work for a fraction of what Americans earn. Such prospective competition puts a damper on wage increases. And if the jobs do move offshore, displaced American workers may lose not only their jobs but also their pensions and health insurance. These people can be forgiven if they have doubts about the virtues of globalization.\n\nWe economists assure folks that things will be all right in the end. Both Americans and Indians will be better off. I think that's right. The basic principles of free trade that Adam Smith and David Ricardo taught us two centuries ago remain valid today: Just like people, nations benefit by specializing in the tasks they do best and trading with other nations for the rest. There's nothing new here theoretically.\n\nBut I would argue that there's something new about the coming transition to service offshoring. Those two powerful forces mentioned earlier -- technological advancement and the rise of China and India -- suggest that this particular transition will be large, lengthy and painful.\n\nIt's going to be lengthy because the technology for moving information across the world will continue to improve for decades, if not forever. So, for those who earn their living performing tasks that are (or will become) deliverable electronically, this is no fleeting problem."},
{"url": "https://www.youtube.com/watch?v=rRbY3TMUcgQ", "link_title": "Erlang the Movie II: The Sequel [video]", "sentiment": 0.12142857142857143, "text": "The interactive transcript could not be loaded.\n\nRating is available when the video has been rented.\n\nThis feature is not available right now. Please try again later."},
{"url": "https://developer.ibm.com/wasdev/blog/2015/05/20/java-ee-7-in-liberty-so-far/", "link_title": "Java EE 7 in Liberty\u2026 so far\u2026", "sentiment": 0.23928571428571427, "text": "For the past year, we\u2019ve been releasing monthly betas of Liberty so that you can see what\u2019s happening, including our progress with implementing the Java EE 7 spec on Liberty.\n\nWe\u2019re getting close to having the whole Java EE 7 spec in our beta releases now. Thank you everyone who\u2019s downloaded our betas so far and given us feedback to make them better (like this recent review by Arjan Tijms). We\u2019re loving being that bit more open about what we\u2019re working on.\n\nThanks to your feedback on the betas, we\u2019ve been able to incrementally release some of the Java EE 7 features through our more agile delivery process so that you can use them on Liberty in production sooner.\n\nCheck out what we\u2019ve implemented so far of the Java EE 7 technologies in the Knowledge Center."},
{"url": "https://github.com/mdahiemstra/nagios-check-redis", "link_title": "Nagios plugin to monitor redis with Node.js", "sentiment": 0.0, "text": "Simple plugin for nagios to check status or memory usage of a redis server.\n\nNote on units: when memory size is needed, it is possible to specify it in the usual form of 1k 5GB 4M and so forth.\n\nEdit your commands.cfg and add the following:\n\nNow you can monitor redis servers by adding:\n\nAdd the command to your NRPE configuration on your remote host:\n\nMonitor the service on your Nagios host:\n\nP.S. We could provide the arguments from Nagios to the NRPE configuration (with nrpe dont_blame_nrpe set to true) but this is quicker and easier to manage."},
{"url": "https://www.winwithoutpitching.com/are-you-really-needed/", "link_title": "Are You Really Needed?", "sentiment": 0.20923685900820055, "text": "I love you. I\u2019m here to help. Sit in the circle.\n\nThis post\u00a0is one man\u2019s long over-due intervention for the creative professions.\n\nI love my work. I enjoy helping people, particularly other business owners. I find creative\u00a0people stimulating and the business of the creative professions\u00a0fascinating. I get to travel and learn. I\u2019m building a great team of amazing individuals who inspire me. I thrive on the sense of higher purpose that drives it: we are on a mission to change the way creative service are bought and sold the world over.\n\nAnd we\u2019re doing it. We\u2019re doing it\u00a0better than anyone else on the planet, I believe. Our impact keeps\u00a0growing and\u00a0I expect it\u00a0will be felt long after we are all gone.\n\nAll this is incredibly fulfilling, but it didn\u2019t start out that\u00a0way. I didn\u2019t pursue my passion in the beginning \u2013 I was just trying to feed my family. I had uprooted us from the city, moved us to a little mountain village for quality of life reasons and had no idea\u00a0where the money would come from. The final decision was between launching Win Without Pitching or buying the local\u00a0fly fishing shop. It was a close decision that could have gone the other way.\n\nThis work has\u00a0always been interesting to me (or I wouldn\u2019t do it) but it was never \u201cmy passion.\u201d About five years ago I was speaking to a friend from my big agency days\u00a0and he said to me, \u201cYou\u2019re lucky.\u00a0You\u2019re pursuing your passion and you\u2019re doing it from a beautiful place.\u201d My reaction shocked even me. I screwed up my face, a bit horrified, and blurted out, \u201cThis isn\u2019t my passion!\u201d\n\nAgain, I love what I do; I\u2019m driven by the never-ending grand mission and the idea that I can make a lasting impact on a small part of the world, but I started this business because I needed to feed my family. Of the two choices in front of me, I could pursue my passion (fly fishing) or I could chase what I saw as a massive unmet need in a market I understood but was hoping to escape from. I chose to chase the need.\n\nToo many creative professionals\u00a0start creative\u00a0firms for the wrong reason \u2013 a designer\u2019s\u00a0passion for design and his\u00a0desire to design for a living.\n\nWhile passion is a good thing, it alone is not enough rationale to start a business, particularly a business in a field that is as crowded as design. (I use design here as a surrogate for advertising and other creative services as well.) This passion is at the root of many poorly-run businesses and poor business practices like free pitching.\n\nI want to pose some questions for you to consider before I begin to address them:\n\n1. Why did you start your firm?\n\n2. You\u2019re in the marketing business but are you a marketer?\n\n3. Should you really be in business at all?\n\nI believe there are two reasons most people\u00a0go to work in the morning: fun and money. Now ask yourself, when you started your firm was your primary motivation the fun or the money? Creative firms are almost always started in pursuit of the fun. It\u2019s the thrill, the passion, the creating and the adrenaline rush of presenting. You might go years sacrificing money for fun, telling yourself and your employees that to do so is an investment in the future. You might take work that you shouldn\u2019t, because \u201cit\u2019s good for the portfolio.\u201d You\u2019ll stay up all night crafting ideas that you might pitch for free in the morning because it\u2019s a \u201conce-in-a-lifetime opportunity\u201d. Then you\u2019ll do it again the next week.\n\nAnd in the beginning it is fun. You\u2019re in business for yourself, eating what you kill. You\u2019re a professional artist and people are validating your artistry by actually paying for your work. You\u2019ve got a cool office, a thin phone and great eyeglasses. Life is like a rock \u2018n roll tour bus: everyone is young and good looking and staying up late working on the latest project, sure that it will be a hit. There\u2019s a combined sense of creating something that\u2019s bigger than all of you. The music is always on, there\u2019s always beer in the fridge and sometimes there\u2019s even sex in the washroom.\n\nAnd then, just like in all the rock n\u2019 roll movies, you wake up after awhile \u2013 somewhere before ten years if you\u2019re lucky, maybe as late as 25 years if you\u2019re not \u2013 and realize it\u2019s not fun anymore. You face the fact that there is no money. The people around you that were once part of the fun are now a burden, dragging you down creatively and financially. Now you want nothing more than to get off that bus before you end up looking like Keith Richards. You\u2019re tired of having fun and you realize that what you really want is to make money.\n\nLet me tell you about my dentist. I live in a village of one thousand people in the middle of nowhere. It\u2019s on the shore of a 92-mile long lake set between two mountain ranges in the interior of British Columbia somewhere between the Rocky Mountains and the Pacific Ocean \u2013 a short nine-hour drive from Vancouver. This remote village is filled with people who have no business being here. In this place we all ask each other the question, what are you doing here? We\u2019re all doing the same thing \u2013 living a certain lifestyle trying to obtain a higher quality of life, but the question that immediately follows is, How did you get here?\n\nMy dentist is great at what he does and he clearly loves his work. When I asked him for his story \u2013 how did you get here \u2013 he told me, \u201cWell, I fell in love with this place and wanted to live here, but there was no work. So I looked around and asked, \u2018what does this place need?\u2019 It didn\u2019t have a dentist so I went to dentistry school and then started this practice.\u201d\n\n\u201cYou mean you weren\u2019t a dentist first \u2013 you weren\u2019t passionate about dentistry,\u201d I asked?\n\nI was stunned. \u201cIt must have taken you a while to retrain as a dentist?\u201d\n\nThere are two fundamental perspectives on business: a product-based perspective, and a market-based perspective. Producers try to sell what they know how to build. Marketers build what they determine they can sell.\n\nThere are many definitions of marketing, but the one that rings truest to me is this: \u201cIdentifying a need in the marketplace and matching a product or service to that need at a profit.\u201d\n\nThis definition clearly describes an assessment of market need before the question of product or supply is even broached.\n\nThe world is filled with design firms that were founded by people who know how to, and have a passion for, design, regardless of the market\u2019s need for more graphic or web design services. It is these firms \u2013 the producers who produce what so many others produce \u2013 that cannot Win Without Pitching.\n\nMaking the transition from a pitch-based agency to a Win Without Pitching firm\u00a0begins with a reversal of perspective \u2013 looking to the marketplace and posing the dentist\u2019s question: what\u2019s missing? There are an almost infinite numbers\u00a0of answers to that question, but another web design firm or another full-service advertising agency are\u00a0almost certainly\u00a0not among them.\n\nOne of the reasons a client can ask a creative firm to solve their problem as proof of the firm\u2019s ability to solve their problem is the client has the power in the buy-sell relationship. The power is rooted in the availability of substitutes \u2013 the alternatives available to hiring the firm. Winning without pitching begins by altering the power structure in the buy-sell relationship through reducing the number of legitimate alternatives to the firm\u2019s offering.\n\nThis is the power of a market-based perspective. Marketers intentionally build businesses and brands for the expressed purpose of meeting unmet or poorly-met need. As a result, they deal from a position of strength which allows them to control the buy-sell process and command a price premium.\n\nProducers, on the other hand, love to produce, so they do so and hope they can find someone to purchase what they make. They find themselves at the opposite end of the power spectrum from marketers, dealing with price pressures and forces that increasingly commoditize their offerings.\n\nIn every speech I deliver there is a certain percentage of the audience that cannot even imagine what it\u2019s like to have the marketer\u2019s power in the buy-sell relationship. To them, winning without pitching seems impossible. These are the producers, and usually they are the most passionate about their craft.\n\nIt is not the primary purpose of your business to make money, but it must make money. Consider profit the primary by-product of your purpose. With no or little profit it does not make sense to frame your purpose as a commercial enterprise. Remember the last part of our definition of marketing: \u201c\u2026at a profit.\u201d\n\nDo you subvert profitability for fun, consciously or otherwise? Do you, years into your enterprise, still only earn what you might as someone else\u2019s employee \u2013 but with the added uncompensated risk of having your own employees? Do you rationalize a lack of financial reward by telling yourself how much fun you\u2019re having or how fulfilled you are creatively?\n\nPassion for design is a wonderful thing, and properly harnessed it can be a competitive advantage. Passion alone is not enough reason to start a business. This passion to produce without regard for the market\u2019s needs has created a glut of undifferentiated advertising and design that is driving the global free pitching problem.\n\nMy dentist would tell you that his passion is hiking, fishing and other outdoor pursuits, and his commercially-framed purpose, some variation of helping people. He built a lucrative business of purpose that allows him to pursue his passion (that is why he loves what he does), but it was a business for which he made sure there was a need.\n\nMy own passion, at the time I made the decision to start this business, was fly fishing. In the end I choose not to make fly fishing\u00a0my business, instead going where I saw the larger opportunity.\n\nI ask again, is there really a need for what you do? If your firm disappeared today, what meaningful consequences would your market suffer? Would you really be missed, or would\u00a0other, similar firms seamlessly step in and\u00a0fill the void?\n\nIf your answers to these questions leave you deflated then congratulations \u2013 you\u2019ve just come face to face with the reason why new business development is so hard, why financial reward is so elusive and why it feels like you\u2019ll never be free of the pitch.\n\nThe real journey begins right here."},
{"url": "http://dev.theladders.com/2015/05/design-principles-and-goals-being-expressive-in-code/", "link_title": "Being Expressive in Code", "sentiment": 0.18638740542946153, "text": "Welcome back to the last post in our series on our design goals and principles. We\u2019ve talked about our High Level Architecture and Our Trip Back to OO, and now we\u2019re going to dig into how we write and compose our code, and how we try to be expressive in what our code says.\n\nYou should have to think hard to write code, not read it. One of the things we focus on a lot is how the code reads. Is the code full of programmer terms like \u2018build\u2019 and \u2018create\u2019 or does it read like a fluent paragraph? Some might consider it a small detail and fluffy, but it can directly impact how maintainable code is. A lot of times we write code in ways that only programmers understand, and while we speak the language, it can still add a small mental overhead that really isn\u2019t needed.\n\nJavaBeans is great as a naming convention if you\u2019re relying on reflective frameworks to read properties out of data structures, but inappropriate for true domain models.\n\nEven the small things \u2013 this isn\u2019t a direct example, but similar to what we run into a lot. Why do we have to write:\n\nWhen you read it out loud, you get \u201creturn build Jobseeker premium Subscription get status.\u201d Huh? Why can\u2019t we write:\n\nWhen you read that out loud, you get \u201creturn Jobseeker with premium Subscription status.\u201d Makes a little more sense \u2013 and anyone, developer or not, should be able to understand it just by reading words.\n\nWe do this sort of thing on both a small and large scale \u2013 we name things for their calling context and intent. We might rename something just so it reads better in the code that is calling it. It\u2019s important to note that we\u2019re talking about our end application \u2013 code that one project owns and consumes \u2013 so we can make assumptions about how the code will be called and see it easily. The same ideas don\u2019t necessarily apply to things like reusable libraries.\n\nSome other examples from our code:\n\nThere are a lot of times we have to do things that are \u201cmechanical\u201d that don\u2019t really have any inherent meaning to the task at hand, but are really just supporting code. Things like constructing specific Date instances, interfacing with other library/framework components, or looping over collections. This supporting code serves a purpose, but lacing it within the more expressive code of business logic only serves as clutter. Separating mechanical code from business code leaves the important code cleaner and simpler, highlighting what\u2019s really happening.\n\nA real world example for retrieving the jobs that a Jobseeker has applied to:\n\nIn 5 seconds \u2013 can you tell me what\u2019s going on? Probably not \u2013 there are a lot of mechanics clouding the way. We have some meaningful things buried in anonymous functions and a lot of lines that are just compiler syntax. We make a pretty liberal use of Google\u2019s Guava library to implement functions and transforms, but when left to roam free in our application code they can wreak havoc. With a few small changes: the addition of a ProcessedApplications wrapper class and a class just to create our Guava transform from ProcessedApplication to Job, we can make our high level application code much more expressive.\n\nWhat we ended up with:\n\nNow all the mechanics of looping over applications and creating Guava functions are hidden away in lower components, and we\u2019re left with the quick makings of a sentence, \u201cReturn applications that are successful, transformed to jobs, to representations for the jobseeker.\u201d We could probably have gone one step further to:\n\n\u201cReturn applications that are successful, transformed to job representations for the jobseeker\u201d\n\nOne of the things I like to do when looking at code is to ask where I would expect to find something. \u201cWhat happens when ABC does XYZ?\u201d \u201cWhere do I start looking for code related to ABC?\u201d A lot of times these are questions you either have to already know the answer to, hunt for, or ask someone who knows the system. We\u2019re gunning for a better answer \u2013 start by looking for class ABC and a method called XYZ.\n\nSo asking the questions \u2013 \u201cWhat happens when a Jobseeker upgrades\u201d and \u201cWhere do I start looking for code related to upgrades?\u201d Did anyone immediately say to themselves, \u201cwell it would be in UpgradeService or UpgradeManager?\u201d It\u2019s a common paradigm, and one we\u2019ve used extensively in the past.\n\nSometimes we have to take a step back and think about how we\u2019re modeling and interacting. An example is our transition from a stateless service for completing upgrades to domain models that complete the upgrade and reflect a language we can relate to. Originally, there was an UpgradeService that had a method accepting a Jobseeker, ProductOption, and PaymentMethod. It then went on to organize the interactions of these together to complete the upgrade. This left the higher level application(client of the domain) code less expressive.\n\nAfter some refactoring to allow the models to complete the work on their own, we were still left with a question \u2013 what does the interaction look like?\n\nTaking a step back, the important question is \u2013 how would we have a conversation about this? How would a Product Manager explain this use case? They would probably say \u201cthe Jobseeker is upgrading to the selected product option, and paying by credit card.\u201d So that\u2019s what the code looks like \u2013 a sentence (or close to it) that says just that. So in the end, the application code looks like:\n\nupgradingTo returns an Upgrade with a payBy method that has meaningful steps within it:\n\nSo back to our questions:\n\nWhat happens when a Jobseeker upgrades? Open the Jobseeker class and find the method for upgrading.\n\nWhere do I start looking for code related to upgrades?\u201d Open the Upgrade class.\n\nRather than passing around Collection , we frequently create meaningful classes to encapsulate the collection. This provides a natural home for many operations like sorting, transforming, filtering, etc. This reduces clutter in calling code and makes it more expressive (noticing a pattern yet?), hiding away the mechanical details of iterating over collections in order to do something.\n\nIf we take a look at an earlier example, we have a Users class that handles things like filtering and sorting:\n\nsome methods from our Users class:\n\nto do the same thing with regular collections, our calling code would probably look something more like:\n\nand who wants to read that?? By using first class collections, we can write more declarative high level code that almost reads like English.\n\nCode that reads well and overall quality are things that we highly value. We want our code to express itself to the next developer that has to come and make changes \u2013 and that\u2019s at the heart of our principles: change. If code never changed, we wouldn\u2019t need any of these values. But that\u2019s the point of software \u2013 it\u2019s soft. And that\u2019s what this series is really about \u2013 high level architecture, OO principles, expressive code \u2013 all working together to embrace change and make it easier, because that\u2019s where the value really is."},
{"url": "https://github.com/jazzychad/GMail-Intro-BCC-Button", "link_title": "Gmail Intro BCC Button Made with InboxSDK", "sentiment": 0.10000000000000002, "text": "This is an InboxSDK app for GMail which adds an Intro BCC button to the reply composer. This is useful for when you receive an introduction and want to reply to the 3rd party directly and move the mutual 2nd party to BCC (this is a common behavior when replying to an intro).\n\nThis app comes in the form of a Chrome Extension which you can enable by opening and making sure is checked. Then you can from the cloned repo directory.\n\nSee this tweet for screenshots."},
{"url": "http://www.businessinsider.com/csc-sues-eric-pulier-for-alleged-fraud-2015-5", "link_title": "CSC sues Eric Pulier for alleged fraud", "sentiment": 0.044839208410636995, "text": "These are strange times for the 56-year-old IT giant, Computer Sciences Corp.\n\nCSC has filed a lawsuit accusing\u00a0one of its star executives, Eric Pulier, of fraud connected to an alleged bribery scandal in Australia.\n\nCSC is accusing Pulier \u2014 former CEO of ServiceMesh, which CSC bought \u2014 of making \"unauthorized payments\" to two Australian bank executives.\n\nThe company is seeking to recoup the full $98 million it paid to ServiceMesh shareholders, according to CSC's statement sent to Business Insider.\u00a0 \n\n\n\nPulier is apparently denying the allegations, according to information included in CSC's lawsuit; we have not been able to contact him.\n\nPulier joined CSC to lead its cloud-computing efforts after CSC bought a company he cofounded, ServiceMesh, for over $260 million in 2013.\n\nLast week, CSC filed a lawsuit against Pulier seeking to get a good chunk of its money back.\n\nPulier resigned in late April after allegations of bribery surfaced. He quit right before CSC was about to fire him, a CSC spokesperson told Business Insider.\n\n\n\nThe CSC lawsuit accuses Pulier of making illicit payments to IT executives at an Australian bank, Commonwealth Bank of Australia, which signed multiple big contracts with ServiceMesh.\n\nThis contract helped Pulier earn an extra $25 million equity \"earnout\" bonus from CSC, after it bought the company, the lawsuit says.\n\nAll told, CSC paid $47 million cash to Pulier, the largest shareholder of ServiceMesh, and another $26 million granted as restricted-stock units.\n\nCSC paid over $260 million to buy ServiceMesh, including $93.1 million in cash, it said in the lawsuit.\n\nCSC agreed to pay more than the original cash price if ServiceMesh brought in more than $20 million of revenue within a few months after the deal closed. Specifically, it offered to pay more than $10 for every additional $1 of extra business brought in, up to a max of $33.5 million, explains the lawsuit.\n\nAn additional $12 million of revenue in that period came from a bunch of contracts from the Australian bank, according to the lawsuit. With those contracts, the lawsuit says, the $20 million floor was exceeded by nearly $9.7 million. So CSC shelled out another $98 million-plus to ServiceMesh equity holders. \"That included a $25.3 million+ bonus to Pulier,\" CSC says in its lawsuit.\n\nAfter Pulier received the $25 million, he founded a company called Ace, which paid more than $2 million to the two senior IT executives at the bank, the lawsuit claims. The two IT executives at the Australian bank have been arrested on charges of commercial bribery, according to the lawsuit. They are pleading innocent.\n\nAlthough Pulier could not be reached for comment, Pulier's lawyers have accused the banks of \"making false statements about him,\" according to the CSC lawsuit.\n\nCSC says that it is cooperating with all authorities and says that Pulier refused to fully cooperate with its internal investigation.\n\nTo be clear: Although there have been arrests, a lawsuit, and plenty of accusations, no one has yet been convicted of any crimes. Pulier has only been hit with civil litigation and has not even been charged with a crime.\n\nMeanwhile, Pulier is difficult to locate. His Twitter account is gone. Neither his address nor his legal representatives were named on the lawsuit CSC filed. (We reached out to several people who know him, including others from ServiceMesh, and will update this post when we hear back.)\n\nPrior to this scandal, Pulier was a respected software exec in his field; he founded several companies, including Akana (which just changed its name to SOA Software).\n\nPulier was chairman of SOA. Akana no longer lists him on its leadership page, although it also doesn't say that someone else is serving as chairman. (We repeatedly contacted Akana asking for comment and will update if we hear back.)\n\nPulier also cofounded several other bubble-era companies: Interactive Video Technology, Desktone, Digital Evolution (bought by US Interactive).\n\nHe was an adviser to Al Gore and a contributor to Bill Clinton's Clinton Global Initiative. And he's been a board member to a long list of other tech companies.\n\nThis not a happy situation for CSC and its CEO, Mike Lawrie. Like other old-school IT companies such as IBM and Oracle, Lawrie has been trying to transform CSC into a cloud-computing powerhouse and Pulier, along with ServiceMesh, was key to his plans.\n\nCSC has been on shaky ground in recent years, with revenue shrinking, as companies increasingly turn to cloud computing, instead of old-school data-center tech.\n\nUnder pressure to turn CSC around, Lawrie and team just this week announced they were going the Hewlett-Packard route and would split the venerable old company into two publicly traded companies.\n\nMeanwhile, prior to this scandal, Pulier had been big on the speaker circuit, representing the new CSC. Here's just one tweet from a talk he gave last year.\n\nHere's the full statement sent to us by CSC:\n\nOn May 11, 2015, CSC sued Eric Pulier, the former CEO of ServiceMesh, who resigned from CSC in late April shortly before his employment was to be terminated by CSC due to violations of CSC's code of business conduct related to conflicts of interests and appearance of improprieties. The lawsuit concerns unauthorized payments Mr. Pulier made to two executives of Commonwealth Bank of Australia, a CSC client, shortly after he received tens of millions of dollars from CSC's 2013 acquisition of ServiceMesh.\n\nCSC\u2019s lawsuit seeks to recover the damages caused by Mr. Pulier's fraud, breach of contract, and breach of fiduciary duties, including recovery of all payments to Mr. Pulier under the acquisition agreement and the full amount of the $98 million earnout payment paid to ServiceMesh equityholders under the acquisition agreement. If not for revenue from contracts between ServiceMesh and Commonwealth Bank of Australia, Mr. Pulier and the other ServiceMesh equityholders would have received no earnout payment at all.\n\nCSC's investigation into the Australian authorities' allegations of bribery involving Mr. Pulier and the ACE Foundation is continuing and CSC is cooperating with all relevant authorities."},
{"url": "http://qz.com/409878/the-us-has-space-experts-worried-about-an-extra-terrestrial-land-grab/?utm_source=nextdraft&utm_medium=iosapp", "link_title": "The US has space experts worried about an extra-terrestrial land grab", "sentiment": -0.010464061800268697, "text": "Plans to make money in space are\u00a0missing one of the fundamental ingredients to any business: property rights.\n\nIf you go mine an asteroid, as several companies plan to do, and bring some minerals back to earth, can you sell them? If you build a moonbase, as entrepreneur Robert Bigelow is contemplating, and someone else wants to land a rocket there, what\u2019s to stop them?\n\nAsteroid miners eager to raise funds to raid space rocks\u2014some of which are packed with minerals\u00a0valued in the trillions of dollars\u2014are faced with a legal code that was never meant to apply to\u00a0private enterprise in space, since it was written well before it took anything less than the resources of a national government to get to orbit.\n\nThe 1967 Outer Space Treaty, for example, was designed largely to keep nuclear weapons out of orbit at the height of the Cold War and deter a potential \u201ccolonial competition\u201d in outer space. When it comes to property, it simply says that nations cannot claim sovereign territory in space, and that they\u00a0are responsible for any space activities by their nationals.\u00a0The last time the world attempted to re-write space law, with the Moon Treaty of the 1970s, the negotiators came up with a document that banned private property and sovereign claims on celestial bodies. But no country that could put people in space signed on. (Eleven non-space-faring countries did, though.)\n\nNow, with private companies routinely re-supplying the International Space Station and vying to put a manned mission in orbit by the end of 2018, there\u2019s a clamoring for clarity\u2014especially among asteroid companies, which have been pressuring the US Congress for help.\n\nThis week the House of Representatives took a step forward, passing a commercial\u00a0space bill designed to clear a path for these businesses. Among other things, it would\u00a0delay any airline-style regulation of space tourism until at least 2025, provide liability protection for some launches, and streamline permitting processes.\n\nIf enacted, it also would create new commercial property rights in space\u00a0that have some space lawyers\u2014yep, they exist, in small quantities\u2014worried about a major backlash.\n\nThe legislation recognizes two specific kinds of property rights, both arguably within the rules of the 1967 Outer Space\u00a0Treaty: First, if you can obtain a resource, it\u2019s yours\u2014akin to the moon rocks brought back by astronauts or meteorites you can buy on E-Bay. Second, if you\u2019re doing something in space, someone can\u2019t just come and interfere with what you\u2019re up to.\n\n\u201cWhat the bill tries to do is say that the US will recognize claims of property rights of US citizens who go out and mine asteroids,\u201d space lawyer James Dunstan explains. \u201cThat\u2019s the goal, to say, if you expend the resources and go do it, and bring that stuff back, we will agree\u2014recognize it\u2014as your property.\u201d\n\nBut he and others fear that because the bill doesn\u2019t contemplate a method to resolve competing claims from representatives of different nations, it could spur a new, ugly space race based on territory grabs\u2014or set back efforts for property rights.\n\n\u201cIf the US approaches this in a way that seems like cowboy imperialism, you could either revive interest in the Moon Treaty,\u201d\u2014which, reminder, bans private property in space\u2014\u201dor just spur a very narrow reading of the outer space treaty,\u201d says Berin Szoka, head of the libertarian technology policy think tank TechFreedom. \u201cEither case could end up setting the cause of private property rights back significantly.\u201d\n\nBoth\u00a0Szoka and Dunstan support property rights in space but say the bill needs\u00a0modification before it goes forward.\n\nThey argue that the rushed process to write this bill\u2014there wasn\u2019t even a public hearing about it\u2014could lead to unexpected and detrimental precedents. For instance,\u00a0this bill is limited\u00a0to asteroids, but that might not stop other countries from leveraging it with other celestial bodies, like the moon.\n\n\u201cChina could apply that precedent. \u2018We\u2019ll pass our own law for all space resources,\u2019 China might say, something like the\u00a0200-km [exclusive economic zone] that Bigelow has been asking for,\u201d Szoka\u00a0says.\u00a0\u201cIf you took it to that\u00a0extreme, China could land first on the pole of the moon and claim non-interference rights for the entire pole\u2014all of the polar ice.\u201d\n\nThe potential for scenarios like these, Dunstan says, could lead US president Barack Obama\u2019s diplomatic and national security advisers to recommend that he veto this bill, should it reach his desk. Both Dunstan and Szoka are hopeful that when the Senate considers its version of the bill, it will ask the White House to propose an international settlement mechanism akin to what is used to recognize\u00a0deep seabed mining claims without claiming national sovereignty.\n\nOtherwise, they fear that space businesses beyond tourism may be dead in the water.\n\n\u201cPrivate companies can be a spur\u2014this would not be happening if there weren\u2019t two real companies behind it\u2014but they are not going out and doing it and hoping the law catches up,\u201d Dunstan says. \u201cThey are pushing Congress to act so they know they are protected.\u201d"},
{"url": "http://rauchg.com/slackin", "link_title": "Slackin. A little server that enables public access to a Slack server", "sentiment": 0.18047758692919985, "text": "I registered the Socket.IO channel on Freenode right after I started the project. Even though it was super easy to start, we still had a major problem: onboarding new people to IRC.\n\nIt\u2019s hard to create a welcoming community around something fundamentally difficult to use. It lacks basic features like the ability to receive messages offline without setting up a server.\n\nOne of the best decisions for the project was starting a public Slack community around it instead. For the uninitiated, it\u2019s a cloud chat service with first-class mobile support and a polished mobile and desktop experience, with lots of great features.\n\nI wrote a quick integration between our website and Slack to show how many users we had online, and to automate the process of inviting new ones.\n\nIn less than two weeks, we already had a community of more than 400 users. Not only did it dramatically outpace the growth rate of our IRC server, but it fostered a much stronger sense of community. I\u2019ve already seen a great exchange of technical knowledge, new ideas and even business opportunities for those involved.\n\nA lot of people reached out to ask how we\u2019d managed the integration, so I\u2019m open sourcing a little project that will help you do it efficiently.\n\nIt\u2019s called slackin and it will do three things for you:\n\nA few technical highlights of the project:\n\nThe presence counts update in realtime. It sets clear expectations of the level of activity users are going to find upon joining your channel(s). The badge uses an interesting technique I first saw implemented by Facebook for their \u201cMessage\u201d widget button. It consists of a script that injects an iframe, and communicates with it to detect interaction. When clicked, it injects another iframe for the inline dialog and positions it accordingly. This guarantees near-perfect isolation of style and behavior across all browsers. Embedding the badge then becomes as easy as copy-pasting the script: It\u2019s entirely written in ES6, with the help of 6to5."},
{"url": "http://nymag.com/scienceofus/2015/05/what-motivates-extreme-athletes.html", "link_title": "What Motivates Extreme Athletes to Take Huge Risks?", "sentiment": 0.04373544392115821, "text": "Last weekend, famed extreme athlete Dean Potter, along with fellow climber Graham Hunt, died in a BASE jumping accident\u00a0as the pair attempted a wingsuit jump off Taft Point, an overlook that towers over Yosemite Valley. And on Monday, a YouTube video surfaced of a 73-year-old BASE jumper named James E. Hickey, who died\u00a0earlier this month\u00a0after setting his parachute on fire, in what was supposed to be a stunt. These three deaths seem to confirm the inherently tragic implications of\u00a0the extreme athlete's impulsive, thrill-seeking personality -- what else could explain a hobby that involves diving 7,500 feet off a cliff (with or without a flaming parachute)?\n\nThese are, after all, truly dangerous pursuits. Wingsuit jumping, for the uninitiated, is a little like transforming yourself into a human flying squirrel: the suit has parachute-like flaps of fabric under the arms and between the legs that allow the wearer to \u201cfly\u201d along with the wind. It\u2019s a form of BASE jumping \u2014 BASE standing for building, antenna, span and Earth (as in, the broad categories of very high things from which one can jump) \u2014 which is in itself incredibly risky, to phrase it mildly. One recent University of Colorado School of Medicine\u2013led study found, for example, that 72 percent of the 106 BASE jumpers interviewed had witnessed the death or catastrophic injury of a fellow jumper.\n\nSo it makes sense to think that people who engage in these activities are taking foolish risks purely for the exhilaration of it all. (Potter himself lost Clif\u2019s sponsorship\u00a0last year after a documentary showed him taking chances the company wasn\u2019t comfortable with.) But this isn\u2019t an accurate depiction of the individuals Eric Brymer, a psychologist at Queensland University of Technology in Brisbane, Australia, has encountered in more than a decade of studying experienced extreme athletes. On the contrary, Brymer said his work has suggested that many extreme athletes are the opposite of impulsive; not only are they careful and thoughtful planners, but they actually avoid thrill-seekers \u201clike the plague,\u201d he said.\n\nWhen he began his work, he explained, most of the scientific literature on psychology and extreme sports linked the activity to a certain set of characteristics, \u201cand not very good ones at that \u2014 thrill-seeking, hedonism, that they were doing this because they liked risk.\u201d And yet none of these things accurately described the people he\u2019d met at the outdoor adventure company where he worked while in grad school. As he looked into it further, he found that the bulk of the research up to that point had been done on teenagers and young adults, who tend to be high in impulsivity and poor decision-making anyway.\n\nBut when he conducted research specifically on experienced extreme-sports enthusiasts, he found little evidence that participants are reckless, or have some kind of Freudian death wish. Instead, Brymer has found that \u201colder\u201d extreme athletes \u2014 as in, those who are past their mid-20s \u2014 exercise deep care in equal proportion to the high risk involved. \u201cA lot of these people are highly intelligent people, methodological and systematical,\u201d Brymer said. Those he\u2019s interviewed don\u2019t take one spontaneous trip to REI and then sail off a cliff; rather, they spend years studying the environment and the mechanics of, for example, parachutes, before taking any action, \u201cin order to make it as safe as it possibly can be.\u201d\n\nIf the approach is more thoughtful for these athletes than the rest of us might suspect, so are the motivations that drive them to extreme sports in the first place. They\u2019re not just seeking an adrenaline rush, he said: rather, what keeps many of them coming back is something akin to the flowlike state achieved through mindful meditation, one in which \u201cyou\u2019re so in the moment that everything else drops away,\u201d Brymer said. \u201cYou\u2019re focused on the here and now.\u201d\n\nPotter once described it this way to ESPN: \u201cMy vision is sharper, and I\u2019m more sensitive to sounds, my sense of balance and the beauty all around me. \u2026 Something sparkles in my mind, and then nothing else in life matters,\u201d he said. Athletes interviewed by Brymer have expressed similar sentiments. \u201cThe activity itself enables experiences that are beyond the everyday,\u201d he said. \u201cPeople talk about their senses being alive, about being able to see things much more clearly. It gives them a glimpse of what it means to be human as in the capacities they have that we don\u2019t tap into in everyday life.\u201d\n\nAnother common misconception about extreme athletes is that they must have a weaker fear response than the rest of us, who might feel woozy just watching a video of Potter slack-lining in a Yosemite mountain range. \u201cPeople assume because you\u2019re doing things like base jumping, you have no fear,\u201d Brymer said. \u201cIn reality, fear is an important part of the experience.\u201d It isn\u2019t about the absence of fear, or ignoring it when the feeling does creep in \u2014 rather, it\u2019s about learning to use that feeling.\n\nPeople tend to divide emotions into \u201cgood\u201d and \u201cbad,\u201d and the unpleasant anxiety of fear means it gets placed in the \u201cbad\u201d category. But that\u2019s probably not the best way to think about the feeling. Fear wakes you up, making you more alert to the potential threats or things that could go wrong \u2014 all things that are very useful in a potentially dangerous situation. (Brymer has interviewed BASE jumpers who say they don\u2019t like to jump with people who aren\u2019t afraid.) If, when standing at the edge of a cliff, the jumper gets a little scared, this becomes a time to check in with the prep work: their physical readiness, the environmental conditions, the gear itself. If something isn\u2019t quite right \u2014 if the wind isn\u2019t blowing correctly, for instance \u2014 the seasoned extreme athlete will pack it in and come back another time.\n\nBut if, after ticking through that mental list, everything checks out, it\u2019s time to push past that fear. \u201cThere seems to be a link between that experience of fear and being able to move through it with the proper knowledge and expertise and training,\u201d Brymer said. \u201cInstead of fear stopping them, it gets turned into this way of saying, Okay, I need to really pay attention and be serious here.\u201d The presence of fear is, counterintuitively, what ultimately gives them \u201cthe ability to move through fear \u2026 it\u2019s part of what allows them to have these experiences.\u201d"},
{"url": "http://www.spinellis.gr/blog/20150522/index.html", "link_title": "Grady Booch on the Future in Software Engineering", "sentiment": 0.16769090909090906, "text": "Grady Booch on the Future in Software Engineering I was priviledged to hear Grady Booch deliver a keynote on the Future in Software Engineering. Here are my notes of some important statements and interesting soundbytes. Software is an invisible thread, and hardware is the loom on which we weave the fabric of computing. Developing software is like making love: there are mechanics involved, but in the end its key element is love. (Regarding the use of computers at wartime): Computing is woven on the loom of sorrow. The entire history of software engineering is a rise in the levels of abstraction. Early structured methodologies were very waterfallish, because the risks of failure were very high. The core system in the US IRS is written in IBM 360 assembly language. Old code never dies: you have to kill it. My manager has the illusion of managing me, and I accept the illusion I can be managed. Every line of code you write has human and ethical implications, for we are changing the world we live in. It's easy to apply agile in small teams, but difficult to teams of teams. (Inside a GE MRI scanner): Oh my God I know the people who wrote the software for this! I'm really happy they used UML and formal methods. The fundamental premise of science is that the cosmos is understandable; the fundamental premise of our domain is that the cosmos is computable. Grady continued by offering some important challenges for our field. There is no really need for new programming languages, apart from those that nonprogrammers can use. What can we do to better assist the work of teams? What are the best practices for systems of systems whose components are guaranteed unreliable and untrustworthy? What higher-order languages do we need for programming massive neural networks? You think debugging is hard now? Wait till you have to deal with systems that learn. How can we debug such systems? The keynote finished with the following statements and a well-deserved standing ovation. Software is the invisible writing that whispers the stories of possibility to our hardware. It is a privilege to be software developer, because we change the world. It is a responsibility to be software developer, because we change the world. Read and post comments"},
{"url": "http://yec.co/2015/05/yec-member-company-grasshopper-acquired-by-citrix/", "link_title": "Citrix acquires Grasshopper", "sentiment": 0.13538461538461538, "text": "The acquisition allows Grasshopper to offer even more to its users as they grow.\n\nYEC members Siamak Taghaddos and David Hauser announced last week that their company Grasshopper\u00a0has been\u00a0acquired by Citrix. Grasshopper, a Needham, Massachusetts based business, provides call forwarding, answering, voicemail transcription and other services to entrepreneurs and small businesses.\n\nFounded in 2003, Grasshopper\u2019s aim is to help entrepreneurs run their businesses from their cell phones. Citrix, a multinational company that provides SaaS, application\u00a0and other software services, is well known for their work providing small businesses with communication and collaboration solutions.\n\n\u201cWe\u2019re still on a mission to empower entrepreneurs to succeed, and will continue to offer the same great service. By joining forces with Citrix, we\u2019ll now have access to even greater resources and services to help you start, run and grow your businesses,\u201d said Grasshopper.\n\n\u201cWith the acquisition we expand the breadth of our communication and collaboration solutions for small businesses, including GoToMeeting, GoToTraining, GoToWebinar, ShareFile and OpenVoice,\u201d said Citrix Vice President Chris Battles in a release.\n\nThis announcement\u00a0comes at a time when the need for mobile solutions for business communications is at an all time high.\n\nLearn more about this exciting news here."},
{"url": "http://engineroom.ft.com/2014/05/29/big-flashing-devops-thing/", "link_title": "Big Flashing DevOps Thing", "sentiment": 0.17699303158163918, "text": "What you are about to read is a story of DevOps monitoring,\n\n and the solution one brave team found.\n\nAs the FT moves into the brave new world of DevOps a number of challenges have arisen. One of them, the topic of this article, was how best to be kept informed as to the health of our stack. Is everything working, is something working, is anything working? These questions are particularly relevant as we build a new API and the environment and the applications change constantly! \u00a0Keeping track of what is working and what isn\u2019t has proven to be a royal pain!\n\nNagios is a great tool that practically everyone uses in one form or another. But it doesn\u2019t have a common interface across multiple nagios servers. Nagios kindly sends an email every time a check changes state. Emails are a great way to keep in touch with friends and to tell your boss what you think of him or her, but are a terrible method for communicating that your critical application has just crashed.\n\nI imagine most people do exactly what I do; create a google filter to send all Nagios emails straight to the bin. *@ft.com > bin.\n\nOur Nagios servers have been configured to check every important parameter, from basic disk and CPU \u00a0checks to HTTP, application, database and jconsole via Jolokia. All we need is some way to communicate clearly when a check fails.\n\nWe love monitor screens. We have them hanging from the ceiling all over the office. Displaying information. It looks like good information. I have no idea what all that information is.\n\nOur screens have a viewing angle of about 10 degrees. They are small and have low contrast. The screens display too much information and cycle through different pages every 10 seconds or so. It never seems to show the page I want. I\u2019m still waiting for a page to scroll by that informs me if my app is working.\n\nI\u2019m not bashing monitoring screens. They can be and often are extremely useful. It\u2019s just ours are not.\n\nAn Ops Cop is a reaction to the failure of the above two communication methods. An Ops Cop is a member of the team tasked for a week to keep an eye on Nagios emails, Monitor Screens, etc, and respond accordingly. I\u2019m glad I have never been asked to be an Ops Cop; something about this seems wrong.\n\nNone of the above satisfied our needs. Something is missing. When something fails I want an alarm bell, a siren, or a flashing light that is so bright my eyes explode. A warning system that is in everyone\u2019s face. No escape. There should be no excuse for anyone to not know when something in the stack has broken. \u201cWhat do you mean you didn\u2019t know the site was down, there is a mongoose running around the office ! \u201c\n\nWell I did spend my evenings and weekends making this so forgive me the naming it after myself.\n\nI scaled back my plans for a herd of mongoose and a 50 gigawatt light bulb. I bought a BlinkyTape, a\u00a0\u201csuper-cool LED strip with full-color RGB LEDs and an integrated microcontroller\u201d, with 60 independently controllable LEDs.\n\nBought some perspex, some glue, sellotape, nuts and bolts, wired it to a Raspberry Pi. Wrote some bash and Python ( OK, so, after my bad attempts a friend wrote most of the Python, thanks Mark ) . Now cue the music !\n\nA good monitor system should display the health status of the stack to as many people as possible in as simple format as possible. The more people that know the health state of the stack the better chance of someone picking it up and resolving the issue quickly.\n\nSAWS simply shows by grouping LED\u2019s if each nagios server has an error. Green, orange, yellow, red and flashing red LED\u2019s representing OK, Unknown, Warning, Critical or Critical for over 30 minutes. Blue LED\u2019s swoosh back and forth like a Cylon to indicate the python script is running and the data is up to date.\n\nSince installing the SAWS device above the desk of the content team the reaction has been surprising. It\u2019s not an exaggeration to say people love it. The question now is why ?\n\nMaybe because it displays just enough information and nothing more. When a nagios server has an alert the LED\u2019s change to a corresponding colour, it doesn\u2019t tell you what check is failing, just something is failing. It\u2019s bright and colorful like candy and is visible from every corner of the office.\n\nAnother reason why people have taken to it might be because for the first time in human history there is a monitoring system that is fun !\n\n\u201cHey; I think the photos have been edited, I don\u2019t believe it can be that bright?\u201d\n\n \u201cNo, Way\u201d\n\nThe source code can be found on GitHub :\u00a0https://github.com/muce/SAWS"},
{"url": "https://medium.com/synapse/training-for-discontent-42591cf57baf", "link_title": "The Doublespeak of Parenting and the Double Blade of Ambition in Silicon Valley", "sentiment": 0.0857960095460095, "text": "I volunteer at my son\u2019s school in Menlo Park, which means once a month I read stories and talk about feelings with overachieving sixth graders. My son\u2019s not in the class I visit, because I\u2019m here, with Project Cornerstone, to be an additional \u201ccaring adult\u201d for other kids, not a helicoptering mom for Wilson. The stated goal is for me to thwart self-destructive and peer bullying behavior. I\u2019m supposed to encourage positive personal skills and relationships, something the majority of Silicon Valley kids\u200a\u2014\u200aaccording to studies, and despite all their immense privilege\u200a\u2014\u200aaren\u2019t on track to develop.\n\nMenlo Park is an Oz-like bubble of privilege and possibility filled with egg-headed tech wizards, quirky characters (most, despite press about rampant greed and moral bankruptcy, with big brains and big hearts), and shiny red Teslas. We believe in \u201cfree to be you and me,\u201d same-sex marriage, and philanthropy over proselytizing. We also enjoy an ever-present, yellow brick sun. I swim and run outside all year round, and though the startup world can be stressful and a dump of a house costs a million dollars, there is no chance I\u2019ll come down with seasonal affective disorder. \u201cLucky\u201d is the word that is often used to describe us here, and for these and a lot of reasons, we are. My sons play their beloved soccer, and are coached by national stars, every beautiful rainless day of the year. I pay nothing to send them to the public Blue Ribbon School across the street, where each year the parent foundation raises more than $2 million, and the teachers have PhDs, Olympic medals, or spouses who run startups.\n\nYou want your kid to take computer programming or robotics after school with a Stanford-trained engineer? We have that. You want diverse offerings like hip hop dance, lacrosse, origami, orchestra, cooking and Mandarin? We have those too. You want the best shot at today\u2019s upper middle class American dream for your child? This is where you\u2019ll find it. In Menlo Park, an impressive number of local high school students attend Stanford.\n\nNone of the kids I know are being bullied, and Wilson seems well adjusted\u200a\u2014\u200afor a first-born Type A Silicon Valley kid\u200a\u2014\u200aand happy. Thriving, even. But a year ago I was fired from my own startup and I made the somewhat radical decision to drop out and stay home. I felt aimless\u200a\u2014\u200asometimes even worthless\u200a\u2014\u200aand had a lot of time on my hands. I needed a project. Then someone showed me this report, by Stanford Challenge Success, of adolescent experiences at Wilson\u2019s public, average-sized middle school, from a few years back:\n\nStress levels are high for 57% of students. Students experience sleep deprivation and some physical health issues. Students report homework hours above those recommended nationally. 11 students have used prescription or illegal stimulants to help stay up to study. 16 students have used over-the-counter or legal drugs to help stay up to study. 7 students have been so upset or angry that they have cut themselves.\n\nThe truth is I\u2019d never liked the idea of parents working in the classroom after sometime around the fourth grade. It seemed voyeuristic, and in our uber involved district, unnecessary. But after I read the report, I made another radical decision, and started volunteering.\n\nI\u2019m reading the book Today I Feel Silly and Other Moods That Make My Day by Jamie Lee Curtis and I\u2019m aware that it is too childish for sixth graders, so I look for eye rolling and rush through the story. The lesson plan is about how to \u201cturn your frown upside down.\u201d I expect the discussion to be relatively trivial and aloof. There are fifteen tweens siting on the carpet in front of me. Their teacher is out today, and a substitute sits in the back of the room.\n\n\u201cWhat\u2019s a bad day?\u201d I ask the class, when I\u2019m done reading.\n\n\u201cA school day,\u201d Lea, an adorable, slender blonde with bright blue eyes who is wearing the \u201cin\u201d Menlo Park tween uniform of black yoga pants and tan colored UGG boots, says. Lea has always seemed perky and upbeat.\n\nI start to laugh as if Lea\u2019s comment is a joke, but no one else does. Her eyes look watery. The boy beside her nods.\n\nI ask Lea why, and she tells me she is stressed out and unhappy, \u201cevery single day\u201d during the week. She says she skips swim practice, though she hopes to swim in the Olympics someday, because she has her homework and piano lessons, plus studying for tests she has to get A\u2019s on.\n\n\u201cWhy do you have to get A\u2019s?\u201d I ask.\n\n\u201cI have older sisters. One\u2019s a junior in High School, the most important year,\u201d she says, because it is the one that will determine where her sister gets in to college, and, \u201cyou know, what the rest of her life will be like.\u201d\n\nThe kids stare at me blankly as if to say: Duh.\n\n\u201cYah,\u201d Lea says, \u201ceveryone\u2019s always yelling about school at my house. So I know it will only get worse for me.\u201d\n\n\u201cYah,\u201d her friend Jenny says, \u201cit only gets harder.\u201d\n\n\u201cI hear you,\u201d I say to Lea. This is the first time I\u2019ve heard anything like this, first hand. Yes, I know about the stressed teens of others and all those issues in Palo Alto. And yes, I read the report about Wilson\u2019s school. But I figured the cutters were outliers, or at least eighth graders. These are eleven-year-olds I know in Menlo Park. An alarm bell begins to go off in my spine. Is this also my son?\n\n\u201cI can\u2019t take it anymore,\u201d she says. Her voice breaks and I clear my throat.\n\nWhen I look up, ten other hands are raised. I want to call on a boy to make things even, and maybe, though it pisses me off when my friends attribute sentimentality to girls and doggedness to boys, because I suspect it will make the atmosphere less emotional.\n\n\u201cJeremy,\u201d I say to a studious kid. Jeremy has a wry sense of humor, is well liked, and won the science fair last year. \u201cWhat\u2019s a happy day for you?\u201d\n\n\u201cOh.\u201d He seems surprised that I switched tactics. \u201cI was gonna say I agree school days are bad but, um, I guess a happy day is a weekend day?\u201d\n\n\u201cI can play,\u201d he says. \u201cThere\u2019s more time.\u201d\n\n\u201cYou don\u2019t like the weekdays either?\u201d I ask.\n\nHe scans the room for their teacher, and remembers they have a sub. \u201cNot anymore,\u201d he admits. And then more quietly: \u201cI hate them.\u201d\n\n\u201cI\u2019m sorry,\u201d I say. \u201cThey\u2019re stressful?\u201d He nods. \u201cDoes anything help you deal with the stress?\u201d\n\nLots of hands go up. I call on Max.\n\n\u201cTaking a break to text,\u201d he says. \u201cOr look on Instagram.\u201d\n\nMy gut tightens. I have thus far refused to get Wilson a phone. He\u2019s eleven.\n\n\u201cMy buddies. I just say \u2018Hi\u2019, check in. Sometimes send funny emoji.\u201d\n\n\u201cYah,\u201d Mackensie says. \u201cMy friends send me crazy smiley faces. They make me feel better even though sometimes I don\u2019t know what they mean.\u201d\n\nMy God, I think. Have emoji, sent furtively and sometimes without care, come to count as empathy? I suspect they have. I use them too. I start to feel warm, panicky. I haven\u2019t wanted to admit this, but these kids are telling me straight up: the predominant personal skills they need to thrive here are straight A\u2019s and social media etiquette.\n\n\u201cDo you tell your parents, or anyone, when you\u2019re feeling this way?\u201d I ask the class.\n\n\u201cNo,\u201d Jennifer calls out. \u201cI just text, or surf Insta.\u201d Twenty heads nod.\n\n\u201cOkay,\u201d I say. I try to reiterate before posing another question. \u201cIt sounds rough though, guys? A lot of pressure in school, and also when you leave school. What does that feel like?\u201d\n\nA polished and bright student named Stephanie shoots her hand up. I call on her.\n\n\u201cI feel like I want to bash my head in,\u201d she says.\n\nHer classmates nod and I hold my breath for a beat until we all seem to remember the levity of the book\u200a\u2014\u200aToday I Feel Silly!, a young child\u2019s story I have read to a group of eleven-year-olds who are too often treated like little adults\u200a\u2014\u200aand decide as a group to receive this lightheartedly. There\u2019s no other option. There is nervous laughter and someone says yah. Stephanie slouches into the rug and I recognize she is recoiling from the prospect of having this taken too seriously. If she is really feeling this way, will she have to talk to the counselor? Will her mom be called? She lets the conversation move away from her.\n\nThe substitute takes a break from reading at a back desk and looks at me with her eyebrows cocked.\n\n\u201cMan,\u201d I say, \u201cthat sucks.\u201d I get a few smiles because they like it when I speak their language. I sit at the helm trying not to let my mouth hang open. I didn\u2019t realize, I keep thinking. How did I not see?\n\nWhen I leave the classroom I am stressed. Does Wilson want to bash his head in? If he is stressed, is his only outlet texting fat, yellow-faced emoji? It can\u2019t be, because I won\u2019t let him have a phone yet. So then, have I refused him his only escape?\n\nHe\u2019s complained about homework, and I notice it takes an hour or two sometimes, but that\u2019s the extent of it. He does it, and gets A\u2019s. And I admit, I like seeing him work hard.\n\nBut that night my husband, Noah, and I secretly sit down and do a typical night of Wilson\u2019s math homework, to see how long it will take two Stanford engineering grads. The concept is simple for us, and it still takes us thirty-five minutes. We figure it would take a tired, bored tween who understands the concept more than an hour. Wilson has five courses at school that have nightly homework or ongoing projects. I, like Lea, assume his workload will only get heavier.\n\nBefore bed I tell Wilson I am sorry I haven\u2019t been more patient with his meandering homework process. \u201cCome on, Wilson,\u201d I often said. \u201cYou\u2019re wasting your time.\u201d To a certain extent, I was right. He is a sixth grade boy: of course he procrastinates and fumbles with the underutilized iPad the school gave him. Of course he\u2019s tired after two hours of soccer training, and wants to stroll through the kitchen every thirty minutes for a snack. But now that I\u2019ve heard the other kids, I\u2019m repentant. I\u2019m also confused. Yes, Wilson has more homework than I did, and seems to care more about his grades at eleven years old than I did. A lot more. Is that bad, or good? Is it a sign of dangerous unnecessary pressure, or of promising and praiseworthy ambition?\n\nThis is the double blade of parenting. I want Wilson to be driven and confident, to feel alive and ambitious, the way I have felt. I also want him to be content, and satisfied without having to feel \u201cexceptional,\u201d the way I am not able to. This is the double blade of my own ambition. How much does where I live sharpen the blade?\n\nI wonder if my son is being asked to do too much, or if this is how he really wants to be challenged, and needs to be trained. Trained for what: for living here? Why is the entrepreneurial narrative of Silicon Valley so tangled up in depression and suicide?\n\nYes, a handful of teens\u200a\u2014\u200amany of them legacies, like I was\u200a\u2014\u200ago to Stanford. But an astounding percentage of other high school students step forward to their death in front of the high-speed commuter train a few blocks away, in Palo Alto. You\u2019ve heard about them, and all the grieving. Guards in orange vests on \u201csuicide watch\u201d now man the most likely station. \u201cWhat\u2019s their job?\u201d my boys ask when we ramble over the tracks, and I tell them.\n\nYou haven\u2019t heard about the others. Last year, a seventeen-year-old leapt off the Sand Hill Road 280 overpass into oncoming highway traffic a few blocks from our house, and a fifteen-year-old who\u2019d tried to hang himself shared an ER room with Wilson (after he broke both his arms jumping out of a tree on his birthday). Recently three people I know checked in to in-patient treatment programs for depression. Almost every adult I know here, myself included, takes antidepressants, self-medicates with alcohol or Ambien, and/or works or exercises compulsively. I know from experience\u200a\u2014\u200amy own childhood clinical depression and a long line of inherited mental illness\u200a\u2014\u200anot to blame the environment I live in completely. Discontent and suicidal tendency do not spring naturally from rigor and hard work. And yet, there must be some kind of confluence.\n\n\u201cWhy do they kill themselves?\u201d my boys ask when we pass over the highway. I tell them what I know, and remind them that growing up is hard but there is always hope, that feeling bad, just like feeling good, is part of life, and telling us about it, getting help, is part of what they will need to do to survive life here, or anywhere. I don\u2019t actually know for sure about anywhere. I have only lived here.\n\nAnd I was happy\u200a\u2014\u200aif not quite satisfied or fulfilled I was validated daily by achieving highly measurable milestones and winning praise\u200a\u2014\u200awhile I was studying engineering at Stanford, then doing product management at Cisco, and then working night and day to be the admired female tech entrepreneur.\n\nWho or what is to blame for my current discontent, now that I finally have the privilege to do what I really want to do, and for the anxiety of these kids, who are privileged enough to have the opportunity to be anything they choose, but instead want to bash their heads in, or take leaps in front of fast moving trains?\n\nThe next day I email the teacher and tell her what I heard in class.\n\n\u201cYes, sixth grade here is tough,\u201d she says. She is kind, and open to the feedback, but does not seem appalled by the anecdotes I give her. Could it be that she is used to children who feel desperate?\n\nI email all the sixth grade parents I know to see if they\u2019re aware of the stress level. I explain a tiny bit about what I heard, and ask them to reply with, \u201cYes, I agree stress level is high,\u201d or \u201cNo.\u201d Overnight, twenty-five rambling anecdotes flood in.\n\nIn Laura\u2019s son\u2019s math class, the kids correct their homework, and then read their scores aloud. \u201cHe says everyone in class always gets above 90%,\u201d she tells me. \u201cOne day Adam got an 89%. He was ashamed, so he said he forgot to do it, and took a zero.\u201d\n\nKim\u2019s 6th grader\u2019s daily routine is: come home, work on homework until dinner, eat briefly, and then work until bedtime at 9. \u201cThis past week he had tantrums, wanted to break things, and even locked himself in the bathroom at 7:45 AM while the carpool waited outside,\u201d she says. \u201cHe was afraid he wouldn\u2019t do as well as the other kids on the three tests that day.\u201d\n\nRebecca\u2019s eleven-year-old daughter, Danielle, checks School Loop\u200a\u2014\u200aan \u201cachievement management system\u201d that sends a daily email to every student and their parent detailing every grade and assignment in every course they take\u200a\u2014\u200aobsessively. \u201cDani says School Loop is motivating her to get great grades, but I think it is hindering her health.\u201d Rebecca compares her daughter\u2019s monitoring the School Loop account to an anorexic weighing herself.\n\nI need an enemy, stat. I assume it\u2019s the school.\n\nI meet with the administration the following week, after I send the document of parent feedback to them, and include the Stanford report.\n\nThe superintendent, Mrs. Granata, is a practiced politician. She listens patiently, and then reiterates what I\u2019ve said without adding much until the end.\n\n\u201cYes,\u201d she says, \u201csixth grade is tough.\u201d In the group of anecdotes I supply, no one admits to drugs, cutting, or starving yet. Perhaps that\u2019s the problem.\n\n\u201cIt seems more than tough,\u201d I say. \u201cYou read the document, right?\u201d\n\n\u201cYes,\u201d she sighs. \u201cBut for every complaint like yours, we have an equal complaint that we aren\u2019t pushing kids hard enough.\u201d\n\nI\u2019m surprised that this surprises me, but I recover quickly. \u201cReally,\u201d I say. The truth is just a year ago, I probably would have agreed with making my kid work harder, since he seemed to do fine with it, but I was too busy working hard myself to consider his work load. Just a few days ago, I wouldn\u2019t have believed that an entire classroom of kids would tell me they feel like bashing their heads in. \u201cBut what about these kids?\u201d\n\n\u201cLook,\u201d she says, clearly frustrated. \u201cIf it were up to me there\u2019d be zero homework here. I don\u2019t believe in it.\u201d\n\nI\u2019m incredulous, then encouraged. There is a large local movement, spurred by the very type of Stanford research done at our school, for less homework and more family time. We could leverage that.\n\n\u201cBut it\u2019s your school,\u201d I say. A clear policy, a compromise at least, should be easy to implement. \u201cSo then why\u2026?\u201d\n\nHer short laugh sounds bitter. \u201cYou parents would burn me alive.\u201d\n\nHow ridiculous, I think at the time.\n\nI send out all the anonymous anecdotes, a review of my meeting with the administration, and another email to parents asking how we can pressure the school to make changes. Given the angst I\u2019ve heard from the kids and the helplessness I sense from the parents, I expect an outpouring of positive responses, and I do get some. I also get these:\n\n\u201cWait though. Yes, I am concerned for my child\u2019s wellbeing, but I\u2019m not willing to consider dumbing down the curriculum if it threatens to reduce their chances for a top university.\u201d\n\n\u201cWe\u2019re applying to private schools for next year, so she\u2019ll have that golden ticket to the right high school. I need her to be prepared.\u201d\n\nThey come from people I actually like, and respect, and I empathize with what they are saying even as it terrifies me. Wait, I finally start to think, what the fuck is really going on here?\n\nWhen I ask about this, friends in different cities also lament the withering of their children\u2019s spirits due to educational pressures and follow these laments with their own modified plans to achieve success, despite them. Dropping out as a kid, as I have done as an adult, isn\u2019t an option.\n\nI find that in Menlo Park, the stakes are not so different from in Madison, Wisconsin or Baltimore, Maryland. All over America, the race for success and security has escalated. Parents are worried for the future, and teens are being pushed to excel and compete. The contest has been exacerbated and \u201cgreat\u201d is the new \u201cgood.\u201d As I interview more parents, I begin to feel dizzy, like I\u2019m standing inside a carnival house of mirrors. But one mirror is the largest, and most distorted. It is the one directly in front of me. In Menlo Park, the fates are amplified: escalated by our profound privilege and sped up by elite opportunity.\n\nChildren here know they have to have that one brilliant thing\u200a\u2014\u200aa national science award, an already profitable startup, a published book, international humanitarian aid experience, a real shot at the Olympics (an actual medal would be ideal), that kind of thing\u200a\u2014\u200ato set them apart from the whole entire world of other people who are trying to beat them in order to get in to Stanford, or some other top tier elite college. \u201cBut I don\u2019t have that one thing,\u201d my friend recently tearfully told me her twelve-year-old son said to her. \u201cWhat\u2019s my one thing, mom?\u201d This kid speaks three languages and has already traveled more widely than me. More, my friend has never hinted to her son that he needs to go to an elite university. But he lives in Menlo Park, and his dad went to Stanford. His classmates are already applying for and getting in to private middle schools and talking about their \u201cgolden ticket\u201d into the most elite local high schools. The finish line keeps moving.\n\nI tell my sons that no matter what they are exceptional and worthy, and we try to show them this daily. I have been able to afford to stay home to be with my sons after school, to have the extraordinary privilege to show them, at least in deed, that I value being with them\u200a\u2014\u200adowntime\u200a\u2014\u200aover work and achievement. And yet I can\u2019t help but worry sometimes that I am teaching them to be lazy. I can\u2019t help but notice that when I have an unproductive day of writing I often sound more stressed out or angry than I did when I was running a startup, when validation was scripted and easier to come by.\n\nI play soccer and board games after school, go to practices, and we all eat dinner together most every night. I talk and listen. For now, what I hear from them is a lot of ambition and joy. I want to luxuriate in it, and most times I can. But sometimes I wonder: what happened to mine?\n\nWilson decided he\u2019d be going to Stanford, to play on the soccer team, back when he was five. So cute! we always thought. Ambitious! And yes, Noah and I went there, my dad went there, and our family buys season tickets for Stanford football and basketball, we have a combined total of 23 Stanford t-shirts and hats, and Wilson first slept in the Stanford freshman dorms at an overnight soccer camp when he was ten. But somehow, as Noah and I started companies and gathered the family at our alma mater for sporting events, I convinced myself that Wilson\u2019s version of the early decision process was purely of his own doing.\n\nIn the past I was fond of telling people how awful it was to \u201cgroom your child\u201d for an elite university, while I also believed Wilson\u2019s desire was innate. For as long as I can remember, he\u2019s had a plan, and I, (I had thought) did not come up with it. The plan is simple and seems to keep him confident: he doesn\u2019t do religious school or Boy Scouts (neither of which we believe in), music (which would be fine but we didn\u2019t push or require), or secondary sports (which is too bad because we\u2019d also like him to play basketball). He intends to get straight A\u2019s, and master soccer, which Noah and I have both played all our lives, and still play weekly. So far, he\u2019s on target, and has plenty of extra time to hang out with his friends and our family. Good for him! I always thought, even as I rather emptily reminded him that there is life after soccer, and other schools are options too.\n\nI am surprised, then, to realize how thoroughly I\u2019ve engaged in doublespeak.\n\nStanford looms in our lives like a birthright, so close and promising, so heralded and prized, and yet I know that an elite education and a prosperous career don\u2019t guarantee contentment, and in fact it may serve to train one for discontent. But how can I decry what Stanford gave me: an undeniable leg up (at no cost, thanks to a scholarship) toward financial independence and staunch self-reliance? I have been able to choose to drop out of the system only because the system set me up for it financially. Maybe the question is: What, exactly, has it set me up for?\n\nI came to Stanford broke. I have enough money now, and yet somehow, never enough validation.\n\nEnd Note: Wilson is nearing the end of seventh grade now, and I don\u2019t volunteer with Project Cornerstone anymore. Before I stopped, I started a petition telling the administration parents wanted less homework (because this was the easiest stress factor to define), and at least as far as I can tell, they listened. Wilson\u2019s workload has gotten lighter, not heavier. He seems more interested in what they do during school hours, and never complains about homework. He has also decided he doesn\u2019t want to go to Stanford, because it \u201cseems too close to home.\u201d Good for him, I think now. I hope his path will be his own.\n\nI didn\u2019t stop volunteering because I don\u2019t care about those other kids or the ones, like my youngest son, that will come after them. I stopped because I saw that the journey through today\u2019s educational waters are as complicated and individual as every child out there\u200a\u2014\u200athat there is no single external enemy to fight\u200a\u2014\u200aand right now I need to be vigilant about the mental health of myself and my own sons. I saw that I am the one who has the potential to be their, and my own, worst enemy.\n\nNote: the names, and some of the details, of those not in my family in this piece have been changed."},
{"url": "http://www.nature.com/srep/2015/150521/srep10431/full/srep10431.html", "link_title": "Tracing the life story of a Bronze Age female", "sentiment": -0.018953578472809242, "text": "To trace the Egtved Girl\u2019s origin, we performed a strontium isotope analysis of enamel from the left mandibular first molar tooth. With the exception of the third molar, tooth enamel mineralizes during early childhood (e.g. the 1st molar mineralizes between peri-natal to 3 to 4 years of age) and it does not remodel thereafter, hence carrying childhood information on geographic origin6 (Supplementary Information). The Egtved Girl\u2019s tooth enamel yielded a 87Sr/86Sr value of 0.71187 (\u00b1 0.0002; 2\u03c3; Table 1). Similarly, we measured the strontium isotope signature of the associated child\u2019s compact part of the occipital bone, the pars petrosa which was recently shown to be a valuable archive preserving origin information5 (Supplementary Information). The occipital bone yielded an 87Sr/86Sr value of 0.71190 (\u00b1 0.0002; 2\u03c3; Table 1), a value which is indistinguishable from the Egtved Girl\u2019s tooth enamel. Studies aimed at delineating the range of bioavailable strontium isotope compositions characteristic for present day Denmark (excluding the island of Bornholm and hereafter referred to as \u201cDenmark\u201d) resulted in a baseline range defined by 87Sr/86Sr values of 0.708 to 0.71118, 19, 20, 21 (Supplementary Information). Bioavailable strontium from the Egtved burial site itself is defined by isotopic compositions which lie in the lower end of this scale with 87Sr/86Sr values ranging from 0.70852 to 0.70874 (Table 1). Hence, comparing the strontium isotope results from the Egtved Girl and accompanying child with the Danish baseline, it implies that both individuals originated from outside present day Denmark (Fig.1).\n\nTo trace the Egtved Girl\u2019s mobility during the final months of her life, we divided her 23-cm long scalp hair into 4 segments covering a total growth period of, at least, 23 months prior to death (Fig. 3 and Supplementary Information). The oldest period represented by the hair (segment 4, Table 1) which corresponds to, at least, 23 to 13 months prior to death, is characterized by an elevated strontium isotope signature (87Sr/86Sr\u2009=\u20090.71255). The middle segments 2 and 3 represent a period of, at least, 9 months, and have similar lower strontium isotope signatures (87Sr/86Sr\u2009=\u20090.71028 to 0.71086). These two middle segment values are compatible with bioavailable signatures characteristic for Denmark18, 19, 21. However, the youngest scalp hair segment 1, corresponding to, at least, the final 4 to 6 months of the Egtved Girl\u2019s life, again reveals an elevated strontium isotope signature (87Sr/86Sr\u2009=\u20090.71229) similar to that measured in the oldest part of the hair. Finally, data of the three segments from one of her fingernails (87Sr/86Sr\u2009=\u20090.71235 to 0.71240) corroborate with the youngest hair segment signature, which together cover the same final 4 to 6 months of her life.\n\nStable isotope signatures (\u03b415N\u2009=\u20098.6\u2030; \u03b413C\u2009=\u2009\u221221.6\u2030) of a c. 6-cm-long scalp hair corresponding to the same period as the youngest scalp hair segment analyzed for its strontium isotope composition indicate a terrestrial diet (Supplementary Table S3, Supplementary Fig. S3 and Supplementary Information). The partial sigmoidal curve defined by \u03b413C and \u03b415N could be suggestive of a seasonal diet variation, although the variance in nitrogen isotopes too, may potentially be interpreted as resulting from physiologically-related influences (Supplementary Fig. S3). Additionally, micro-morphological investigations of several scalp hairs reveal marked constrictions along shafts which may reflect periods of reduction/availability of protein22 (Supplementary Fig. S5 and Supplementary Information).\n\nDNA was extracted from the Egtved Girl\u2019s scalp hair, and using high throughput sequencing technology we obtained >28 million DNA sequences (Supplementary Table S2 and Supplementary Fig. S2) which we intended to use for elucidating population affinity and phenotypic characters (Supplementary Information). However, the proportion of non-duplicated sequences identified as human was extremely small (0.04% and 0.13%, respectively in two extracts) and did not exhibit the molecular characteristics expected for ancient DNA with increased levels of cytosine deamination damage towards the termini. We therefore conclude that there is minor, if any, retrievable ancient human DNA preserved in the hair sample, most likely due to the acidic pH of the burial environment, combined with years of exhibition23.\n\nThe measurements of the wool fibres indicate extensive selection and processing (Supplementary Fig. S6, Supplementary Table S4 and Supplementary Information), indicative of high quality textiles. The strontium isotope compositions from the animal fibres from the textiles and the underlying oxhide reveal a large range from 87Sr/86Sr\u2009=\u20090.71168 to 0.71551 (Table 1, Supplementary Table S1 and Supplementary Information), revealing that her outfit was made of raw materials gathered from outside Denmark. Only the raw materials from a wool cord placed in the container with the cremated remains of the child (ad 11847a, Table 1) yielded strontium isotope signatures that could imply local origin (87Sr/86Sr\u2009=\u20090.70982 to 0.71044)."},
{"url": "http://www.mrmoneymustache.com/2015/05/20/what-im-teaching-my-son-about-money/", "link_title": "What I\u2019m Teaching my Son about Money", "sentiment": 0.21991491162377247, "text": "I\u2019m not going to lie to you \u2013 being wealthy is a lot of fun.\n\nAnd I\u2019m not just talking about novelty fun that you get from\u00a0driving around in a fancy car. True wealth is more of a big picture thing \u2013 freedom from negative stress and a higher confidence about how great life is. It hits you like a pack of wild butterflies every morning when you wake up. Holy shit, here comes another great day.\n\nI want to pass this gift along to my son if at all possible, because it is truly a great way to live. After all, as parents we are really in the business of producing the happiest and most capable adults we can, given the constraints of the real world. If my boy eventually ends up as happy with his lot in life as his parents are, we will be more than satisfied.\n\nSurely every parent wants the same thing \u2013 to pass on their happiness if life is good, or if not, to give their kids a better life than they had. \u00a0So they do their best to dish out financial advice, and to model good behavior for their offspring to emulate. Unfortunately, the results are not always good.\n\nIn a country where Ridiculous is Ubiquitous, most people\u2019s best attempts at getting ahead are in fact recipes for financial disaster. I get emails from high school and university students telling me, \u201cDad advised me to finance a reliable NEW\u00a0car with 4WD, so I can be safe in the winter and spend less on repairs.\u201d Other people rack up $200,000 in student loans for a elite degree with few job prospects, because their parents cautioned \u201cYou can\u2019t get a good job without a degree these days.\u201d Still other families stress over how much to spend on olympic-caliber toddler birthday parties, how to afford ivy league preschools and how to fit in with the other high-income families in their private schools.\n\nWhile any one of these pieces of advice might work fine for a family with infinite money, they have trickled deep down into the middle classes where they become unhelpful for those wanting to truly get ahead.\n\nI just read a book called The Opposite of Spoiled, by Ron Lieber. While the book was thoughtful and thoroughly researched, I\u00a0was still fascinated by how much things have changed since I was a kid.\u00a0There were chapters on how to handle the social pressures of a high-income neighborhood. What do you do when the other kids have nicer stuff than your kids, or vice versa? How do you say no to your kids when they want things that you can\u2019t afford for them? How do you handle allowances, jobs, paying for education, mobile phones, cars, and giving to others?\n\nAll of these perceived social pressures of the Wealthy New York style of childraising were unfamiliar to me. It was three decades ago in a small town in a different country that I approached my own teenage years, and we followed a much simpler model of family finance back then.\u00a0Much like the Unfrozen Caveman Lawyer, I found myself wondering \u201cWhat the hell are these modern people fussing about?\u00a0Do they really worry about this stuff?\u201d\n\nIt seems to me that if we bring the financial values of a small working-class 1980s town forward to today, life gets a lot simpler for kids. And in the long term, richer.\n\nIn our household, money is an open subject without any attached baggage or taboos . Our 9-year old knows exactly how money is earned, what happens when you spend it (it\u2019s gone), and what happens if you invest it instead (it works for you, for life).\n\nSince we retired just before he was born, he has grown up with the idea of financial independence \u2013 if you\u00a0own assets like rental houses or shares of businesses, they provide income which means you don\u2019t have to leave home for 9 hours every day and commute to an office unless this is your idea of fun. He sees this every day by comparing the daily routine of his own parents, to what other parents do each day.\n\nSo ever since he has been old enough to have a use for money himself (age six or so), I have tried to give him a chance to learn for himself how it works.\n\nBeing a kid is quite a lucrative proposition these days. On top of the free rent, he gets occasional cash gifts from relatives and a salary from me that consists of 10 cents for every mile walked or biked as part of family life. These tend to add up in a mostly-car-free family, as he already has more than 1200 miles on the little 20\u2033 tires of his mountain bike and we wear through quality shoes before growing out of them.\n\nOver the coming years, I\u2019m expecting him to move from these little-kid sources of income into more independent ones. Whether he pursues traditional employment or hardcore full entrepreneurship right off the bat is up to him*.\n\nSome parents like to focus on academics:\u00a0\u201cUntil you graduate, getting good grades is your only job.\u201d But I like to think of a good education as a highly diverse set of experiences. Working and earning your own money at any age \u2013 even if it includes stocking shelves and assembling wheelbarrows at a hardware store \u2013 is a key part of this. School is just a tiny part of a kid\u2019s education, and not even the most important part. In fact, most my most vibrant experiences from high school were side effects of work rather than classes at school.\n\nThis is where things get a bit unconventional. Instead of a physical piggy bank, my boy prefers to keep his money in the Bank of Mr. Money Mustache, a spreadsheet that contains every transaction he makes with money. To make a deposit, he just hands me some cash. To withdraw, he asks me for cash or has me buy something for him online.\n\nBut for every dollar that remains in the account, he accrues interest at a 10% annual rate with monthly compounding. I\u2019m excited about the teaching value of this, because it shows him that\n\nWhere the Money Goes:\n\nRight now, he has only a few true consumer loves in his life: PC games, books, and the occasional phone or tablet app. So he has spent over\u00a0$100 on those things (quite a large percentage of income) in the past year. But in most cases, he has felt the fun value has been worth the spending.\n\nInterestingly enough, he has already started to display a high degree of generosity. When something breaks in the house\u00a0or another kid doesn\u2019t have enough money to pay for something they want, he immediately offers up a large sum of his own money to cover the shortfall.\n\nMeanwhile, I still cover the basic infrastructure of educational childhood fun \u2013 to build his computer I bought about $500 of parts and we assembled them together into a pretty spiffy gameworthy PC. We build robots with a $400 kit of VEX IQ stuff, and many books, bits of outdoor equipment and trips come for free as part of being in the family. Any organized activities also come from this freebie budget, at least until he reaches his teenage years. \u00a0But like his Dad, he has shown a strong preference for self-guided activities with friends rather than adult-organized ones so far, and I\u2019m happy to let him continue with this style.\n\nLiving By Example, and Giving:\n\nIn Ron Lieber\u2019s book, the tricky subject of \u201cwhy do we have so much, when these other people have so little?\u201d comes up. It\u2019s a good one, because this observation is often the gateway to taking an interest in helping other people. But for me, it would be hard to answer a question like that while living at the pinnacle of American luxury with multiple homes, boats, and jets. Since our annual spending of around $25,000 is lower than average for our own country, and it stays that way even in years when we make many\u00a0times that amount, I\u2019m hoping the example of \u201cspending does not need to scale with income\u201d will jump to the next generation.\n\nWhen your own needs are capped, it becomes only logical to find an efficient outlet for the surplus money. So there is an understanding that we operate with an informal, non-billionaire\u2019s version of the \u201cgiving pledge\u201c, meaning there will be no large Mustache family inheritance \u2013 each generation will be left free to generate its own massive surplus.\n\nFor me, this is where the rubber really meets the road. \u00a0If you can\u2019t leverage money to live more happily, then what good is it? And yet consider the stunning case study of the children of the nation\u2019s uber-wealthy enclaves like Palo Alto, California. Despite incredible wealth and some of the best educational institutions money can buy, kids there are more stressed, less happy, and more likely to commit suicide than others\u00a0who live with a fraction of their privilege.\n\nThe problem arises when high-achieving parents assume that their kids need to be pushed to achieve more themselves, to beat out the other high achievers and gain access to the most elite schools, in order to compete in this incredibly challenging modern world.\n\nRemember way back when I started this blog in April 2011? Right there in the first paragraph of the first post, we hit this sentence:\n\nTo me, raising kids to feel pressure and fear so they can be COMPETITORS is bullshit. Life is not a competition. It\u2019s a gigantic collaboration, and the world welcomes and rewards people who see it that way.\n\nIt may be that most parents of the very-upper-middle class are still operating from a scarcity mindset. If they are addicted to a high consumption lifestyle, earning $600,000 per year but still making car and house payments, they will assume that their children will need to earn and consume just as much in order to be happy. This of course dictates a job in the top fraction of the top percent of the economy, and education with enough prestige to secure such a job.\n\nOn the other hand, having crossed the threshold of having more than enough money\u00a0for a good life almost a decade ago, I cannot even imagine my son not earning a plentiful and permanent surplus very early on in his adult life. Thus, there is no need to fight for traditional elite status. It is much more efficient to rise up to into your own niche without the constant drag of material addiction telling you you aren\u2019t good enough. Paradoxically, this path is rare enough that you might end up earning even more money in the end.\n\nWhat I Really Want Him to Learn:\n\nAll of this kid stuff is just the groundwork for the bigger (and slightly radical) perspective on money that I want to instill over his lifetime: that money is something you can master and control, rather than letting it control you.\n\nObserve the following statements and see if you agree with them. While you can poke holes and find exceptions to each one, the overall philosophy is remarkably true if applied forcefully over a lifetime:\n\nThis is my experience so far in raising a Junior Mr. Money Mustache. Although I feel the foundation is solid, like everything in life it is an ongoing experiment. I\u2019ll let you know how it turns out.\n\n* My first jobs were paper route, lawn maintenance, and gas pump jockey. Out of these three, I\u2019d only discourage a teenager from pursuing gas station work \u2013 avoiding toxic vapors during key periods of brain growth seems wise in retrospect.\n\nA Technical Note:\u00a0Due to higher traffic these days, our current server can\u2019t keep up with traffic unless I have the comments feature disabled. We are still in the process of moving to a bigger system and some\u00a0more modern comment-handling technology. I hope to have everything in order, better than ever, very soon.\n\nA Fun Note: I started a sub-page of this blog called \u201cShould we Meet?\u201d \u2013 this is just a place where I keep track of places I\u2019ll be passing through in the near future, just in case you want to come out and meet some real-life Mustachians with me. This page is just my tiny personal version of the Forum\u2019s Meetup Section, where people are hanging out all over the world."},
{"url": "http://www.cnbc.com/id/102702029", "link_title": "Microsoft offered to acquire Salesforce for $55B", "sentiment": 0.05450980392156863, "text": "Microsoft and Salesforce.com had significant talks earlier this spring about a purchase of Salesforce by Microsoft, according to a number of people familiar with the situation. While the two sides failed to reach a deal and have not re-engaged, the talks advanced to a level of detail that indicates they were serious.\n\nUltimately, the two companies remained far apart on a price, with Microsoft said to be willing to offer roughly $55 billion for the company, while its founder and CEO Marc Benioff is said to have kept raising his expectations to as high as $70 billion.\n\nThe deal envisioned Microsoft using a significant portion of its $95 billion cash pile to pay for Salesforce, but there was discussion of allowing Benioff to roll his 5.7 percent stake in Salesforce into Microsoft stock, while other shareholders would have gotten paid in cash. Benioff would have had a management role at Microsoft under the deal, according to people close to the talks.\n\nSalesforce was engulfed in takeover rumors late last month when Bloomberg reported on an approach of an unnamed suitor that was not Microsoft, for the company. Bloomberg also reported earlier this month that Microsoft was evaluating a bid for Salesforce, but said no talks between the two companies were taking place. Both reports sent shares of Salesforce sharply higher. \n\n"},
{"url": "http://www.vox.com/2015/5/22/8639717/reid-hoffman-the-alliance", "link_title": "Reid Hoffman on the relationship between employers and employees", "sentiment": 0.10932312925170064, "text": "In his new book\u00a0The Alliance, Reid Hoffman argues that the relationship between employers and employees is built on \"a dishonest conversation.\"\n\nHoffman would know. As co-founder and executive chairman of LinkedIn, he sits atop the largest, most data-rich hiring platform the world has ever seen. As a venture capitalist who made early investments in everything from Facebook to Airbnb, he's helped some of the era's most successful companies grow.\n\nAnd now he wants both workers and employers to begin having honest conversations with each other \u2014 conversations that admit employment isn't for life, that loyalty only lasts so long as it coincides with self-interest, and that the relationship doesn't have to end when the worker leaves.\n\n\"The biggest lie is that the employment relationship is like family,\" Hoffman says.\n\nHe goes on to describe two versions of the lie. \"One is where the employer is actually deluding themselves.\" Employers may want to believe their workplace really is like a family, and, in that moment, they may convince themselves it actually is like a family.\n\nThe other version of the lie comes because the employer wants the employee to believe it. \"They really want the employee to be loyal to the company,\" Hoffman continues. \"That's when it gets deceptive.\"\n\nBut the employer-employee relationship isn't like a family. \"You don't fire your kid because of bad grades,\" Hoffman says.\n\nBut it's not just employers who lie. Prospective employees do, too.\n\n\"They know that employers want loyalty,\" Hoffman says. \"They know they want to hear, 'Oh, I plan on working here for the rest of my career.' But most employees recognize that career progression probably requires eventually moving to another company. But that never comes up.\"\n\nThis is core to Hoffman's idea that both employers and employees should look at a particular job less as a lifetime contract and more as a \"tour of duty\" \u2014 a limited-time engagement meant to achieve specific ends on both sides. But until employers stop pretending employees are family and employees stop pretending their aim is a job they'll never leave, neither side can have that conversation.\n\nLinkedIn is an organization dedicated to helping other companies hire talent. It has access to more hiring data than arguably any other corporation on Earth. So I asked Hoffman: how do they hire? What do they ask that most companies don't?\n\n\"All of our managers and recruiters ask about how working here will be transformational to your career,\" Hoffman says. \"So we'll ask, 'What's the next job that you would like to have post-LinkedIn?' That's not because we don't want our stars to stay at LinkedIn for a long time. It's because we're so committed to the idea that we're going to be transformative in the prospective employee's career. So we need to know, what's the next job after this? What do you want it to be?\"\n\n\"No,\" Hoffman says. \"It's framed as, 'We're planning on having a huge impact in your career if you're working here.' And they find that liberating. It brings some honesty to what is otherwise kind of a collective self-deception dance. And it also means that when they leave, we still care about them.\"\n\n\"I was at an Airbnb board meeting and I ran into two former LinkedIn employees who walked up to me and said, 'Hey, how's it going? I'm working here now. I'd love to tell you about some of the stuff that I'm learning.' They know the way that we operate, is not, 'Oh, you've left LinkedIn, so you're no longer part of our tribe.' We continue to be allies. We can continue to try to help each other. That lets them come up and start telling me things about things could be really helpful to LinkedIn.\"\n\nA key part of every hiring process I've ever been a part of \u2014 both as the applicant and as the employer \u2014 is the job interview. And I've never felt very good about it. Don't job interviews bias you toward gregariousness? Is there any real reason to believe shy employees perform worse than extroverted ones?\n\n\"I think you can learn some useful things from an interview,\" Hoffman says. \"You just have to be clear about what it is you're actually trying to learn. I think you can learn about chemistry and fit. I think you can learn about a person's immediate response to a challenge. But if you told me, 'Pick one \u2014 you could either get references or an interview,' I would pick references every day of the week.\n\n\"I advise all the companies that I affiliate with to take reference checking very seriously. References actually tell you how people work, what their work ethic is. That is a critical piece of data that cannot be put aside or done casually. Frequently employers are so casual about references they either a) don't check them, or b) only check the ones the prospective candidate gives them. In fact, you want both those references and others.\"\n\nHoffman's chief of staff, Ben Casnocha, wrote an interesting piece on leadership lessons he's learned from Hoffman. This one in particular surprised me: \"If you\u2019re choosing between working with someone who\u2019s a trusted friend and a 7 out of 10 on competence, versus a stranger who\u2019s a 9 out of 10 on competence, who should you pick? Answer: if the trusted friend is a fast learner, pick the trusted friend.\"\n\nThe normal management guidance is don't hire your friends. According to Nick Bilton's\u00a0history of Twitter, when Evan Williams asked legendary CEO coach Bill Campbell what the worst mistake he could make is, Campbell replied: \"Hire your fucking friends!\" So I asked Hoffman why he believes in hiring friends.\n\n\"You need to handle it well,\" Hoffman replied. \"If I get to the point where I'm hiring a friend, I say, 'Look. Here's how we keep the friendship and the work stuff different. Here's how I'm going to treat you a little differently as a friend. Here's how you're going to act a little differently as a friend.' I'm going to be clear about the fact that I'm not going to privilege them at all in the continuum to the job and promotions and bonuses. All of that will be done in a very fair way.\n\n\"On the other hand, I will actually, as a friend, go out of my way to invest even more energy than I normally do to make this work. I'm committing to put in a little bit more energy. In return, one thing is I want you, as a friend, to do the same. The benefit you get from this is both a) a higher level of trust, and b) you get to work with people that you actually really like to spend time with. Which usually facilitates a generally positive working relationship anyway.\"\n\nHoffman's background isn't typical. He didn't study computer science or get an MBA. He studied philosophy. And he thinks he's better off for it.\n\n\"One of the things that philosophy is very helpful on is how to think pretty precisely about arguments, and an investment thesis is fundamentally an argument. Part of philosophical training is making you really understand how good an argument is and how to think through the alternatives. Philosophy is really good at posing the question, 'If the universe were such that this data would be different or the universe was such that this framework would be wrong, what happens to the argument then?' Questioning those premises really helps you figure out why someone smart might actually hold a different point of view.\n\n\"We live in a probabilistic universe, and we tend to think in determinist ways. If A is data-driven and I think I have that data, how certain am I that I have that data? What could I discover that might actually tell me that that data is formulated wrongly? When you dig into it, most of your arguments are actually probabilistic. They're not certain, even when you have data. You're really trying to get a sense of whether you have a reasonable bet on the probability.\""},
{"url": "http://blog.pitchbook.com/uber-grew-its-value-by-120-million-per-day/", "link_title": "Uber grew its value by $120M per day", "sentiment": 0.10378787878787876, "text": "In the sports world, for example, Alex Rodriguez has long been a\u00a0detestable\u00a0figure due in part to gaudy salary figures that once\u00a0equated to nearly $60,000 per at bat (not to mention the performance enhancing drugs, of course). But the VC world has some heavy hitters of its own, so we used the PitchBook Platform to look up some head-spinning valuation increases from the industry\u2019s biggest players.\n\nTake Uber. Did you know the ridesharing giant effectively increased its value\u00a0by $120 million per day between financing rounds\u00a0in June and December last year?\n\nSay what you will about the justifiability of soaring valuations, but Uber isn\u2019t alone. Several other tech companies have seen\u00a0historic rates of increasing value, and you\u2019ll notice in the table\u00a0of select\u00a0companies below that many of the financings\u00a0have been raised within the past year or so.\n\nIt\u2019s worth noting that the graphics\u00a0above only take into account absolute\u00a0value increase and not relative value increase. When measuring the rate of the\u00a0company\u2019s growth against its previous post-valuation, the numbers look much different. The best example is Twitter, which saw roughly 32,000% annualized growth between its Series A ($127,500 valuation) and Series B ($22.75 million valuation) in just a matter of months."},
{"url": "https://github.com/matthewbauer/reddwall", "link_title": "Show HN: ReddWall \u2013 An Automatic Reddit Wallpaper Changer", "sentiment": 0.10183566433566432, "text": "ReddWall sets your wallpaper to a random image from one of the many Reddit wallpaper subreddits.\n\nOn the first run, ReddWall will find a random wallpaper from /r/wallpapers and minimize to the status bar. By default, it will select a new wallpaper every 1 hour. This behavior can be changed in the preferences window that can be opened by clicking the ReddWall icon in the status bar.\n\nTo run from source, first make sure you have Git, Python 2.7, setuptools, and wxPython 2.7 installed. Then, clone the repo:\n\nand install it using the setup.py script:\n\nThis program has not been tested on many different platforms. The cross platform toolkits used (wxWidgets, Python, and PRAW) mean that it should work without a problem. However, problems are bound to arise. If you have any issues with ReddWall, please file an issue and I'll be sure to get back to you."},
{"url": "https://www.h2check.org", "link_title": "Show HN: H2Check \u2013 An HTTP/2 server checker", "sentiment": 0.22023809523809523, "text": "Check your server for HTTP/2 or SPDY support. We support both NPN and ALPN as negotiation mechanisms, and IPv6. Please contact us if you have any problems or notice a result that doesn't seem quite right. HTTP/2 is negotiable over plain HTTP, but most browsers have chosen to support it only over HTTPS, so we support checks only over this protocol.\n\nAdd a successful result to the list below"},
{"url": "http://shsceo.com/2015/05/22/who-cares-about-money/", "link_title": "Who cares about money?", "sentiment": 0.21785473785473786, "text": "We recently closed a round for Onus, and I\u2019m taking a fairly low salary. I\u2019ve been asked, \u201ccan you live off of that?\u201d Which is pretty important, and I kinda sorta can. But what I didn\u2019t want, is for people to think that I\u2019m making Onus to make me money. I check our bank account every day, just to look at the money, and wait for me to be excited about it, or feel something\u2026 but everytime it\u2019s just a number on my screen. It\u2019s like looking at photos of celebrities, and knowing that it\u2019s all photoshop and makeup\u2026 it\u2019s almost all fake.\n\nFor me, money is the end result of making an amazing product for a lot of people that they love. I strongly believe, that you can only do that when your focus isn\u2019t on short-term monetary gains for yourself\u2026 or even the company.\n\nI absolutely want my investors to get an extremely high return, but we only accomplish that by making an amazing product first.\n\nToo many times, do I see my friends, fellow CEOs, founders, etc., talk about and day dream about what they\u2019re going to do with all of this money that will come once they make something and sell it. As if the easy thing is to make an amazing product, and sell it, and the hard thing is to figure out which car they are going to buy.\n\nMy passion isn\u2019t software, or hardware, my passion is making products that make people productive. Because I\u2019m not going to change the world, but if I help someone get more things done in their life, maybe they will. At the end of it, all that matters is how much we\u2019ve done and how far we\u2019ve gone\u2026 that\u2019s what I care about. Making a product that helps people achieve more\u2026 because I want to achieve more.\n\nMoney is an end result, not part of the equation. Luck is not by chance, it follows those who are passionate. Care about the right things."},
{"url": "https://www.indiegogo.com/projects/record-all-lambdaconf-2015-talks", "link_title": "Campaign to record all LambdaConf 2015 talks", "sentiment": 0.2101660401002506, "text": "Last year, 130 attendees from all around the world gathered at LambdaConf 2014 for a day of incredible workshops and presentations on functional programming. Unfortunately, we had no budget for recording the talks, so all that remains of LambdaConf 2014 is a collection of slide decks, code snippets, blogs, and tweets.\n\nThis year for LambdaConf 2015, we tried very hard to find a commercial sponsor for video. While we found a handful of companies interested in helping us reimburse travel expenses for speakers, we were unable to find any companies willing and able to sponsor the very substantial costs of video recording and production.\n\nSo based on the suggestions of attendees, we are trying something different: crowdfunding! We have 70 talks, and more than 70 hours of amazing content, most of it completely unique to LambdaConf! We want every single talk recorded and published online\u00a0free of charge\u00a0for generations of programmers to learn from!\n\nSound exciting? Then please help us reach our goal!"},
{"url": "https://gist.github.com/raine/d12d0ec3e72b2945510b", "link_title": "Using ramda-cli to process and display data from GitHub API", "sentiment": 0.20365936147186145, "text": "In this tutorial we'll use ramda-cli with GitHub's Repos API to get a list of @jeresig's most starred repos.\n\nramda-cli is a command-line tool for processing JSON using functional pipelines. As the name suggests, its utility comes from Ramda and the wide array of functions it provides for operating on lists and collections of objects. It also employs LiveScript for its terse and powerful syntax.\n\nOn the way, there's a gentle introduction to some functional programming concepts such as currying and function composition. Step by step, we'll build a pipeline of functions that takes a list of repos and returns the ten most starred repos in descending order as a list of objects. Finally, we'll print the result as table.\n\nCopy-pasting this into a shell session will make all the examples runnable:\n\nLet's first use to get the list of repos in JSON format and pipe it to to get an idea of what we're working with.\n\nAs in programming, in ramda-cli data is manipulated by applying a function to the data. The result will by default be written to standard output in JSON format.\n\nSince stands for a function that simply returns its argument, our command will pipe the JSON payload unchanged through to stdout in a more readable ( is for pretty) format.\n\n@jeresig has a lot of repos and the API returns a ton of info we don't care about, so we'll go ahead and see how the output could be reduced to just a list of the names of those repos.\n\nFor those unfamiliar with Ramda's curried API or LiveScript, this will require some explaining.\n\nIn LiveScript, as in CoffeeScript, parentheses are optional when calling a function. Backslash preceding a word is sugar for string. Therefore, compiles into in JavaScript.\n\nis a function that for a given key and a list of objects, returns a list of values corresponding to that key from all the objects in the list. Ramda's functions are all by design curried, so we can partially apply with just the key we want, , and thus get back a function that will be waiting for the second argument, a list of objects.\n\nSince curl gives us a list of repos, it's a great match for a function that is waiting for a list objects to get the properties from.\n\nLooks like the output contains repos that are forks. Not a big deal but for the sake of example we could get just the repos that are originally by @jeresig.\n\nHere, set up a with a spec object ( ) creates a predicate function to be used with . is now waiting for the second argument that curl will provide, a list of repos to filter.\n\nNotice that two independent pieces of code are now passed to . What happens here is our program is still evaluates into a single function, but it's composed under the hood by ramda-cli from the given functions in order from left to right. Therefore, what we just did is equivalent to explicitly using for function composition:\n\nThe list is first filtered, then property is plucked from each object. In this way we can build a pipeline of operations to be applied on our data in a specific order.\n\nNow that we have a list of repo names that are not forks, we can make the output more interesting by grabbing also the number of stars.\n\nInstead of we need an operation that picks specific fields from a list of objects. In Ramda, there's a function called for just that.\n\nBefore we can make the output more visually appealing, we have a few steps to add in our pipeline. First, sorting by in descending order.\n\ntogether with sorts a list of objects according to the field given to .\n\nFinally, we apply to get the most starred projects first and limit the list to first 10 items with .\n\nGood, now that the data is getting transformed into a shape that has the info we want, it can be presented in a more readable format.\n\nUsing the option, a list of objects may be printed as a table in such way that the objects' keys become the table headers. It's convenient because all we need is an uniform list of objects to get a pretty table. So we'll do just that. Remove the flag from before and slap at the end.\n\nThis concludes the tutorial. If you're new to Ramda and want to learn more, check out the list of articles in the wiki. For ramda-cli, the README provides helpful information and examples.\n\nThanks to buzzdecafe for providing feedback on this article.\n\nAs the pipeline grows, it becomes increasingly more manageable option to write the pipeline in a separate script file. For this, ramda-cli provides the option:"},
{"url": "https://www.eff.org/press/releases/victory-photo-hobbyist-prevails-over-junk-patent-bully", "link_title": "Victory: Photo Hobbyist Prevails Over Junk-Patent Bully", "sentiment": 0.165668044077135, "text": "Camden, New Jersey \u2013 Patent bully Garfum has abandoned its lawsuit against an online photo hobbyist, just one day after a federal judge set the date for a face-off in court against lawyers for the Electronic Frontier Foundation (EFF).\n\nEFF together with Durie Tangri LLP represent Bytephoto.com, which has hosted user-submitted photos and run competitions for the best since 2003. In 2007, a company called Garfum.com applied for a patent on the \u201cMethod of Sharing Multi-Media Content Among Users in a Global Computer Network.\u201d The patent takes the well-known concept of a competition by popular vote and applies it to the modern context of generic computer networks, and Garfum claims that it covers the rights to online competitions on social networks where users vote for the winner\u2014despite the fact that courts have ruled that this kind of abstract idea using generic computer technology cannot be patented.\n\nGarfum used this patent to accuse EFF\u2019s client of infringement, filing a federal lawsuit without warning. EFF moved to dismiss the complaint earlier this year, arguing that the junk patent should be declared invalid. But after all the briefing had been completed and just one day after the court scheduled a hearing on the motion to dismiss, Garfum capitulated: it dropped its case with a promise not to sue Bytephoto.com again rather than defend its patent before a judge.\n\n\u201cWe\u2019re pleased that Garfum has abandoned its claims against our client. But it\u2019s a travesty that this case was ever filed in the first place,\u201d said EFF Staff Attorney Daniel Nazer, who is also the Mark Cuban Chair to Eliminate Stupid Patents. \u201cOur client began running online \u2018favorite photo\u2019 competitions years before this patent was filed. The idea that you could patent this abstract idea, find innocent enthusiasts online and demand settlement money\u2014and then slink away once challenged and before the court issues a ruling\u2014goes against any sense of fair play.\u201d\n\n\u201cPatent bullies count on not having to defend their weak patents in a court of law. They drive up costs with baseless lawsuits and then bow out before getting a decision they don\u2019t like,\u201d said EFF Staff Attorney Vera Ranieri. \u201cSo while we are glad our client doesn\u2019t have to worry about Garfum anymore, there\u2019s still a lot of work to do the fight against bad patents.\u201d\n\nJoe Gratz of the law firm Durie Tangri LLP and Frank Corrado of Barry, Corrado & Grassi, PC are co-counsel with EFF.\n\nFor more on this case:\n\n https://www.eff.org/cases/garfum-v-reflections-ruth"},
{"url": "http://malan-app.com/", "link_title": "Show HN: Malan.app \u2013 app to build superhuman strength", "sentiment": 0.029761904761904757, "text": "one of the world's strongest powerlifters.\n\nSmart training and nutrition planning Malan.app will automatically generate training and nutrition plan based on your goals, gender, experience and age.\n\nand his training team for\n\nIn-app coaching and personalized advice from Andrey Malanichev and his training team for $99,99 /month.\n\nShare your personal and competition records with other lifters around the world. Inspire. Be inspired."},
{"url": "https://medium.com/@mhausenblas/containerama-612207a7686e", "link_title": "Containerama", "sentiment": 0.16794642857142858, "text": "The general ingredients of containers and the building blocks of Linux containers in special\u200a\u2014\u200acgroups, namespaces and chroot\u200a\u2014\u200ahave been already around for a while, up to 8 years. I recommend the following resources from Justin Weissig\u2019s awesome Sysadmin Casts to gain a deeper understanding:\n\nThe first generation of containers was mainly lacking one thing: usability. There was little or no common way to define, package and distribute the container images.\n\nDocker is probably the best known and most hyped example in the 2nd generation of container technologies, addressing the usability issue. Though it was certainly the first attempt to establish a def-facto standard, others are emerging and should not be under-estimated, especially the appc specification and CoreOS\u2019s reference implementation rkt.\n\nHowever, let us not forget about other, related things in this space: YARN containers and JVMs.\n\nIn this context, questions as the following typically arise:"},
{"url": "http://letitcrash.com/post/119598493862/akka-2-4-m1-announcement", "link_title": "Akka 2.4 M1 is released", "sentiment": 0.06022727272727273, "text": "we\u2014the Akka committers\u2014proudly present the first development milestone for Akka 2.4. Since the release of Akka 2.3.0 (already 14.5 months ago) much has happened, in particular around Streams & HTTP. It may therefore surprise you that these additions to the Akka toolkit are not yet part of the 2.4 development branch, but if you read on for a little while the pieces of the puzzle will fall into place. Besides a plethora of small improvements the main changes relative to the 2.3.x series are:\n\nBeing binary compatible means that applications and libraries built on top of Akka 2.3.x continue to work with Akka 2.4.x without recompilation (subject to the conditions below), which implies that Akka Streams & HTTP as well as the upcoming Play Framework 2.4 can be combined with Akka 2.4.\n\nWhat remains to be done before we can release 2.4.0-RC1 is to\n\nWe will perform this work within the next weeks, releasing further milestones when appropriate. Please do what you usually do so well: try out our latest and report back when things break, not work as advertised, feel strange, or even when you are happy :-) Especially concerning binary compatibility we will need help from the community (you!) since we cannot run all possible programs ourselves; we base our BC efforts on the MiMa plugin but that is no perfect guarantee that everything will work out of the box.\n\nAkka 2.4.x is backwards binary compatible with previous 2.3.x versions (exceptions listed below). This means that the new JARs are a drop-in replacement for the old one (but not the other way around) as long as your build does not enable the inliner (Scala-only restriction). It should be noted that Scala 2.11.x is is not binary compatible with Scala 2.10.x, which means that Akka\u2019s binary compatibility property only holds between versions that were built for a given Scala version\u2014 is compatible with but not with .\n\nBinary compatibility is not maintained for the following:\n\nThe dependency to Netty has been updated from version 3.8.0.Final to 3.10.3.Final. The changes in those versions might not be fully binary compatible, but we believe that it will not be a problem in practice. No changes were needed to the Akka source code for this update. Users of libraries that depend on 3.8.0.Final that break with 3.10.3.Final should be able to manually downgrade the dependency to 3.8.0.Final and Akka will still work with that version.\n\nThe dependency to Typesafe Config has been updated from 1.2.1 to 1.3.0 which should be binary compatible for the vast majority users, except for obscure edge cases as its changelog points out. This change was made in order in order to use new JDK8 specific features in the library as well as to align Akka with Play which is now also depending on 1.3.0.\n\nWhen migrating a code base to 2.4 please refer to the migration guide in order to profit from some of the improvements.\n\nWe are extraordinarily proud of the long list of contributors to this release, looking at the commit history we find more than 100 names! We are particularly thankful to hepin1989 (a.k.a. kerr) who contributed already 19 pull requests to Akka. Other noteworthy mentions besides Andrei Pozolotin\u2019s, Dominic Black\u2019s and Richard Marscher\u2019s (which are already listed above) are:\n\nThe complete list of closed tickets can be found in the 2.4-M1 github issues milestone.\n\nFor the full stats see the announcement on the website."},
{"url": "http://www.jobhop.co.uk/blog/jobhop/-robin-silcock-chats-about-her-career-as-a-game-designer-", "link_title": "Robin Silcock Chats About Her Career as a Game Designer", "sentiment": 0.10881619633499329, "text": "I had the pleasure of interviewing Robin Silcock about how she got into games art and design and what a typical day for a game designer would be.\n\nWhen did you become interested in games art and design?\n\nMy first gaming experiences were playing Tony Hawk skating and Golden Eye with my brother at an early age. We have an 8 year age-gap and video games were a much-loved bridge between us at what can be very different ages (picture a 7 year old with a 15 year old), and they still are today! I became more interested in them as media when I attended an open day at NUA, and heard Marie-Claire speak on their artistic and cultural potential, and from then on I was hooked on the topic.\n\nWhat qualifications did you get?\n\nI graduated this summer with a First Class BA (Hons) in Games Art and Design from NUA.\n\nAre you a gamer or an artist first?\n\nThat's a hard question to answer as my degree successfully blurred those lines for me, but I would say at my heart I am a lover of games. I do not play them in a conventional way however, and that is where I think the artist and designer in me shines through. I for example like to explore in the opposite direction of wherever the game is encouraging me to go in order to find out what might be hidden, or spend a good 10 minutes examining an ornate window frame or rock formation to see how it was made, often right in the middle of a game-sequence.\n\nAre you a programmer as well as a games designer?\n\nSadly programming does not come naturally to me, but I am trying to learn all I can, and at least develop a good understanding of programming for games (even if I am never able to 'write' code) so that when I am working with a team, I understand their potential and their constraints.\n\nIs there a particular area of game design that appeals to you most?\n\nI am fascinated by how virtual worlds in games can create new experiences for people. They develop keen attachments to characters from a game, or a place in a game. Virtual tourism through gaming is another interesting topic, supported by the pain-stakingly re-created historical worlds seen the Assassin's Creed series. I remember virtually visiting the Colosseum, and then visiting it in person the following year - picking out features I had learnt from the game, and comparing the versions of it I knew.\n\nAnother personal intrigue of games for me is their potential for re-creating memories and experiences virtually. For my final degree project I re-created my childhood kitchen from memory and a few reference photographs, filling the virtual space with emotionally-charged objects such as my childhood teddy bear, and my childhood drawings decorating the space, having scanned them into my computer to use.\n\nSome people say that to be a game designer you must have a love for video games, is this true?\n\nI would say that the industry demands passion in what you do. You cannot be a \u2018kind of interested\u2019 person, and do well at your job in the games industry. That is not to say that you have to be passionate about games specifically in order to be a good games-designer. Many students of architecture, film, animation, psychology and more find a passion in interactive design and games design for various backgrounds, and ultimately, you should want a varied group of people contributing to a game\u2019s design to make it really something special.\n\nWhat is a typical work environment like for a game designer?\n\nThe work environment for a games designer or artist varies vastly depending on the size of the company that they join. Each company has its own office-culture, and if you join a small company you could well be doing more than one role within the development team. In a much larger company, you could be providing one very specific part of a much larger process.\n\nWhat is a typical day for a game designer?\n\nMy advice would be to research the company you would be thinking of joining as much as you can, check out their company site, see where their employees have worked previously, and get in touch with them if you can to see what their \u2018working day\u2019 is like.\n\nWhen I was studying, I contacted a lot of artists and designers to ask them about the realities of their roles, and I received some excellent replies - I would thoroughly recommend reaching out on Twitter or Linked In to find out more.\n\nYour Linkedin profile says that you\u2019re a co-organiser for the Norfolk Indie Game Developers, can you explain what that is.\n\nI work with Alastair Aitchison in helping organise and support the local Norfolk Community of Game Developers. We try to arrange social and networking events for the group as often as we can to help with the exchange of ideas, skills and support, and organised the Norwich Gaming Festival last year in order to help generate more press and support of the industry in this area.\n\nYour Linkedin profile\u00a0also mentions that you organise the Norwich Gaming Festival\u00a0Would you recommend potential games designers going along to it?\n\nI most certainly would! The Norwich gaming festival\u00a0is very much for everyone and anyone who has an interest in games. Last year we had a great range of talks, workshops and activities for those interested in writing for games, audio and music for games, language localisation, board game design, game development, and much more. We hope to encourage those with a passion for games as well as those who might not ordinarily consider games as either a career or even a pass-time, and allow us to present the vast and diverse realities of how games are created.\n\nThe Norwich Gaming Festival began with The Forum Trust wishing to expand their already successful Retro Arcade event. They approached and invited Norfolk Indie Game Developers to help them put on an amazing event to help champion the local digital creative industry. Without their support the festival would not be the celebration it is today.\n\nDo you foresee more of a need for game designers in the future?\n\nThe games industry is now larger by the film industry by quite a margin, and I expect it only to grow and diversify in the years to come. Games will become more integrated into our daily routines, and the principals of games design, and user-experience design will in turn become more in demand by companies and industries outside that of the games industry. (So, the short answer is, yes!)\n\nRobin what\u2019s you number one most valuable tip to all would be game designers?\n\nI actually have 3 tips (I hope that\u2019s alright!) 1) Attend every networking event, conference, talk, convention possible to meet as many people as possible. 2) Your online presence is key - get yourself on Twitter and keep it reasonably professional, get yourself on Linked and make sure you follow up networking events with a healthy boost in your connections! And 3) Read as much as you can on games design, on art, science, narrative and drama, and read from different perspectives and opinions on games. People often think that \u2018research for games designers is just playing games\u2019, in reality it should almost be the opposite! \u2018Rules of Play\u2019 by Salen and Zimmerman is a great place to start, but is not for the light-hearted! \u2018Reality is Broken\u2019 by Jane McGonigal discusses very interesting ideas and case-studies on how games can change our lives, and our culture. And one final book-suggestion would be \u2018The Art of Game Design: A Book of Lenses\u2019 by Jesse Schell - all should be widely available online.\n\nYou can look at some of the games that Robin has worked on here"},
{"url": "http://selfmadesuccess.com/manage-pinterest-followers-unfollow-people-who-dont-follow-back/", "link_title": "Manage Pinterest Followers \u2013 How to Unfollow People Who Don't Follow Back", "sentiment": 0.07439703153988868, "text": "Have you ever tried increasing your pinterest followers by following people who are interested in your niche, only to get a massive amount of people who don\u2019t follow back?\n\nWhen you start out trying to increase your Pinterest followers, following people and getting follow backs can help a lot. The problem is that there is not tool, software or app that is built to help you manage pinterest followers.\n\nThey have them for Instagram, Twitter, and even Google+, but not for Pinterest. Sure, there are a couple of softwares or apps out there, but either they don\u2019t work at all or they have too many bugs to work fast and efficiently.\n\nSo what do you do? How do you manage your Pinterest followers and unfollow people who don\u2019t follow back? How do you sort through and know who followed you and who didn\u2019t?\n\n This video will show you exactly what you need to know to get by until someone creates a tool that actually works for Pinterest:\n\n1. First off, to manage your pinterest followers, you will need either Microsoft Office Excel or Google Docs. If you don\u2019t have Microsoft Office installed on your computer, I recommend just signing up to use Google Docs for free.\n\n2. After you\u2019ve picked one, open up a new spreadsheet and give it a title (like Pinterest Followers).\n\n3. Next go to your Pinterest account, click on your profile and then click on \u201cfollowers\u201d.\n\n4. Now that you see a list of your followers, highlight every single name on the page, then right click and select \u201ccopy\u201c.\n\n5. Click on the first row, first space in your spreadsheet and then paste the list.\n\n6. Now, you need to go to \u201cedit\u201c, then \u201cfind and replace\u201d and enter in each number, symbol or punctuation, one at a time, and don\u2019t put anything in the \u201creplace\u201d space.\n\nWhen you do this, each of those characters will be deleted from the list and will help you clean it up until there are only names.\n\n7. After you have nothing but a row of names from your Pinterest followers, go back to your Pinterest profile and click \u201cfollowing\u201c.\n\n8. Now that you see a list of people you are following on Pinterest, copy them all just like you did with the other list and paste them in a different column of your spreadsheet.\n\n9. Do the same thing you did before to find and replace unwanted characters in the list and then you should be looking at just the names of your followers in one column and the names of who you are following in the other.\n\n10. Next highlight the \u201cfollowers\u201d column, go to \u201cdata\u201d and click \u201csort sheet by column A, A \u2013> Z\u201c. Do the same thing for the other column so you can clearly see both in alphabetical order.\n\n11. Now, you can go through and compare who followed back and who didn\u2019t by looking at the names in the \u201cfollowing\u201d column and seeing if the same names are in the \u201cfollower\u201d column.\n\nOf course, to actually unfollow them, you have to go back to Pinterest.com and find the person\u00a0to unfollow.\n\n12. You can then create a third column and add the names of the people who you unfollow to keep track of in the future if you want to keep from following the same person multiple times while trying to grow your following on Pinterest.\n\nAnd there you have it. That is how you can manage Pinterest followers and unfollow people who don\u2019t follow back, at least until a decent software is created that does this or until you don\u2019t need to use the \u201cfollowing people strategy\u201d anymore.\n\nEventually, if you keep posting high-quality content, you won\u2019t need to worry about manually trying to grow your Pinterest following because you will naturally be getting new followers everyday.\n\nDon\u2019t be one of those people who is following 10,000 Pinterest users and only has 200 followers. It doesn\u2019t look good for your brand.\n\nMake sure you consistently manage Pinterest followers, so you aren\u2019t connected with a bunch of people not interested in your brand. Until a decent software is finally made for this purpose, unfollow people who don\u2019t follow back by using my spreadsheet method."},
{"url": "https://blog.picnichealth.com/picnichealth-and-ubiome-are-partnering-to-further-ibd-research/", "link_title": "Ubiome, PicnicHealth Partner to Correlate Clinical Data with Microbiome Data", "sentiment": 0.14066617259552047, "text": "Last summer, we were both participants in Y Combinator, a program for entrepreneurs that changed both of our lives. We came together as part of a group of female founders, who were also leading companies in the health / life sciences space. uBiome was founded to analyze the microbiome by giving participants access to their own microbiome data. PicnicHealth was founded to give patients access to their complete medical records.\n\nIt was a crazy summer, but one of the things we learned was the importance of collaboration, launching things quickly, not being afraid to do something bold and new.\n\nToday, we announce a ground-breaking partnership to advance research on role of the microbiome in Inflammatory Bowel Disease, using validated clinical data. The partnership will give hundreds of patients access to their medical records and to an analysis of their gut microbiome, free of charge.\n\nWe couldn\u2019t be more excited! Here\u2019s why:\n\nIBD is pretty awful. Patients suffer from chronic inflammation of the digestive tract, which can lead to pain, diarrhea, fatigue, and other symptoms. Currently, treatment options include anti-inflammatory drugs, immune system suppressors, antibiotics, and in severe cases, even surgery to remove parts of the intestines. More than a million and half Americans have been diagnosed and Noga Leviner, PicnicHealth\u2019s Co-Founder and CEO, happens to be one of them. (Here are some more personal words on why this is meaningful to her as a patient.)\n\nAs part of this study, participants will have access to their own data, something that almost never happens in medical research. IBD patients will get access to all of the medical records we collect through a personalized health timeline created by PicnicHealth. Because we clean up and structure all of the data, they\u2019ll be able to see things like trends in key lab tests that they have never seen before. They\u2019ll also get to learn about the ecosystem of microbes and see how it compares with other participants and relates to current scientific research.\n\nGetting all of this data together in one place is a big deal. The partnership will produce a first of its kind data set that has real prospects for making an important contribution to science and hopefully, eventually, for actually improving people\u2019s lives. Pulling together these two data sources is non trivial and remains a major barrier for researchers.\n\nHow\u2019d we do it? By putting patients at the center of our work. By organizing scientific research around you, the participant, we can generate better data. It is our goal to use this data to produce meaningful scientific results that can lead to new diagnostics and therapeutics.\n\nAs it turns out, lots of people really want their personal data and lots of people really want to help contribute to science. We hope you\u2019ll join us!\n\nLearn more about the partnership and sign up: \n\n PicnicHealth.com/ubiome\n\n ubiome.com/pages/picnichealth"},
{"url": "http://www.webstreaming.com.ar/articles/safe-connections-for-apps/", "link_title": "How to Secure Apps Connections", "sentiment": -0.01940476190476191, "text": "It isn\u2019t rare that Android Play Store classifies the Apps for maturation time. Fixing application bugs is a constant work. But, we can fix some security bugs when making connections with the server.\n\nGenerally the amateur programmers think that no one may try to hack their code because the information isn\u2019t \u201cimportant\u201d, or because it is simply impossible to hack (for lack of knowledge).\n\nWhen we create a connection App\u2013API (server), we do internal connections sending data or getting request.\n\nThe Representational State Transfer or REST is a type of web development architecture that relies totally on the standard HTTP.\n\nREST allows us to create services and applications that can be used by any device that understand HTTP, so that\u2019s why it is incredibly simple and conventional compared to other alternatives such as, SOAP or XML-RPC.\n\nBy using a single REST request header we can develop applications quickly but with \u00a0vectors for attack.\n\nAn amateur programmer could use basic http request without ssl believing that other people won\u2019t realize, or that it isn\u2019t necessary because the information handle by the app is not private.\n\nIf a hacker connects to a net (such as coffee bar net) they can divert the net traffic to their computer.\n\nAn attacker enables the router and type\u00a0to\u00a0evade the detection of intruders system. Then the attacker can change the ARP table (ARP positioning). When an IP is not\u00a0found on the\u00a0table, they send the IP to all devices (broadcast) and the device sends his MAC Address.\n\n Now, the net traffic can pass through\u00a0the device of the attacker (Man In The Middle), and then send data\u00a0without the user suspecting it.\n\nThe most common hacks are intended to obtain\u00a0messages,\u00a0images or log in credentials. If we don\u2019t build a secure application\u00a0this can affect the application users and impair our business.\n\n If our application doesn\u2019t send credentials and only requests information it doesn\u2019t seem dangerous, but the attacker could\u00a0know how to operate\u00a0our\u00a0API server such as shown\u00a0on the previous print-screen, the data package shows the Request URL, headers and parameters.\n\n This information can be used to\u00a0send custom request, and to get data for\u00a0custom applications, malicious applications (in order\u00a0to steal information, access the\u00a0device etc).\n\nOnce the attacker is a \u201cMan in the middle\u201d, he can act as a Fake DNS server (or dns spoof), \u00a0the DNS is used for a domain name resolution converting for example webstreaming.com.ar to an IP address, 104.236.3.236. By compromising the protocol, the attacker could re direct someone looking for a domain name to his malicious website for example activating an apache server, only to send similar replies obtainecd from the stoled packages.\n\nDepending on how the application is programmed., the hacker can steal information, apply Social Enginne to get personal information, user credentials, install malicious applications etc. If some data is used directly on SQL statement (a serious programing mistake), it can allow SQL injection to update or delete data. Or for example, save data to change our app functionality as redirecting it to another server, the possibilities are tons.\n\nThe fake data not only allows to mislead the user but they can also be a vector to detect vulnerabilities, to crash the application, denial of service, or memory corruption among other things.\n\nBut once having the information from captured packages (image 2), they can also affect our server to detect vulnerabilities on services, code, etc. One tip to avoid this is not to allow \u00a0our API server to support multiple data formats. It might not seem important but for example sometimes a service is programmed to use one format and the server may accept multiple data formats and we do not realize or we did not anticipate it.\n\nFor example, the JSON endpoints may allow XXE attacks (or XML External Entity) parsing xml on the server.\n\nWe notice that uses application/json on headers \u00a0and this sends a Json string. Such as,\n\nBut if we change content/type\n\nThen we receive something like\n\n{\u201cid\u201d:{\u201cerror\u201d:\u201cParseException: XML document structures must start and end within the same entity.\u201d}\u2026\n\nNow, we know that the server allows xml and an attacker can inject code into xml similar to..\n\nand look for information on the server. It also works the other way around-\n\nTo guarantee the transmission of information, we used ssl on httpRest connections, we ensure a secure information travel.\u00a0And for example if a hacker is re directing the net traffic, they would probably not realize that happens, but on the application would appear a \u201cHandshake exception\u201d (Hey, we could develop a warning message for this).\n\n-1 the request without ssl -2 the request with ssl\n\nGenerally to protect Online Transaction we use a secure data transmission on forms, emails, file transfers. So, by using ssl and preventing errors on the code, we can keep \u00a0safe applications."},
{"url": "https://www.facebook.com/HappyPeopleHappy/videos/859227964131267/", "link_title": "Great public transportation in Japan", "sentiment": 0.6666666666666666, "text": "\n\n \n\n \uff1c\u5c08\u696d\u7cbe\u795e\u3001\u4e00\u7d72\u4e0d\u82df\uff01\uff1e\n\n \n\n Salute!\n\n \n\n Source: Charli James\n\n \n\n #japan #train #miracle #japanese #happypeople\n\n \n\n \u201cThe best way to cheer yourself is to try to cheer someone else up\u201d - Mark Twain Source: Charli James\u201cThe best way to cheer yourself is to try to cheer someone else up\u201d - Mark Twain //The \"7-minute MIRACLE\" in Japan// \uff0d Please SHARE\uff1c\u5c08\u696d\u7cbe\u795e\u3001\u4e00\u7d72\u4e0d\u82df\uff01\uff1eSalute!"},
{"url": "https://www.vogogo.com/web/wave/2015/05/crypto-adoption-growth/", "link_title": "Crypto Adoption Growth", "sentiment": 0.13190497002997, "text": "One way of encouraging crypto adoption amongst the general public is to increase the number of retailers that accept cryptocurrency. Not only will crypto enthusiasts have another place to spend their coins but cryptocurrency will gain greater credibility as a form of payment. There were some interesting developments in crypto adoption over the last few weeks, here are some of the highlights.\n\nBitnet partners with Universal Air Travel Plan (UATP)\n\n At the start of February, payment processor Bitnet partnered with Universal Air Travel Plan (UATP), a\u00a0travel payment platform owned and run by the world\u2019s airlines, particularly popular with the\u00a0corporate travel sector. The partnership means that over 260 of UATP\u2019s member airlines can now\u00a0accept Bitcoin. UATP processed over $14 billion worth of transactions in 2014 and counts some of\u00a0the most high profile airlines amongst its members including American Airways, British Airways and\u00a0Lufthansa\n\nBitpay partners with Adyen\n\n Bitpay recently announced a partnership with fiat payment processor Adyen. While Adyen isn\u2019t\u00a0necessarily a household name, it processed an impressive $25 billion worth of transactions in 2014\u00a0and recently raised $250 million in venture capital funding, valuing the company at $1.5 billion. To\u00a0start with, users of Jagex, the developer of MMORPG RuneScape, will have the opportunity to pay in Bitcoin. This partnership could grow significantly though because Adyen also provides payment\u00a0services for some of the most well-known online brands including Facebook, Uber and Spotify.\n\nStripe integrates Bitcoin\n\n Following an extensive testing period, payment platform Stripe now allows merchants in its network\u00a0with a US dollar account to start accepting payment in Bitcoin. Stripe offers payment services to\u00a0some more of the biggest online brands including Shopify and Kickstarter and was named an\u00a0Applepay partner at the end of 2014. Although the company doesn\u2019t disclose transaction volumes, it\u00a0recently raised $70 million in a second round of funding which values the 6 year old company at a\u00a0staggering $3.5 billion.\n\nDell accepting Bitcoin in Canada and UK\n\n Dell recently announced that the company is now accepting bitcoin payments in Canada in the UK.\u00a0When the tech giant started accepting Bitcoin in the US in July 2014, the crypto industry received a\u00a0major boost from the backing of such a high profile brand. Dell has decided to extend the program\u00a0outside of the US based on positive customer feedback and demand, making it the second largest\u00a0merchant to accept Bitcoin after Microsoft.\n\nParkt offers Bitcoin rewards\n\n Parking app Parkt recently integrated Bitcoin into its rewards program. Parkt is an innovative app\u00a0that rewards shoppers with points for using certain merchants. Users can redeem the points against\u00a0parking fees, or as cash, or Bitcoin. Currently an early stage startup, Parkt is launching in Seattle\u00a0initially but plans to expand across the US and internationally. The company is also exploring the\u00a0possibility of further integrating bitcoin to allow users to pay for parking with it too.\n\nWhile some pretty high profile brands are mentioned above, even the lesser known merchants still\u00a0processes substantial volumes of transactions which should help establish crypto\u2019s credibility even\u00a0further."},
{"url": "http://consequenceofsound.net/2015/05/the-lyrics-of-recent-no-1-singles-average-at-a-third-grade-reading-level/", "link_title": "The lyrics of recent No. 1 singles average at a third grade reading level", "sentiment": 0.10053297956523763, "text": "No one would ever dare to compare the writing\u00a0prowess\u00a0of artists like Macklemore, Nicki Minaj, and\u00a0Katy Perry to\u00a0Chaucer and\u00a0Ginsberg, but a new study from Andrew Powell-Morse reveals just how dumbed down the lyrics are for songs currently dominating\u00a0the Billboard charts.\n\nPowell-Morse analyzed the reading levels for 225 songs that spent three or more weeks atop Billboard\u2019s Pop, Country, Rock, and Hip-Hop song charts.\n\nWhereas chart-toppers in 2005 read between a third and fourth grade level, a decade later that average is declining, and fast. In 2014, the reading level of a Billboard No. 1 single averaged between a second and third grade reading level, with the bar trending downward in five of the last 10 years.\n\nOf the four genres analyzed, country music came out with the highest average reading level (3.3), followed by pop (2.9), rock \u2018n\u2019 roll\u00a0(2.9), and\u00a0R&B/hip-hop (2.6).\n\nAt an individual level, the data is even more\u00a0fascinating. The average reading level of Eminem is a grade-and-a-half higher\u00a0than Beyonc\u00e9, while Nickelback (!) tops Foo Fighters by nearly a number\u00a0letter grade. In the world of pop music, superstars\u00a0like Mariah Carey and Adele rank a full number\u00a0grade higher than the likes of Lady Gaga and Ke$ha.\n\nOf all 225 songs analyzed in the studio, the highest-scoring rock song was Red Hot Chili Peppers\u2019 \u201cDani California\u201d with a reading level of 5.5. Meanwhile, Three Days Grace\u2019s \u201cThe Good Life\u201d is the \u201cdumbest\u201d with a reading level of 0.8.\n\nBelow, you can find a few more infographics illustrating\u00a0the data. The full report can be read here."},
{"url": "http://twentytwowords.com/like-engineer-sales-meeting/", "link_title": "What it\u2019s like to be an engineer in a sales meeting", "sentiment": 0.1, "text": "When the sales team brings an engineer into a final sales meeting as an \u201cexpert\u201d reference to help put the client at ease and close the deal, the engineer knows he\u2019s really just there to say, \u201cYes, we can do that,\u201d regardless of the glaring truth\u2026\n\nDon\u2019t forget to commiserate with your engineer friends on Facebook or Twitter\u2026"},
{"url": "http://www.www2015.it/brin-and-page-win-the-first-seoul-test-of-time-award/", "link_title": "Brin and Page Win the First Seoul Test of Time Award", "sentiment": -0.002525252525252522, "text": "Sergey Brin and Larry Page, the founders of Google, have won a new Test-of-Time award for what is one of the most influential works ever published in computer science: their paper \u201cThe Anatomy of a Large-Scale Hypertextual Web Search Engine\u201d that was presented at the World Wide Conference in Brisbane back in 1998 and introduced Google to the world.\n\nThe prize was presented at the World Wide Web Conference in Florence on May 22. Andrei Broder, a Google Distinguished Scientist, and one of the three keynote speakers at the conference accepted the glass trophies on behalf of the Google founders. Brin and Page sent a video message to the conference delegates expressing appreciation for the award.\n\nProfessor Dame Wendy Hall, Chair of the International World Wide Web Conference Committee (IW3C2) said, \u201cIt is impossible to overestimate the importance of Sergei\u2019s and Larry\u2019s paper in 1998. I cannot think of any scientific paper that has had such an impact on society. On any metric, the Brin and Page paper has to be the clear winner of the first award. Google changed everything, and you heard about it first at a World Wide Web Conference.\u201d\n\nThe World Wide Web was conceived in 1989 by Tim Berners-Lee at CERN in Geneva, Switzerland. The first ever World Wide Web Conference was organized by Robert Cailliau and held at CERN in 1994. Since then the IW3C2 conference has become an annual event rotating among locations in Europe, Asia, and North- and South-America, attracting well over a thousand international delegates every year.\n\nThe WWW Conference series aims to provide the world a premier forum for discussion and debate about the evolution of the Web, the standardization of its associated technologies, and the impact of those technologies on society and culture. The conferences bring together researchers, developers, users and commercial ventures \u2013 indeed all who are passionate about the Web and what it has to offer.\n\nThe official name of the new prize is \u201cThe Seoul Test of Time Award.\u201d It was made possible by a generous contribution from the team of computer scientists that organized the 2014 World Wide Web Conference in Seoul. \u201cThis prize will be given each year to the author, or authors of a paper, presented at a previous World Wide Web Conference, that has, as the name suggests, stood the test of time.\u201d explained Professor Hall. \u201cOn behalf of the organizing committee I would like to thank Professor Chin-Wan Chung, who led the organization of the Seoul Conference for his generosity and foresight in setting up this award.\u201d"},
{"url": "http://aeon.co/video/society/final-draft-scripting-the-apocalypse-a-nuclear-attack-how-to/", "link_title": "1973: secret UK committee drafts message to be played in case of nuclear attack", "sentiment": 0.08853754940711461, "text": "In 1973, fearing a Soviet nuclear strike, a UK government committee was formed to write a message to be played from the British Broadcasting Corporation\u2019s secret bunker in Scotland\u00a0during a worst-case-scenario attack. Informing your citizens of their imminent annihilation is difficult enough, but drafting a doomsday script raised even greater challenges for the secret team. How can you write a convincing, useful message when you don\u2019t know the specifics of the situation? And how can you reassure the public that the message is being read live, and the BBC hasn\u2019t been completely destroyed? Moreover, how long might it take for the entire committee to approve a final draft?\n\nIrreverently constructed using declassified documents and scenes from the BBC\u2019s drama-documentary The War Game (1965),\u00a0Final Draft: Scripting the Apocalypse\u00a0is a darkly comic, Kubrickian examination of the deep weirdness of modern warfare.\n\nFor more by the US director Scott Calonico, watch\u00a0You Can\u2019t Always Get What You Want, which candidly chronicles the private phone conversations of President Lyndon B Johnson.\n\nWar. What is it good for? Join the conversation on\u00a0Aeon Ideas."},
{"url": "http://info.meteor.com/blog/meteor-and-a-galaxy-of-containers-with-kubernetes", "link_title": "Meteor and a Galaxy of Containers with Kubernetes", "sentiment": 0.24499624525134728, "text": "Meteor is building Galaxy, the best way to run Meteor apps in production. Galaxy will scale from free test apps to production-suitable high-availability hosting. Want to help us? We're hiring.\n\nCloud-scale application hosting is shifting to the use of containers, and Galaxy will be no exception to this.\u00a0A big driver there was Docker: Docker did an amazing job of popularizing containers, in particular the one-process model for easier management and easy image creation and distribution.\n\nContainers offer efficient isolation compared to full virtual machines, and now enjoy full support in the mainline Linux kernel. They empower a new way of thinking about applications: a move away from machine-first, to thinking about processes and how your processes communicate.\n\nIn the container world, you rapidly end up with a large number of containers, which presents significant operational challenges. \u00a0Those problems only get harder when you want to run across multiple machines, especially in the cloud, and when you want to do this at huge scale for thousands of tenants.\n\nKubernetes is an open source project led by Google, and is their way of releasing their internal technology called Borg. \u00a0Everything at Google runs inside containers - crawling, search, gmail, even google compute VMs - and it has all run inside Borg for the past 10 years. \u00a0Kubernetes is developing rapidly and\u00a0the technology has probably logged more machine hours than all of AWS.\n\nWe\u2019re very happy to be basing Galaxy on that same battle-tested technology, by building on top of Kubernetes. \u00a0Kubernetes manages the basic resources of computing - compute, networking and storage - and makes sure that your containers reliably get their fair share of each, and stay running even as the underlying systems may fail. \u00a0Most of all, it lets us think in terms of multiple containers which run a service.\n\nFor you as a Galaxy user: this will mean that your meteor apps run as services (or micro-services), running in multiple containers that automatically reconfigure themselves to work around failure, and can be easily scaled when your app hits 'ProductLaunch'. \u00a0This is devops best practice, but done \u201cthe meteor way\u201d - it just works; you\u2019ll still but now you\u2019ll be deploying to a production hosting environment.\n\nThere\u2019s a lot we\u2019re building in Galaxy. \u00a0Meteor has taken the lead on work to make sure that Kubernetes itself is production ready on AWS rather than its GCE roots given the volume of developers working with AWS and therefore the demand for that functionality.\n\nWe\u2019re also building the other parts of a complete cloud runtime that any Meteor app needs: a stateful routing tier \u00a0that has first-class support for websockets - not just HTTP - and a way to fall back to long-polling when neccessary.\u00a0We\u2019re also building the infrastructure on top of Kubernetes to make sure your app is available even as we upgrade\u00a0the operating system, Kubernetes, Galaxy, or your application itself, or in the case where a process or whole VM suddenly fails.\u00a0Of course, we have a lot of plans for post-V1 too!\n\nWe\u2019re really excited about Galaxy, and hope you are too. If you\u2019re excited enough to help us build it, then we\u2019re hiring!\n\nFor more discussion, you can click here to listen to a hangout with myself and Arunoda of MeteorHacks talking Docker, Kubernetes and Galaxy."},
{"url": "http://www.forbes.com/sites/joshsteimle/2015/05/22/9-tips-for-a-successful-startup-on-foreign-soil/", "link_title": "Tips for Starting Your Company on Foreign Soil", "sentiment": 0.14781079427818558, "text": "Growing and scaling a startup, whether it\u2019s in an exciting new market or a competitive and saturated one, may not be as straightforward as you\u2019d like \u2014 especially if you want to launch in a foreign country. From inadvertently designing offensive logos, to failing to protect your intellectual property or simply being unable to penetrate your target market, a lot can go wrong. It takes some savvy business skills to ensure you don\u2019t return home with your tail between your legs.\n\nWhat advice do those who are experts on creating startups on foreign soil give?\n\n\u201cProfessional advice can seem expensive when you\u2019re starting out, but heeding unprofessional advice will undoubtedly be even more expensive,\u201d says Simon Galpin, Director-General at Invest Hong Kong. He continued, \u201cWhile it\u2019s true that capital is the lifeblood of a startup, the cost of bad legal advice, poor translation and cheap accountants will, in most cases, end up being even higher than what you would have paid for good, local professional advice.\u201d\n\n2. Pack your bags and go\n\nVisiting a country where you want to do business is the best way to find out how your customers live, think and operate. In today\u2019s world of Skype, WhatsApp and IM, doing business from your desk is easy but as much as you need to trust and empower your team abroad, going onsite to meet people face-to-face is equally important. This is true especially in countries across Asia, where in-person interaction is more appreciated than it is in western cultures that rely heavily on conference and video calls for communication.\n\nNon-profit organization Slush.org makes technology startup conferences happen on a global scale. \u201cEven if you don\u2019t have a physical presence in your target market, pay a visit to the grassroots community,\u201d says Martin Talvari, CEO of Strategy at Slush. \u201cThey\u2019ll make you feel like a local and you\u2019ll get to know the right people who can help you move forward. Don\u2019t blatantly sell your product or ask to be promoted\u2013be useful for them and make it a win-win situation.\u201d After you\u2019ve made the personal connection, Talvari, who also founded Science Knows No Country says, \u201cLater on, it\u2019s easy to stay in touch via email or Skype.\u201d\n\n3. Do what you know and love\n\nMawgan Batt is Founder and Director of The HK Hub, a comprehensive online guide to living, working and playing in Hong Kong. She says with 80 per cent of startups failing to get off the ground, it\u2019s important to be passionate about your product. \u201cWhen I moved to Hong Kong, I was overwhelmed by the pace of life and the density of the city. I didn\u2019t know where to find the best shops, bars, restaurants and things for my children and initially relied on word of mouth to find my feet. This made me realise the need for a website that pulled together all the fantastic things on offer all in one place and so The HK Hub was born!\u201d Batt says, \u201cI don\u2019t think it would have been so successful if I wasn\u2019t 100 per cent in love with the idea of providing the best guide in the city.\u201d\n\nSo you\u2019ve started your business and want to be present across the whole of Asia? Depending on your business, this could require pretty heavy investment in technology, people and infrastructure in places you\u2019re probably not too sure about. Let the business lead your investment. Sunita Kaur, Managing Director at Spotify in Asia says, \u201cAt Spotify we ensure we have a strong local music catalogue and a demand for the product before we take the plunge into a new territory. We believe that if you\u2019re going to roll out a music service to music fans who demand excellence, you want to be sure everything is perfectly in place for launch. We all need to think about the long-term game.\u201d\n\n5. Find partners who can help\n\n\u201cTrusted local partners with market expertise are key to expanding and scaling in emerging markets,\u201d says Paul Ahlstrom, Managing Director of Alta Ventures, a leading venture capital fund in Latin America. \u201cBefore expanding into any new market, we start traveling years in advance to those countries. We map the market and find the right trusted partners to work with, and research the risks and upside of doing business in that geography.\u201d When scaling internationally, find partners with local market knowledge of business basics like tax, legal, and regulatory compliance, and seek relationships with those who can assist with early customer traction and distribution.\n\nEvery company has different touch points and requirements for rapid scalability across global markets. CQS International is an eCommerce insurance solution provider, offering consumers the coverage they need at a price they can afford. CQS CEO, William Nobrega says, \u201cAs an eCommerce company focused on insurance products, our success strategy consists of building a highly advanced proprietary technology platform, and then acquiring small to medium-sized insurance brokers in each country. This provides us with an immediate customer base, database, licenses, infrastructure, and most importantly, a pool of highly experienced and talented management assets.\u201d\n\n\u201cPost acquisition integration is never easy, but by overlaying our online technology over the current infrastructure, we can quickly scale up revenues and build a common brand and culture,\u201d Nobrega says.\n\n7.\u00a0Seek opportunities to learn from each country entry\n\nRecent research has found technology-based startups (in particular) are more likely to succeed globally early on, especially compared to traditional industries, where internationalization often happens at a more gradual pace.\n\n\u201cMy colleagues studying the internationalization of software firms found that there was a specific structure and ordering to the more successful companies in what they learned along the way,\u201d says Chuck Eesley, Assistant Professor, Management Science and Engineering at Stanford University and board member of Startup Chile. He continues, \u201cFirst were the rules or heuristics for selecting which country to enter and how. Secondly there was timing and how to tell if a market was ready; and thirdly, \u2018prioritizing\u2019 which countries were best to enter after they\u2019d been successful in a neighboring country.\u201d\n\nHong Kong, for example, is a special administrative region within China that sets its own tax rates, has its own convertible currency pegged to the US dollar, enjoys free flow of talent and information, and operates a judicial system that is independent from the government. In other words, it combines access to China (and the rest of Asia) with low barriers to entry with a stable environment.\n\nEntrepreneurs are usually leaders by nature but blindly leading from the front, in a market that you don\u2019t know and understand could lead to bad outcomes. Cultural sensitivities, language barriers, local customs, ambiguous regulations and other factors mean that leading from the trenches may, in many cases, be the clever thing to do.\n\nMarketing and business start up specialist Matthew Reede says, \u201cThere is always a difference between markets, especially in the Asian region. As Founder and Director at Habitat Travel he says, \u201cSpending time on your UX (User Experience) by testing your product in a foreign market and understanding local buying habits before you take a tech idea live is paramount to success.\u201d\n\nAjit Melarkode is Managing Director at Rackspace Asia-Pacific, a managed cloud company. His advice, \u201cPick a platform where you can fail quickly and pivot very fast if required!\u201d He says, \u201cToday most businesses from taxis to healthcare are becoming IT businesses of sorts, so this invariably means picking an IT platform that can help you to achieve this. Cloud computing allows you to test ideas quickly and achieve results or not, fast.\u201d\n\nLaunching a business in a country where you\u2019re not a native has its challenges, but it\u2019s far from impossible, and the benefits can be sizable. Have you launched a startup in a foreign land? What tips would you add?\n\nJoshua Steimle is the CEO of MWI, a digital marketing agency with offices in the U.S. and Hong Kong."},
{"url": "http://www.wired.com/2015/05/faraday-porteur-ebike-video/", "link_title": "We got a chance to take the Faraday Porteur for a spin", "sentiment": 0.18055555555555558, "text": "More people are using bikes for transportation\u2014this is a good thing. But depending on your commute, your local terrain, and the amount of stuff you need to tote back and forth, it can also be a tall order. Enter electric bikes, which can turn an impossible trip into a joyride. Over the past month, we got a chance to take the Faraday Porteur for a spin; the bike, which began its life as a Kickstarter project back in 2012 and is finally available for general purchase, is one of the most elegant and zippy e-bikes we\u2019ve tried. Watch the video for a dive on its features, its benefits, and its not-so-benefits (at 39 pounds, you won\u2019t be slinging it around all that easily)."},
{"url": "http://www.hongkiat.com/blog/tools-to-coding-online/", "link_title": "15 Websites to Test Your Codes Online", "sentiment": 0.1798767848082916, "text": "Modern trends and webapps have dramatically changed the way web developers can build. Obviously you need some type of IDE to code new files and save them for deployment. But what about just testing your code snippets? There are more tools available now than ever before!\n\nIn this article I want to outline 15 interesting web apps for testing your code online. All of these apps require an Internet connection, and some of the more advanced editors offer pro plans to upgrade your account features. But most of these tools will surely come in handy when you\u2019re scrambling to debug a block of JavaScript or PHP.\n\nOriginally created by Steven Hazel, Codepad is a unique web app where you can share code syntax across the Web. Instead of just debugging, Codepad allows you to copy/paste important bits of code to share online.\n\nThe output screen displays any error messages associated with your code. The left-hand menu radio buttons allow you to change the parsing language from C/C++, Perl, PHP, Python, Ruby, and tons more. I would argue Codepad is really for software engineers who need to collaborate and debug their more confusing programs.\n\nThe main website for WriteCodeOnline.com actually redirects to their JavaScript editor. You can choose among JS, PHP, and basic URL encoding. Their application is very safe to use and feels lighter than other alternatives.\n\nWhat\u2019s interesting is that you\u2019ll see the output results directly underneath the text field. So when you hit \u201crun code\u201d it will parse through everything and display the result for you to see. It can be tough debugging some larger PHP scripts because you need to include other files.\n\nHowever for just testing the waters on a new idea, you can get a lot of value from this application.\n\nTinkerbin may actually be my favorite online code editing resource. It supports web developers coding in HTML5/CSS3/JS and renders the output directly on-screen. The application is still in Alpha development, but most of the tools work perfectly and can quickly catch bugs.\n\nThe rendering engine also supports more obscure languages such as Coffeescript and Sass within CSS. Their console is very advanced and clearly supports many of the same trends you\u2019d expect moving into the future of web design.\n\nAnother interesting note is how the most popular functionality actually supports keyboard shortcuts! This is something you hardly see on any webapp, let alone an in-browser source code editor. As you type new tags, the IDE will auto-indent new lines. Tinkerbin is truly the best frontend tool you can have in your web developer\u2019s toolbox.\n\nIn a similar fashion as above, jsbin is a simple JavaScript debugging console. Their pitch involves a collaborative effort where you can share a private link with other developers and write together in real time.\n\nTheir interface may be a bit confusing to newcomers. The developers have setup some online tutorials which you can read through if interested. Basically you can select between any number of JS libraries \u2013 jQuery, JQuery UI, jQM, Prototype, MooTools, there are dozens to choose from.\n\nAs you\u2019re coding different elements the drafts will autosave. You have the ability to download your final product or keep the source code saved online. Their system is much more advanced for exporting and keeping your code as a bare template.\n\nAnybody who has browsed through Stack Overflow must know about jsFiddle. Their interface is a whole lot difference compared to JS Bin, along with support for more complex functions.\n\nRight away you can signup for a free account and start saving your code samples online. jsFiddle offers a short URL which you can share around the Web via Twitter, Facebook, even Stack. But notice you do not need an account to start coding. It\u2019s just a handy feature to keep everything organized.\n\njsFiddle also supports the inclusion of libraries such as Prototype and jQuery. You can include additional external resources to JS/CSS files into each testing document. Incredibly their app even supports XHR Ajax where you can pass data back-and-forth between the server and client browser window.\n\nMoving from the world of scripting into stylesheet language, we have CSSDesk. You\u2019ve got a similar setup like all the rest, with your source code on the left and final webpage render on the right. This webapp is great for building small webpage templates and testing the longer CSS3 properties with gradients and box shadows.\n\nThis app also allows you to download source code as files to your computer. It can be a solid replacement in situations where you\u2019re working on a laptop without any IDE software. Or additionally, you can generate a short URL link to share online. Then other developers may come in and edit what you\u2019ve already created \u2013 definitely an interesting solution!\n\nHere you can share JavaScript, HTML5 and CSS3 code snippets. Their app is not as open as the others, requiring a much more formal registration process. This requires connecting into any other social network such as Facebook, Twitter, Google, or Github. Then you select a username and off we go to code.\n\nSome of the apps appear to have Japanese writing which makes me believe it was originally created somewhere in Asia. But what I love about their interface is how you can actually upload files you\u2019ve already created and store them into a project. It\u2019s such an easy process to store full webpage mockups online where you can access and edit them from any computer.\n\nI\u2019m surprised how many developers are not familiar with the Google code sandbox. You have full access to their APIs and can debug all your code right from the same window.\n\nWhen I talk about APIs I mean that you can pull data from the biggest Google products. Listing blog posts from Blogger, markers from Google Maps, and even video players directly from YouTube. As you click through these different examples the live preview box will update accordingly.\n\nI would recommend saving this tool only as a resource. It\u2019s not perfect for debugging everything you write. But Google is a huge company with a lot of open source API data. If you ever need to pull content from YouTube or custom Google Searches, this is the sandbox you want to use.\n\nIDE One is another tool based around deep programming and software development. Their online editor supports syntax highlighting for some very prominent languages. These include Objective-C, Java, C#, VB.NET, SQL and dozens more.\n\nWhat\u2019s so great about their app is how you can quickly debug many different programming languages from the same page. You can also store this source code via a unique URL to share around the Web. However I do feel that their layout is very cluttered with ads and other content, it makes using their website difficult. It would be really cool to see the option of including alternate code libraries, such as Cocoa Touch for iPhone app development.\n\nThis webapp also named Codepad is hosted on a website viper-7.com, which also redirects to the same online editor. Their debugging tools are setup for PHP output where you can change between PHP5 and PHP4.\n\nIf you create an account you can use their service as a personal storage system. Much like other online editors, you can name each PHP project and keep them hosted online for free. It\u2019s such a powerful code editor because you don\u2019t need any software on your computer at the time. As you parse each script the editor will offer additional meta details, such as browser request & response headers.\n\nThe self proclaimed JavaScript Code Quality Tool has to be JSLint. Their website is a bit strange, yet the code editor works exactly as you would expect.\n\nYou might find the options to be very confusing if you haven\u2019t used their framework before. It\u2019s possible to work with open source code such as Node.js if you have the skillset. But much of the source code doesn\u2019t even support syntax highlighting, a big letdown when you have so many other options to choose from. I would check out JSLint if you have the time, but it may not become your go-to online JavaScript debugger.\n\nWe saw earlier the power of a web application like jsFiddle. Now we can see SQL Fiddle which works in the same way, except for SQL database syntax. I have yet to find another alternative for testing database code and this is by far my favorite choice.\n\nAll of the output data from your SQL code will appear in a table beneath the editors. You can write some code to implement new data on the right and generate a schema on the left. This database schema is SQL code you can save to export your current database and re-install everything on a new server.\n\nIf you aren\u2019t familiar with databases or SQL language then this app won\u2019t be much assistance. But even for developers who are new yet interested in learning SQL, this is brilliant! Check out one of their basic code examples so you can get an idea of how the app works.\n\nIn my opinion Cloud9 is the best source code editors you can find online. It\u2019s not just an editor, but an entire system of tools and resources and you can store all your code repositories on their servers.\n\nAccount signup is free for all public projects. But if you need private development space this costs $15/month which only adds up to $180/year. You can share these private code repos with anybody you choose. This brings the opportunity for collaboration between web developers on many different projects.\n\nEach new project is stored in a subfolder where you can generate real physical files. HTML, CSS, JS, PHP, anything you need to code will be saved locally in your account. Then you can later export these files as a whole project and download them to your computer.\n\nThe amount of things you can accomplish with Cloud9 is extraordinary. I highly recommend toying around in a free account for even 10 or 15 minutes, you\u2019ll be amazed at the UI performance. And their company is always growing so I\u2019m expecting to see very cool features released in the next few years.\n\nThe CodeRun IDE is an online editor for any dynamic web application. Their text editor looks very similar to Microsoft Visual Studio, and you can even code in C# for ASP.NET. Their libraries include 3rd party resources such as Facebook Connect and Sliverlight.\n\nBut aside from Microsoft-based web applications you can also code in straight JavaScript or PHP. The application runs very similar to Visual Studio where you create a new Website Project and develop over individual files. Towards the bottom of the screen you\u2019ll find debugging tools and output from the console window.\n\nCodeRun is fantastic if you do have any experience working with Visual Studio. The interface behaves almost exactly the same, and you can even upload/download project files locally to your computer. This is another tool experienced web developers may consider bookmarking for future reference.\n\nHere we have another desktop-style online IDE Compilr with a similar template as Windows applications. You can work with open tabbed documents and edit files right on the fly. However you do need to register an account before you can create any new projects.\n\nSince their layout is designed similar to a regular desktop application it\u2019s much easier to work with having no prior experience. I don\u2019t think any developers would struggle, although their tools do support true programming practices with C++, C#, and Visual Basic. If anything Compilr should be one more app you have in reserve for testing and debugging source code.\n\nWith more computers connected online, it\u2019s getting easier for developers to work together and collaborate in the browser. We\u2019re seeing more and more technologies shift from local applications, and who knows how far this trend will go?\n\nI hope this collection of code testing tools can get you thinking about the modern development environment. It\u2019s so easy to quickly put together an HTML/CSS web project and within minutes have a small demo preview. Remember these are only tools to help guide you along the path to constructing your final product. If you have any suggestions or questions about the article feel free to share your thoughts in the discussion area below."},
{"url": "http://noammor.github.io/noammor/ipy/str-join-benchmark.html", "link_title": "Python idioms performance characteristics", "sentiment": -0.0225, "text": "I've been meaning to check this out for a long time.\n\nThe setup is simple. We write a monstrous decorator which modifies a simple function that iterates n times, to a function that runs the former on a number of inputs and returns how long each run took.\n\nI'm not in the business of 4% performance improvements here. I'm after quadratic behavior!"},
{"url": "http://brettterpstra.com/2015/05/22/logging-snippets-for-sublime-text/", "link_title": "Logging snippets for Sublime Text", "sentiment": 0.19984126984126988, "text": "Following most of the same patterns as TextMate snippets, Sublime Text snippets can be a great timesaver. One of the things I\u2019ve been doing recently is assigning keyboard shortcuts to snippets instead of tab expansions, allowing me to apply them to selected text with the placeholder. That, combined with some text mutation offers some serious timesaving options. To illustrate, I thought I\u2019d share a couple of very handy logging snippets.\n\nThese are added to the default user keybindings file located in your Sublime application support folder, in the file (substitute your OS as needed).\n\nThe first thing to note is that you can add a language scope to the keybindings, so that the same keybinding inserts different snippets depending on what language you\u2019re currently working in.\n\nHere are the two logging snippets I use most commonly, one for JavaScript, and one for Objective-C:\n\nThese add an (Option-Shift-L) keybinding. When you trigger it in a JS file, it will insert a statement. If there is a selection when it\u2019s triggered, the selection will be moved into the arguments for the command. Initially the keyword is selected, so that you can change it to , , or any of the available methods in the console API. Hitting TAB from there jumps into the arguments for the command, where you can add a string or object to log. One more TAB jumps the cursor to after the closing semicolon.\n\nIn the Objective-C snippet, \u2325\u21e7L will produce . Any current selection will be added to the format string arguments, and you can use placeholders to reference the arguments. The is automatically selected for editing. If you delete the from the format string, the trailing comma will automatically be removed. If you add back any placeholder, the comma will return and TAB will place the cursor into the format arguments.\n\nBetween the two of these examples, you should have enough to come up with snippets for debug logging in any language. If you have some awesome snippets to share, please gist them and shoot me a link here or on Twitter!"},
{"url": "http://realm.io/news/swift-summit-joseph-lord-performance/", "link_title": "How Swift is Swift?", "sentiment": 0.12163616361636159, "text": "Swift is designed to be fast, very fast. Static typing helps, as do value types. There is no point to aliasing unless you actually use the pointer types when it has something to do. Constants and copy-on-write all help. And so on.\n\nI started looking at performance in Swift back in August, and I thought it should be faster. People were experimenting, but compiled code wasn\u2019t running as fast as they expected, compared to other languages. So, I started playing with Swift myself to see what helped performance, and I found a number of things that made it run faster. By looking at other people\u2019s code, it\u2019s slightly clearer where the optimizations can be found. I would look for bottlenecks, and adjust the code to see what helped \u2014\u00a0all slightly more revealing that working with my own codebase. I looked for situations where developers had problems, and were complaining; I\u2019d look at their solutions, and see what wasn\u2019t performing well.\n\nI\u2019ve been looking at the recent betas of Swift, so most things in this talk are for the lastest Xcode beta at the time I\u2019m speaking \u2014 6.3 beta 2. They will apply to earlier betas, and as a history, in some cases were more significant in the earlier days of Swift. As a mark of Swift\u2019s impressive developmental pace, there are performance tricks that were necessary in Swift 1.1 that are often automatically done by Swift 1.2. Swift 1.2 beta 1 made things substantially faster, particularly for debug builds, and beta 2 basically removed the need for several things that I had been doing before. Good progress!\n\nThe ability to statically dispatch, and avoid indirection, allows the inlining of code, which is really important for real performance gains. Much of your optimizing will be about enabling it.\n\nMost applications don\u2019t really need much optimization. Most of the app\u2019s time is spent in API calls, network calls, and blocking library things, so if you\u2019re looking to optimize, those are the first thing to sort out. I won\u2019t be looking at these. The next thing, particularly in Swift, is the build settings you need to setup. Debugging is hugely slower than the release builds, because it doesn\u2019t take any shortcuts. All the data must be there for you to step through and debug properly, so the performance is wildly different.\n\nThe next thing you do is profile and measure which parts are slow, and where it really matters. Quite obviously, if you speed something up by a hundred times but it was only running a fraction of a percent of the time, you haven\u2019t done much for your program. The profiler, which you should really get to know in Xcode Instruments if you\u2019re doing anything performance related, won\u2019t allow you to read anything, but it\u2019s worth knowing that it\u2019s there. It\u2019s quite useful for finding where the problems are. For example, you can stop when a call is happening from a function that you hoped would be inlined.\n\nOn top of the things I recommend, there are a few other things that will make your Swift code run faster. If you\u2019re going for serious performance, for many applications you will want to step out of your code for a second, and use Metal, Accelerate, or full on parallelism.\n\nDebug builds default to . Even in 1.2, this is typically about a hundred times slower for performance critical code, which means really quite slow. For release builds the default is , which is pretty fast. Choosing the builds will remove any precondition checks (and a few other checks) you might include in your code. Right now, it doesn\u2019t make a huge difference, as unchecked is usually about 10% faster.\n\nNew in Swift 1.2 is a setting for whole module optimization, which turns off incremental builds. This slows down the build because, if you enable it, it will build the entire module or application in one big compile, so it\u2019s all in one file. Your builds will get slower, but it does allow inlining and optimization between all those files. With it turned on, you no longer have to move code to the right file to get the best performance, which can make quite a big difference. It will also allow the creation of specific versions of generic functions where they are being used, which can be helpful.\n\nIf you look at side by side comparisons of software running in different builds, with the main object changing from a struct to class, you will see that the class builds run significantly slower. In situations where you are using the CPU intensively, optimization becomes really important. To emphasize, in my test scenario, the entirely unoptimized version takes half an hour to reach a point that only takes the optimized version 40 seconds. This should give you a clear idea of just how much difference a debug build makes. The lesson here: if you have a big project, you might want to consider doing debugs in some parts of the code, and not in others.\n\nTo help with this, you can make two targets for your application. In one of them, make a framework target that will be used by the main application. Put the performance-critical code, and all the things it calls into, in that framework. This lets you optimize the framework, run it with whole module optimization, and separate out the remaining code into a debug build calling into it. When you do a release build you can optimize the whole thing. It\u2019s a way of setting up your project to get the best of both worlds; extra hassle, sure, but if you\u2019re really having problems with the slowness of the builds, or you need the debug in other areas of the code, it can be very useful. And if something really didn\u2019t work in Swift, you could always drop into C or C++ in the inner loop.\n\nSo, the big question: is C in Swift faster than Objective-C? The question is simple, the answer is not. C is valid Objective-C and it\u2019s fast. -based code with NSArrays retain and release from ARC and so on, but it will be a lot slower. So, the unhelpful philosopher\u2019s answer: it depends what you mean by Objective-C. If you\u2019re using Swift with structs, it will usually be nearly as fast as C, and quite a lot faster than Objective-C, so for now, let\u2019s say it\u2019s likely to be somewhere between the two.\n\nIn comparisons between Swift, C, and C++, I\u2019ve been tweaking other people\u2019s projects online to see how close I can get performance. In most cases, I find that it gets within around 20%, but there are exceptions. Primate Labs, who make Geekbench, published the source code for their Swift performance testing implementations, along with an article detailing the results of a comparison with their C++ benchmark code. After some work with the latest betas, two of the three tests, FFT and Mandelbrot, were near enough equal between Swift and C++. But the third test, GEMM, was four times faster in C++. Part of the reason I believe, is that the test used \u201cfast math\u201d, which is less accurate, may have allowed further optimizations, and is not available in Swift.\n\nOne other test was done by David Owens, with a gradient render that performed seven times faster in C. He detailed his results in a series of posts, demonstrating that there definitely are cases where C is a lot faster. Those tests used fast optimization in C, but when built with the OS (which is perhaps more typical), C was only two times faster than Swift \u2014\u00a0still significantly faster, but it is certainly within bounds for the Swift compiler to improve. Indeed, as a general point about Swift, the knowledge the compiler posesses about your code should let it do at least as good a job as C.\n\nSo, how do you get your Swift to be Swift? I have already mentioned a few. To start with the simple things: use structs where you can, as they will be much faster than objects. If you must use objects, make as many as you can . That allows much more direct calls into the code, and removes steps of indirection. Use constants with , which is good for not only optimization, but in my view, also for code design. With most classes, unless you design them really carefully, your methods are often at risk from being overridden by others in ways that don\u2019t meet expectations \u2014 especially if it mutates the object and does so in a different way. So is, I think, good for correctness as well. And structs are good for correctness, as are constants.\n\nThen there are some dangerous optimizations you could do. The operators , , and don\u2019t do overflow checks; you\u2019ll have to manage that yourself, or risk the overflow. With , you can avoid ARC calls which will help with speed, but you need to make sure that the object isn\u2019t released.\n\nA caveat: this only matters for highly performance critical code. For most of the code you write, do what\u2019s appropriate for the code style, and so on. Don\u2019t be a slave to optimization. However, when you come to performance critical parts, some things are very bad things to do! Global classes and static variables can be very slow to access. The compiler can\u2019t optimize away access because it could change from other locations. For performance critical code, avoid function calls that can\u2019t be inlined. Ideally, you don\u2019t want any function calls at all, because they\u2019re quite expensive. And if you can, avoid access via protocols, or to non-final class methods or properties, as well as Objective-C compatible ones.\n\nCheck your build settings. Profile it. Measure! An obvious thing to say perhaps, but set up and measure the critical parts of your code, so you know what changes are occurring, and what affects you\u2019re having, so you can see whether your changes actually help.\n\nAnd thank you!\n\nFor additional commentary on this talk, please see Joseph\u2019s blog. Thank you to Simon Gladman for the original Cellular Automata code used as a basis for some of these performance optimizations.\n\nQ: How do you make sure functions can be inlined?\n\n Joseph: You have to think about whether the compiler can know where it\u2019s calling to. That means it needs to be in the same file or module, if you have whole module optimization turned on. It\u2019s not via a protocol. There might be some exceptions where it can get away with that, but I don\u2019t know them. If it\u2019s in a class it needs to be final, otherwise it needs to worry about the inheritance. Although in the latest two betas, it will automatically mark things as final if they\u2019re in the same module and internal, or in the same class and private. The compiler can work if it can\u2019t be inherited from. You have to try and determine whether the compiler can know exactly what function it\u2019s going to call. Then, you can check whether it has worked by looking at the profiler, or looking at the SIL code, or looking at the assembler.\n\nQ: Are there things that we have to worry about when we optimize our Swift code, like particular parts of behavior that we can change? And can we test that somehow?\n\n Joseph: I think the behavior only changes for the unchecked case where the preconditions are skipped, but the compiler can assume that they are true. I don\u2019t believe any results will change in the fast case. There may be other differences in the unchecked, but I don\u2019t have a full handle on exactly what it skips out, and what it then does in that case.\n\nQ: Last year you found an issue with a float that had been put into an int, or vice versa, and since then Apple have made big improvement to that. Is this still relevant?\n\n Joseph: Yes, there was a case that I found where, because of the bridging to Foundation classes, there was a typo. Someone had written a signed little seven to a float. I think it worked via converting to and then back again, and things like that. I think there might still be some edge cases, but it\u2019s improved in the new beta because it doesn\u2019t automatically cast from Foundation types to Swift types. If you do include Foundation, especially in Swift 1.1, be careful! It changes the type safety because things will cast through between different types. That\u2019s slow because it goes to an object, as well as being potentially incorrect in places.\n\nQ: Have you run the same test across multiple Swift versions like 1.0, 1.1, 1.2, and was there a difference?\n\n Joseph: I haven\u2019t done a direct comparison. On the Geekbench tests, there are some results on their website. What I seem to remember is between 1.1 and the first beta, a couple of tests became a couple of times faster. Between beta one and beta two I could remove some of the ugly things I had to do previously. became automatic in that case, and also, I had been using an instead of an array, because it did provide big performance improvements in 1.1. In 1.2 that seems to have largely gone away, and arrays seem faster. I think they were over-checking quite a lot of things. I hope they\u2019re still checking enough, but I think they\u2019ve sped up arrays a lot. It depends at what point in Swift\u2019s development you started from, to see how big a jump it was. If you started without my optimizations, beta 2 compared to 1.1 could be 10 times faster or more, but that gain would be a lot less if you had already optimized in the ugly ways I suggested."},
{"url": "http://thenodeway.io/posts/testing-essentials", "link_title": "Testing Essentials for Node.js", "sentiment": 0.2107692840393733, "text": "Setting up good testing can be tricky no matter what language you\u2019re in. JavaScript\u2019s flexibility can make your tests powerful in terms of access, but if you\u2019re not careful that same flexibility can leave you tearing your hair out the next day. For instance, how do you test API callbacks? How do you deal with require? Without a proper setup, whether TDD is dead or not will end up meaning very little.\n\nThis post will explain the tools needed for efficient testing with Node.js. Together, they form an essential testing suite that will cover almost any project. The setup was designed to cover just the essentials, valuing simplicity over cleverness and advanced features. If that sounds counter-intuitive\u2026 read on.\n\nBefore introducing the tools, it\u2019s important to emphasis why you should be writing tests in the first place: confidence. You write tests to inspire confidence that everything is working as expected. If something breaks you want to be sure that you\u2019ll catch it, and quickly understand what went wrong. Every line of every test file should be written for this purpose.\n\nThe problem is that modern frameworks have gotten incredibly clever. This is ultimately a good thing, advanced tooling and technology can only benefit the developer. But it also means you\u2019ll need to be careful: this extra power is easily gained at the expense of clarity. Your tests may run faster or have more reusable code, but does that make you more or less confident in what is actually being tested? Always remember: There are no points for clever tests.\n\nTest clarity should be valued above all else. If you framework obfuscates this in the name of efficiency or cleverness, then it is doing you a disservice.\n\nWith that out of the way, lets introduce the essential Node testing kit. As you may remember, The Node Way values smaller, swappable, single-purpose tools over large do-everything-under-the-sun testing frameworks. In that spirit, the essential toolkit is a collection of smaller tools that each do one thing exceptionally well. They are:\n\nThe first and most important thing you\u2019ll need is a testing framework. A framework will provide a clear and scalable bedrock for all of our tests. There are a ton of available options in this area, each with a different feature set and design. No matter which framework you go for, just make sure you chose one that supports writing clear, maintainable tests.\n\nFor Node.js, Mocha is the gold standard. It has been around forever, and is well tested and maintained. Its customization options are extensive, which makes it incredibly flexible as well. While the framework is far from sexy, its setup/teardown pattern encourages explicit, understandable, and easy-to-follow tests.\n\nWith a new testing framework in place, you\u2019re ready to write some tests. The easiest way to do that is with an assertion library.\n\nThere are a ton of different libraries and syntax styles available for you to use. TDD, BDD, assert(), should()\u2026 the list goes on. BDD has been gaining popularity recently thanks to its natural-language structure, but it should all come down to what feels best to you. Chai is a great library for experimentation because it supports most of the popular assertion styles. But if you\u2019re a dependency minimalist, Node.js comes bundled with a simple assertion library as well.\n\nUnfortunately, assertions alone can only get you so far. When testing more complex functions, you\u2019ll need a way to influence the environment and test code under explicit conditions. While it\u2019s important for good tests to stay true to the original code, sometimes we need to be certain that some function will return true, or that an API call will yield with an expected value. Sinon allows us to do this easily.\n\nSinon includes a collection of other useful tools for testing, such as fake timers and argument matchers. In addition to Stubs, there are also Spies for watching functions are called and with what arguments, and Mocks for setting expectations on some behavior. Start simple and feel free to experiment as you go.\n\nYou\u2019re almost ready to start writing tests, but there\u2019s still one last problem in your way: . Because most calls to happen privately, you have no way to stub, assert, or otherwise access external modules from your tests. To really control them, you\u2019ll need to control .\n\nThere are a few different ways to accomplish this, depending on how much power is needed. Mockery lets you populate the internal module cache and replace modules to be loaded with objects of our own. Just be sure to disable & deregister mocks after the tests have run.\n\nRewire is another popular mocking tool that is much more powerful than Mockery. With it, you can get and set private variables within the file, inject new code, replace old functions, and otherwise modify the original file. This may sound like a better deal, but with all this power comes the cliched responsibility. Just because you can check/set a private variable doesn\u2019t mean you should. While tempting, rewriting and rewiring the private module scope like that brings your code farther away from the original module behavior, and can easily get in the way of writing good tests.\n\nTo see these tools all working together check out a working example on GitHub. While I singled out a few favorite libraries above, there are plenty of good alternatives in each of the categories listed. Think I missed something important? Let me know in the comments, or on Twitter at @FredKSchott."},
{"url": "http://www.newyorker.com/business/currency/patagonias-anti-growth-strategy?sf9473030=1", "link_title": "The Patagonia Clothing Company\u2019s Anti-Growth Strategy", "sentiment": 0.16046224018298494, "text": "Earlier this month, a peculiar vehicle appeared on the streets of Manhattan and Brooklyn: a biodiesel-fuelled, reclaimed-wood camper that could have been a food truck selling vegan \u201cish\u201d and chips. But instead of a meal, the truck was made to sell a message on behalf of Patagonia, the outdoor-clothing company.\n\nThe camper, dubbed Delia, was on a six-week cross-country road trip, repairing outdoor gear and selling used Patagonia products along the way. The amount of fixing that went on was humble in scale: ninety-three garments in New York City and about twenty-one hundred nationwide. The tour, which ended May 12th in Boston, is better thought of as the latest embodiment of the company\u2019s ongoing campaign to encourage a national conversation about the threat posed to the planet by a global economy that depends on relentless growth and consumerism.\n\nThat conversation\u2014despite being spurred, in recent years, by such figures as the author-activists Bill McKibben (a former staff writer) and Naomi Klein; the economists Robert Costanza, Tim Jackson, and Peter Victor; and the participants in a thinly spread \u201cdegrowth\u201d movement\u2014has so far failed to reach the volume even of mainstream Internet buzz. Yet anti-consumerism is clearly helping to build the Patagonia brand. Indeed, the company is seeing double-digit annual growth.\n\nRick Ridgeway, Patagonia\u2019s vice-president of environmental affairs, told me that the company\u2019s approach was inspired by a 2009 Times story he read about consumer spending during the last days of the Great Recession. The article noted that the financial squeeze was putting \u201cvalue in vogue\u201d\u2014and not only in the predictable form of bargain hunting. Impulse buying and conspicuous consumption had slowed, and some shoppers were seeking goods that offered enduring worth, such as fuel-efficient vehicles and gardening tools that allowed them to grow their own food.\n\n\u201cThat really caught my eye, because that is our value proposition. That is what we\u2019re trying to deliver to our customers\u2014those kinds of products,\u201d Ridgeway said. \u201cI thought, Wow, if at least some small cohort of people are recognizing that, then those people are our people, and how could we do a better job of giving them what they need to live more responsibly, not just in recession but any time?\u201d\n\nThe company\u2019s anti-materialistic stance ramped up on Black Friday, 2011, with a memorable full-page advertisement in the Times that read, \u201cDon\u2019t Buy This Jacket.\u201d The ad\u2019s text broke down the environmental costs of the company\u2019s top-selling R2 fleece sweater and asked consumers to think twice before buying it or any other product. The attention the ad received helped to bump Patagonia\u2019s 2012 sales significantly.\n\nIn September, 2013, the company launched its Responsible Economy campaign. In an accompanying essay (which, like the rest of the campaign\u2019s material, is no longer available at Patagonia.com but still can be read in Google\u2019s cache), under a graphic that declared \u201cgrowth is a dead end,\u201d Ridgeway argued that global environmental crises such as climate change, toxic pollution, and resource depletion were only symptoms of a larger problem. Annual, compounded economic expansion, of the kind that the Club of Rome warned against in its 1972 book, \u201cThe Limits to Growth,\u201d was the \u201celephant in the room.\u201d Since Ridgeway published his essay, Patagonia\u2019s own expansion has continued unabated: this year, the company expects to gross about six hundred million dollars.\n\nAll of this would be jet fuel for the engines of modern cynicism, if not for the fact that Patagonia, a privately owned corporation now in its fifth decade, has a distinguished record of environmental philanthropy and investment. The company has often made risky choices in favor of its ecological and social ethics, including early bets that consumers would pay more for products made with organic cotton or Fair Trade certification, the latter of which is now available on thirty-three of its products. The Responsible Economy campaign similarly backed talk with action. Patagonia is trying second-hand-clothing sales at its shop in Portland, Oregon, and has made product repair and recycling a growing part of its business model. It recently invested in Yerdle\u2014a Web startup whose stated mission is to reduce new-product purchases by twenty-five per cent\u2014as a way for people, and even the company itself, to swap or give away used Patagonia gear.\n\nThis spring\u2019s truck tour was part of a related campaign called Worn Wear, which is an attempt to draft a new compact between Patagonia and its customers. The company promises to make products that endure, and to repair, resell, or recycle them as necessary, while consumers, in turn, pledge to buy only what they need, and to similarly steward their purchases from new garment to storied heirloom to the recycling bin.\n\nIt is confounding to try to draw lines around when Patagonia\u2019s marketing encourages sales and when it discourages them. The gimlet eye will find no shortage of contradictions. Watch enough of the company\u2019s promotional videos, which feature real Patagonia customers, and you might start to believe that the United States is mainly a nation of earthy, physically fit people who are handy with a framing hammer, enjoy rock climbing, and know their way around Bhutan (one Worn Wear spot depicts an actual mountain guide turned family farmer). But visit the company\u2019s stores in locations like the Upper West Side, Hong Kong, and Chamonix, and you will also see the affluent recreational shoppers who helped to inspire the nickname Patagucci. Online, the company offers D.I.Y. garment-repair tutorials, produced in partnership with iFixit, but these also feature a thirty-dollar \u201cexpedition sewing kit\u201d that resembles a prop designed for the Khaki Scouts in Wes Anderson\u2019s \u201cMoonrise Kingdom.\u201d Click for more information on the kit and a typical marketing stratagem plays out: you will be offered six other Patagonia products.\n\nCorporate-social-responsibility theorists say that successful activist companies go though a \u201csense making\u201d process that renders their efforts meaningful both within the corporation and among its customers. In Patagonia\u2019s case, self-inquisition was part of the campaign. Even two years ago, with the launch of the Responsible Economy concept, Ridgeway was publicly questioning Patagonia\u2019s steady expansion. \u201cCompanies, including ours, are reducing the environmental footprint of our individual products but increasing the footprint of our company as a whole as we grow,\u201d he told Adweek. To GreenBiz, he said this: \u201cIt\u2019s our hunch that all these sustainability innovations put together are not going to be enough to offset the continued increase in our human footprint that comes from this tie to growth.\u201d\n\nThese days, the company is less ambivalent. \u201cWe\u2019re not afraid of growth\u2014we\u2019re excited about it,\u201d Adam Fetcher, the company\u2019s director of global P.R. and communications, told me. Ridgeway was more expansive: \u201cThere is a point out there where our own growth is going to likely create more problems than it does solutions,\u201d he said. \u201cBut as far out on the horizon line as we can see right now, we\u2019re continuing to produce products that allow people to live a more responsible life with the apparel that they choose. As long as there\u2019s a lot of other people out there that don\u2019t do that, and that are creating more problems than they are solutions, then we should be growing.\u201d\n\nThat narrative explains the Patagonia paradox: there\u2019s bad growth, and then there\u2019s good growth. An expanding economy driven by ever greater individual consumption of ever more disposable products is bad. In a more sustainable future, people will buy fewer things at higher prices, technological innovation will reduce the impact of those products\u2019 manufacture, and the goods themselves will be made to last and then be recycled at the end of their useful lives. Since those are the kinds of products Patagonia is striving to make, and the kinds of relationships to products that Patagonia is trying to foster, then the more that Patagonia expands its market share, the better. The new economy must grow out from beneath the old one.\n\nIt makes sense\u2014provided that is what\u2019s actually happening. Company spokespeople say that Patagonia\u2019s impressive growth over the past few years is explained by the fact that, like explorers in search of a lost tribe, they have made contact with a largely untapped market of sophisticated customers who support the brand\u2019s anti-consumerism by thoughtfully consuming its products. \u201cThese new customers are the ones inspired by our approach with Worn Wear and other programs that stem from our values,\u201d Fetcher said. Maybe so, but more conventional explanations are also available: for example, that the company is growing because it has expanded its reach (Patagonia has doubled its scale of operations in the past six years, and has opened forty new stores worldwide since 2011) and the impact of its marketing. Cognitive dissonance can cut both ways: it\u2019s quite possible that Patagonia\u2019s philosophy has attracted many shoppers to the brand without deeply affecting their buying habits, as suggested by the way that \u201cDon\u2019t Buy This Jacket\u201d translated, for many, into \u201cBuy This Jacket\u201d in 2012.\n\nPatagonia can\u2019t say for sure if its growth is in fact the good kind or the bad. Fetcher says that they\u2019ve had an \u201cenormous outpouring of interest\u201d from their customers, but that the company cannot currently provide any numbers on how many people are hanging onto their Patagonia jackets and board shorts for one more year rather than buying something new. Company records do indicate that Patagonia recycles about twenty thousand pounds of gear per year (roughly equal by weight to twenty thousand R2 jackets) and repairs some forty thousand garments; their Reno repair shop is the largest facility of its kind in North America. Given Patagonia\u2019s sales figures, it is clear that the company repairs and recycles only a very small fraction of the number of products sold each year. Not a revolution at this point, then, though surely a significant gesture\u2014corporate social responsibility\u2019s version of propaganda of the deed.\n\nPatagonia\u2019s boardroom is now enough at ease with growth that they\u2019re applying their business model in new directions. A foodstuffs division called Patagonia Provisions (shades of Wes Anderson again) aims to sell quality products that address the ecological consequences of farming, fishing, and livestock husbandry. So far, it\u2019s small potatoes: \u201cJust a tiny little million-plus-dollar business,\u201d Ridgeway said. Products on offer are limited to sustainably caught Alaskan salmon; an organic toasted-grain-flour soup mix based on tsampa, the staple food of Tibet; and all-natural energy bars. \u201cWe are imagining a point out in the future where that business is probably going to eclipse the apparel business,\u201d Ridgeway said, \u201cbecause that\u2019s where the biggest problems reside, and that\u2019s also where the biggest solutions reside.\u201d\n\nA sense-making narrative survives only as long as it is believed, and it depends on being both true and perceived to be true. It is still the early days of Patagonia\u2019s appeal to \u201cgood growth,\u201d and the company\u2019s core customers, sometimes referred to as Patagoniacs, appear to be patient. Lydia Baird, a student at the Fashion Institute of Technology who founded a campus cotton-muslin-composting project, was on hand when the Worn Wear truck made its Manhattan stop on the timeworn bricks of Greene Street, in front of Patagonia\u2019s SoHo store. None of her Patagonia gear needed mending, but the repair crew was cool with patching a hole in a competing brand\u2019s jacket, and they delivered her dad\u2019s favorite Patagonia vest to Reno, free of charge, for a more complicated zipper fix. Environmentally literate, socially engaged, and with a clear sense that contemporary consumerism is a warm bath of contradictions, Baird is the shopper that Patagonia wants to be in business for\u2014whether or not it really is.\n\n\u201cTheir growth now is only positive in my eyes. The bigger they get, the more impact they can have,\u201d Baird told me. \u201cWe definitely have to consume less, and there\u2019s no way the world is going to be a perfect place. But maybe production can be done better. Maybe production doesn\u2019t have to be a bad thing. And maybe Patagonia can lead us there.\u201d"},
{"url": "http://timesofindia.indiatimes.com/world/uk/Hundreds-evacuated-as-World-War-II-live-bomb-uncovered-near-national-stadium-in-London/articleshow/47389814.cms", "link_title": "Hundreds evacuated as World War II live bomb uncovered in London", "sentiment": 0.11713286713286715, "text": "LONDON: An unexploded 50kg World War II live bomb has been uncovered near UK's national football stadium here, forcing authorities to evacuate hundreds of people living nearby. \n\n\n\n Residents and traders have been evacuated from nearby buildings, and a 400m cordon has been erected in the area, the Metropolitan Police said in a statement. \n\n\n\n A \"Britain's Got Talent\" studio was also evacuated after the bomb was uncovered by builders near Wembley Stadium in London. \n\n\n\n The 50kg device, thought to date from the early 1940s German air raids on London, was found yesterday. \n\n\n\n \"This bomb is a live munition in a potentially dangerous condition so it's important that people listen to the police and evacuate their homes if asked. \n\n\n\n \"We will do all we can to minimize the disruption, but ask the public to bear with us. Any bomb, even under a controlled explosion, could cause significant damage to property and there is a genuine risk to life,\" an army official was quoted as saying by the BBC. \n\n\n\n The bomb also threatened the Football League play-off finals, which are due to begin tomorrow at the stadium. \n\n\n\n In March a 250kg (550lb) bomb was found in Bermondsey, south-east London. It was safely defused and taken to Kent to be destroyed."}][{"url": "http://www.financemagnates.com/fintech/bloggers-6/big-opportunities-small-investors-new-equity-crowdfunding-rule/", "link_title": "The JOBS Act and Equity Crowdfunding: Big Opportunities for Small Investors", "sentiment": 0.09952630702630702, "text": "Alessandro Ravanetti is co-founder and CMO of Crowd Valley, a global fintech company.\n\nSecurities-based crowdfunding is finally here. From May 16, 2016, it\u2019s possible for registered platforms to offer securities under Title III of the JOBS Act, for companies to use crowdfunding to offer and sell securities, and to retail investors to place their money on the next big thing.\u00a0The number of opportunities arising for retail investors through equity crowdfunding is huge.\n\nThe SEC adopted the final rules and forms on October 30, 2015, implementing the new Section 4(a)(6) of the Securities Act of 1933, as amended (the \u201cSecurities Act\u201d), allowing issuers to sell securities to the public under certain circumstances without registering such securities with the SEC. But what will these new rules mean for small investors?\n\nBasically, anyone will be able to invest, but given the risks involved by investing in early stage companies, there is an investment limit which depends on your net worth and annual income.\n\n\u2013 If either your net worth or your annual income is < $100,000, then you can invest, in any 12 month period, up to the greater of either $2,000 or 5% of the lesser of your annual income or net worth\n\n\u2013 If both your annual income and your net worth are = or > than $100,000, then you can invest, in any 12 month period, up to 10% of annual income or net worth, whichever is lesser. In any case you can\u2019t exceed $100,000 invested per year.\n\nA few examples in the table below taken from the SEC investor bulletin:\n\nIn order to calculate your net worth you should add all your assets and subtract all your liabilities. If you calculate your income or assets jointly with your spouse, each of your investments together cannot exceed the limit that would apply to an individual investor.\n\nExamples of net worth calculations, in order to determine the investment limits, are showed in the following table:\n\nMaking an investment in a crowdfunding offering will be very easy. It will be just needed to open an account with the crowdfunding intermediary (broker-dealer or funding portal ).\n\nEven if it\u2019s a great opportunity to invest early on in a promising startup, there are various risks that you should consider when you place your money in early stage ventures. The main risks are listed below:\n\n1. This is a speculative investment and you should be able to afford to lose the entire amount invested.\n\n2. It\u2019s an illiquid investment, as you will be limited in your ability to resell your investment for the first year. You may have to locate an interested buyer in order to resell your crowdfunded investment.\n\n3. There are cancellation restrictions, but there are up to 48 hours prior to the end of the offer period to cancel your investment commitment for any reason.\n\n4. The valuation of private companies is particularly difficult, and there is the risk to overpay the percentage of equity that you receive.\n\n5. A part of your investment may be used as compensation of the company\u2019s employees, including its management.\n\n6. As it happens with other investments, there is no guarantee that crowdfunding investments will be immune from fraud.\n\n7. No early-stage investors, as for example angel investors and venture capital firms, can translate into a lack of professional guidance.\n\n8. The required financial disclosure is limited compared to public listed companies.\n\nThis last point, about the limited disclosure required for a private company compared to a publicly listed company, is one of the main differences between a crowdfunding investor and a shareholder in a publicly traded company.\n\nPublic listed companies are generally required to disclose information about their performances every quarter, while private companies raising money through crowdfunding are required to disclose the results of their operations and financial statements on an annual basis. All the persons representing the company must identify themselves, and broker-dealers/funding portals are required to have transparent communication channels to allow the investors to discuss the pros and cons of an investment opportunity, as well as be able to ask questions directly to the management of the company.\n\nAnother big difference, in the points listed above, is the illiquidity of the shares you buy. You cannot resell your shares for the first year unless the shares are transferred:\n\n\u2013 to the company that issued the securities;\n\n\u2013 in connection with your death or divorce or other similar circumstance;\n\n\u2013 to a trust controlled by you or a trust created for the benefit of a family member;\n\n\u2013 as part of an offering registered with the SEC.\n\nEven if there are requirements and risks that you should think about when you consider investing your money through equity crowdfunding, the investment could be definitely worth it. E-Car Club, the UK\u2019s first entirely electric car sharing club for businesses and communities, is one of the most successful case so far, as the 63 investors which helped the company to raise \u00a3100,000 in 2013 have received a \u201cmultiple return\u201d on their investment following the sale of the business to Europcar in July 2015.\n\nMany more exits with big returns are expected in the year ahead, are you ready to place your bets?"},
{"url": "http://www.oneskyapp.com/blog/trello-localization-global-marketing/", "link_title": "Here's How Trello Nailed Localization and Global Marketing", "sentiment": 0.20149201013348925, "text": "There are some things that just about everyone loves, no matter where the person comes from. Laughter. Time with someone they care about. A good meal.\n\nAnd, perhaps, your product.\n\nIt\u2019s hard enough to build a software that\u2019s truly universal. But, even when you pull that off, there\u2019s still another set of challenges: making sure people everywhere have heard about your product, and then getting them to actually use it. Many software companies struggle to go truly international; even if they start getting active users from around the globe, few companies\u00a0know how to leverage this interest to get real traction in new markets.\n\nBut some do. Like Trello.\n\nEveryone who uses Trello loves it, and the company recently localized into a total of\u00a020 languages. We had the opportunity to sit down with Alexia, Trello\u2019s International Marketing Lead, and pick her brain about their international expansion strategies. In this blog post, we\u2019re going to share what we learned with you.\n\nNumbers don\u2019t lie, and Trello knows that. From the very beginning, the company has started with the data: first, they look at the information they already have, then hypothesize what strategies would be best, and, after that, use experiments to see if they were right. Only then do\u00a0they get started with the actual plan.\n\nThis approach resulted in a three-part localization strategy. First, they conducted two experiments. And, once those proved to be successful, they went for a huge global launch.\n\nA localization initiative that spans 20 countries takes time, money, and effort. And Trello knew they didn\u2019t want to commit too much of any of those things without being sure that localization would have the greatest possible impact.\n\nBased on user data, the company chose Brazil, Spain, and Germany\u2014three of their most popular markets\u2014as test cases. For each country, they localized the product quickly, using professional translation services. But, when it came to marketing campaigns, they went with three different tactics.\n\nComparing the results, they found huge differences between the countries. The effectiveness of localization ranged from almost nil, to successfully doubling signups during the launch period. After this first experiment, there was no question that localization could work for Trello\u2014but how they did it would matter.\n\nTrello then added a new market to the equation: France. Here, in addition to localizing the product, they capitalized on a number of marketing strategies to drive the number of signups to triple the number they usually would expect during a launch period. Even after the launch had ended, they still found that performance was 25% better than it had been before.\n\nAnd, beyond taking this opportunity to test even more marketing campaigns, they also experimented with their approach to translation. In France, rather than using professional translators, they tried crowdsourcing the task.\n\nAnd the results were surprising.\n\nVolunteer translators, whom they recruited, were able to finish 47,000 words of translation within a month\u2026.and the quality was much better than expected. Sure, these volunteers made mistakes sometimes, but they were also more familiar with the tone of Trello. The text came out friendly, and with a sense of humor\u2014exactly the voice that Trello prides itself on.\n\nThe France experiment was a success. Now that it was clear that volunteers would be eager to help with the translation (and the results would be better for it), Trello was ready to use crowdsourcing for other countries. All that was left was to make it happen.\n\nAfter the expansion into France, Alexia and the Trello team analyzed all the data to answer a number of questions: how signups varied based on country, which browsers different locals used, what the default languages were on user devices, etc. Based on this analysis, they selected dozens of languages to translate. And then, they got down to business.\n\nThere was a flurry of hard work\u2014crowdsourced translation, troubleshooting, collaborating with local markets\u2014and, in just four months, they were ready. Trello rolled out a global launch with\u00a016\u00a0new languages: Finnish, Norwegian, Swedish, Russian, Polish, Hungarian, Ukrainian, Czech, Dutch, Italian, Turkish, Thai, Japanese, Traditional Chinese, Simplified Chinese, and Vietnamese.\u00a0Add in the languages they localized during their experiments (Spanish, German, Portuguese, and French), and you end up with the coverage shown in the map below.\n\nIt\u2019s a big world, and Trello is now easily accessible to most of it. That\u2019s no small feat.\n\nTrello already had a strong user base\u2014which meant that they also already had access to a wealth of data. That\u2019s the perfect starting point for a global push.\u00a0Alexia explains, \u201cInstead of looking for what [languages] you want to have, choose the languages that your users really want. When you have a sizable community, you can understand what languages your users really want. Instead of wasting time on languages that people wouldn\u2019t actually use, choose those\u2026.\u201d\n\nBy starting and experimenting with information specific to your app, you\u2019re going to create a strategy that responds to your users\u2019 needs. Plus, choosing languages that your users already know\u2014and finding ways for them to feel invested in your product\u2014can have other advantages. Alexia notes, \u201cCrowdsourcers can do a lot of things, because we have a very engaged community.\u201d\n\nAlexia should know. Trello\u2019s users did all the translations for their global launch\u2014for free.\n\nCrowdsourcing gets a lot of flak. All you have to do is say the word to some people, and you\u2019ll get a speech about poor translation quality, unfair treatment of volunteers, etc.\n\nAlexia is definitely not one of those people. When Trello started crowdsourcing, the company ended up getting an incredible amount of positive attention from the Trello community. Their product has been free since the beginning, and it turned out that their users felt a lot of gratitude\u2014even love\u2014for the product and the company.\n\n\u201cI was once at a tourist spot in Mexico with my Trello tee,\u201d Alexia remembers. \u201cI was so surprised that there were people lined up behind me and [they] asked for\u2026photos, just because they use Trello and love it.\u201d\n\nAnd people don\u2019t just love Trello\u2014they want to help. Alexia adds, \u201cWhat we learned from French localization was that we could translate easily with crowdsourcing, and people are interested in doing that. And we realized, we could actually open any language [within the community].\u201d\n\nNow, they\u2019ve recruited anywhere from 30 to 50 translators for each language, working with a total of 520 people to build and maintain the 17 languageshey\u2019ve translated via crowdsourcing. And they\u2019ve made the requirements simple. To participate in Trello\u2019s crowdsourcing, you simply have to be (a) a native speaker of the target language, (b) able to communicate in English, and (c) an active Trello user.\n\nThis system works. To localize Trello into a new language, approximately 47,000 words have to be translated. That means that, in that four-month period of localization, a total of 800,000 words of translation were produced\u2014and almost all of it was done by volunteers.\n\nCrowdsourcing does come with some challenges, though. In this section, we\u2019ll lay out some tips and tools for proper product\u00a0(and translator) management.\n\nEight hundred thousand words, four months, 16 languages (French was already completed), and 520 people\u2014with numbers like that, Trello\u2019s project could have been a headache. This is especially true because volunteers were involved. As\u00a0passionate and kind as your contributors may be, the whole setup can feel like it will collapse at any second.\n\nThat is, unless you have great tools and smart workflows. With the right systems, Trello was able to pull off this project\u00a0with incredible success.\n\nTrello has found that the best tool is a\u00a0translation management system (TMS), which is crucial for the localization process. A TMS automatically imports any new content in the original language (in Trello\u2019s case, English) from the relevant website or app; when the translation is done, the TMS drops this new text back into the product. That means, as soon as new content appears, the crowdsourcers\u2014or professional translators, if you go that route\u2014are notified, and they can use the built-in translation tool to work with the content then and there.\n\nWhen a TMS does these tasks, you don\u2019t have to. Since the TMS automates so many steps of the localization process, you save both management and development time\u2014which helped Trello turn around its launch in a matter of months.\n\nFor product management and communication, Trello has exactly the tool it needs in, well, Trello.\n\nAlexia uses her own product to manage localization projects. She\u2019s created a Trello board for each language, and adds the volunteer translators to these spaces. On each Trello board, she can share instructions, project timelines, and the translation glossary. For the translators, it\u2019s great because they can discuss the work they\u2019re doing; for Alexia, it\u2019s great because she can receive notifications about all the projects in one place. (And, of course, what\u2019s better than having translators build comfort\u00a0with the very product that they\u2019re helping to present?)\n\nAlexia has assembled just the right toolbox to make localization a breeze\u2014for her, and for her translators as well.\n\nWhen you crowdsource a project, you\u2019re counting on people to contribute because they love your product. That\u2019s great.\n\nBut it\u2019s also not a contract. Your volunteers can withdraw from a project at any time, or ease off on their commitment, and there\u2019s nothing you can do once they\u2019ve decided that they want to quit.\n\nSo, it\u2019s important to make sure they don\u2019t want to quit\u2014and that means rewards. Alexia noticed that, in the first two weeks, her volunteers were eager to contribute. But, after that, they would often become less passionate about the project. That\u2019s when they needed a boost, and Alexia delivered. Here are her two tricks:\n\nNot everyone loves crowdsourcing, and it\u2019s often criticized for producing poor results. But Alexia argues that translation errors are, in fact, inevitable no matter which method you use. \u201cProfessional translations services could also have mistakes,\u201d she notes. \u201cThe question is whether you implement good quality measures.\u201d\n\nAnd, if your quality measures are good, then the resulting translations won\u2019t simply be equal to professional work\u2014they\u2019ll be better. The advantage of crowdsourcing is that the translators are actually familiar with the product. Alexia explains, \u201cTrello is famous for its humorous and friendly voice. Crowdsourcers can capture Trello\u2019s voice more easily and reflect it in their translation.\u201d\n\nSo, wait. What quality measure should you be using? Two words: translation glossary.\n\nA translation glossary is a collection of the key terms in the source language. It improves translation quality by keeping translations consistent (and it speeds up the whole process). And Trello takes the task of creating it very, very seriously.\n\nInstead of using all their volunteers to crowdsource this part, Trello generally asks only local bloggers and journalists\u2014people who have already been writing about Trello in their local language\u2014for help with developing the translation glossary. Trello works with them to nail down the official translation for a couple different types of terms: the words that will be used for key features (such as \u201cboards\u201d and \u201ccards\u201d), and words that clearly need to have a consistent translation (such as \u201cdownload\u201d and \u201cemail\u201d).\n\nA translation glossary is important, but it\u2019s not enough on its own. Alexia would also prepare quality assessment materials to ensure that the content for translation comes with plenty of context.\n\nFor the product\u2019s content, such as in-app text, she provides information from the development side of Trello. And for the marketing content, such as the website and landing page, she provides the\u00a0fullest contest, such as\u00a0a\u00a0screenshot of the webpage.\n\nAnd, when translators have a problem, they know not to hide it. In the true spirit of crowdsourcing, there\u2019s a discussion board where they can hash problems out together.\n\nSo, you have the tools. Who should be using them, so you get the best results? Alexia has the answer for this one too. You need reviewers. And you need a critical mass of translators to make crowdsourcing successful.\n\nReviewers have more responsibilities than your average participant. They\u2019re the ones who will check the quality of work, and they will ensure that there is consistency across all the translations. For each language, you should have three or four reviewers, who are chosen from the group of translators.\n\nWith a critical mass of crowdsourcing participants, you\u2019ll also have additional, built-in reviewers. \u201cWhen your crowdsourcers reach a critical mass, they are actually able to review each other\u2019s translation and spot translation errors,\u201d Alexia says \u201cThe translation will only get better and better.\u201d\n\nNo matter how well organized you are, though, sometimes the deadline comes before you\u2019re ready. As the end of Alexia\u2019s four months of efforts approached, she realized that they were not going to finish some of the Asian languages on time. So, she switched to professionals.\n\n\u201cProfessional translation services are reliable when it comes to service delivery,\u201d Alexia says. Being willing to use a mixture of translation styles also means that you can choose the right people for the right project. For example, when it comes to Trello\u2019s updates to users, she uses professional services. \u00a0\u201cTrello ships on a weekly basis. They really cannot count on crowdsourcers\u2019 passion to keep up the pace.\u201d\n\nTrello\u2019s localization project was big. That wasn\u2019t only because it involved a lot of text; it also covered a lot of platforms: the Android and iOS apps, the store descriptions for the two app stores, the web app, the website, the Trello shop, and server notifications. For each of these, Trello\u2019s team needed to get content ready for translation, in a process that is known as \u201cinternationalization.\u201d\n\nThat\u2019s where the devs come in. Alexia works closely with the product\u2019s devs to ensure that content has been extracted correctly, that it will connect to the TMS well, and that, overall, internationalization is done right.\n\nAnd that\u2019s not an easy task. Languages aren\u2019t universal, and code isn\u2019t able to be either. Take pluralization for example. Different languages have different rules for plural forms. Russian has three ways of pluralizing a word; many Asian languages, such as Japanese and Korean, have none. \u00a0Alexia and the dev team had to work together to create new strings that would support different patterns of pluralization.\n\nThey were able to tackle some of the problems, but not all. Other issues, like the role of gender in some languages, or the fact that some languages read from right to left, were not dealt with in the internationalization process. For these, Alexia has to work with her translators to fix them (e.g. asking them to use gender-neutral translations). But, with close coordination, Trello will be able to fix these issues too\u2014one at a time.\n\nFor the most part, Trello is a free tool. So, when choosing marketing strategies, the company has learned to focus on tactics that foster organic growth, rather than paid acquisition. And, when you think about it, localization itself is a great organic growth strategy. (It\u2019s so much more appealing to use an app in your own language!)\n\nBut translation is just a start. According to Alexia, \u201cA big lesson we learned from the first experiment stage was that only being available in local languages is not enough.\u201d\n\nWe mentioned earlier that Trello started their localization project by testing it out in three countries: Brazil, Spain, and Germany. In each, they tried a different marketing tactic. For example, Brazil saw a huge investment in local PR and content marketing, while Spain received only the translated content. Turns out that there wasn\u2019t much of a change in signups in Spain\u2014while Brazil got twice as many as would normally be expected.\n\nYou can\u2019t assume that users will magically discover that your app is now available in their language. \u201cTo make localization successful, you have to tell the local users that these languages are available,\u201d says Alexia.\n\nSo, when they expanded to France for further testing, they scaled up the marketing effort even more. They invested in content marketing. They held more than forty events. They recruited local ambassadors. The C-suites\u00a0even flew to France twice to promote Trello.\n\nBasically, they did a lot. And they got a lot in return: triple the normal signups during the launch period, and, even after the launch, they continued to do 25% better than they had been. It was clear that Trello had a plan that worked.\n\nIn Brazil, they\u2019ve also tried something else:\u00a0a localized blog. Rather than just having the company\u2019s posts translated into English, they also recruit local bloggers to write original posts. And the content, as a result, is just right for the region.\n\nLocalization is all about, well, the local, but it\u2019s also about the global. When Trello was preparing for their worldwide launch, they made sure to craft high-quality content that would appeal to a broad base of users (and potential users!). The result: an exciting landing page, made especially to promote the global launch.\n\nThe page includes the results from a recent survey the company conducted about its users\u2019 work habits and productivity hacks, illustrated through a series of appealing infographics. Toss in a lot of PR and local marketing, and you have the makings of an international splash.\n\nIn the future, Alexia wants to go even further, scaling up the global marketing efforts through tactics such as international SEO and stronger local marketing teams. She also wants to increase the number of services that have been translated, expanding localization efforts to the help desk and a welcome email, so that users will become even more engaged.\n\nTrello may be just one company, but the strategy they developed\u2014like them\u2014could quickly become universal. Alexia gave us three main takeaways:\n\nWant to know more about how to kick-start localization? Check out OneSky\u2019s guide to app localization. We walk you through\u00a0everything you could possibly want to know about the process, from start to finish. And, if you\u2019re already familiar with the basics, the guide is still worth a look: it goes in-depth on specific challenges of localization, from how to collect the best market data to step-by-step instructions for internationalization.\n\nThank you for reading!\n\nAlexia leads international marketing at Trello, where she recently launched the product in 21 languages through an innovative crowdsourcing effort. She has previously helped numerous companies create expansion strategies for Latin America. Alexia has lived and worked in three different continents. She is originally from France and lives in Brazil. Follow her (@AlexiaO_ian) on Twitter and connect with her on LinkedIn.\n\nTrusted by millions, Trello is a visual collaboration tool that creates a shared perspective on any project. Trello\u2019s boards, lists and cards enable you to organize and prioritize\n\n your personal and work life in a fun, flexible, and rewarding way.\n\n \n\n \n\n"},
{"url": "http://arstechnica.com/gadgets/2016/05/windows-hardware-specs-going-up-for-the-first-time-since-2009/", "link_title": "Windows hardware specs going up for the first time since 2009", "sentiment": 0.1870478983382209, "text": "Windows Vista was a shock to many Windows users, as its hardware requirements represented a steep upgrade over those required to run Windows XP: most 32-bit versions required a 1GHz processor, 1GB RAM, DirectX 9 graphics, and 40 GB of mass storage with 15GB free. But those 2006-era requirements looked much less steep once Windows 7 rolled out in 2009: it required almost the same system specs, but now 16GB of available disk space instead of 15. Windows 8 again stuck with the same specs and, at its release, so did Windows 10.\n\nBut the Windows 10 Anniversary Update (referred to in documentation as version 1607, so it ought to ship in July) changes that, with the first meaningful change in the Windows system requirements in almost a decade. The RAM requirement is going up, with 2GB the new floor for 32-bit installations. This happens to bring the system in line with the 64-bit requirements, which has called for 2GB since Windows 7.\n\nThe changed requirements were first spotted by Nokia Power User and WinBeta.\n\nThese hardware demands are only particularly relevant for system builders; they'll need to meet the new specs for machines that ship with Windows 10 preinstalled. Windows will still install and run on machines with less than 2GB; it'll just run better on systems with more memory.\n\nThis isn't the only hardware change that comes this summer. The initial Windows 10 specifications said that after July 28, all new systems must ship with Trusted Platform Module (TPM) 2.0. The TPM is used for various cryptographic purposes, including storing disk encryption keys. Until this cut-off date, OEMs could choose between TPM 1.2 and 2.0; TPM 2.0 adds a number of additional encryption capabilities to the 1.2 version.\n\nThe new specs also change the acceptable screen sizes for Windows 10. Previously, Windows 10 Mobile could ship on phones and tablets with screens up to 7.9 inches, with full Windows 10 on devices with screens of 8 inches or greater. Both of these ranges are now expanded, with Windows 10 Mobile accepted on screens less than 9 inches, and desktop Windows 10 now allowed on anything with a screen of 7 inches or greater.\n\nPrevious updates to the specs enabled the support for Qualcomm's new Snapdragon 820 in Windows Mobile. However, ARM support remains stubbornly restricted to 32-bit, with no 64-bit support for any of the 64-bit ARM processors. Microsoft representatives said at its Build conference in 2015 that a 64-bit ARM compiler was in development, and the company hinted that it would become available by the end of last year. It seems that it is still not finished."},
{"url": "http://tutorials.pluralsight.com/ruby-ruby-on-rails/react-vs-angular-2-integration-with-rails", "link_title": "React vs. Angular 2 integration with Rails", "sentiment": 0.16188426279602752, "text": "One of the most recent debates in the web development world has been the choice of constructing the view layer of modern applications. In this guide, we will go through React and Angular 2 integration Ruby on Rails. The two approaches will be analyzed in terms of ease of integration and ease of use with the Rails.\n\nBecause the focus on the tutorial is on getting React or Angular integrated into a Rails application, the Rails application itself will be as simple as possible - it will contain one action which will return arbitrary JSON that is going to be rendered on the page using React or Angular. Open up your terminal or command prompt and type:\n\nThis will scaffold a default Rails application and install all the required dependencies to make it run. Go to the directory of the application and locat the Gemfile. For this application, you will need only the following gems:\n\nSQLite3 is the gem for the Ruby-based database, Sprockets is for the asset pipeline and rack-cors is used to enable cross-origin HTTP requests (CORS) so that the React/Angular client-side can communicate with the server. In your console, run:\n\nThis will install the gems in the Gemfile. To configure , add this code snippet to the configuration file of the Rails applicaiton:\n\nThis configuration will permit all types of requests from any source ( denotes any address) to be accepted. For development, this setup is acceptable. However, for production, you need to replace the asterisk with the URL of your client-side application.\n\nThe next step is to make a controller and an action that will return JSON data and make a route for them. For the sake of simplicity and for the tutorial, let's put the action in the application controller:\n\nAn action called is going to respond with the JSON object when requested through http://localhost:3000. Let's put a route for the action:\n\nAngular 2 has two specifics - it is a framework and it uses TypeScript. These specifics come with certain requirements when it comes to integration with the Rails applicaiton:\n\nBecause Angular 2 is a framework and not a library, it would be best if is put in a separate directory where all its files are going to reside. This means that, instead of putting it into the Rails asset pipeline ( ), the Angular 2 app will reside in the Rails application's directory, separated from the compilation and the logic of the Rails application. This will allow a clearer separation of concerns between the Rails and the Angular 2 applications and their dependencies.\n\nAngular 2 uses TypeScript, a superset of JavaScript. As of today, there hasn't been devised a way for TypeScript to be implemented into Rails' asset pipeline, which means that a transpiler has to be configured in the root directory of the Rails application. Transpilers (short for transcompilers) in JavaScript are tools that read code (or CoffeScript or similar) and transpile it to pure JavaScript that can be interpreted by the browser.\n\nThere hasn't been devised a way for TypeScript to be implemented into Rails' asset pipeline, which means that the transpiler has to be configured in the root directory of the Rails application intead of . Because of TypeScript's requirements, there are three files that need to be created in the root directory in order the environment to be set up for an Angular 2 application.\n\nTo install these requirements, you need to have the Node Package Manager (npm) installed on your computer. Let's go through each of the files:\n\nThe package.json file that contains a list of all the packages required to integrate Angular 2 with Rails:\n\nPaste the contents in a file in the root directory of the Rails application and name it * package.json. The file contains typings, a package that is used for configuring the behaviour of TypeScript and the typescript package itself as *devDependencies (dependencies of the dependencies). Obviously, angular2 is also one of the packages as well as libraries such as systemjs and es6-shim that add EcmaScript 6 functionality, which is requried for Node 5.\n\nAlso in your root directory, create a file named typings.json . This file will be used to configure the dependencies of TypeScript after the packages are installed. You can also do it manually by running in your console.\n\nThe last file you need to create in the root directory is tsconfig.json:\n\nThe file contains standard configuration for the behavior of TypeScript in the Angular 2 application. One thing that requires a paricular attention is the property which defines that the Angular 2 application will reside in the directory.\n\nAfter these three files are added to the root directory of the Rails application, write the following command in your console:\n\nAnd wait as the packages are installed. Once the command completes, there will be an extra directory named in the root directory that will contain the installations of all the packages.\n\nThe configuration is almost finished, but there is a gotcha - the directory will not load in the application's assets since it is not in the directory. Thus, it must be added explicitly in the configuration of the Rails applicaiton:\n\nThe Rails applicaiton is now ready to load an Angular 2 application that resides in its directory. To start off, let's create a root html document has to be created that is going to load all the JavaScript files:\n\nBetween the tags, the systemJS library will configure the modules and import the file, which is going to be included later in the guide. Another interesting snippet in the file is the tag, where the built-in Angular 2 router component is going to be mounted.\n\nIn the directory, add an directory. This is where all the Angular 2 files are going to be put. Let's start with the first component -\n\nOne of the most notable features of TypeScript is that it provides classes and ability to and snippets of code. Such features make the code more maintainable and more familiar to programmers who are not used to JavaScript's idiosyncracies.\n\nHere, you can see how the provider is imported into the file and then made available in the constructor of the class. Doing http requests is done similarly to Angular 1's service, but the syntax of the promise is handled a little bit differently - acts similarly to and the functions for resolving the promise use EcmaScript 6's arrow syntax. To make it more familiar, the Angular 2 translates to in Angular 1. Another interesting feature is that variables can have a strong type, as it is demonstrated with the component variable.\n\nLet's add the template of the component:\n\nThe template will simply print the message that was fetched from the Rails application.\n\nThe first component is ready, but there must be a way through which it can be reached withing the Angular 2 application. Because Angular 2 is a framework, it comes with its own built-in router that can be imported and configured to your liking:\n\nIn this file, the router component is imported and configured. You can see that the component is bound to the tag. And the built-in router directives are put under the property. contains an array of route objects.\n\nA route object can contain a path , name and a component that it uses. Let's add in there:\n\nThe last thing that needs to be done is to add the file that is going to bootstrap the application:\n\nYou can regard as the initialization file of the Angular 2 application. It imports the that was defined earlier and bootstraps it, making all its routes and components reachable through their paths.\n\nThis step completes the Angular 2 integration into your Rails application. Start the server using:\n\nOpen your browser and go to http://localhost:3000. You should see the home.component with the message displayed on the server. if you encounter any errors, you can check the GitHub repository of what we just implemented.\n\nUnlike Angular 2, which is a full-fledged framework, React is simply a library that provides bare-bones features for constructing user interfaces. React uses JSX to render its components, which is much simpler and lighter than TypeScript. Unlike TypeScript, JSX can be integrated into Rails' asset pipeline, which significantly speeds up the integration of React with Rails.\n\nBecause React's JSX easily integrates with the asset pipeline, you can setup the environment by simply installing the react-rails gem. It comes with a great set of useful features - server-side rendering, component generators and more. Add the gem to your Gemfile:\n\nRun the generator provided by the gem to make the Rails application work with React:\n\nThis will create an initialization file named in the directory and a new directory, for the rest of the components. It is also going to add the React dependencies to the asset pipeline:\n\nReact is configured, now all we need is to put a controller, a view and a route for our Ruby on Rails application to render the components. First, let's create a simple controller. Go to and create a file named :\n\nPut the controller's action as a root for your Rails application:\n\nThis is everything that is required to setup the environment React for the Rails applicaiton. Next, we are going to render the data from the server.\n\nTo make a new component, you can simply run the generator and it will be created for you:\n\nThe generator will create an component in in EcmaScript 6 syntax. Open the file and put the following code snippet:\n\nis used to initialize the component's state variables. In the constructor, an empty object will be initialized in the component's state. is a common Object-Oriented pattern, here itused to inherit the props and context from the class. The function is used to render html into the component. In this case, it will render the state of the component. contains the state of the component, which is usually private or component-specific variables. is one of them. When there are several nested components however, the data between them is passed through .\n\nWhat we want is to render the data from the server into , so let's do that:\n\nis a method that will be called when the component becomes mounted into the DOM. Simply said, it is called when the component is rendered on the page. In it, the function makes a request to and uses EcmaScript 6's arrow function to get the callback when the request succeeds. this.setState() will set the property with the property of the object which will contain .\n\nLast, make a view to render the component: Go to , create a directory named and create a file named and put the following snippet in it:\n\nis a built-in helper method provided by react-rails that is going to render the component into the Rails view.\n\nWith this step, the Rails application integrates React. Go to http://localhost:3000 and see the results. If you encounter any problems, check the GitHub repository .\n\nHere is a overview of the three different ways the view layer in Rails can be hanlded:\n\nTo sum up, both React and Angular 2 come with their pros and cons. However, in terms of integration and setting up with Rails, React is a clear winner. The integration with the asset pipeline and server-side rendering make React easy to set up and highly configurable with Ruby on Rails. Angular 2, on the other hand, tends to be bulkier and trickier to implement. Part of the problem is TypeScript's complexity when it comes to integrating with the asset pipeline, another part is that Angular 2 is quite new and the Rails ecosystem hasn't dug deep in it yet."},
{"url": "https://medium.com/major-league-hacking/theres-no-such-bad-thing-as-a-bad-type-of-hackathon-they-re-just-not-all-aimed-at-you-6efa66f1fb10#.j0stwjj7y", "link_title": "There\u2019s no such bad thing as a bad type of hackathon, they\u2019re not all for you", "sentiment": 0.2345180305131761, "text": "Last August, I gave a talk at Hackcon EU called \u201cThe Hackathon Zoo\u201d. In the talk, I covered the different categories of hackathons, and what organisers who run student events can take away from each of them in order to make theirs as awesome as possible.\n\nOne of the key things I reiterated is that the hacker community, and by extension the developer community, is massively diverse. When I say diverse in this context, I am talking about background, skills and motivators.\n\nAnd that\u2019s exactly why \u2018startup weekend\u2019 hackathons are totally fine, as are less results-driven community-focussed events\u200a\u2014\u200athey\u2019re not necessarily aimed at the same kind of person.\n\nRegardless of what event you are running, regardless of the style and regardless of the format, there are some things which are not optional.\n\nThe first thing now, and always, is the safety of everyone involved. Many people feel like this is only your responsibility if you\u2019re hosting under 18s, and that simply isn\u2019t true. You should think about mitigating risk for everyone, and having a clear procedure in place for dealing with any issues.\n\nIt\u2019s also critical that the ownership of the ideas and projects built between the attendees, sponsors and organisers is incredibly clear from the start. In certain scenarios, I\u2019d be fine with saying \u201cyou don\u2019t own what you build\u201d, but that should never be an afterthought and be very explicit from the outset.\n\nFinally, inclusivity is key. This could mean many things\u200a\u2014\u200abeing welcoming to those with lower skill, making the effort to diversify the audience beyond white men, or in terms of nationality.\n\nThere are surprisingly few of these events around now, mainly due to risk aversion and the perceived extra work needed to keep everyone safe. When they do happen though, they are largely learning-based events with more focus on the hacker-mentor relationship.\n\nIt\u2019s really important, for this demographic which often have lower levels of experience, that there is ample support provided throughout.\n\nIn terms of environment, it should be as low-pressure as possible. Some hackathons I\u2019ve been to are really intense (which is suitable for the challenges), but many young people may freak out at something where they feel somewhat out of their depth.\n\nIt\u2019s also important to listen to the hackers\u2019 needs, and discoveries, as it will differ from person to person. Be attentive and make sure you learn from them\u200a\u2014\u200awhether it\u2019s how to better explain a concept, liken their experience to something more relevant, or even a technique that you hadn\u2019t thought of.\n\nSuccess for this kind of event is having people leave having learnt loads and wanting more. After all, there\u2019s lots for them once they get a bit older.\n\nFor the last year, I\u2019ve been involved with the EU hackathon community, and more specifically the group of roughly 50 people who organise these events through Major League Hacking.\n\nLike events for young people, these are largely learning-based, with the aims being to build, learn and share. To that point, it\u2019s just as important for these events to create a caring and collaborative environment.\n\nThat said, I\u2019d argue that it\u2019s even more important at these events that we focus on building awesome and resilient communities of people in the regions that remain outside of the events. These communities can rely on each other for support, and can work together for longer periods of time on ideas.\n\nOne thing I think more events could place focus on is the social opportunities, and carving time for them, at events. One of the reasons that Werewolf is such a fantastic mini-game is that it encourages the social interaction between people who otherwise may not have done so.\n\nThese events are about building a community for building awareness around social action. I\u2019ve seen many of these happen\u200a\u2014\u200afor charities, in response to crises and around societal challenges such as homelessness.\n\nWhat\u2019s key here is providing ample background which allows people with an interest in doing good, but little knowledge, to still understand why and how they should build their projects.\n\nThis can be done in a number of ways\u200a\u2014\u200afrom a strong opening presentation from a subject matter expert, mixing technical people with them throughout the event, or creating a briefing pack of sorts which contains links/data to refer back to. You should also try and encourage cross-team collaboration as these events are often for a common cause.\n\nWhat many organisers don\u2019t do, however, is keep the conversation going once the event is over. It\u2019s incredibly important to have real impact beyond \u2018look at some cool stuff we made.\u2019\n\nThese events are much more focussed on the business goals of the sponsors. The most important thing is that you should be a lot more prescriptive about what you are looking for\u200a\u2014\u200athere\u2019s absolutely nothing wrong with this and one of common improvement points for hackathons is about this communication.\n\nAt the start of this post I spoke about ownership, and it is even more important for this type of event. It should be incredibly clear, from the beginning, who owns the projects built. If the ideas and code won\u2019t belong to the attendees, you should be giving them something in return\u200a\u2014\u200ait could be cash or something in-kind\u200a\u2014\u200abut an actual transaction should take place.\n\nYou don\u2019t need to like every kind of event, and that\u2019s totally okay. There\u2019s no single way for a hackathon to be run, and each kind is aimed at people with certain motivating factors.\n\nRegardless of the type though, there are some pointers from each that every organiser can take away in order to make their event more successful.\n\nThese, of course, are just my thoughts. I\u2019d be really interested in having conversations with people who agree or disagree with things I\u2019ve written."},
{"url": "http://minutehack.com/opinions/why-its-important-to-embrace-the-difference", "link_title": "Why It's Important to Embrace the Difference", "sentiment": 0.1587589126559715, "text": "Emerging brands copy successful ones all the time. Here's why that's a marketing strategy you need to avoid.\n\nAs a child I remember being told off by my parents and them saying sternly to me \u201cIf he jumped off a cliff, would you?\u201d I am sure I was caught doing something I shouldn\u2019t have, and my likely response was the classic \u201cWell HE did it!\u201d. Looking back at it now, it makes perfect sense that copying someone without considering the consequences isn\u2019t a smart move.\n\nWith that in mind, you would be shocked to learn how many times companies approach us to work with them, and pretty much the first sentence out of their mouth is \u201cwe would love to see your creativity, but, if we could look like X competitor that would be great\u201d.\n\nAs branding consultants (and through our Brand thinking\u2122 process) we work very hard with our clients to understand their individual needs, their individual USPs and the reasons that their customers choose them over their competitors.\n\nSo to hear the words \u201cwe want to look like\u2026\u201d sends chills down our spine. With that in mind, I felt it was worth exploring the reasons why this happens and what clients feel they can gain, but in reality what they miss out on with their own brands.\n\nThe perception of what is to be gained\n\nClients often ask to look like who they believe to be the market leader. This could be a perception which is built out of experience, hearsay or a historical knowledge that X competitor is the the best. The problem with this logic can often be that the chosen business may be seen historically as a market leader within the sector, but externally (and most importantly to potential customers) they could be seen as out-of-touch, old-fashioned or just generally not appealing.\n\n5 Branding Trends That Took Shape In 2015\n\nWhat Can You Do When Customers Love Your Products But Not Your Brand?\n\nThe Importance Of Building Your Personal Brand (And How You Can Do It)\n\nThis is also true when clients ask to look like the biggest organisation in the market. We understand that the client feels like \u201cit worked for them, so it will work for us\u201d, but that is very rarely true.\n\nCompany X owns that look and owns that brand perception in the mind of the customer \u2013 by trying to move into that sphere you only appear to be a \u2018wannabe\u2019, a company devoid of ideas, or worse, just a company that copies others. For all of our clients, that is certainly not the case and it\u2019s a trap we try and steer them away from.\n\nLet\u2019s take the supermarket sector as an example:\n\nHistorically Tesco have been the largest organisation in the sector. If Aldi and Lidl went to their design teams and said \u201cwe want to look like Tesco\u201d, what would that have done for their business? They would have been creating a brand and visualisation that was tired, dated, had poor brand perception and ultimately their own brands would have been damaged.\n\nHowever, they looked at their situation, understood their position, assessed their USPs of price and quality and they delivered new brand campaigns, built a brand which is focussed at their target market \u2013 \u00a0and have created a buzz with new and existing customers.\n\nIf they had blindly said \u2013 \u201cwe want to look like Tesco\u201d, they would not be taking a 10% share of British grocery sales \u2013 an increase of 50% from only three years ago and rising.\n\nThe big boys are not always the best\n\nThere is a perception in some sectors, that the biggest players do it best \u2013 and that just isn\u2019t true.\n\nYes there are some large companies whose brand, message and offering you can stand back and admire, for example the Apples and BMWs of this world. But, more often than not, the biggest player is by no means the best when it comes to branding, messaging, offering and generally engaging with their customers.\n\nLet\u2019s take the estate industry sector as an example. Very often clients will come to us and say \u201cwe want to look like Foxtons\u201d \u2013 and our response is a flat-out \u201cwhy?\u201d. A myriad of answers will come flying back but normally it boils down to \u201cthey are the biggest, therefore they must be doing it right\u201d But is that really true?\n\nDoes Foxtons fill you with brand warmth? Do they make you feel energised to work with them? Or does their brand fill your brain with images of walls of Peroni in fridges? Or Minis running around with a massive \u2018F\u2019 in a circle on the side? What does the \u2018F\u2019 stand for other than Foxtons? Is looking like a glass pub what your brand really needs or wants?\n\nAs a consumer/competitor you know the Foxtons brand because of the sheer scale of the business. If you live or have visited London you can\u2019t fail to see the \u2018F\u2019 chasing down houses or on what feels like every other street corner, but, if your estate agency is in Newcastle and called Taylors, what relevance would a capital T in a circle have to anyone? None! You have not earned the right to be known by your brand yet \u2013 you have to build that through brand understanding.\n\nAll joking aside, Foxtons have been a stand-out brand success \u2013 but it\u2019s certainly not just from putting an \u2018F\u2019 in a circle. They have worked very hard at growing quickly and aggressively, and building a brand which is very specific for a London audience.\n\nThey considered the market, did their research, understood their potential and most importantly they did something different! As a species and a culture, we applaud difference when we know it is a success, but before that point we look upon difference with suspicion and fear.\n\nAs branding consultants, our role is to work in the realms of difference, to see the gap in the market and to find the hook that takes our clients from \u2018just another business\u2019 to the brand which resonates with the target audience on an emotional level \u2013 the business they are drawn to, like a moth to the flame, when the time comes that they need their services.\n\nWhat that hook is depends on each individual client and their goals \u2013 but we are certain the answer does not lay in looking at competitors and blindly following their lead.\n\nOur top 5 tips for companies who want to stand out:"},
{"url": "http://techcrunch.com/2016/05/23/facebook-live-video-engagement-graph/", "link_title": "Facebook Live lets you skip to the good part", "sentiment": 0.13450944822373398, "text": "Facebook\u2019s newest feature could fundamentally change how you watch video. Until now, you either sat through a video until it got too boring, waited for the interesting part or fast-forwarded hoping to spy something worth seeing. But for clips that weren\u2019t immediately exciting, especially monologues or selfie-streams where the action was in the audio, it was tough to tell if a video deserved your time.\n\nYet Facebook knows when the good part of a Live video is coming. When entertained or riled up, viewers can fire off Live comments and reaction emojis during the stream that the broadcaster can see, similar to Facebook Live competitor Periscope\u2019s hearts.\n\nNow Facebook tells me it\u2019s putting reactions to work to power a visualized timeline of when a Live video receives the most engagement. When you go to fast-forward through the recorded replay of a Live clip, you\u2019ll see the graph of reaction volume overlaid on the progress bar.\n\nEssentially, you\u2019ll be able to see when the video gets interesting and skip there if you want.\n\nThat could influence how videos are shot and paced, and make amateur streams more compelling, but also encourage anxious skipping around that breaks a video\u2019s narrative.\n\n\u201cAround two-thirds of the watch time for Facebook Live happens when the video is no longer live, which tells us that people are interested in watching live videos even if they can\u2019t catch them while they\u2019re happening,\u201d Facebook\u2019s head of video Fidji Simo tells me. That might be why Periscope just launched its #Save feature\u00a0that mimics Facebook, so its users can finally\u00a0show off replays permanently instead of only for 24 hours before they\u2019re deleted.\n\nSimo notes that \u201cWhen people watch a live video after the fact, the engagement graph provides a valuable signal that can help people explore the video and easily identify highlights that they may find engaging, which could encourage people to spend more time with a video that they might have otherwise skipped over.\u201d\n\nFacebook says the engagement graph is rolling out to some users now. As shown at the top of this article, users will see blue peaks and valleys representing high and low volumes of engagement so they can easily scrub to the best scenes.\n\nIt\u2019s a bold experiment in content consumption. A distant relative might be how SoundCloud visualizes both a song\u2019s sound wave so you can find the big bass drop, and shows timed comments pegged to certain moments of a song. Simo explains that \u201cThe engagement graph is designed to help people easily navigate a video that was live \u2014 especially longer ones \u2014 to find the moments that drew the most engagement.\u201d\n\nFacebook also revealed that it\u2019s starting to show Live video reaction replays that appear in sync on recorded versions of broadcasts so it feels like you\u2019re watching in real time. You\u2019ll see the emojis for Likes, Hahas, Sads, and Angrys plus the faces of friends who left them overlaid on the video.\n\nThe enabling of impatience could have a profound impact on how people create and consume Live video, or all video if Facebook expands the feature there. For now, it\u2019s rolling out to some users on Live video replays only.\n\nAs for coming to traditional videos, Simo says there are \u201cNo plans to share. We think the engagement graph is particularly useful for live video, as they are often longer than typical short-form, on-demand videos, and can help people easily discover the parts of the video they might find most interesting.\u201d\n\nThe announcement comes as Facebook continues to rapidly push advances to its Live video platform. It now allows Continuous Live Video feeds like nature cams, as well as geo- and age-gating to make streams visible only to certain people.\n\nThe engagement graph feature could make creators feel more comfortable filming slow build-ups to big climaxes, because viewers can peek to see that something special is coming up and zip there if they\u2019re antsy. Primetime TV shows have to be scripted with mid-episode cliffhangers to keep audiences glued in through the commercial breaks. Similarly, the engagement graph could push broadcasters to pepper their streams with moments of delight.\n\nIt could help amateur livestreamers get friends to at least watch the highlights of the streams even if the first few seconds that auto-play in the News Feed look drab. That\u2019s Simo\u2019s theory. And if users are less worried about boring their friends to death, they might be more confident about hopping in front of the camera.\n\n\u201cNothing beats watching a live video while it\u2019s happening, but it\u2019s not always possible to catch-all broadcasts live,\u201d Simo shared. \u201cWhile we can\u2019t totally replicate the experience of watching live, we want to help people feel \u2018in\u2019 on the action after the fact.\u201d\n\nAt the same time, viewers might quickly bounce from broadcasts that don\u2019t show an upcoming spike in engagement. Clips could see a sudden exodus if they drag on past their brightest moment. Any sense of coherent, linear storytelling might be fractured by itchy trigger fingers. The ability to preview the future entertainment value of a video could make the format more utilitarian and less like art if viewers merely opt for the visual crib notes.\n\nThe mobile live streaming medium is so new that norms are still emerging. We\u2019ll have to wait and see how engagement graphs impact Facebook Live.\n\nFacebook built the News Feed itself to help us skip to the good parts of our social graph\u2019s collective experience.\u00a0As the democratization of video creation tools leads to an explosion of the quantity of content produced, we\u2019ll need ways to sort through it, too."},
{"url": "http://arstechnica.co.uk/information-technology/2016/05/how-the-internet-works-submarine-cables-data-centres-last-mile/", "link_title": "How the Internet works: Submarine fibre, brains in jars, and coaxial cables", "sentiment": 0.13, "text": "Ah, there you are. That didn't take too long, surely? Just a click or a tap and, if you\u2019ve some 21st century connectivity, you landed on this page in a trice.\n\nBut how does it work? Have you ever thought about how that cat picture actually gets from a server in Oregon to your PC in London? We\u2019re not simply talking about the wonders of TCP/IP, or pervasive Wi-Fi hotspots, though those are vitally important as well. No, we\u2019re talking about the big infrastructure: the huge submarine cables, the vast landing sites and data centres with their massively redundant power systems, and the elephantine, labyrinthine last-mile networks that actually hook billions of us to the Internet.\n\nAnd perhaps even more importantly, as our reliance on omnipresent connectivity continues to blossom, the number of our connected devices swells, and our thirst for bandwidth knows no bounds, how do we keep the Internet running? How do Verizon or Virgin reliably get 100 million bytes of data to your house every second, all day every day?\n\nWell, we\u2019re going to tell you over the next 7,000 words.\n\n"},
{"url": "http://gizmodo.com/astronomers-might-have-just-solved-a-key-mystery-about-1778095567", "link_title": "Astronomers Might Have Just Solved a Key Mystery About the Origin of Life", "sentiment": 0.11599994655550212, "text": "If a massive solar storm struck the Earth today, it could wipe out our technology and hurl us back to the dark ages. Lucky for us, events like this are quite rare. But four billion years ago, extreme space weather was probably the norm. And rather than bringing the apocalypse, it might have kickstarted life.\n\n\n\nThat\u2019s the startling conclusion of research published in Nature Geoscience today, which builds on an earlier discovery about young, sun-like stars made with NASA\u2019s Kepler Space Telescope. Baby suns, it turns out, are extremely eruptive, releasing mind-boggling amounts of energy during \u201csolar superflares\u201d that make our wildest space weather look like drizzle.\n\nNow, NASA\u2019s Vladimir Airapetian has shown that if our sun was equally active 4 billion years ago, it could have made the Earth more habitable. According to Airapetian\u2019s models, as solar superflares pounded our atmosphere, they initiated chemical reactions that yielded climate-warming greenhouse gases and other essential ingredients for life.\n\n\u201cThe Earth should have been in a deep freeze four billion years ago,\u201d Airapetian told Gizmodo, referring to the \u201cfaint young sun paradox\u201d first raised by Carl Sagan and George Mullen in 1972. The paradox came about when Sagan and Mullen realized that Earth had signs of liquid water as early as 4 billion years ago, while the sun was only 70 percent as bright as it is today. \u201cThe only way [to explain this] is to somehow incorporate a greenhouse effect,\u201d Airapetian said.\n\n\n\nAnother early Earth puzzle is how the first biological molecules\u2014DNA, RNA and proteins\u2014scavenged enough nitrogen in order to form. Similar to today, the ancient Earth\u2019s atmosphere was composed primarily of inert nitrogen gas (N2). While specialized bacteria called \u201cnitrogen fixers\u201d eventually figured out how to break N2 and turn it into ammonia (NH4), early biology lacked this ability.\n\nThe new study offers an elegant solution to both of these problems in the form of space weather. The research began several years back, when Airapetian was studying the magnetic activity of stars in NASA\u2019s Kepler database. He discovered that G-type stars (stars like our sun) are like dynamite in their youth, frequently releasing pulses of energy equivalent to over 100 trillion atomic bombs. The most powerful solar storm ever experienced by humans, the 1859 Carrington event that caused worldwide power outages, pales in comparison.\n\n\u201cIt is a crazy amount of energy. I can hardly comprehend it myself,\u201d Ramses Ramirez\u2014an astrobiologist at Cornell University who was not involved with the study but collaborates with Airapetian\u2014told Gizmodo.\n\nIt soon occurred to Airapetian that he could use this discovery to peer back into the early history of our solar system. He calculated that 4 billion years ago, our sun could have been releasing dozens of superflares every few hours, with one or more grazing the Earth\u2019s magnetic field every single day. \u201cBasically, the Earth was under constant attack from super Carrington-sized events,\u201d he said.\n\nUsing numerical models, Airapetian then showed that solar superflares would be strong enough to dramatically compress Earth\u2019s magnetosphere, the magnetic shield that encircles our planet. Not only that, charged solar particles would bust a hole clean through the magnetosphere near our planet\u2019s poles, entering the atmosphere and colliding with nitrogen, carbon dioxide and methane. \u201cSo now you have these particles interacting with molecules in the atmosphere and creating new molecules\u2014like a chain reaction,\u201d Airapetian said.\n\nThese solar-atmospheric interactions produce nitrous oxide, a greenhouse gas with 300 times the global warming potential of CO2. Airapetian\u2019s models suggest that enough nitrous oxide could have been produced to dramatically warm the planet. Another product of the endless solar storm, hydrogen cyanide (HCN), could have fertilized the surface with the nitrogen needed to form the early building blocks of life.\n\n\n\n\u201cPeople have looked at lightning and falling meteorites as ways to initiate nitrogen chemistry,\u201d Ramirez said. \u201cI think the coolest thing about this paper is that nobody had really thought about looking at solar storms.\u201d\n\nIt\u2019ll be up to biologists to determine whether the exact mix of molecules produced via superflares would have been enough to jumpstart life. That investigation is already underway. Researchers at the Earth Life Sciences Institute in Tokyo and elsewhere are now using Airapetian\u2019s models to devise new experiments that simulate conditions on the ancient Earth. If those experiments can produce amino acids and RNA building blocks, that would go a long way toward supporting the idea that space weather helped get life started.\n\nIn addition to helping piece together our origin story, Airapetian\u2019s models could shed light on the past habitability of Mars, which appears to have also been wet four billion years ago despite receiving even less radiation from the young sun. The study could have implications for life beyond our solar system, too.\n\nWe\u2019re just starting to figure out what constitutes a star\u2019s \u201chabitable zone,\u201d where planets with liquid water oceans might exist. But the current habitable zone definition only factors in the brightness of the parent star. With more detailed information on a star\u2019s explosive activity, we might be able to glean more about the chemistry of exoplanet atmospheres, and the potential for a strong greenhouse effect to take hold.\n\n\u201cUltimately, this will inform us whether the energy from a star is available in a way that can create the chemistry to create biomolecules,\u201d Airapetian said. \u201cWithout that, it would be a miracle to have life.\u201d"},
{"url": "http://www.theguardian.com/commentisfree/2016/may/24/robots-future-work-humans-jobs-leisure", "link_title": "If robots are the future of work, where do humans fit in?", "sentiment": 0.08042671614100183, "text": "Robin Hanson thinks the robot takeover, when it comes, will be in the form of emulations. In his new book, The Age of Em, the economist explains: you take the best and brightest 200 human beings on the planet, you scan their brains and you get robots that to all intents and purposes are indivisible from the humans on which they are based, except a thousand times faster and better.\n\nFor some reason, conversationally, Hanson repeatedly calls these 200 human prototypes \u201cthe billionaires\u201d, even though having a billion in any currency would be strong evidence against your being the brightest, since you have no sense of how much is enough. But that\u2019s just a natural difference of opinion between an economist and a mediocre person who is now afraid of the future.\n\nThese Ems, being superior at everything and having no material needs that couldn\u2019t be satisfied virtually, will undercut humans in the labour market, and render us totally unnecessary. We will all effectively be retired. Whether or not we are put out to a pleasant pasture or brutally exterminated will depend upon how we behave towards the Ems at their\u00a0incipience.\n\nWhen Hanson presents his forecast in public, one question always comes up: what\u2019s to stop the Ems killing us off? \u201cWell, why don\u2019t we exterminate retirees at the moment?\u201d he asks, rhetorically, before answering: some combination of gratitude, empathy and affection between individuals, which the Ems, being modelled on us precisely, will share (unless we use real billionaires for the model).\n\nOpinion on the precise shape of the robot future remains divided: the historian Yuval Noah Harari argues, in Homo Deus: A Brief History of Tomorrow, that artificial intelligence robots will be the first to achieve world domination. This future is bleaker than Hanson\u2019s \u2013 lacking empathy, those robots wouldn\u2019t have a sentimental affection for us as their progenitors \u2013 but essentially the same. Harari predicts the rise of the useless class: humans who don\u2019t know what to study because they have no idea what skills will be needed by the time they finish, who can\u2019t work because there\u2019s always a cheaper and better robot, and spend their time taking drugs and staring at screens.\n\nThese intricacies, AI versus Ems, AI versus IA (intelligence amplification, where humans aren\u2019t superseded by our technological advances but enhanced by them) fascinate futurologists. Hanson argues that AI is moving too slowly, while only three technologies need coincide to make an Em possible: faster and cheaper computers, which the world has in hand; brain scanning, which is being worked on by a much smaller but active biological community; and the modelling of the human mind, \u201cwhich is harder to predict\u201d.\n\nBut all the predictions lead to the same place: the obsolescence of human labour. Even if a robot takeover is some way away, this idea has already become pressing in specific sectors. Driverless cars are forecast to make up 75% of all traffic by 2040, raising the spectre not just of leagues of unemployed drivers, but also of the transformation of all the infrastructure around the job, from training to petrol stations.\n\nThere is always a voice in the debate saying, we don\u2019t have to surrender to our own innovation: we don\u2019t have to automate everything just because we can. Yet history teaches us that we will, and teaches us, furthermore, that resisting invention is its own kind of failure. Fundamentally, if the big idea of a progressive future is to cling on to work for the avoidance of worklessness, we could dream up jobs that were bolder and much more fulfilling than driving.\n\nThere are two big threats posed by an automated future. The first \u2013 that we will irritate the robots and they will dominate and swiftly obliterate us \u2013 is for Hollywood to worry about. There is not much apparatus we can build in advance to make ourselves less annoying. There will undoubtedly be those who believe our obliteration is so inevitable that every other anxiety is\u00a0a\u00a0sideshow.\n\nIf you can hold your nerve against that, the critical question becomes: in a world without work, how do we distribute resources? It is a question articulated precisely by Stephen Hawking last year, when he noted: \u201cEveryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine owners successfully lobby against wealth redistribution.\u201d\n\nLike so many things, from debt cancellation to climate change, the reality of the situation is easily understood by scientists, academics, philosophers from the left and right, activists from within and without the establishment; and the only people who staunchly resist it are the self-styled political \u201crealists\u201d.\n\nThe question of how to distribute wealth in the future curves back round to meet a conundrum raised by the past: how do we remake the social safety net so that it embodies solidarity, generosity and trust, rather than the welfare state of the present, rickety with the woodworm of mutual suspicion.\n\nThe idea of a universal basic income is generally framed as a way to \u201cshift from the Beveridge principle of national insurance based on contributions and the sharing of risk, to a system of income as of right\u201d (as described in a paper by Howard Reed and Stewart Lansley). In its simplest iteration, all citizens receive the same income. There is work to be done on the numbers \u2013 whether this income needs to be supplemented for housing, in what form it has its most progressive effect, whether and how it is taxed back in the higher deciles, how it can be affordable at the same time as genuinely livable.\n\nThere is also work to be done on the surrounding incentives, whether a basic income would capsize the work ethic and leave the world understaffed while we await the robot takeover (a pilot scheme in Canada concluded the only groups who worked less with an income were mothers of young babies and teenagers still in education; other pilots are under way in Kenya and across Europe).\n\nEnter the future, with its possibility that many vocations will be unnecessary, and we face more existential questions: how do we find meaning without work? How do we find fellowship without status? How do we fill leisure intelligently? These mysteries possessed Bertrand Russell and John Maynard Keynes, then fell out of currency as we realised we could consume our way out of futility, and ignite our urge to earn by spending it before it arrived.\n\nEven absenting the constraints of the globe, that plan has failed. Consumption may have lent necessity to work, but it didn\u2019t confer meaning upon it. And perhaps the most profound accommodation we have to make with the future isn\u2019t whether or not we are capable of sharing, but where we will find our impetus.\n\n\u201cCan you just write,\u201d Hanson asked at the end of our conversation, \u201cthat even though I\u2019m talking about dire and dramatic things, I\u2019m a friendly guy who smiles a lot?\u201d I\u2019m not sure how much this helps. Some of his predictions are only bearable if you assume that you\u2019ll have died before they come to pass.\n\nHanson doesn\u2019t insist that his is the only possible outcome. Rather, \u201cyou should expect that, whatever change is going to happen, it\u2019s going to happen pretty fast. Like, five years from nothing different that you\u2019d notice to a completely different world. What I want is to have people understand how urgent it is, when this thing shows up, to have made a plan.\u201d"},
{"url": "http://allianceforscience.cornell.edu/blog/mark-lynas/gmo-safety-debate-over", "link_title": "GMO safety debate is over", "sentiment": 0.08265082921773063, "text": "The GMO debate is over \u2014 again. Last week, the prestigious National Academies of Science, Engineering and Medicine issued what is probably the most far-reaching report\u00a0ever produced by the scientific community on genetically engineered food and crops. The conclusion was unambiguous: Having examined hundreds of scientific papers written on the subject, sat through hours of live testimony from activists and considered hundreds more comments from the general public, the scientists wrote that they \"found no substantiated evidence that foods from GE crops were less safe than foods from non-GE crops.\"\n\nThe National Academies process was both impressively inclusive and explicitly consensual. As noted in the preface to their report, the scientists \"took all of the comments\" \u2014 however ludicrous \u2014 \"as constructive challenges\" and considered them carefully. Thus the expert committee patiently gave yogic flyer-turned-anti-GMO activist Jeffrey Smith\u00a0a generous 20-minute slot within which to make his customary assertion that genetically engineered foods cause just about every imaginable modern ailment. Greenpeace also offered invited testimony. So did Giles-Eric Seralini, the French professor who suffered the ultimate scientific indignity of having his paper claiming rats fed GMOs suffered tumors\u00a0retracted in 2013.\n\nEach of their claims was examined in turn. Do GE foods cause cancer? No \u2014patterns of changing cancer incidence over time are \"generally similar\" between the US, where GMO foods are ubiquitous, and the United Kingdom, where they are virtually unknown. How about kidney disease? US rates have barely budged over a quarter century. Obesity or diabetes? There is \"no published evidence to support the hypothesis\" of a link between them and GE foods. Celiac disease? \"No major difference\" between the US and UK again. Allergies? \"The committee did not find a relationship between consumption of GE foods and the increase in prevalence of food allergies.\" Autism? Again, evidence comparing the US and UK \"does not support the hypothesis of a link.\"\n\nIn a rational world, everyone previously fearful about the health effects of GMOs would read the report, breathe a huge sigh of relief and start looking for more evidence-based explanations for worrying trends in health issues like diabetes, autism and food allergies. But psychological associations developed over many years are difficult to break. A Pew Center poll\u00a0in 2015 found only 37 percent of the public thought GE foods were safe, as compared to 88 percent of scientists, a greater gap than on any other issue of scientific controversy, including climate change, evolution and childhood vaccinations. These entrenched attitudes are not about to disappear \u2014 especially since they are continually reinforced by a vocal and well-funded anti-GMO lobby.\n\nThere is also political path dependence. Vermont's GMO labeling law, scheduled to throw US food manufacturers and retailers into chaos when it comes into force on July 1, is predicated on the explicit assumption that GE foods may be unsafe. \"There is a lack of consensus regarding the validity of the research and science surrounding the safety of genetically engineered foods,\" Vermont's Act states\u00a0in its preamble. Indeed, such foods \"potentially pose risks to health [and] safety. Will Vermont's legislature reconsider its Act now that it stands so clearly on the wrong side of a rock-solid scientific consensus? Of course not.\n\nThe National Academies report should make particularly uncomfortable reading for the environmental movement, many of whose leading member groups now exhibit all the hallmarks of full-scale science denialism on the issue. A spokeswoman from Friends of the Earth dismissed the report as \"deceptive\" before she had even read it. The group's website claims that \"numerous studies\" show GE foods can pose \"serious risks\" to human health. Another environmentalist group, Food and Water Watch, issued a pre-publication rebuttal that conspiratorially accused the National Academies of having undisclosed links with Monsanto, before reasserting its view that \"there is no consensus, and there remains a very vigorous debate among scientists... about the safety and merits of this technology.\"\n\nBut despite these shrill denials, the truth is that there is no more of a debate on the safety of GE crops than on reality of climate change, the scientific consensus on which all these same green groups aggressively defend. And the irony goes deeper: many of the strategies now being employed to demonize GMOs come straight out of the climate denialist playbook. There's the same promotion of false 'no consensus' statements by groups of self-appointed experts. Why, more than 300 \"scientists and legal experts\" signed a 'no consensus on GMO safety' statement\u00a0last year, Greenpeace reminds us. That sounds like a lot, until you compare it with the 30,000 \"American scientists\" who have supposedly signed a petition\u00a0claiming that there is \"no convincing scientific evidence\" linking CO2 with climate change, which Greenpeace (rightly in my view) ignores.\n\nThere's also a worrying trend towards the harassment of bona fide scientists. Just as senior Republicans have shamefully targeted climate experts with politically-motivated subpoenas, so an anti-GMO group called US Right to Know has slapped dozens of geneticists and molecular biologists working at public universities with repeated Freedom of Information Act requests demanding access to thousands of their private emails. In some cases, scientists have as a result of subsequent campaigns received death threats, and had their laboratory and home addresses circulated menacingly on social media.\n\nThere is still plenty of room for genuine dissent moreover. The National Academies report is zealous in pointing out some of the experienced difficulties and drawbacks of GMOs. The overuse of GE crops has indeed led to the evolution of resistance, both in weeds and insects, it finds. Also, industry domination of the technology might restrict access of small farmers in poorer countries to improved seeds. And mandatory GMO labelling might well be a good way to raise public trust in a more transparent food system.\n\nBut these real areas of debate do not include GMO safety. That issue has now been definitively put to bed. So let's be clear once again: the safety debate is over. If you vaccinate your kids and believe that climate change is real, you need to stop being scared of genetically modified foods.\n\nMark Lynas is a writer and campaigner on climate change and a visiting fellow at the Cornell Alliance for Science"},
{"url": "http://www.wsj.com/articles/can-you-carbo-load-your-way-to-good-health-1463670677?utm_source=nextdraft&utm_medium=email", "link_title": "Can You Carbo-Load Your Way to Good Health?", "sentiment": 0.1874405181381926, "text": "The bread arrived by UPS, heavy as flesh, wrapped in brown paper. Its springy crust belied a two-day journey from baker Avery Ruzicka at Manresa Bread in Los Gatos, Calif.\n\nIf shipping bread cross country seems like a wanton act of locavore disobedience, consider that I\u2019m not talking about just any loaf. The one Ms. Ruzicka sent me was made using Oregon-grown Edison wheat berries, ground to flour shortly before being mixed with water, naturally fermented for 24 hours, then baked to tangy, tender goodness. This bread is imbued with all the nutritional virtues of the wheat kernel\u2014perhaps the most misunderstood ingredient in modern America. Forget juicing. Forget bone broth. With bread like this, many chefs and bakers have come to believe, you can carbo-load your way to optimal health.\n\nAsk Adam Leonti, who, like Ms. Ruzicka, grinds his own flour for pasta, bread and pizza dough. Mr. Leonti shed 15 pounds eating sourdough breads made from wheat pulverized by the 10,000-pound stone mill at his Brooklyn Bread Lab.\n\n\u201cThere\u2019s fiber in there, which is missing from people\u2019s diets altogether,\u201d Mr. Leonti said. \u201cYou have all these enzymes that are alive and volatile, which are extracted from white flour to make it shelf stable. Those are the things your body is searching for to make digestion happen, to make nutrition happen.\u201d\n\nThe farm-to-table revolution has transformed most of the restaurant pantry, but even sophisticated kitchens still largely craft baked goods and pastas from lily-white commodity flour, an ingredient short on flavor and nutrition. Thus, baguettes and bucatini have come to be understood in some quarters as deviations from a wholesome diet.\n\nRecently, though, a growing number of bakeries and restaurants around the country have begun to grind their flour fresh, or work with local mills to do it. The practice boils down to more than effete notions of superior flavor or artisan virtue; this flour teems with vitamins, minerals and antioxidants freshly liberated from the wheat kernel\u2014elements that have been largely absent from American baked goods for generations.\n\nChad Robertson, whose Tartine bakery in San Francisco sits at the epicenter of the artisan-bread movement, is in the midst of taking his milling in house. Last year Marco Canora installed a mill at Hearth in New York City, and he uses it to grind corn and wheat for polenta, pasta and pastries. At Union Restaurant in Pasadena, Calif., Bruce Kalman makes his pasta from durum and other wheats ground by a local miller, Grist & Toll. At Vetri Ristorante in Philadelphia, head baker Claire Kopp McWilliams now mills all the flour for their breads, pasta and pastries, from wheat procured directly from farmers. Add to their number Richard Bourdon at Berkshire Mountain Bakery in Western Massachusetts, David Bauer at Farm & Sparrow in North Carolina, Chris Bianco at Pizzeria Bianco in Arizona and Justin Slojkowski at Bruno Pizza in New York\u2014committed millers, all.\n\nTo understand flour, you must know the wheat kernel, which comprises a fibrous outer layer, the bran; a starchy middle layer, the endosperm; and the vitamin-rich core, or germ. For most of human history all wheat was milled whole. White flour, a modern invention, is produced by grinding only the endosperm for shelf-stable starch that is later enriched with a handful of vitamins and minerals.\n\nFreshly milled flour is also worlds apart from the so-called \u201cwhole wheat\u201d flours and baked goods on supermarket shelves. Typically, those are made by mixing white flour with a small amount of wheat bran but with the wheat germ omitted altogether because its oils limit shelf life. (For those playing along at home: Community Grains, Grist & Toll and Carolina Ground are three excellent online sources for whole-milled flour. The flour starts losing its flavor immediately after milling, and should be used within a week for best results. Store it in the freezer to preserve flavor and prevent it from turning rancid.)\n\nThis generation of whole-grain milling is still in its infancy, and nutritional data on freshly milled flour is scant, though it\u2019s clear enough what highly processed flour is missing. \u201cWheat is incredibly nutritious, but when you mill in such a way that you remove the bran and germ, you\u2019re losing the micronutrients that we need the most,\u201d said Dr. David Killilea, a nutritional biochemist at the Children\u2019s Hospital of Oakland Research Institute. \u201cWhen you compare what\u2019s removed from wheat to make commercial flour, it tracks pretty well with the nutrients that are most deficient in the U.S. population.\u201d A long ferment\u2014letting bread rise 24-48 hours\u2014has the further beneficial effect of breaking down gluten, making it easier to digest.\n\n\u201cOne huge difference I find is when you\u2019re eating this type of whole food product, freshly milled and long-fermented, it fills you up,\u201d said Mr. Robertson. \u201cI eat bread every day, at a lot of meals, but I don\u2019t eat a ton of it because I\u2019m eating stuff that\u2019s more whole-grain, and it\u2019s satisfying, and you feel it in your body in a different way.\u201d\n\nFor an increasing number of chefs and bakers, that difference makes it worth putting up with the challenges that attend fresh milling, which are many. Rick Easton, who closed his acclaimed Pittsburgh bakery, Bread & Salt, a few months ago, said he paid $1.40 per pound for a whole-milled product, while commodity flour runs around 20 cents a pound. \u201cMost of my customers just wanted to know why the bread doesn\u2019t cost $2 a loaf,\u201d he said. Sourcing wheat from local farmers subjects bakers to the whims of their varying harvests, and flour milled this way lacks the consistency of industrial flour, making it more finicky to bake with. Despite all that, Mr. Easton hopes to re-open his bakery in New York City.\n\nJ.D. McLelland, whose forthcoming documentary \u201cIngrained\u201d chronicles the rebuilding of regional grain economies, predicts that improvements in milling technology and more widespread cultivation of grains fit for whole milling will soon make high-quality flour more accessible and affordable. He likens wheat kernels to coffee beans: Grinding to order was rare 20 years ago, but now it\u2019s expected. \u201cThe biggest element that could improve grain is treating it like a fresh product,\u201d he said. \u201cAroma equals flavor, and flavor equals nutrition.\u201d"},
{"url": "https://github.com/googlechrome/lighthouse", "link_title": "Lighthouse - Auditing and Performance Metrics for Progressive Web Apps", "sentiment": -0.031111111111111096, "text": "status: prototype extension and CLI available for testing\n\nSome basic unit tests forked are in and run via mocha. eslint is also checked for style violations.\n\nThe same audits are run against from a Chrome extension. See ./extension.\n\nThe return value of each audit takes this shape:\n\n. ({ name , tags [ ], description , // value: The score. Typically a boolean, but can be number 0-100 value , // rawValue: Could be anything, as long as it can easily be stringified and displayed, // e.g. 'your score is bad because you wrote ${rawValue}' rawValue {}, // debugString: Some *specific* error string for helping the user figure out why they failed here. // The reporter can handle *general* feedback on how to fix, e.g. links to the docs debugString // fault: Optional argument when the audit doesn't cover whatever it is you're doing, // e.g. we can't parse your particular corner case out of a trace yet. // Whatever is in `rawValue` and `score` would be N/A in these cases fault some reason the audit has failed you, Anakin });\n\nWe're using JSDoc along with closure annotations. Annotations encouraged for all contributions.\n\n> > . Use wherever possible. Save for emergencies only.\n\nThe traceviewer-based trace processor from node-big-rig was forked into Lighthouse. Additionally, the DevTools' Timeline Model is available as well. There may be advantages for using one model over another."},
{"url": "https://www.youtube.com/watch?v=xRj9rYKV48k", "link_title": "Build, Ship and Run the Docker Way", "sentiment": 0.35, "text": "This presentation was recorded at GOTO Berlin 2015\n\nhttp://gotober.com\n\n\n\nMaxime Heckel - Software Engineer & Space Enthusiast\n\n\n\nABSTRACT\n\nMaxime will talk about fully automating the developer workflow from Git Push to deploying in production.\n\nHe will walk through the concepts and functions you need to know [...]\n\nDownload slides and read the full abstract here:\n\nhttp://gotocon.com/berlin-2015/presen...\n\n\n\nhttps://twitter.com/gotober\n\nhttps://www.facebook.com/GOTOConference\n\nhttp://gotocon.com"},
{"url": "http://qz.com/689806/a-controversial-theory-may-explain-the-real-reason-humans-have-allergies/", "link_title": "A controversial theory may explain the real reason humans have allergies", "sentiment": 0.059308844432347435, "text": "For me, it was hornets.\n\nOne summer afternoon when I was 12, I ran into an overgrown field near a friend\u2019s house and kicked a hornet nest the size of a football. An angry squadron of insects clamped onto my leg; their stings felt like scorching needles. I swatted the hornets away and ran for help, but within minutes I realized something else was happening. A constellation of pink stars had appeared around the stings. The hives swelled, and new ones began appearing farther up my legs. I was having an allergic reaction.\n\nMy friend\u2019s mother gave me antihistamines and loaded me into her van. We set out for the county hospital, my dread growing as we drove. I was vaguely aware of the horrible things that can happen when allergies run amok. I imagined the hives reaching my throat and sealing it shut.\n\nI lived to tell the tale: my hives subsided at the hospital, leaving behind a lingering fear of hornets. But an allergy test confirmed that I was sensitive to the insects. Not to honey bees or wasps or yellow jackets. Just the particular type of hornet that had stung me. The emergency room doctor said I might not be so fortunate the next time I encountered a nest of them. She handed me an EpiPen and told me to ram the syringe into my thigh if I was stung again. The epinephrine would raise my blood pressure, open my airway\u2013and perhaps save my life. I\u2019ve been lucky: that afternoon was 35 years ago, and I haven\u2019t encountered a hornet\u2019s nest since. I lost track of that EpiPen years ago.\n\nAnyone with an allergy has their origin story, a tale of how they discovered that their immune system goes haywire when some arbitrarily particular molecule gets into their body. There are hundreds of millions of these stories. In the US alone, an estimated 18 million people suffer from hay fever, and food allergies affect millions of American children. The prevalence of allergies in many other countries is rising. The list of allergens includes\u2013but is not limited to\u2013latex, gold, pollen (ragweed, cockleweed and pigweed are especially bad), penicillin, insect venom, peanuts, papayas, jellyfish stings, perfume, eggs, the feces of house mites, pecans, salmon, beef and nickel.\n\nAllergies are not simply a biological blunder. Instead, they\u2019re an essential defense against noxious chemicals.\u00a0Once these substances trigger an allergy, the symptoms can run the gamut from annoying to deadly. Hives appear, lips swell. Hay fever brings sniffles and stinging eyes; allergies to food can cause vomiting and diarrhea. For an unlucky minority, allergies can trigger a potentially fatal whole-body reaction known as anaphylactic shock.\n\nThe collective burden of these woes is tremendous, yet the treatment options are limited. EpiPens save lives, but the available long-term treatments offer mixed results to those exhausted by an allergy to mould or the annual release of pollen. Antihistamines can often reduce sufferers\u2019 symptoms, but these drugs also cause drowsiness, as do some other treatments.\n\nWe might have more effective treatments if scientists understood allergies, but a maddening web of causes underlies allergic reactions. Cells are aroused, chemicals released, signals relayed. Scientists have only partially mapped the process. And there\u2019s an even bigger mystery underlying this biochemical web: why do we even get allergies at all?\n\n\u201cThat is exactly the problem I love,\u201d Ruslan Medzhitov told me recently. \u201cIt\u2019s very big, it\u2019s very fundamental, and completely unknown.\u201d\n\nMedzhitov and I were wandering through his laboratory, which is located on the top floor of the Anlyan Center for Medical Research and Education at the Yale School of Medicine. His team of postdocs and graduate students were wedged tight among man-sized tanks of oxygen and incubators full of immune cells. \u201cIt\u2019s a mess, but a productive mess,\u201d he said with a shrug. Medzhitov has a boxer\u2019s face\u2013massive, circular, with a broad, flat nose\u2013but he spoke with a soft elegance.\n\nMedzhitov\u2019s mess has been exceptionally productive. Over the past 20 years, he has made fundamental discoveries about the immune system, for which he has been awarded a string of major prizes. Last year he was the first recipient of the \u20ac4 million Else Kr\u00f6ner Fresenius Award. And though Medzhitov hasn\u2019t won a Nobel, many of his peers think he should have: in 2011, 26 leading immunologists wrote to Nature protesting that Medzhitov\u2019s research had been overlooked for the prize.\n\nNow Medzhitov is turning his attention to a question that could change immunology yet again: why do we get allergies? No one has a firm answer, but what is arguably the leading theory suggests that allergies are a misfiring of a defense against parasitic worms. In the industrialized world, where such infections are rare, this system reacts in an exaggerated fashion to harmless targets, making us miserable in the process.\n\nMedzhitov thinks that\u2019s wrong. Allergies are not simply a biological blunder. Instead, they\u2019re an essential defense against noxious chemicals\u2013a defence that has served our ancestors for tens of millions of years and continues to do so today. It\u2019s a controversial theory, Medzhitov acknowledges. But he\u2019s also confident that history will prove him right. \u201cI think the field will go around in that stage where there\u2019s a lot of resistance to the idea,\u201d he told me. \u201cUntil everybody says, \u2018Oh yeah, it\u2019s obvious. Of course it works that way.\u2019\u201d\n\nThe physicians of the ancient world knew about allergies. Three thousand years ago, Chinese doctors described a \u201cplant fever\u201d that caused runny noses in autumn. There is evidence that the Egyptian pharaoh Menes died from the sting of a wasp in 2641 BCE. Two and a half millennia later, the Roman philosopher Lucretius wrote, \u201cWhat is food to one is to others bitter poison.\u201d\n\nBut it was a little more than a century ago when scientists realized that these diverse symptoms are different heads on the same hydra. By then researchers had discovered that many diseases are caused by bacteria and other pathogens, and that we fight these invaders with an immune system\u2013an army of cells that can unleash deadly chemicals and precisely targeted antibodies. They soon realized that the immune system can also cause harm. In the early 1900s, the French scientists Charles Richet and Paul Portier were studying how toxins affect the body. They injected small doses of poison from sea anemones into dogs, then waited a week or so before delivering an even smaller dose. Within minutes, the dogs went into shock and died. Instead of protecting the animals from harm, the immune system appeared to make them more susceptible.\n\nOther researchers observed that some medical drugs caused hives and other symptoms. And this sensitivity increased with exposure\u2013the opposite of the protection that antibodies provided against infectious diseases. The Austrian doctor Clemens von Pirquet wondered how it was that substances entering the body could change the way the body reacted. To describe this response, he coined the word \u2018allergy\u2019, from the Greek words allos (\u2018other\u2019) and ergon (\u2018work\u2019).\n\nTwo and a half millennia later, the Roman philosopher Lucretius wrote, \u201cWhat is food to one is to others bitter poison.\u201d\u00a0In the decades that followed, scientists discovered that the molecular stages of these reactions were remarkably similar. The process begins when an allergen lands on one of the body\u2019s surfaces\u2013skin, eye, nasal passage, mouth, airway or gut. These surfaces are loaded with immune cells that act as border sentries. When a sentry encounters an allergen, it first engulfs and demolishes the invader, then decorates its outer surface with fragments of the substance. Next the cell locates some lymph tissue. There it passes on the fragments to other immune cells, which produce a distinctive fork-shaped antibody, known as immunoglobulin E, or IgE.\n\nThese antibodies will trigger a response if they encounter the allergen again. The reaction begins when an antibody activates a component of the immune system known as a mast cell, which then blasts out a barrage of chemicals. Some of these chemicals latch onto nerves, triggering itchiness and coughing. Sometimes mucus is produced. Airway muscles can contract, making it hard to breathe.\n\nThis picture, built up in labs over the past century, answered the \u201chow?\u201d part of the allergies mystery. Left unanswered, however, was \u201cwhy?\u201d And that\u2019s surprising, because the question had a pretty clear answer for most parts of the immune system. Our ancestors faced a constant assault of pathogens. Natural selection favored mutations that helped them fend off these attacks, and those mutations accumulated to produce the sophisticated defenses we have today.\n\nIt was harder to see how natural selection could have produced allergies. Reacting to harmless things with a huge immune response probably wouldn\u2019t have aided the survival of our ancestors. Allergies are also strangely selective. Only some people have allergies, and only some substances are allergens. Sometimes people develop allergies relatively late in life; sometimes childhood allergies disappear. And for decades, nobody could even figure out what IgE was for. It showed no ability to stop any virus or bacteria. It was as if we evolved one special kind of antibody just to make us miserable.\n\nReacting to harmless things with a huge immune response probably wouldn\u2019t have aided the survival of our ancestors.\u00a0One early clue came in 1964. A parasitologist named Bridget Ogilvie was investigating how the immune system repelled parasitic worms, and she noticed that rats infected with worms produced large amounts of what would later be called IgE. Subsequent studies revealed that the antibodies signaled the immune system to unleash a damaging assault on the worms.\n\nParasitic worms represent a serious threat\u2013not just to rats, but to humans too. Hookworms can drain off blood from the gut. Liver flukes can damage liver tissue and cause cancer. Tapeworms can cause cysts in the brain. More than 20 per cent of all people on Earth carry such an infection, most of them in low-income countries. Before modern public health and food safety systems, our ancestors faced a lifelong struggle against these worms, as well as ticks and other parasitic animals.\n\nDuring the 1980s, several scientists argued forcefully for a link between these parasites and allergies. Perhaps our ancestors evolved an ability to recognize the proteins on the surface of worms and to respond with IgE antibodies. The antibodies primed immune system cells in the skin and gut to quickly repel any parasite trying to push its way in. \u201cYou\u2019ve got about an hour to react very dramatically in order to reduce the chance of these parasites surviving,\u201d said David Dunne, a parasitologist at the University of Cambridge.\n\nAccording to the worm theory, the proteins of parasitic worms are similar in shape to other molecules we regularly encounter in our lives. If we encounter those molecules, we mount a pointless defense. \u201cAllergy is just an unfortunate side-effect of defense against parasitic worms,\u201d says Dunne.\n\nWhen he was an immunologist in training, Medzhitov was taught the worm theory of allergies. But ten years ago he started to develop doubts. \u201cI was seeing that it doesn\u2019t make sense,\u201d he said. So Medzhitov began thinking about a theory of his own.\n\nThinking is a big part of Medzhitov\u2019s science. It\u2019s a legacy of his training in the Soviet Union in the 1980s and 1990s, when universities had little equipment and even less interest in producing good scientists. For his undergraduate degree, Medzhitov went to Tashkent State University in Uzbekistan. Every autumn the professors sent the students out into the cotton fields to help take in the harvest. They worked daily from dawn to dusk. \u201cIt was terrible,\u201d said Medzhitov. \u201cIf you don\u2019t do that, you get expelled from college.\u201d He recalls sneaking biochemistry textbooks into the fields\u2013and being reprimanded by a department chair for doing so.\n\nGraduate school wasn\u2019t much better. Medzhitov arrived at Moscow State University just as the Soviet regime collapsed. The university was broke, and Medzhitov didn\u2019t have the equipment he needed to run experiments. \u201cI was basically spending all of my time reading and thinking,\u201d Medzhitov told me.\n\nMostly, he thought about how our bodies perceive the outside world. We can recognize patterns of photons with our eyes and patterns of air vibrations with our ears. To Medzhitov, the immune system was another pattern recognition system\u2013one that detected molecular signatures instead of light or sound.\n\nTo Medzhitov, the immune system was another pattern recognition system\u2013one that detected molecular signatures instead of light or sound.\u00a0As Medzhitov searched for papers on this subject, he came across references to a 1989 essay written by Charles Janeway, an immunologist at Yale, titled \u201cApproaching the Asymptote? Evolution and revolution in immunology.\u201d Medzhitov was intrigued and used several months\u2019 of his stipend to buy a reprint of the paper. It was worth the wait, because the paper exposed him to Janeway\u2019s theories, and those theories would change his life.\n\nAt the time, Janeway was arguing that antibodies have a big drawback: it takes days for the immune system to develop an effective antibody against a new invader. He speculated that the immune system might have another line of defense that could offer faster protection. Perhaps the immune system could use a pattern-recognition system to detect bacteria and viruses quickly, allowing it to immediately launch a response.\n\nMedzhitov had been thinking about the same thing, and he immediately emailed Janeway. Janeway responded, and they began an exchange that would ultimately bring Medzhitov to New Haven, Connecticut, in 1994, to become a postdoctoral researcher in Janeway\u2019s lab. (Janeway died in 2003.)\n\n\u201cHe turned out to speak very little English, and had almost no experience in a wet laboratory,\u201d says Derek Sant\u2019Angelo, who worked in the lab at the time. Sant\u2019Angelo, now at the Robert Wood Johnson Medical School in New Jersey, recalls coming across Medzhitov at the bench one night. In one hand, Medzhitov held a mechanical pipette. In the other hand, he held a tube of bacteria. Medzhitov needed to use the pipette to remove a few drops of bacteria from the tube and place them on a plate on the lab bench in front of him. \u201cHe was slowly looking back and forth from the pipette down to the plate to the bacteria,\u201d says Sant\u2019Angelo. \u201cHe knew in theory that the pipette was used to put the bacteria on the plate. But he simply had absolutely no idea how to do it.\u201d\n\nMedzhitov still marvels that Janeway agreed to work with him. \u201cI think that the only reason that he took me in his lab is that nobody else wanted to touch this idea,\u201d he recalled.\n\nWith help from Sant\u2019Angelo and other members of the lab, Medzhitov learned very quickly. Soon he and Janeway discovered a new class of sensor on the surface of a certain kind of immune cell. Confronted with an invader, the sensors would clasp onto the intruder and trigger a chemical alarm that promoted other immune cells to search the area for pathogens to kill. It was a fast, accurate way to sense and remove bacterial invaders.\n\nMedzhitov and Janeway\u2019s discovery of the sensors, now known as toll-like receptors, revealed a new dimension to our immune defenses, and has been hailed as a fundamental principle of immunology. It also helped solve a medical mystery.\n\n\u201cThirty years ago, it was, \u2018Whatever causes septic shock is bad.\u2019 Well, now we know it\u2019s not.\u201d\u00a0Infections sometimes produce a catastrophic body-wide inflammation known as sepsis. It is thought to strike around a million people a year in the USA alone, up to half of whom die. For years, scientists thought that a bacterial toxin might cause the immune system to malfunction in this way \u2013 but sepsis is actually just an exaggeration of one of the usual immune defenses against bacteria and other invaders. Instead of acting locally, the immune system accidentally responds throughout the body. \u201cWhat happens in septic shock is that these mechanisms become activated much more strongly than necessary,\u201d said Medzhitov. \u201cAnd that\u2019s what kills.\u201d\n\nMedzhitov isn\u2019t driven to do science to cure people; he\u2019s more interested in basic questions about the immune system. But he argues that cures won\u2019t be found if researchers have the wrong answers for basic questions. Only now that scientists have a clear understanding of the biology underlying sepsis can they develop treatments that target the real cause of the condition\u2013the over-reaction of the toll-like receptors. (Tests are ongoing, and the results so far are promising). \u201cThirty years ago, it was, \u2018Whatever causes septic shock is bad.\u2019 Well, now we know it\u2019s not,\u201d said Medzhitov.\n\nMedzhitov kept thinking after he and Janeway discovered toll-like receptors. If the immune system has special sensors for bacteria and other invaders, perhaps it had undiscovered sensors for other enemies. That\u2019s when he started thinking about parasitic worms, IgE and allergies. And when he thought about them, things didn\u2019t add up.\n\nIt\u2019s true that the immune system makes IgE when it detects parasitic worms. But some studies suggest that IgE isn\u2019t actually essential to fight these invaders. Scientists have engineered mice that can\u2019t make IgE, for instance, and have found that the animals can still mount a defence against parasitic worms. And Medzhitov was sceptical of the idea that allergens mimic parasite proteins. A lot of allergens, such as nickel or penicillin, have no possible counterpart in the molecular biology of a parasite.\n\nThe more Medzhitov thought about allergens, the less important their structure seemed. Maybe what ties allergens together was not their shape, but what they do.\n\nWe know that allergens often cause physical damage. They rip open cells, irritate membranes, slice proteins into tatters. Maybe, Medzhitov thought, allergens do so much damage that we need a defense against them. \u201cIf you think of all the major symptoms of allergic reactions\u2013runny noses, tears, sneezing, coughing, itching, vomiting and diarrhoea\u2013all of these things have one thing in common,\u201d said Medzhitov. \u201cThey all have to do with expulsion.\u201d Suddenly the misery of allergies took on a new look. Allergies weren\u2019t the body going haywire; they were the body\u2019s strategy for getting rid of the allergens.\n\nMaybe, Medzhitov thought, allergens do so much damage that we need a defense against them.\u00a0As Medzhitov explored this possibility, he found that the idea had surfaced from time to time over the years, only to be buried again. In 1991, for example, the evolutionary biologist Margie Profet argued that allergies fought toxins. Immunologists dismissed the idea, perhaps because Profet was an outsider. Medzhitov found it hugely helpful. \u201cIt was liberating,\u201d he said.\n\nTogether with two of his students, Noah Palm and Rachel Rosenstein, Medzhitov published his theory in Nature in 2012. Then he began testing it. First he checked for a link between damage and allergies. He and colleagues injected mice with PLA2, an allergen that\u2019s found in honey-bee venom and tears apart cell membranes. As Medzhitov had predicted, the animals\u2019 immune systems didn\u2019t respond to PLA2 itself. Only when PLA2 ripped open cells did the immune system produce IgE antibodies.\n\nAnother prediction of Medzhitov\u2019s theory was that these antibodies would protect the mice, rather than just make them ill. To test this, Medzhitov and his colleagues followed their initial injection of PLA2 with a second, much bigger dose. If the animals had not previously been exposed to PLA2, the dose sent their body temperature plunging, sometimes fatally. But the mice that had been exposed marshalled an allergic reaction that, for reasons that aren\u2019t yet clear, lessened the impact of the PLA2.\n\nMedzhitov didn\u2019t know it, but on the other side of the country another scientist was running an experiment that would provide even stronger support for his theory. Stephen Galli, chair of the Pathology Department at Stanford University School of Medicine, had spent years studying mast cells, the enigmatic immune cells that can kill people during allergic reactions. He suspected mast cells may actually help the body. In 2006, for example, Galli and colleagues found that mast cells destroy a toxin found in viper venom. That discovery led Galli to wonder, like Medzhitov, whether allergies might be protective.\n\nTo find out, Galli and colleagues injected one to two stings\u2019 worth of honey-bee venom into mice, prompting an allergic reaction. Then they injected the same animals with a potentially lethal dose, to see if the reaction improved the animal\u2019s chance of survival. It did. What\u2019s more, when Galli\u2019s team injected the IgE antibodies into mice that had never been exposed to the venom, those animals were also protected against a potentially lethal dose.\n\nMedzhitov was delighted to discover Galli\u2019s paper in the same issue of Immunity that carried his own. \u201cIt was good to see that somebody got the same results using a very different model. That\u2019s always reassuring,\u201d Medzhitov told me.\n\nMedzhitov predicts that these experiments will show that allergen detection is like a home-alarm system. \u00a0Still, the experiments left a lot unanswered. How precisely did the damage caused by the bee venom lead to an IgE response? And how did IgE protect the mice? These are the kinds of questions that Medzhitov\u2019s team is now investigating. He showed me some of the experiments when I visited again last month. We sidled past a hulking new freezer blocking a corridor to slip into a room where Jaime Cullen, a researcher associate in the lab, spends much of her time. She put a flask of pink syrup under a microscope and invited me to look. I could see a flotilla of melon-shaped objects.\n\n\u201cThese are the cells that cause all the problems,\u201d said Medzhitov. I was looking at mast cells, the key agents of allergic reactions. Cullen is studying how IgE antibodies latch onto mast cells and prime them to become sensitive\u2013or, in some cases, oversensitive\u2013to allergens.\n\nMedzhitov predicts that these experiments will show that allergen detection is like a home-alarm system. \u201cYou can detect a burglar, not by recognizing his face, but by a broken window,\u201d he said. The damage caused by an allergen rouses the immune system, which gathers up molecules in the vicinity and makes antibodies to them. Now the criminal has been identified and can be more easily apprehended next time he tries to break in.\n\nAllergies make a lot more sense in terms of evolution when seen as a home-alarm system, argues Medzhitov. Toxic chemicals, whether from venomous animals or plants, have long threatened human health. Allergies would have protected our ancestors by flushing out these chemicals. And the discomfort our ancestors felt when exposed to these allergens might have led them to move to safer parts of their environment.\n\nToxic chemicals have long threatened human health. Allergies would have protected our ancestors by flushing out these chemicals. \u00a0Like many adaptations, allergies weren\u2019t perfect. They lowered the odds of dying from toxins but didn\u2019t eliminate the risk. Sometimes the immune system overreacts dangerously, as Richet and Protier discovered when the second dose of anemone allergen killed the dogs they were experimenting on. And the immune system might sometimes round up a harmless molecular bystander when it responded to an allergy alarm. But overall, Medzhitov argues, the benefits of allergies outstripped their drawbacks.\n\nThat balance shifted with the rise of modern Western life, he adds. As we created more synthetic chemicals, we exposed ourselves to a wider range of compounds, each of which could potentially cause damage and trigger an allergic reaction. Our ancestors could avoid allergens by moving to the other side of the forest, but we can\u2019t escape so easily. \u201cIn this particular case, the environment we\u2019d have to avoid is living indoors,\u201d said Medzhitov.\n\nScientists are taking this theory very seriously. \u201cRuslan is one of the most distinguished immunologists in the world,\u201d said Galli. \u201cIf he thinks there\u2019s validity to this idea, I think it gets a lot of traction.\u201d\n\nDunne, on the other hand, is skeptical about the idea that Medzhitov\u2019s theory explains all allergies. Medzhitov is underestimating the huge diversity of proteins that Dunne and others are finding on the surface of worms\u2013proteins that could be mimicked by a huge range of allergens in the modern world. \u201cMy money\u2019s more on the worm one,\u201d he said.\n\nOver the next few years, Medzhitov hopes to persuade skeptics with another experiment. It\u2019s unlikely to end the debate, but positive results would bring many more people over to his way of thinking. And that might eventually lead to a revolution in the way we treat allergies.\n\nSitting on Cullen\u2019s lab bench is a plastic box that houses a pair of mice. There are dozens more of these boxes in the basement of their building. Some of the mice are ordinary, but others are not: using genetic engineering techniques, Medzhitov\u2019s team has removed the animals\u2019 ability to make IgE. They can\u2019t get allergies.\n\nMedzhitov would just be happy to get people to stop seeing allergies as a disease, despite the misery they cause. \u00a0Medzhitov and Cullen will be observing these allergy-free mice for the next couple of years. The animals may be spared the misery of hay fever caused by the ragweed pollen that will inevitably drift into their box on currents of air. But Medzhitov predicts they will be worse off for it. Unable to fight the pollen and other allergens, they will let these toxic molecules pass into their bodies, where they will damage organs and tissues.\n\n\u201cIt\u2019s never been done before, so we don\u2019t know what the consequences will be,\u201d says Medzhitov. But if his theory is right, the experiment will reveal the invisible shield that allergies provide us.\n\nEven if the experiment works out just as he predicts, Medzhitov doesn\u2019t think his ideas about allergies will win out as quickly as his ideas about toll-like receptors. The idea that allergic reactions are bad is ingrained in the minds of physicians. \u201cThere\u2019s going to be more inertia,\u201d he said.\n\nBut understanding the purpose of allergies could lead to dramatic changes in how they\u2019re treated. \u201cOne implication of our view is that any attempt to completely block allergic defenses would be a bad idea,\u201d he said. Instead, allergists should be learning why a minority of people turn a protective response into a hypersensitive one. \u201cIt\u2019s the same as with pain,\u201d said Medzhitov. \u201cNo pain at all is deadly; normal pain is good; too much pain is bad.\u201d\n\nFor now, however, Medzhitov would just be happy to get people to stop seeing allergies as a disease, despite the misery they cause. \u201cYou\u2019re sneezing to protect yourself. The fact that you don\u2019t like the sneezing, that\u2019s tough luck,\u201d he said, with a slight shrug. \u201cEvolution doesn\u2019t care how you feel.\u201d\n\nThis article first appeared on Mosaic and is republished here under a Creative Commons license. We welcome your comments at ideas@qz.com."},
{"url": "http://www.theguardian.com/commentisfree/2016/may/22/peter-thiel-paypal-donald-trump-silicon-valley-libertarian", "link_title": "Peter Thiel \u2013 The Only Living Trump Supporter in Silicon Valley", "sentiment": 0.1237069676355391, "text": "The most interesting discovery of the week was not that IBM, Citigroup and Microsoft were unwittingly running ads on (and therefore providing funds to) an Indonesian jihadi website \u2013 though they were \u2013 but that Peter Thiel is supporting Donald Trump in his bid to become the next president of the United States.\n\n\u201cPeter who?\u201d I hear you say. Mr Thiel is not exactly a household name in these parts, but in Silicon Valley he\u2019s a big cheese, as a co-founder of PayPal and the first investor in Facebook. He is therefore rich beyond the dreams of avarice. But he is also: a philosophy graduate; a lawyer; a former bond trader; a hedge-fund manager; a venture capitalist; a philanthropist; a far-out libertarian; and an entertaining author. So what is a guy like that doing supporting Trump?\n\nOne answer might be that he\u2019s as much of an irritant to the Silicon Valley crowd as Trump is to the Republican establishment. Although the Valley\u2019s tech titans like to portray themselves as non-statist disruptors, in fact most of them are \u2013 politically speaking \u2013 Democratic party supporters, albeit of an unusual kind. They may detest trade unions, for example, but they\u2019re very keen on immigration \u2013 so long as the immigrants have PhDs from elite Indian or Chinese universities. And they\u2019re not opposed to big government, so long as it\u2019s \u201csmart\u201d, whatever that\u00a0means.\n\nPeter Thiel doesn\u2019t fit this template at all. In 2009, he published an intriguing essay entitled The Education \u00a0of a Libertarian. \u201cI remain committed to the faith of my teenage years\u201d, it began: \u201cto authentic human freedom as a precondition for the highest good. I stand against confiscatory taxes, totalitarian collectives, and the ideology of the inevitability of the death of every individual. For all these reasons, I still call myself \u2018libertarian\u2019.\u201d But, he confessed, \u201cover the last two decades, I have changed radically on the question of how to achieve these goals. Most importantly, I no longer believe that freedom and democracy are\u00a0compatible.\u201d\n\nSo what changed his mind? Answer: the 2008 banking collapse, which Thiel describes as \u201ca financial crisis caused by too much debt and leverage, facilitated by a government that insured against all sorts of moral hazards \u2013 and we know that the response to this crisis involves way more debt and leverage, and way more government. Those who have argued for free markets have been screaming into a hurricane. The events of recent months shatter any remaining hopes of politically minded libertarians. For\u00a0those of us who are libertarian in 2009, our education culminates with the knowledge that the broader education of the body politic has become a fool\u2019s errand.\u201d\n\nThe emerging theme is that democratic politics is irretrievably broken. \u201cIn our time,\u201d Thiel says, \u201cthe great task for libertarians is to find an escape from politics in all its forms \u2013 from the totalitarian and fundamentalist catastrophes to the unthinking demos that guides so-called \u2018social democracy\u2019. The critical question then becomes one of means, of how to escape not via politics but beyond it.\u201d\n\nIn 2009 Thiel could only see three possible escape routes. The first was cyberspace: \u201cBy starting a new internet business,\u201d he wrote, \u201can entrepreneur may create a new world. The hope of the internet is that these new worlds will impact and force change on the existing social and political order.\u201d The second was \u2013 wait for it \u2013 outer space: \u201cBecause the vast reaches of outer space represent a limitless frontier, they also represent a limitless possibility for escape from world politics.\u201d And finally there was what Thiel called \u201cseasteading\u201d \u2013 floating islands in international waters run as libertarian paradises, presumably with free copies of Ayn Rand\u2019s books on every bedside table.\n\nSadly, none of these ideas has \u2013 as yet \u2013 borne much fruit. The internet has been captured by governments and huge corporations. Colonising Mars and escaping to other galaxies is a proposition only for Hollywood and the Starship Enterprise. And seasteading, though technically less impracticable, remains the fantasy of dreamers and flakes of Cadbury\u00a0proportions.\n\nFaced with these cruel disappointments, what is a billionaire fantasist to do? Why, hitch his wagon to that of another billionaire fantasist, of course. And Trump and Thiel have more in common than perhaps they realise. In his 2009 essay, for example, Thiel wrote: \u201cSince 1920, the vast increase in welfare beneficiaries and the extension of the franchise to women \u2013 two constituencies that are notoriously tough for libertarians \u2013 have rendered the notion of \u2018capitalist democracy\u2019 into an oxymoron.\u201d Trump\u00a0is hoping to turn that oxymoron into a reality."},
{"url": "http://packagemain.blogspot.com/2016/05/data-model-generation-for-postgresql.html", "link_title": "Data Model Generation for PostgreSQL", "sentiment": 0.026406926406926403, "text": "that is useful to produce Microservice applications starting from the database schema.\n\nThe current release (2.0.beta) contains all the templates and producers for providing microservices that can operate on Postgres.\u00a0This DinGo release is still\u00a0in beta because it requires a few tests and there are also some limitations. Postgres array types are not supported and I don't know if they will be in the future.\n\nPostgreSQL offers two methods for retrieving meta-data about the table's schema using the system catalogs (pg_class, pg_user, pg_view, ...) or querying the information_schema (available since Postgres 7.4).\n\nDinGo has adopted the second method retrieving all the\u00a0tables meta-data\u00a0querying the information_schema. This approach is very similar to that used with MySQL, queries on the PostgreSQL information_schema require only few changes starting from the previous ones.\n\nDinGo configuration file requires some additional parameters to connect Postgres.\u00a0This is an example of configuration\n\nThe new property DatabaseType can now assume one of the two values\n\n\"MySQL\" or \"Postgres\" and the PostgresSchema property is used for querying the Information_Schema and get the meta-data of the selected set of tables."},
{"url": "http://www.headforpoints.com/2016/05/24/curve-card-pauses-amex-functionality/", "link_title": "Curve card pauses support for American Express", "sentiment": 0.1103658536585366, "text": "Let\u2019s keep Curve discussion here, please, to stop other threads getting clogged up.\n\nI will do an analysis on this tomorrow. \u00a0My gut feeling in the short term is that \u2013 assuming you can easily spend \u00a31,000 on Curve in the next three months (and \u00a3600 of that can be free ATM withdrawals linked to a MasterCard or Visa, earning points) \u2013 you might as well do that.\n\nYou will get the \u00a335 credit, which if you have the basic card means you are in the same financial position as taking a refund, and you still have the Curve card for overseas use (if you don\u2019t have a 0% card) and ATM use and for places where it is treated as a debit card.\n\nAnd, of course, if you take a refund you would need to pay \u00a335 again later if / when Amex functionality returns. If you have the premium version, the maths is different and you may find the full refund better than a \u00a350 credit.\n\nIt is always fun and games working with start-ups \u2026.!\n\nI\u2019m Shachar, the CEO of Curve. Today we\u2019ve got some disappointing news. American Express have asked us to pause their functionality with Curve \u2013 which means you will not be able to use your Curve card with your Amex for the time being. This begins at midday on May 31st UK time \u2013 you\u2019ll shortly receive another email with practical details of what will happen when we pause the American Express functionality \u2013 your Curve will continue to work with MasterCard and Visa.\n\nAmerican Express want to ensure there is a seamless customer support process for their Members when using their Card with Curve. We\u2019re continuing discussions with their management, and hope that American Express decide to come back to Curve soon.\n\nWe\u2019re continuing to build something big at Curve. The first ever platform to connect you to your everything money \u2013 transforming the way you handle your finances, bringing value and saving you time and money. We\u2019re already connecting up all your bank cards, tracking your expenses in real time and saving you money with zero FX fees when you travel. This is just the beginning \u2013 we\u2019re working hard on future features such as the ability to move charges between funding cards after you\u2019ve paid, bespoke loyalty rewards and cash-backs, peer-to peer payments and much more.\n\nAs a big thank you for being one of Curve\u2019s early adopters we are giving Blue card members \u00a335 and Black card members \u00a350 worth of Curve Points if you spend a total of \u00a31000 over the next three months. You\u2019ll be able to spend your Curve Points at any merchant that accepts MasterCard using your Curve. Please see our FAQs for further details.\n\nWe hope you choose to stay with us to continue the exciting journey we\u2019ve begun together. Please write to me directly at shachar.bialick@imaginecurve.com \u2013 I\u2019m happy to answer any questions you might have."},
{"url": "http://koaning.io/human-entropy.html", "link_title": "An inverse turing test", "sentiment": 0.028785142061004135, "text": "You may not want to read this blogpost on mobile. The d3 stuff can be a bit heavy.\n\nIn this document we'll do an experiment to see if humans can generate random numbers effectively. We will use you, a human to do this. Please click the heads/tails button as randomly as you can. You may also use the (heads) or (tails) keys on your keyboards (which probably is a whole lot faster if you are on a laptop).\n\nYou've currently generated 0 numbers, please ensure that you got about 100 before moving on. You are likely to cheat if you scroll down, which is fine when you try this a second time but it would be best to do a clean attempt beforehand.\n\nLet us do an inverse turing test.\n\nYou've just given your sequence of 'random' numbers. There are many axes for judging if you have given random data. In this document we will focus on the markov chain that we can learn from the generated input. The counts for these markov models are graphed below. You may see no bias in the first or second chart, but as you scroll down it may become more and more biased to a certain pattern.\n\nOften, humans fall into a pattern instead of delivering true randomness. Especially repeating or is common. Using the barcharts it may become evident if this is the case.\n\nLet us go a step further. It is well possible that you are such a terrible machine that an actual machine can predict your 'randomness'. We can use the counts from before to generate simple markov models $M_k$ (where $k$ the number of nodes in the markov chain). Each markov chain can then say how likely it is to see a heads (or a 1).\n\nWe can also combine these models. We train $M_1,...,M_5$ in real time and combine these via a naive ensemble rule.\n\nThis is a blunt model, especially because we're sticking to discrete-land while a beta distribution would be much better here. Still, this should be able to pick up a lot of common human patterns. We'll also introduce some smoothing in the beginning to prevent a very early overfit. We encourge the reader to try and changing their pattern a few times to see how the markov chains respond.\n\nWhen we do this the predictions over time are show below. The lines $p_1, ..., p_5$ correspond to the predictions of markov chain $M_1,...,M_5$ and corresponds to the prediction from $P(H_t | M_1,...,M_k X_{t-k-1})$.\n\nThe accuracy of this naive ensemble is depicted in the plot below. We show the total accuracy as well as the accuracy of the last 15 predictions. We do this to also demonstrate how the markovs can learn new patterns.\n\nSo with these numbers, how random might the supplied data be? Well, if the data truly was random then the number of correct predictions needs to come from the following distribution;\n\nThis means that our found accuracy can help us determine how likely it is that the data is generated randomly. In maths, with the given data; $\\sum_i P(x_i \\geq a | H_0) = $ 1. This is by no means the only axis where we can measure randomness, but it is able to filter out a lot of human behavior.\n\nMainly, coding this was a lot of fun.\n\nYou may be wondering what the result is if an actual robot filled this in. Press the button below to find out.\n\nThis document was created together with @jbnicolai_, props to him!"},
{"url": "https://motherboard.vice.com/read/phineas-fisher-sme?asd", "link_title": "A Notorious Hacker Just Released a How-To Video Targeting Police", "sentiment": -0.004516250944822379, "text": "The pseudonymous hacker behind the catastrophic breach of notorious police surveillance tool seller Hacking Team is now teaching others how to hack.\n\nPhineas Fisher, who also goes by the handles Hack Back! and GammaGroupPR, has released a tutorial video showing would-be vigilantes one way of stripping data from target websites.\n\nIn this case, Phineas Fisher targeted a website of Sindicat De Mossos d'Esquadra (SME), or the Catalan police union. The data obtained, which was dumped by the hacker, appears to include names, bank details, and more personal details on police officers.\n\nMost of the information, Phineas Fisher writes in the video\u2019s description, is essentially public, but can be used to connect an officer to their badge number. The hacker also claimed to have temporarily defaced the police union's Twitter account.\n\nThe video lasts some 39 minutes, and blasts out NWA's \u201cFuck the Police,\u201d and other hip hop songs. Using Kali Linux, a popular operating system that comes bundled with a host of hacking and penetration testing tools, Phineas Fisher walks through the steps of probing a site to see if it's vulnerable to a particular type of SQLi injection, before launching an attack and downloading the data.\n\n\"Hehe, mossos are probably too busy tear gassing protesters right now to pay attention to their server logs,\u201d Phineas Fisher writes at one point. Throughout the video, the hacker displays images of people who have been on the receiving end of abuse from the Mossos d'Esquadra. (At the time of writing, SME's website is down).\n\nIn the video, Phineas Fisher points out that the date is May 15, \"so it's likely the majority of Mossos were literally out on the streets at that moment harassing protestors out for Global Debout and the anniversary of 15M,\" referring to recent protests and the anti-austerity movement in Spain, they wrote in an email.\n\nThe point of this video is, naturally, to show others how to \u201chack back,\u201d which follows Phineas Fisher's previous comments of encouraging others to follow suit.\n\n\u201cThat's the plan,\u201d the hacker told Motherboard in an email. \u201cLike subverso says in the lyrics of the song at the end of the video, \u2018el que comparte lo que aprende, es peligroso.\u2019\u201d\n\nOr, \u201cThe one who shares what he learns, is dangerous.\u201d\n\nThis story has been updated to add a comment from Phineas Fisher."},
{"url": "https://blog.risingstack.com/node-hero-node-js-authentication-passport-js/", "link_title": "Node.js Authentication using Passport.js", "sentiment": 0.13251725751725751, "text": "This is the 8th part of the tutorial series called Node Hero - in these chapters, you can learn how to get started with Node.js and deliver software products using it.\n\nIn this tutorial, you are going to learn how to implement a local Node.js authentication strategy using Passport.js and Redis.\n\nBefore jumping into the actual coding, let's take a look at the new technologies we are going to use in this chapter.\n\nPassport is an authentication middleware for Node.js which we are going to use for session management.\n\nWe are going to store our user's session information in Redis, and not in the process's memory. This way our application will be a lot easier to scale.\n\nFor demonstration purposes, let\u2019s build an application that does only the following:\n\nYou have already learned how to structure Node.js projects in the previous chapter of Node Hero, so let's use that knowledge!\n\nWe are going to use the following structure:\n\nAs you can see we will organize files and directories around features. We will have a user page, a note page, and some authentication related functionality.\n\nOur goal is to implement the following authentication flow into our application:\n\nTo set up an authentication strategy like this, follow these three steps:\n\nWe are going to use Express for the server framework - you can learn more on the topic by reading our Express tutorial.\n\nWhat did we do here?\n\nFirst of all, we required all the dependencies that the session management needs. After that we are created a new instance from the module, which will store our sessions.\n\nFor the backing store, we are using Redis, but you can use any other, like MySQL or MongoDB.\n\nPassport is a great example of a library using plugins. For this tutorial, we are adding the module which enables easy integration of a simple local authentication strategy using usernames and passwords.\n\nFor the sake of simplicity, in this example, we are not using a second backing store, but only an in-memory user instance. In real life applications, the would look up a user in a database.\n\nOnce the returns with our user object the only thing left is to compare the user-fed and the real password to see if there is a match.\n\nIf it is a match, we let the user in (by returning the user to passport - ), if not we return an unauthorized error (by returning nothing to passport - ).\n\nTo add protected endpoints, we are leveraging the middleware pattern Express uses. For that, let's create the authentication middleware first:\n\nIt only has one role if the user is authenticated (has the right cookies) it simply calls the next middleware; otherwise it redirects to the page where the user can log in.\n\nUsing it is as easy as adding a new middleware to the route definition.\n\nIn this Node.js tutorial, you have learned how to add basic authentication to your application. Later on, you can extend it with different authentication strategies, like Facebook or Twitter. You can find more strategies at http://passportjs.org/.\n\nThe full, working example is on GitHub, you can take a look here: https://github.com/RisingStack/nodehero-authentication\n\nThe next chapter of Node Hero will be all about testing Node.js applications. You will learn concepts like unit testing, test pyramid, test doubles and a lot more!\n\nShare your questions and feedbacks in the comment section."},
{"url": "https://github.com/containous/traefik", "link_title": "A modern reverse proxy that supports etcd consul messos docker..", "sentiment": 0.1426666666666667, "text": "Tr\u00e6f\u026ak is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. It supports several backends (Docker, Swarm, Mesos/Marathon, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API, file...) to manage its configuration automatically and dynamically.\n\nImagine that you have deployed a bunch of microservices on your infrastructure. You probably used a service registry (like etcd or consul) and/or an orchestrator (swarm, Mesos/Marathon) to manage all these services. If you want your users to access some of your microservices from the Internet, you will have to use a reverse proxy and configure it using virtual hosts or prefix paths:\n\nBut a microservices architecture is dynamic... Services are added, removed, killed or upgraded often, eventually several times a day.\n\nTraditional reverse-proxies are not natively dynamic. You can't change their configuration and hot-reload easily.\n\nTr\u00e6f\u026ak can listen to your service registry/orchestrator API, and knows each time a microservice is added, removed, killed or upgraded, and can generate its configuration automatically. Routes to your services will be created instantly.\n\nRun it and forget it!\n\nHere is a talk (in french) given by Emile Vauge at the Devoxx France 2016 conference. You will learn fundamental Tr\u00e6f\u026ak features and see some demos with Docker, Mesos/Marathon and Lets'Encrypt.\n\nYou can access to a simple HTML frontend of Tr\u00e6fik.\n\nYou can find the complete documentation here.\n\nPlease refer to this section.\n\nYou can join to get basic support. If you prefer a commercial support, please contact containo.us by mail: support@containo.us.\n\nThese projects use Tr\u00e6f\u026ak internally. If your company uses Tr\u00e6f\u026ak, we would be glad to get your feedback :) Contact us on\n\nZenika is one of the leading providers of professional Open Source services and agile methodologies in Europe. We provide consulting, development, training and support for the world\u2019s leading Open Source software products.\n\nFounded in 2014, Asteris creates next-generation infrastructure software for the modern datacenter. Asteris writes software that makes it easy for companies to implement continuous delivery and realtime data pipelines. We support the HashiCorp stack, along with Kubernetes, Apache Mesos, Spark and Kafka. We're core committers on mantl.io, consul-cli and mesos-consul. .\n\nKudos to Peka for his awesome work on the logo"},
{"url": "http://techcrunch.com/2016/05/23/google-plans-to-bring-password-free-logins-to-android-apps-by-year-end/", "link_title": "Google plans to bring password-free logins to Android apps by year-end", "sentiment": 0.160265774629411, "text": "Google\u2019s\u00a0plan to eliminate passwords in favor of systems that take into account a combination of\u00a0signals \u2013 like your typing patterns, your walking patterns, your current location, and more \u2013 will be available to Android developers by year-end, assuming all goes well in testing this year. In an under-the-radar announcement Friday\u00a0afternoon at the Google I/O developer conference, the head of Google\u2019s research unit ATAP (Advanced Technology and Projects)\u00a0Daniel Kaufman offered\u00a0a brief update\u00a0regarding the status of Project\u00a0Abacus, the name for a\u00a0system that\u00a0opts for biometrics over two-factor authentication.\n\nAs you may recall, Project Abacus was first introduced at Google I/O last year, where it was described as an ambitious plan to move the burden of passwords and PINs from the user to the device.\n\nToday, secure logins \u2013 like those used by banks or in the enterprise environment \u2013 often require more than just a username and password. They tend to also require the entry of a unique PIN, which is generally\u00a0sent to your phone via SMS or emailed. This is commonly referred to as two-factor authentication, as it combines something you know (your password) with something you have in your possession, like your phone.\n\nWith Project Abacus, users would instead unlock devices or sign into applications based on a cumulative \u201cTrust Score.\u201d This score\u00a0would\u00a0be calculated using a variety of factors, including your typing patterns, current location, speed and voice patterns, facial recognition, and other things.\n\nGoogle has already implemented similar technology on Android devices (running Android 5.0 and higher) called \u201cSmart Lock,\u201d which lets you automatically unlock your device when you\u2019re in a trusted location, have a trusted Bluetooth device connected, when you\u2019re carrying your device, or\u00a0when the device recognizes your face. (Smart Lock for Passwords, meanwhile, simply saves passwords to websites\u00a0and apps,\u00a0and auto-fills them for you upon your next visit.)\n\nProject Abacus is a bit different. It\u00a0runs in the background on your device to\u00a0continually collect data about you to form its Trust Score.\n\nThis score is basically about how\u00a0confident it is that you are who you say you are. If your score isn\u2019t high enough, apps could revert back to asking for passwords. ATAP had also said previously that apps could require different Trust Scores. For example, your bank might require a higher score than a mobile game.\n\n\u201cWe have a phone, and these phones have all these sensors in them. Why couldn\u2019t it just know who I was, so I don\u2019t need a password? I should just be able to work,\u201d explained Kaufman Friday\u00a0afternoon at Google I/O, describing the problem with password-based authentication.\n\nHe said that engineers in Google\u2019s search and machine intelligence groups have since\u00a0turned Project Abacus\u2019s ideas into\u00a0something called \u201cTrust API,\u201d and this API\u00a0is entering testing with banks starting next month.\n\nIn June, \u201cseveral very large financial institutions\u201d will begin their initial testing of the Trust API, said Kaufman.\n\n\u201cAnd assuming it goes well, this should become available to every Android developer around the world by the end of the year,\u201d he added.\n\nKaufman quickly moved on to other ATAP projects, like its connected clothing, modular smartphone,\u00a0radar sensors, and more. And\u00a0while the other technologies are fun to contemplate, this \u201cTrust API,\u201d as it\u2019s called, could introduce more of a real-world change in how users interact with apps on their smartphones. Plus, it offers a new way of securing the content in apps \u2013 if someone who was not you gained access to your phone and was able to unlock it, all the apps could be locked down automatically simply because that person, as determined by the software, was not you.\n\nLast year, the company said that Project Abacus was in trials with 33 universities across 28 states. Bringing it to banks is a significant step forward. And by making it something Android developers can implement in their own apps by year-end, Google could have its own unique advantage in terms of user authentication to compete with rival systems, Apple\u2019s fingerprint-based TouchID."},
{"url": "https://github.com/PaulBernier/castl", "link_title": "CASTL \u2013  A JavaScript to Lua compiler with a runtime library", "sentiment": 0.18964646464646467, "text": "CASTL (Compile Abstract Syntax Tree to Lua) is a free and open source project that allows you to \"compile\" some JavaScript (ES5 and ES6 through Babel transformation) code to Lua 5.2 or LuaJIT 2 code and run it. (I never tested on Lua 5.3 VM)\n\nCASTL has one dependency called Lrexlib to handle regular expressions: you'll most likely need it, but if your JS code doesn't contain any regexp this dependency is not required. The easy way to install this dependency is to use Luarocks package manager:\n\nYou'll need to have libpcre installed on your system (lrexlib-pcre is only a binding to the libpcre API). On an Ubuntu system for instance you could do: sudo apt-get install libpcre3 libpcre3-dev\n\nAnd then you may want to test:\n\nNote that you can also directly install CASTL globally via npm repository:\n\nThe options of the command line are:\n\nAnnotations are useful to optimize the generated code. Please refer to the file for more information.\n\nHeuristic compilation is an attempt to optimize the generated code by guessing program behavior at execution time. Thus this option may increase the speed of execution of your program ; but as it is based on guessing it may also be wrong sometimes and break you program in some weird cases.\n\nFor now heuristic only applies to numeric for loops. It tries to identify and optimize the following pattern:\n\nYou should try to enable heuristic if you have a lot of numeric for loops. If it breaks you code you may want to consider annotations instead.\n\nCASTL is made of two parts:\n\nThere is also a useful command line tool to easily both compile and execute JS files.\n\nCASTL has one dependency, Lrexlib, which provides a binding of PCRE regular expression library API.\n\nCASTL also needs a JavaScript parser able to produce an AST (Abstract Syntax Tree) compliant with the SpiderMonkey AST structure. You can use Esprima or Acorn for instance (if you installed CASTL as stated above Esprima and Acorn are automatically downloaded as dependencies, you don't need to do anything).\n\nCASTL transpiler is not able to parse ES6 code. If you try to do so you'll get a SyntaxError like 'Error: let instruction is not supported yet'. Luckily thanks to the great Babel library you can transform your code from ES6 to ES5 that will be understood by CASTL transpiler. As a convenience I added an option --babel to the command line tool that'll invoke Babel transform for you before transpiling to Lua.\n\nCASTL lets you execute JS scripts on Lua 5.2 VM. Lua VM is known to be fast and lightweight, and especially it is often used in embedded systems. Thus it could allow you to execute your JS scripts on micro-controllers for example.\n\nThe probably best known example is Tessel which sells a board that \"runs JavaScript\". In fact they do the same as CASTL, the JS code is converted to Lua and then executed in a specific environment. They call it Colony.\n\nNote that CASTL will always be slower than a native Lua script, so take time to learn Lua :).\n\nI have definitively been inspired on many points by Colony. Nonetheless there is some important points of divergence:\n\nEasy, just compile CASTL by itself (and also Esprima for the parsing)! By the way you can find a version already compiled in folder which is used for the eval() function."},
{"url": "http://www.archdaily.com/786202/from-starcraft-to-age-of-empires-when-architecture-is-the-game", "link_title": "From Starcraft to Age of Empires: When Architecture Is the Game", "sentiment": 0.03653368321176539, "text": "In this new collaboration, originally titled \"Architecture for the system\u00a0and systems for architecture,\" Spanish architect and cofounder of the blog MetaSpace, Manuel Saga, reflects on the experience of developing (and taking on) a game where architecture plays a key role for the designer, and for the player. The case studies? No less than four major titles of our times: Starcraft, Age of Empires, Diablo and Dungeon of the Endless.\n\nOn MetaSpace we have introduced a general overview of the challenges that video game designers face when creating buildings, cities and even maps. This time we will go one step further: what happens when a game doesn\u2019t offer a narrative or a fixed, open map, but rather an architectural system that the player can take control of? How does a design team respond to something like that?\n\nA classic example of this type of game is Starcraft: a strategy game in real time, where you have to manage a military base in order to build an army powerful enough to overcome your opponent. In games of this genre, you usually start the game with just a command center and some workers, who need to collect the necessary resources to expand your facilities, generate the appropriate units and tactically respond to various situations that the enemy poses. There is always an ideal\u00a0strategy when facing certain situations, the challenge is knowing how to quickly adapt your style of play.\n\nUltimately, it consists of\u00a0a very well integrated system of resources, buildings and units. Under the aesthetic surface is a network of attributes, damage points and movement speeds. Like a kind of complex, super-chess, the real core of the game is in its rules, which determine its gameplay and complexity. The longer you play, the better you understand the system and can anticipate your opponent's actions. The winner will be the one who knows the system the best and adapts to it.\n\nAll this is fine, but so many numbers and rules would be boring without context. Building a site for gathered materials doesn\u2019t sound that same as teleporting a colossal pyramid from an alien planet, right? Here lies the challenge of the designer. They must be able to translate the system rules into a set of easily recognizable designs, identifiable by their role within the game and adaptable to any situation that may arise. This applies to the design of characters and units, but also to buildings and cities, to the architecture.\n\nThere are the large main buildings that stand above the rest, and others that are medium-sized and small outposts. The design should suit the style of the game. In Starcraft, for example, the Terrans are humans of the future with super-tech buildings that can fly. Meanwhile, the Zergs are an alien race based on organic matter, including an infrastructure of living buildings. In both cases, the constructions of both races should be easily recognizable by the players, in any given configuration. That's the challenge.\n\nThis same problem applies to games in completely different settings, like the well-known Age of Empires. In this game, the logic is exactly the same: build a base and beat your opponent, but instead of controlling aliens from the future you\u2019re Joan of Arc, William Wallace or El Cid. You manage resources, create structures and train your soldiers in a manner analogous to Starcraft --\u00a0but the interaction between these elements is different, as is their contextualization.\n\nAge of Empires stands out with the careful design of its buildings, particularly that of its castles. The game has its own encyclopedia, where you can see the historical role that buildings had or the royal castles that the designs were based on. Its scope has been such that even now, years later, projects have been developed to improve its graphic models, like the Architecture Renovation Mod website, which makes various changes to the architecture to make it more dense, with more detail and realism, even though it\u2019s more inconvenient to play.\n\nOn the other hand, there are games that pose the opposite challenge: rather than\u00a0creating a system that the\u00a0player manages, they create a system with spaces that can be reorganized in each game, adjusting and changing for\u00a0different levels. For example, in the Diablo saga you have to move through a dungeon that\u2019s different every time you enter it, creating a permanent and highly addictive challenge.\n\nHowever, as in any system, these games have permanent rules governing their spaces. Although they may change, they need to maintain a basic consistency. For example, if the player is supposed to go through a city of narrow corridors, the \"architectural pieces\" of the system must comply with this rule. If you want the player lost in a vast desert with few clues for orientation, the rules need to be different.\n\nAnother good example of this approach is Dungeon of the Endless, a title which, as its name suggests, makes you navigate a dungeon that is seemingly endless. To accomplish that effect there is a predetermined system of rooms and corridors that\u00a0are arranged differently for each level. Some rooms have \"outlets\" where you can build machines that help you move, others offer no help or even have traps, monsters, or poisonous clouds. In this sense, the challenge is very interesting, because you never know what you\u2019ll find. It\u2019s completely up to chance. A game can be simple or present challenge after challenge, making you \"suffer,\" but giving\u00a0you a great feeling of accomplishment when you\u00a0\"beat the system.\"\n\nThis example has three elements that are particularly interesting for architects. One is its interface, which is like a kind of freeform plan where the walls appear broken up into sections. The second is its map, designed to manage the various rooms that we mentioned. The third and most important is the door, since entering\u00a0a new room can change the game completely. The title sequence utilizes\u00a0this element, showing heavy doors that slowly open with a creaking noise, causing an unbearable curiosity especially if you\u2019re doing poorly\u00a0in the game. Will you finally find that treasure and win? Or will an interdimensional beast appear, making you unleash a string of expletives?\n\nIf you take a step back, you can see the influence that role-playing maps --\u00a0used since the 70s --\u00a0had\u00a0in Dungeon of the Endless, where the narrator arranges the rooms differently in each iteration. The davesmapper\u00a0tool is a good example of this, generating thousands of random maps for role-playing games.\n\nDesigning a system of random spaces and contextualizing them for\u00a0coherence is not original to video games. From the battlefield of the chessboard, to Chutes\u00a0and Ladders or Monopoly, in our childhood (and well past our childhood) we had fun placing houses and hotels in strategic locations.\n\nIn fact, architecture as a system is a research topic starting in the Modern Movement, seeking to provide lots of solutions from a limited number of elements. Many of these projects were presented as a building game. We all have a vague idea of \u200b\u200bhow the master bedroom (the Dragon\u2019s den), or Dad's office (the throne room) or the basement (the bloody catacombs) should be. If we start from a systemic approach to architecture, the rules that we establish between those elements are what will give consistency to the game.\n\nVideo games teach us that, beyond getting\u00a0the game to work, in order for it to be fun and interesting, you also need to establish a character\u00a0that is innovative and very much its own. You need to let your imagination run wild."},
{"url": "http://www.kryogenix.org/days/2016/05/24/the-importance-of-urls/", "link_title": "The importance of URLs", "sentiment": 0.2074392557031445, "text": "Lots of discussion about progressive web apps recently, with a general consensus among forward-thinking web people that this is the way we should be building things for the web from now on. Websites that work offline, that deal well with lie-fi, that are responsive, that are progressive, that work everywhere but are better on devices that can cope with the glory. Alex Russell, who originally coined the term, talks about PWAs being responsive, connectivity independent, fresh, safe, discoverable, re-engageable, installable, linkable, and having app-like\u00a0interactions.\n\nWe could discuss every part of that description, every word in that definition, for hours and hours, and if someone wants to nominate a pub with decent beer then I\u2019m more than happy to have that discussion and a pint while doing it. But today, we\u2019re talking about the word linkable.\n\nFirst, a little background. Google Chrome attempts to detect whether the website you\u2019re looking at \u201cqualifies\u201d as a Progressive Web App, because if it does then they will show an \u201cinstall to home screen\u201d banner on your second visit. This is a major improvement over the previous state of a user having to manually install a site they like to their home screen by fishing through the menus or using the \u201cadd to home screen\u201d button in iOS Safari. The Chrome team have then created Lighthouse, a tool which invokes a Chrome browser, checks whether a site passes their checks for \u201cthis looks like a PWA\u201d, and returns a\u00a0result.\n\nSo Jeremy\u2019s point is this: Lighthouse is declaring that to be a valid PWA, you have to insist that when you\u2019re added to the home screen, you stop showing the URL bar. And he doesn\u2019t agree,\u00a0because\n\nThis is inspirational stuff, and it\u2019s true. URLs are important. Individual addressability of parts on the web is\u00a0important.\n\nHowever. (You knew there was a \u201chowever\u201d coming.) Whether your web app shows a URL bar is not actually a thing about that web\u00a0app.\n\nA bit more background. In order to qualify as a progressive web app, you have to provide a manifest. That manifest lists various properties about this web app which are useful to operating systems: what its human-readable name is, what a human-readable short name for it is, what its icon should be, a theme colour for it, and so on. This is all\u00a0good.\n\nBut the manifest also lists a , defined in the spec as \u201chow the web application is being presented within the context of an OS (e.g., in fullscreen, etc.)\u201d Essentially, the options for the display mode are (the app will take all the screen; hardware keys and the status bar will not be shown), (no browser UI is shown, but the hardware keys and status bar will be displayed), and (the app will be shown with normal browser UI, ie. as a normal\u00a0website).\n\nNow we see Jeremy\u2019s point. Chrome propose that you only qualify as a \u201creal\u201d PWA if you request \u201cfullscreen\u201d or \u201cstandalone\u201d mode: that is, that you hide the URL bar. Jeremy says that URLs are important; they\u2019re not a thing to hide away or to pretend that don\u2019t exist. And he has a point. The hackability of URLs is surprisingly important, and unsurprisingly dismissed by app developers who want to lock down the user\u00a0experience.\n\nBut, and this is the important point, whether a web app shows its URLs is not a property of that app. It\u2019s a property of how that app\u2019s developer thinks about the\u00a0web.\n\nIf Jeremy and I were both to work on a website, and then discuss what should be in the manifest, we\u2019d agree on what the app\u2019s name was, what a shortened name was, what the icon is. But we might disagree on whether the app should show a URL bar when launched. That disagreement isn\u2019t about the app itself; it\u2019s about whether you, the developer, think it\u2019s OK to hide that an app is actually on the web, or whether you should proudly declare that it\u2019s on the web. That doesn\u2019t differ from app to app; it differs from developer to developer. The app manifest declares properties of the app, but the property isn\u2019t about the app; it\u2019s about how the app\u2019s developer wants it to be shown. Do they want to proudly declare that this app is on the web and of the web? Then they\u2019ll add the URL bar. Do they want to conceal that this is actually a web app in order to look more like \u201cnative\u201d apps? Then they\u2019ll hide the URL bar. The property feels rather less like it\u2019s actually tied to the app, and rather more like it should be chosen at \u201cadd-to-home-screen\u201d time by the user; do you, the bookmarking user, prefer to think of this as a web thing? Include the URL bar. Do you want to think of it as an app which doesn\u2019t involve the web? Hide the URL bar. It\u2019s a preference. It\u2019s not a\u00a0property.\n\nThe above argument stands alone. But there are additional issues with having a URL bar showing on an added-to-home-screen web app. We should discuss these separately, but here I have them in the same essay because it\u2019s all\u00a0relevant.\n\nThe additional issue is, essentially, this. On my desktop \u2014 not my phone \u2014 I add an app to my \u201chome screen\u201d. This might add it to my desktop as a shortcut icon, or to my Start Menu, or in the Applications list, or all of the above, depending on which OS I\u2019m on. If that PWA declares itself as being then how to handle it is obvious: open it in a new window, with no URL bar showing. Similarly, web apps launched from an icon should be full screen. But what do we do when launching a display-mode web app on a\u00a0desktop?\n\nSince we\u2019re launching something indistinguishable from just another browser tab, it should launch a browser tab, right? I mean, we\u2019re opening something which is essentially a bookmark. But\u2026 wouldn\u2019t it feel strange to you to pick something from your app menu or an icon from your desktop and have it just open a browser tab? It would for me, at least. So maybe we should launch a new browser window, with URL bar intact, as though you\u2019d clicked \u201copen in new window\u201d on a link. But then I\u2019d have a whole new browser window for something which doesn\u2019t really deserve a whole new window; it\u2019s just one more web page, so why does it get a window by itself? I manage my browser windows according to project; window A has tabs relevant to project A, window B has tabs relevant to project B, and so on. I don\u2019t want a whole new window, and indeed I have extensions installed so that links which think they deserve a new window actually get a new tab\u00a0instead.\n\nIt\u2019s not very clear what should happen here. The whole idea of launching a website from an OS-level icon doesn\u2019t actually mesh very well at all with the idea of tabbed browser windows. It does mesh well with the 2002-era idea of a-new-browser-window-for-every-URL, but that idea has gone away. We have tabbed browsing, and people like it.\n\nThe Chrome team\u2019s idea, that basically you can\u2019t add an \u201cOS-level bookmark\u201d for a website which wants to be treated as a website, avoids these\u00a0problems.\n\nJeremy\u2019s got a point, though. Hiding away URLs, pretending that this thing you\u2019re looking at is a \u201cnative\u201d app, does indeed sacrifice one of the key strengths of the web \u2014 that everything\u2019s individually addressable. You can\u2019t bookmark the \u201caccount\u201d page in Steam, or the \u201csettings\u201d window in Keynote or Word or LibreOffice. With the web, you can. That\u2019s a good thing. We shouldn\u2019t give it up lightly. But you already can\u2019t do that for apps which use web technologies but pretend to be native. If Word or iTunes used a WebView to render its preferences dialog, would it be good if you could link directly to it with a URL like ? Yes it would. Would it be good if the iTunes user interface had a URL bar at the top showing that URL all the time? Not really,\u00a0no.\n\nThere is a paternalism discussion, here. URLs are a good thing about the web; the addressability of parts is a good thing about the web. People don\u2019t necessarily appreciate that. How much effort should we put into making this stuff available even though people don\u2019t want it, because they\u2019re wrong to not want it? Do we actually know better than they do? I think: yes we do. But I don\u2019t know how important that is, when we can also win people over to the web by pretending that it\u2019s native apps, which is what people wrongly\u00a0want.\n\nOn balance, therefore, I approve of the Lighthouse team\u2019s idea that you don\u2019t qualify as an add-to-home-screen-able app if you want a URL bar. I can see the argument against this, and I do agree that we\u2019re giving up something important, something fundamental to the web by hiding away URLs. But I think that wanting to see the URL is not a property of an app; it\u2019s a property of how you personally want to deal with apps. So browsers should, when adding things to the home screen, pretend that actually said , but give people who care the ability to override that if they want. And if we want more people to care, then that\u2019s what evangelism is for; having individual app developers decide how they want their app to be displayed just leads to fragmentation. Let\u2019s educate people on why URLs are important, and then they can flip a switch and see the URLs for everything they use\u2026 but until we\u2019ve convinced them, let\u2019s not force them to see the URLs when what they want is a native-like\u00a0experience."},
{"url": "http://www.nextplatform.com/2016/05/23/lustre-daos-machine-learning-intels-platform/", "link_title": "Lustre to DAOS: Machine Learning on Intel's Platform", "sentiment": 0.08525916561314792, "text": "Training a machine learning algorithm to accurately solve complex problems requires large amounts of data. The previous article discussed how scalable distributed parallel computing using a high-performance communications fabric like Intel Omni-Path Architecture (Intel OPA) is an essential part of what makes the training of deep learning on large complex datasets tractable in both the data center and within the cloud. Preparing large unstructured data sets for machine learning can be as intensive a task as the training process \u2013 especially for the file-system and storage subsystem(s). Starting (and restarting) big data training jobs using tens of thousands of clients also make severe demands on the file-system.\n\nThe Lustre* file-system, which is part of the Intel Scalable System Framework (Intel SSF), is the current de facto high-performance, parallel/distributed file-system. According to Brent Gorda (General Manager, Intel HPC Storage), \u201cLustre currently runs on 9 out of 10 of the world\u2019s largest supercomputers and over 70 of the top 100 systems\u201d. \u201cLustre owns the high ground\u201d Gorda said as he pointed out how machines like the Fujitsu K-machine can sustain a 3 TB/s (terabytes per second) read performance and 1.4 TB/s write performance [1]. This makes it attractive for commercial companies who are using Lustre for machine learning in a big way.\n\nLustre is an open-source project as is the forward thinking DAOS (Distributed Application Object Storage). Both projects position Intel to deliver high-performance data for an exascale supercomputing future.\n\nAs the Intel General Manager for the Intel HPC Storage, Gorda can definitively say that, \u201cIntel takes open-source very, very seriously\u201d. His long history with Lustre (Brent co-founded and led Whamcloud, a startup focused on the Lustre technology which was acquired by Intel in 2012) substantiates the success of the Intel Lustre effort when he says, \u201cThere is a lot of confidence in Lustre after the Intel acquisition\u201d, as exemplified by \u201ca convergence to the Lustre single source tree supported by Intel. This was recently amplified with the Seagate announcement that it will adopt Intel Enterprise Edition for Lustre\u00a0(IEEL) as its baseline Lustre distribution.\u201d\n\nIt is possible to train on large and complex data sets using an exascale capable mapping such as the one by Farber discussed in the first article in this series. The following graph shows the performance and near-linear scaling to 3,000 Intel Xeon Phi\u2122 coprocessors SE10P observed on the TACC Stampede supercomputer. Each of these Intel Xeon Phi coprocessors contains 8 GB of GDDR5 RAM. In other words, this hardware configuration can support training using nearly 24 terabytes of high-speed local Intel Xeon Phi processor memory.\n\nThe slight bend in the graph between 0 to 500 nodes has been attributed to the incorporation of additional layers of switches into the MPI application, meaning data packets had to make more hops to get to their destination. The denser switches provided by Intel OPA will reduce that effect.\n\nLustre plays a key role in the pre-processing and handling of big-data training and cross-validation sets as it provides scalable high-performance access to storage. Not surprisingly, the preprocessing of the training data, especially using unstructured data sets, can be as complex a computational problem as the training itself, which is why the performance, scalability, and adaptability of the data preprocessing workflow is an important part of machine learning.\n\nThere are a variety of popular workflow frameworks for data pre-processing. In my classes and via online tutorials [2], I teach students using a click-together framework that I created at Los Alamos National Laboratory that is illustrated in the schematic below. This framework incorporates \u201clessons learning\u201d while performing machine learning at the US national laboratories and in commercial companies since the 1980s.\n\nThis is but one example of a distributed data pre-processing framework that can run across a LAN, via the WAN, or within a cloud as there are many popular work flow frameworks. I teach the click-together framework due to its simplicity and efficiency. Workflows can utilize as many computational nodes as are made available and codes can run on both Intel Xeon and Intel Xeon Phi hardware as well as other devices. The freely available Google Protobufs [3] with its serialization format lets programmers work their favorite language of choice from C/C++ to Python and R to name a few [4]. As I point out to my students, there are a few performance disadvantages to using protobufs \u2013 namely extra copies \u2013 when using offload mode devices. Intel Xeon and the newest Intel Xeon Phi processors (codename Knights Landing) when booted in self-hosted mode will not have this issue. Aside from that, Google protobufs are an excellent, production-proven in the Google data centers serialization method for structured data that is quite fast.\n\nThe disk icons in the schematic show that data can originate from storage and eventually be written back to storage for later use in training and for archival purposes. This particular framework performs streaming reads and writes which can scale to the largest supercomputers and achieve high performance on a Lustre file-system. Archival resilience is also provided by both Lustre and this framework. Lustre HSM (Hierarchical Storage Management) can migrate data to and from petabyte archival products from a number of vendors. The click-together framework utilizes redundant information (to guard against bit-rot) and version numbers to ensure seamless use of data. For example, I still use data from the 1980s on modern machines with the current framework.\n\nSuccinctly, data preprocessing for machine learning (as well as other HPC problems) needs to scale well, which requires a high-performance, scalable distributed file-system such as Lustre. These file-systems also need to have seamless access to archival storage to minimize data management issues for data scientists.\n\nOnce the big data training set is prepared, the focus then becomes on the scalability and performance of the data load. Happily with Lustre, the data load can scale as needed to support the needs of today\u2019s leadership class supercomputers, and institutional compute clusters as well as future systems.\n\nThe schematic below shows that the data load can occur in an MPI (Message Passing Interface) environment simply by having each client open the training file, seek to the appropriate location and then sequentially read (e.g. stream) the data into local memory.\n\nThe scaling graph in Figure 2 shows that the filesystem will receive the open requests from 3,000 MPI clients. These open requests are referred to as meta-data operations. The Lustre meta-data architecture is designed to handle tens of thousands of concurrent metadata operations. Gorda notes that, \u201cLustre has grown to scale up to 80,000 metadata operations per server, which can scale-further by adding of metadata servers\u201d. In other words, a single metadata server can handle 80k metadata operations per second while a ten metadata server configuration can manage a far greater number of metadata operations per second. Further, high-demand portions of the filesystem tree can be isolated so they don\u2019t have a performance impact for other users of the filesystem, which is perfect for data intensive HPC workloads like machine learning.\n\nFor cloud-based machine learning, Lustre provides the storage frameworks for big data in the data center as well as the cloud. For example, both Microsoft Azure and AWS let users configure their cloud instances to use Lustre as the distributed filesystem. The challenge with running in a cloud environment is that HDFS, which is written in Java, appears to be a bottleneck. As can be seen in the graphic below, Lustre provides a Hadoop adapter to provide high-performance storage access.\n\nLustre is part of the forward thinking DAOS (Distributed Application Object Storage) project. DAOS (Distributed Application Object Storage) is a forward-thinking open-source next step in HPC file-systems that utilizes objects rather than files. Lustre is a component in the DAOS effort. Both projects position Intel to deliver high-performance data for an exascale supercomputing future.\n\nThrough the use of innovative technologies such as 3D XPoint\u2122, Intel OPA and DAOS that will keep hot data local to the processors, Gorda believes it will be possible to get much bigger speedups for short I/O\u2019s (vs. large streaming checkpoint files).\n\nThis is the third in a multi-part series on machine learning that examines the impact of Intel SSF technology on this valuable HPC field. Intel SSF is designed to help the HPC community utilize the right combinations of technology for machine learning and other HPC applications.\n\nSuccinctly, exascale-capable machine learning and other data-intensive HPC workloads cannot scale unless the storage filesystem can scale to meet the increased demands for data. This makes Lustre \u2013 the de facto high-performance filesystem \u2013 a core component in any machine learning framework and DAOS a storage project to watch.\n\nRob Farber is a global technology consultant and author with an extensive background in HPC and in developing machine learning technology that he applies at national labs and commercial organizations. He can be reached at info@techenablement.com."},
{"url": "https://pixls.us/blog/2016/05/new-rapid-photo-downloader/", "link_title": "New Rapid Photo Downloader", "sentiment": 0.20527161011031977, "text": "Community member Damon Lynch happens to make an awesome program called Rapid Photo Downloader in his \u201cspare\u201d time. In fact you may have heard mention of it as part of Riley Brandt\u2019s \u201cThe Open Source Photography Course\u201d*. It is a program that specializes in downloading photo and video from media in as efficient a manner as possible while extending the process with extra functionality.\n\n* Riley donates a portion of the proceeds from his course to various projects, and Rapid Photo Downloader is one of them!\n\nThe main features of Rapid Photo Downloader are listed on the website:\n\nDamon announced his 0.9.0a1 release on the forums, and Riley Brandt even recorded a short overview of the new features:\n\n(Shortly after announcing the 0.9.0a1 release, he followed it up with a 0.9.0a2 release with some bug fixes).\n\nSome of the neat new features include being able to preview the download subfolder and storage space of devices before you download:\n\nAlso being able to download from multiple devices in parallel, including from all cameras supported by gphoto2:\n\nThere is much, much more in this release. Damon goes into much further detail on his post in the forum, copied here:\n\nHow about its Timeline, which groups photos and videos based on how much time elapsed between consecutive shots. Use it to identify photos and videos taken at different periods in a single day or over consecutive days.\n\nYou can adjust the time elapsed between consecutive shots that is used to build the Timeline to match your shooting sessions.\n\nHow about a modern look?\n\nFor those who\u2019ve used the older version, I\u2019m copying and pasting from the ChangeLog, which covers most but not all changes:\n\nOf course, Damon doesn\u2019t sit still. He quickly followed up the 0.9.0a1 announcement by announcing 0.9.0a2 which included a few bug fixes from the previous release:\n\nIf you\u2019ve been considering optimizing your workflow for photo import and initial sorting now is as good a time as any - particularly with all of the great new features that have been packed into this release! Head on over to the Rapid Photo Downloader website to have a look and see the instructions for getting a copy:\n\nRemember, this is Alpha software still (though most of the functionality is all in place). If you do run into any problems, please drop in and let Damon know in the forums!"},
{"url": "https://mortoray.com/2016/05/24/is-reference-counting-slower-than-gc/", "link_title": "Is reference counting slower than GC?", "sentiment": 0.11077006327006327, "text": "\u201cReference counting is slower than garbage collection\u201d, a claim often made in the discussion of memory management. I heard it again recently when discussing Leaf; somebody took issue with my arguments against garbage collection. Making a comparison though is extremely difficult, but I will attempt to look at some of the contributing costs here.\n\nThere is a definite overhead to using reference counting for memory, but in practice it\u2019s generally efficient. The overhead is incurred throughout the code during pointer assignment. It comes primarily in the form of memory synchronization between CPU cores, and a little bit in added code size. Though most pointer assignments don\u2019t need this overhead, it still happens enough be noticed in the overall performance.\n\nIn a scanning GC there is no real overhead during pointer assignment. The question of whether an object is free is deferred to a later time. The overhead in the GC comes from having to scan memory at frequent intervals searching for things to release.\n\nThis means that the functional code paths will definitely be faster than reference counting so long as the GC thread doesn\u2019t interrupt it. The moment the GC thread starts working however it will impact all code. A lot of synchronized memory access is required by the GC, causing some caching issues on the CPU (it requires memory flushing much like reference counting\u2019s increment and decrement do).\n\nMany GC\u2019s result in a stop-the-world effect, suspending all processing during some collection cycles. In some domains this type of pause is unacceptable to the application, but in many domains it really isn\u2019t a problem if it\u2019s kept short enough. A more iterative GC may reduce the stopping time, but the CPU cache synchronization will impact the code paths on a continual basis, much like in reference counting.\n\nGC\u2019s overhead relates to the total memory allocated, not just the memory actively used in the code. Each new allocation results in an increased time per scanning cycle. Certainly there are many optimizations involved in a good GC to limit this, but the fundamental relationship is still linear. As a program uses more memory the overhead of it\u2019s GC increases.\n\nReference counting only involves objects involved in an assignment; it\u2019s cost is not related to the total memory used by an application.\n\nReference counting with cycle detection adds an interesting twist. I believe both D and Python do this. In order to eliminate memory loops objects are \u201csometimes\u201d scanned for reference loops. This involves a lot more work than simply touching a reference count \u2014 though still not nearly as much as scanning all allocated memory. The impact of this loop detection depends heavily on how often it is scanned, and what the triggers are.\n\nGC tends to result in more memory being used in total. The trigger for scanning is often low memory conditions, meaning that a lot of memory needs to be allocated prior to a scanning operation. This has significant OS level implications: memory used by the app must be swapped in/out, and also prevents other apps, and file caches, from using the memory.\n\nIt\u2019s my feeling, from experience, that it\u2019s this aspect of GC\u2019d applications that cause the biggest performance impact. The overall system just goes slower due to the large amount of memory involved. It doesn\u2019t seem like it needs to be a fundamental problem of a GC though. The structure of a standard library, and typical programming paradigms, contributes a lot to this effect.\n\nOne problem in comparison is that programmers often design around the limitations of the system. Despite C++ having a reference counted , the vast majority of pointer operations aren\u2019t using that type. Direct pointers, stack objects and memory pools are common ways to reduce allocation overhead. Even manual memory management can used to improve performance well beyond what a GC can achieve.\n\nBut that requires a lot of direct programmer optimization, and those techniques can\u2019t all be easily used in language like Python. Certain techniques, like object pools, can also be used in GC languages, though the setup may be more work.\n\nThe level of complexity in memory management makes comparison of reference counting to a scanning garbage collector quite difficult. We must consider the entire system, not just an individual program. We also can\u2019t just switch from one strategy to another, even if it were an option, since programs tend to be written around their memory manager.\n\nI don\u2019t see how a claim that \u201cgarbage collection is faster\u201d can be substantiated. One would need to construct a test that considers all of the above points. My general feeling is that GC is probably slower, strictly based on the increased total memory use of the applications. Perhaps that\u2019s an issue of bulky and poorly constructed standard libraries more than an issue of GC itself though."},
{"url": "http://techcrunch.com/2016/05/24/the-new-macbook-pro-could-feature-a-touch-id-sensor-and-an-oled-mini-screen/?sr_share=facebook", "link_title": "Apple Macbook Pro is finally getting an update", "sentiment": 0.16877405002405008, "text": "Rumor has it that Apple is about to update the MacBook Pro with a retina display in the coming months. And according to well-informed analyst Ming-Chi Kuo from\u00a0KGI Securities and 9to5mac, it should be much more than a specification bump.\n\nThe company wants to keep two screen sizes with a 13-inch model and a somewhat more powerful 15-inch model. But these laptops should come in a thinner and lighter design, bringing it closer to the MacBook Air.\n\nBut if you have a MacBook Pro, you know that the laptop\u2019s thickness is constrained by the ports. The retina MacBook Pro is roughly as thick as a USB port, or a Thunderbolt port, or an HDMI port \u2014 these three types of ports are more or less the same thickness.\n\nSo how is Apple going to solve that? The answer is USB-C. Apple is already using this thinner USB standard in the 12-inch MacBook with a retina display. And it looks like the company doesn\u2019t want to stop there. USB ports are probably going to become USB-C ports. But you can also make Thunderbolt-compatible USB-C ports, so Apple could also switch to USB-C design for Thunderbolt ports. Similarly, Apple could take advantage of a USB-C port to drop the MagSafe connector. Finally, Apple could drop the HDMI port or replace it with a Mini-HDMI port.\n\nAccording to these reports, Apple is also thinking about adding a Touch ID sensor right on your Mac. It\u2019s nice that you won\u2019t have to type your password again. But it could also open other possibilities, such as Apple Pay on the Mac for online purchases.\n\nThe last addition is a bit more surprising. Apple could be adding a tiny OLED touchscreen above your keyboard thanks a new, smaller metal-injection molded\u00a0hinge. While Apple has never made laptops with touchscreens, this narrow touch screen would feature shortcuts and could change depending on the app you\u2019re using. This could be the most surprising element of the new MacBook Pro.\n\nThe new MacBook Pro will feature the new Intel Skylake processors. It could also feature a redesigned keyboard that would look like the Magic Keyboard.\n\nUnfortunately, today\u2019s reports say that you shouldn\u2019t expect to buy a new MacBook Pro before Q4 of 2016. Many anticipated a big announcement at Apple\u2019s WWDC, but it looks like it won\u2019t be ready just yet."},
{"url": "https://news.stanford.edu/2015/08/17/wireless-optogenetic-mouse-081715/", "link_title": "Stanford engineers develop a wireless implant that stimulates nerves in mice", "sentiment": 0.08228693181818184, "text": "A miniature device that combines optogenetics \u2013 using light to control the activity of the brain \u2013 with a newly developed technique for wirelessly powering implanted devices is the first fully internal method of delivering optogenetics.\n\nThe device dramatically expands the scope of research that can be carried out through optogenetics to include experiments involving mice in enclosed spaces or interacting freely with other animals. The work is published in the Aug. 17 edition of Nature Methods.\n\n\u201cThis is a new way of delivering wireless power for optogenetics,\u201d said Ada Poon, an assistant professor of electrical engineering at Stanford. \u201cIt\u2019s much smaller and the mouse can move around during an experiment.\u201d (See video.)\n\nThe device can be assembled and reconfigured for different uses in a lab, and the design of the power source is publicly available. \u201cI think other labs will be able to adapt this for their work,\u201d Poon said.\n\nTraditionally, optogenetics has required a fiber optic cable attached to a mouse\u2019s head to deliver light and control nerves. With this somewhat restrictive headgear, mice can move in an open cage but can\u2019t navigate an enclosed space or burrow into a pile of sleeping cage-mates the way an unencumbered mouse could. Also, before an experiment a scientist has to handle the mouse to attach the cable, stressing the mouse and possibly altering the outcome of the experiment.\n\nThese restrictions limit what can be learned through optogenetics. People have successfully investigated a range of scientific questions including how to relieve tremors in Parkinson\u2019s disease, the function of neurons that convey pain and possible treatments for stroke. However, addressing issues with a social component like depression or anxiety or that involve mazes and other types of complex movement is more challenging when the mouse is tethered.\n\nPoon had made a name for herself creating miniature, implantable, wirelessly powered devices. Although that capability was badly needed in the optogenetics world, Poon didn\u2019t know it until she attended a neural engineering workshop that brought together faculty from neurosciences and engineering.\n\nAt that event Poon met Logan Grosenick, who was a graduate student from the lab of Karl Deisseroth, a Stanford professor of bioengineering and of psychiatry and behavioral sciences who invented optogenetics. But Grosenick didn\u2019t have time to lead a collaboration.\n\nThrough follow-up conversations, Poon eventually met graduate student Kate Montgomery, who was working in the lab of Scott Delp, professor of bioengineering and of mechanical engineering, and collaborating with Deisseroth. \u201cIt was clear that this could provide a powerful tool for neuroscience.\u00a0 We just needed to prove it would work,\u201d Delp said.\n\n\u201cSince then our labs have established a lasting scientific relationship,\u201d said Montgomery, who has an interdisciplinary fellowship through Stanford Bio-X. \u00a0She and graduate student Alexander Yeh, who worked in Poon\u2019s lab, were co-first authors on the research paper.\n\nBefore a new wave of tinfoil hat designs takes over the Internet, it is important to clarify one point: Optogenetics only works on nerves that have been carefully prepared to contain the proteins that respond to light. In the lab, scientists either breed mice to contain those proteins in select groups of nerves or they carefully and painstakingly inject viruses carrying the protein DNA into nerves the size of dental floss. Shining a light \u2013 whether through a fiber optic cable or a wireless device \u2013 on neurons that haven\u2019t been prepared has no effect.\n\nPoon said that developing the tiny device to deliver light was the easy part. She and her colleagues developed that and had it working a few months after the workshop. What was hard was figuring out how to power it over a large area without compromising power efficiency.\n\nIn behavioral experiments, the mouse would be moving all around, and the researchers needed a way of tracking that movement to provide localized power. Poon knew other labs were tackling the same problem using bulky devices that affix to the skull and complex arrays of coils paired with sensors to locate the mouse and deliver localized power.\n\n\u201cWe were lazy,\u201d Poon said. \u201cThat sounded like a lot of work.\u201d\n\nSo instead she got what she called a crazy idea to use the mouse\u2019s own body to transfer radio frequency energy that was just the right wavelength to resonate in a mouse. Crazy maybe, but it worked, and she published the results Aug. 4 in Physical Review Applied with co-first authors John Ho, a graduate student who is now an assistant professor at the National University of Singapore, and Yuji Tanabe, a research associate in her lab.\n\nPoon had the idea but initially didn\u2019t know how to build a chamber to amplify and store radio frequency energy. She and Tanabe consulted with Tanabe\u2019s father, who had worked at Stanford\u2019s SLAC research center and knew a thing or two about machining such a cavity, and then traveled to Japan to do the initial assembly and testing.\n\nTanabe\u2019s dad referred to their final chamber as a \u201ckindergarten project,\u201d but it worked. However, in its native state the open chamber would radiate energy in all directions. Instead, a grid was overlaid on top of the chamber with holes that were smaller than the wavelength of the energy contained within. That essentially trapped the energy inside the chamber.\n\nThe key is that there\u2019s a bit of wiggle room at the grid. So if something like, say, a mouse paw were present, it would come in contact with the boundary of all that stored energy. And remember how the wavelength is the exact wavelength that resonates in mice? The mouse essentially becomes a conduit, releasing the energy from the chamber into its body, where it is captured by a 2 mm coil in the device.\n\nWherever the mouse moves, its body comes in contact with the energy, drawing it in and powering the device. Elsewhere, the energy stays tidily contained. In this way, the mouse becomes its own localizing device for power delivery.\n\nThis novel way of delivering power is what allowed the team to create such a small device. And in this case, size is critical. The device is the first attempt at wireless optogenetics that is small enough to be implanted under the skin and may even be able to trigger a signal in muscles or some organs, which were previously not accessible to optogenetics.\n\nThe team says the device and the novel powering mechanism open the door to a range of new experiments to better understand and treat mental health disorders, movement disorders and diseases of the internal organs. They have a Stanford Bio-X grant to explore and possibly develop new treatments for chronic pain."},
{"url": "http://torquemag.io/2016/05/woomattics-future-true-ecommerce-democratization/", "link_title": "Woomattic\u2019s Future: True Ecommerce Democratization", "sentiment": 0.2245342455163884, "text": "Since day one, WordPress co-founder and CEO of Automattic Matt Mullenweg\u2019s mission for WordPress has been the\u00a0democratization of publishing.\n\nAnd when\u00a0Automattic last May acquired WooThemes, makers of WooCommerce, the popular ecommerce plugin, Mullenweg shared an idea to create a solution where it\u2019s as easy publish stores online as WordPress has made it to publish websites.\n\nDemocratizing\u00a0publishing is ambitious, and it\u2019s something that WordPress achieves with grace. Users can easily spin up a free site on Automattic\u2019s WordPress.com or for a low cost by signing up with a WordPress host. With thousands of free plugins to choose from, users can elegantly\u00a0add almost any functionality to their site, making it possible for anyone to be a publisher \u2014 true democratization.\n\nThe open-source software is maintained and improved by a growing and thriving community of developers, designers, bloggers, and others, many of whom have built successful WordPress-powered companies. All of these factors fueled\u00a0WordPress\u2019 rise as the platform of choice for small businesses and enterprises alike and\u00a0have been driving forces in its growth.\n\nWooThemes, a successful company with WordPress at the center of its business model, in many ways mirrors the success and growth of WordPress itself.\n\nJust as\u00a0WordPress is the most popular CMS, WooCommerce is the single most popular ecommerce solution. Similarly, it too has a robust and growing ecosystem surrounding it, with a growing number of plugins built specifically to enhance and extend its functionality.\n\nWhere other ecommerce solutions may not offer certain functionality, with WordPress you can find an existing plugin that offers the\u00a0feature you want or ask someone to write it for you.\u00a0With\u00a0WordPress and WooCommerce, anything is possible.\n\nLike WordPress and publishing, WooCommerce is empowering anyone to spin up an ecommerce store, and democratizing the world of online sales.\n\nWooCommerce Connect could be Automattic\u2019s inroad to making WooCommerce as easy to use as services like Shopify or BigCommerce.\n\nThe appeal of WooCommerce Connect is that you don\u2019t have to do the \u201cregister for yet another account -> get API key -> drop API key into settings -> test -> verify\u201d mambo. All of that functionality will be available\u00a0on Automattic\u2019s servers and will simply need to be \u201cactivated.\u201d While real-time USPS shipping rates is currently the only feature available on Connect, I suspect other core functionality like taxes and payment gateways will be added in the near future.\n\nThe new Storefront 2.0 theme features several big improvements and feature changes that make sites look more like a store and less like a blog. Combined with the fact that WordPress is already the platform of choice for small businesses, it\u2019s likely to outcompete solutions like Squarespace and Wix.\n\nAll of the improvements to WooCommerce could be beneficial for the entire community\u00a0as it could help WordPress move away from its antiquated reputation as a \u201cblogging platform\u201d and toward a platform where digital opportunities are endless, whether that\u2019s creating a personal blog or an enterprise ecommerce site.\n\nThe acquisition of WooThemes was a wise\u00a0investment for Automattic. The two companies align perfectly in terms of ethos and mission, and\u00a0it also allows Mullenweg to explore the ecommerce\u00a0ecosystem,\u00a0an area in which he expressed interest during a live interview at TechCrunch Disrupt NY in 2014:\n\nWith WooCommerce, Mullenweg aims to do for online stores what WordPress has done for publishing. I\u00a0look forward to seeing more great things to come from WooMattic as it further democratizes ecommerce."},
{"url": "http://www.echeng.com/journal/2016/5/23/tsa-gave-my-macbook-pro-to-another-passenger-at-lax-and-now-its-gone", "link_title": "TSA gave my MacBook Pro to another passenger at LAX, and now it's gone", "sentiment": 0.022192966247844285, "text": "On Friday, I lost my $2,800 Apple MacBook Pro by following standard TSA security protocols at Los Angeles International Airport (LAX). I look back on the series of events that led to the lost computer with incredulity, and although all of the TSA staff and LAX airport police were courteous, I am still without my computer and am unsure whether or not I will be reimbursed for my loss.\n\nAt 4:00pm on Friday, May 20, I was at the TSA security station at Delta Terminal 5, Lane 3. Per the standard (non-Pre) TSA process, I removed my 15.4\u201d Apple MacBook Pro from my backpack and placed it in a bin. I removed my shoes and incidentals and placed them in a second bin. Behind me, I could hear a woman making a big scene because her \u201cflight [was] at 4:15,\u201d and her gigantic bag was clearly never going to fit on the airplane. She claimed multiple times that \u201cthe lady\u201d told her she could bring it through security. At least 3 TSA agents were dealing with her as we moved closer to the x-ray machine, and most of the people in the area were watching the altercation.\n\nI arrived at the entrance to the x-ray machine, pushed my 2 bags and 2 bins through, and stepped into the body scanner. When I stepped in, I realized that I had left my belt on, and went back to put it through the x-ray machine. I almost always fly using TSA Precheck so I\u2019m accustomed to not removing my belt (and computer), but this particular airplane ticket was booked by a conference, so I was in the normal line. The body scanner line wasn\u2019t long, so this put me 2-3 people between me and my original position. I emerged from the scanner without any problems, collected my 2 bags and incidentals, and proceeded into Terminal 5. After awhile, I looked into my backpack and realized that I didn\u2019t have my computer. My heart skipped a beat! I turned around and rushed to the security area. During the walk back, I was thinking that I didn\u2019t even remember seeing my computer on the belt when I collected my things, but I always assume that human memory is terrible, so I couldn\u2019t be positive.\n\nI arrived at the security area, flagged down a TSA agent, and told her that I must have left my computer there, but that I didn\u2019t remember seeing it on the belt. She left to look around; I wanted for about 5 minutes and never saw her again. I flagged down a second agent, who started looking again. Someone brought over a computer in a bag, but it was a Windows machine in a black case\u2014not mine. Mine was nowhere to be found.\n\nWe moved over to the camera footage station, and a nice agent began to review archived camera footage. After a few minutes, he found me coming through the security line, and sure enough, my computer was not with my bags when I retrieved my belongings. Moving further back in time, we watched as a TSA agent pulled my computer off of the belt as soon as it came out of the machine\u2014there is an area where agents can remove things from the belt before passengers have access to belongings. He moved my computer to a holding area immediately behind the x-ray machine. And then, we watched as the computer was inspected, after which it was handed back\u2026 to a random woman. The woman took my computer and left the security area. Someone remembered that the woman had been with the other woman who had been making the scene, and that they had both been rushing to the 4:15pm flight, but I couldn\u2019t remember whether this was the case or not."},
{"url": "https://blog.bitbucket.org/2016/05/24/introducing-bitbucket-pipelines-beta-continuous-delivery-built-within-bitbucket/?utm_source=twitter&utm_medium=social&utm_campaign=bitbucket_bitbucket-pipelines-bitbucket", "link_title": "Bitbucket Pipelines Beta: continuous delivery inside Bitbucket", "sentiment": 0.256047619047619, "text": "Host your code online in as many public and private repositories as you want. Free for 5 users.\n\nSoftware has changed the world faster than almost any other industrial innovation and it\u2019s only picking up speed. Companies are moving from infrequent, large code deployments to frequent, small and agile deployments. This trend is having a huge impact on the current software development processes. For example, in one of our most recent customer surveys, more than 65% of software teams noted that they are practicing some form of continuous delivery. It\u2019s becoming the norm for software teams.\n\nBut implementing continuous delivery is not easy. Setting up build agents is complicated. Developers have to constantly juggle between different tools. And most of the time, the build is sitting in a queue, or you\u2019re burying yourself in log files digging for information about failures.\n\nBitbucket Pipelines\n\n That is until now. Bitbucket Cloud is introducing Pipelines to let your team build, test, and deploy from Bitbucket. It is built right within Bitbucket, giving you end-to-end visibility from coding to deployment. With Bitbucket Pipelines there\u2019s no CI server to setup, user management to configure, or repositories to synchronize. Just enable it in one click and you\u2019re ready to go.\n\nPipelines is also a great fit for branching workflows like git-flow. Anyone on your team can adapt the build configuration to map the structure of your branches. Here is a quick look at some of the salient features of Bitbucket Pipelines:\n\nBring your own services to Bitbucket Pipelines\n\n We worked very closely with some of the leaders in the industry so you can bring your own services to Bitbucket Pipelines, right out of the box. Whether you want to deploy, test, monitor code quality, or store artifacts, complete any workflow with the tool of your choice: Amazon Web Services, Ansible, bitHound, BrowserStack, buddybuild, Code Climate, JFrog, Microsoft Azure, npm, SauceLabs, Sentry, Sonatype, and TestFairy.\n\n\u201cPipelines provided us with the perfect opportunity to bring the power of automated code quality analysis to Bitbucket users. We\u2019re excited about the awesome potential of Pipelines and they\u2019re only just getting started!\u201d \u2013Michael Bernstein, VP of Community, Code Climate.\n\nCheck out our integrations page for more details.\n\nContinuous Delivery options from Atlassian\n\n We believe that the best way to provide our customers with a top-notch cloud CD solution is to build the service natively within Bitbucket Cloud. That\u2019s why we built Bitbucket Pipelines and also why today, we\u2019re announcing the end-of-life for Bamboo Cloud, which will be discontinued starting Jan 31, 2017. While Bamboo Cloud has helped many customers to adopt CD, we realized that we would not be able to deliver the experience and the quality of service that our customers need. If you\u2019re a Bamboo Cloud customer, click here to learn more about the migration options.\n\nIf you want to build and ship behind the firewall, we\u2019re still heavily investing in Bamboo Server as an on-prem CD solution.\n\nGet early access to Bitbucket Pipelines\n\n With Bitbucket Pipelines we want to empower every team to accelerate their releases. No more time wasted on setup and maintenance, just focus on the work you love. You can sign up for the Bitbucket Pipelines beta program today and request early access.\n\nSign up for the beta program"},
{"url": "https://developer.atlassian.com/blog/2016/05/open-api-initiative/", "link_title": "Atlassian joins Open API Initiative, open sources RADAR doc generator", "sentiment": 0.18579204129204133, "text": "Atlassian was founded by developers building tools to cater to other developers with whom we've always had a close relationship. Very early on we opened our products up with a plugin system allowing others to build upon and extend them, which over time has grown into a large, thriving developer ecosystem that we're very proud of.\n\nOpening a product to external developers requires well-designed, stable APIs that are thoroughly documented. The first plugins were JAR files directly injected into our Java products. To help developers debug, we opened up the source code and to this day all product licenses come with the full source code.\n\nExternal integrations have been supported through XML-RPC, SOAP and eventually REST.\n\nAs cloud services gained momentum we built Atlassian Connect, an entirely web-based add-on system for cloud services, open to external developers.\n\nAPIs, and in particular REST, form the backbone of Atlassian's developer ecosystem which in turn adds substantial value to our products.\n\nREST gained popularity in part because of its simplicity, debuggability and ease of use, but its lack of a commonly accepted interface description standard has sometimes lead to inconsistent APIs and hampered code generation efforts for rapid client development across languages.\n\nOver time, efforts like the Open API Initiative have emerged to address these limitations by defining a schema standard by which to describe REST APIs.\n\nAs a company we're fostering an ecosystem whose success is related to the quality and usability of its REST APIs. This means we and our external developers have a lot to gain from an open, successful and widely-accepted definition language for REST APIs. We've committed ourselves to actively contributing to the standard by becoming an OAI and Linux Foundation member organization, alongside industry leaders like Google, Microsoft, PayPal and others.\n\nWe're really excited to have Atlassian on board. For a technology standard to be successful and see broad adoption, it needs to address the industry's issues and involving its key players I think is a condition for achieving this. Atlassian's focus on interoperability and long history of successfully opening up its products through APIs is a huge boon for us and will contribute to an even stronger specification.\n\nTony Tam, founder of Swagger, the foundation for the Open API Specification\n\nOne of the things the Open API Specification facilitates is the ability to have API documentation automatically generated. As we add Open API support to our products, we'll use it to replace our existing, hand-crafted API documentation.\n\nFor this we've built a custom site generator, RADAR, for Open API specs to host our API documentation. Built on React, it offers searching, browsing and viewing REST documentation for any product and any version of that product. RADAR is a straight implementation of the current version of the Open API specification, not tied to any of our own products. Because of this, RADAR can be used by any Open API provider.\n\nHosted by the Linux Foundation, the Open API members do great work both promoting and actively developing vendor-neutral open source tooling around the specification. With RADAR we're following this model and so we are making it available to the community under the Apache 2 license.\n\nLike so many others in our industry, Atlassian relies on a tremendous amount of open source software. In return, it has traditionally made many of its products freely available to our community and develops core parts of its products like its plugin infrastructure as open source. We're thrilled when the industry acknowledges the important role of open source in ways like this, and takes responsibility by contributing back. As such, we're very happy to work with Atlassian as a new member organization of The Linux Foundation.\n\nWe are committed to continuing its development and are keen to welcome external contributors. The project is hosted at: https://bitbucket.org/atlassian/radar"},
{"url": "http://www.vocativ.com/320111/slack-backlash/", "link_title": "The Slack Backlash Has Arrived, but It\u2019s Already Too Late", "sentiment": 0.1424714339158784, "text": "Everyone loves Slack: It offers chat-style\u00a0communication with coworkers and eliminates the need for a lot of the emails and face-to-face meetings. Yet, everybody hates Slack: It\u2019s a huge distraction that splinters attention, fosters cliquish private communication, and drives\u00a0the pervasive sense that work never ends. A\u00a0growing wave of complaints about the app may test whether the company can wield its power for productivity, without all the noise.\n\n\u201cSlack promises its users will \u2018be less busy,'\u201d Ben Shields, lecturer of managerial communication at the MIT Sloan School of Management who focuses on social media,\u00a0told Vocativ. \u201cIf the reverse is true for some organizations, then Slack should take the charges seriously, as they strike at the core of what the product offers.\u201d\n\nThat product has been drawing converts steadily since its launch in 2013, when it attracted\u00a0some 8,000 users. Today, Slack boasts over two million daily users and the tool is\u00a0a staple of communication for many professional office workers. Slack\u2019s paid users believe the app\u00a0increases productivity by 32 percent, reduces internal email use by almost 50 percent, and knocks inter-office meetings down by 25 percent. Because the app\u2019s archives are searchable and permanent if desired for paid accounts, users say the tool\u00a0increases office transparency by 80 percent. And 79 percent of users said it improved their office culture.\n\nBy some estimates, the average user spends 140\u00a0minutes per weekday in the app. Ask anyone who uses Slack in an office setting and they\u2019ll tell you how it can greatly enhance camaraderie, whether through sharing funny GIFs or just chatting through the more mundane parts of the workday. Like \u201cwhen you send someone a lolcat dick pic and listen to hear them laugh from cubicles away,\u201d a reporter in Nashville tells Vocativ. \u201cYou feed off the energy of the crowd.\u201d\n\nBut despite the conveniences, the Slack backlash\u2014let\u2019s call it a slacklash\u2014has been coming in waves over the last year. In an essay posted in February\u00a0called \u201cSlack, I\u2019m Breaking Up With You\u201d writer Samuel Hulick explains how\u00a0the bloom fell\u00a0off the rose. The app feeds the pressure to always be on. It\u2019s always there, humming away in the background.\u00a0\u201cI\u2019m finding that \u2018always on\u2019 tendency to be a self-perpetuating feedback loop: the more everyone\u2019s hanging out, the more conversations take place,\u201d Hulick notes. \u201cThe more conversations, the more everyone\u2019s expected to participate. Lather, rinse, repeat.\u201d\n\nVice\u2019s tech vertical Motherboard\u00a0announced\u00a0last\u00a0week that it\u00a0would go cold turkey from the app for a week because, in spite of how marvelous it is to be connected to colleagues in Toronto, London, and Oregon, that it\u2019s so distracting that often spirited debates in channels over certain potentially publishable topics actually prevent writers from writing\u00a0them. Motherboard will opt instead for Google Hangouts and G-chat, because at least it\u2019s more deliberate and one-on-one.\n\nA handful of Slack users across the country who spoke to Vocativ were also quick to point out potential pitfalls.\n\nAn alt-weekly\u00a0managing editor says\u00a0it\u2019s just too easy to use and abuse. \u201cThe main drawback (and this could of course be our particular dynamic) is that it\u2019s a bit too easy to dick around on,\u201d he says. \u201cWe have far too many channels, and people sending one another goofy GIFs and that sort of thing.\u201d\n\nThe\u00a0reporter in Nashville says that\u00a0\u201cit\u2019s another level of the grid coworkers expect you to always be available on. Between email, text, Slack, it\u2019s adding to the culture of people always expecting you to be accessible. And I feel like my Slack is just a lot of minutia sometimes.\u201d\u00a0Also, it creates an illusion of access that for certain higher-up workers might be more trouble than it\u2019s worth. \u201cPeople lower on the org chart can harass me with story ideas, and I don\u2019t like that one bit,\u201d he says. \u201cIt\u2019s hard enough swatting away all the annoying pitches from interns in real life.\u201d\n\nAnd as for replacing meetings, Slack leaves much to be desired. \u201cIt distributes low-level information very easily and well,\u201d says an executive creative director at an ad agency. \u201cBut it does not substitute for in-person meetings.\u201d\n\nThe pressure to to field every direct message, maintain the connection with coworkers, and\u00a0keep up your GIF\u00a0game can interrupt the whole point of what you\u2019re supposed to be doing at work: working. In a primer on best Slack practices from die-hard devotees last year, the cofounders of office cleaning service Managed by Q told Fast Company that they require new employees to Slack a giphy on their first day into the general channel.\n\nOther essays have argued that Slack\u2019s always-on status destroys work-life balance. \u201cI love [Slack] so much that I desperately want to delete it from my phone, because I can\u2019t help but check its messages every hour of every day,\u201d Chris Plante wrote at The Verge earlier this year. \u201cYesterday night, I responded to a message at 3AM. Lol, I have no self-control and this is a cry for help!\u201d\n\nAnd rather than ease anxieties about connectivity, Slack\u00a0can create the sense that you\u2019re not clued in. In an essay looking at the pros and cons of group work chat by Jason Fried (who created Slack\u2019s spiritual ancestor, Campfire), he notes that it can basically create an \u201cASAP culture\u201d and an endless workplace version of FOMO. He writes:\n\nGroup chat feels like you\u2019re chasing something all day long. What\u2019s worse, group chat often causes \u201creturn anxiety\u201d\u200a\u2014\u200aa feeling of dread when you\u2019re away for a while and you come back to dozens (hundreds?) of unread lines. Are you supposed to read each one? If you don\u2019t, you might miss something important. So you read up or skip out at your own risk.\n\nPlus, that little green dot is a way of measuring presence at work that says, \u201cif you aren\u2019t green, you aren\u2019t at work.\u201d\n\nDespite these various waves of backlash, Slack\u00a0doesn\u2019t seem to be going anywhere, and is still pushing the\u00a0workplace expectation\u00a0that\u00a0collaboration is king.\u00a0Deleting it seems like an unlikely option, so has Slack already become too essential for companies?\n\nThe app\u00a0is experiencing massive\u00a0growth, attracting 3.5 times as many users in a single year\u00a0and snagging a recent $200 million in new funding. Voice and video chat integration\u00a0is forthcoming, which will make its all-in-one convenience even more enticing, noise fatigue be damned.\n\nFor some companies, it may prove\u00a0easier to strategize around the inundation.\u00a0Fried argues that we need a new set of rules governing the way we work and chat now. In part, that means not expecting everyone to be on all the time. But it also means treating group chat like a sauna. \u201cStay in for a while, then get out,\u201d he advises.\n\nBut this sort of overload is not unique to Slack. In fact, it\u2019s the same problem causing headaches for Twitter and Facebook right now.\n\n\u201cIt\u2019s why Twitter and now Snapchat are implementing a Facebook-like algorithm to help surface content,\u201d Shields tells Vocativ. \u201cThe cycle is familiar: as users and usage increase, the platform experience evolves as well. I suspect Slack will carefully analyze its users\u2019 behaviors and make the requisite platform modifications to better deliver on its promise.\u201d\n\nProductivity expert Marissa Brassfield, who coaches teams through\u00a0technology saturation in Los Angeles via\u00a0her consulting company\u00a0Ridiculously Efficient, tells Vocativ that\u00a0the challenge with workplace communication and technology is often the expectation that one single tool or app will solve everything, like a \u201cmagic pill\u201d approach to weight loss. In her view, this will be an issue with any technology used incorrectly.\n\nBrassfield\u2019s own team uses Slack, and she\u2019s experienced the pros and cons. \u201cBecause it focuses conversations and separates them by workgroups and teams, it\u2019s moved 60-90 percent of my team communication (depending on the day) out of email, text messages and phone calls and into a searchable archive,\u201d she tells Vocativ. \u201cMy team can find the information they want without asking me to forward an email or re-attach a document.\u00a0But it can also be a megaphone for inappropriate work communication. It essentially digitizes the most annoying parts of working in an office (interruptions, poorly timed water cooler conversation) and brings them through your smartphone and straight into your home.\u201d\n\nThese mirror the same praise and criticisms that came with the rise of smartphones, too. Brassfield\u00a0advises teams to set clear rules for what\u2019s allowed, like dedicating certain channels to silliness, or vowing not to put non-work conversation in a public channel. Another guideline: Don\u2019t\u00a0use it for anything involving nuance, where a phone call or meeting is much more effective.\n\nFor its part, Slack seems aware of the growing anxieties.\u00a0Slack CTO Cal\u00a0Henderson told ArsTechnica that cutting through that noise is an imaginable part of the company\u2019s\u00a0future plan. \u201cWe have a lot of information about communication at work, like who talks to who, who reads what,\u201d he said. \u201cSo the question is whether we can be smarter about saying what\u2019s important.\u201d And Slack spokeswoman Katie Wattie provided Vocativ with the following statement:\n\nThere\u2019s always a period of adjustment for norms to develop around new\u00a0forms of communication. Slack is adapting to new ways of working,\u00a0centralizing communication in one place. The team at Slack uses Slack\u00a0all day, so they\u2019re hyper aware of the experience and are always\u00a0working on improvements. Last year they launched \u201cDo Not Disturb\u201d to\u00a0minimize notifications at disruptive times, and there are more\u00a0features like this to come.\n\nBrassfield says part of her coaching at Ridiculously Efficient is\u00a0helping teams understand that Slack is not the solution if the problem is poor communication in the first place.\u00a0Her analogy is that if effective communication is the lifeblood of any organization, than any opportunity for ineffective communication is a \u201cgushing wound.\u201d Slack, Trana, Asello, or any other technologies are just bandages awaiting saturation if the team\u2019s communication isn\u2019t effective, and she works with teams to detox from oversaturation and then build back what works.\n\nWhether or not every company has that luxury remains to be seen."},
{"url": "http://www.cbsnews.com/news/60-minutes-rare-look-at-how-insider-trading-actually-works/", "link_title": "A Rare Look at How Insider Trading Works", "sentiment": 0.14179970075461873, "text": "The following script is from \"Inside Edge\" which aired on May 22, 2016. Bill Whitaker is the correspondent. Deirdre Cohen and Sarah Koch, producers.\n\nInsider trading is one of the hardest crimes to detect, it happens in whispers and phone calls, behind closed doors. But we've been given a rare look at how it actually works, through the experiences of one woman, a former stock analyst named Roomy Khan. She made a fortune in illegal profits -- with inside information, that she and her colleagues called the edge: the inside edge -- using company secrets to make winning investments in the market. But she got caught and became a government informant in one of the biggest insider trading busts in American history.\n\nRoomy Khan helped the government bring down one of the world's largest hedge funds - the Galleon Group - and send its billionaire cofounder, Raj Rajaratnam, to prison.\n\nRoomy Khan's story reveals how she got involved in insider trading, and how easy it was to do.\n\nRoomy Khan: You are pushed and pushed to get this information. You know, you get the high fives after the trade. I was sent flowers after one of the trades. Big thank you, a huge bouquet. Thank you.\n\nBill Whitaker: It sounds like you guys are in a bubble, trading all this information while we sit and look at it and say, 'Well, that's breaking the law.\"\n\nRoomy Khan: Absolutely, that we were breaking the law.\n\nBreaking the law by obtaining confidential information from friends in Silicon Valley - connected to Google and Polycom and other tech companies. In two years, Roomy Khan made $1.5 million from illegal trades alone; her friends and associates made an additional $25 million off her tips, investigators found. It was easy money.\n\nRoomy Khan: It is like if you are taking an exam tomorrow and somebody hands you what's going to be on the test, it's easy to get an A-plus.\n\nRoomy Khan shared her tips with self-made billionaire, Raj Rajaratnam, who built one of the biggest hedge funds in the world, the $7 billion Galleon Group. Federal authorities said Rajaratnam made more than $72 million from illegal tips from Roomy Khan and other sources.\n\nThe two met back in the 1990s when she was working at Intel as a product marketer and had access to proprietary company information. Rajaratnam tapped her for the inside information, so he could trade on it.\n\nRoomy Khan: And he started asking me about 'How's business?' and I used to have access to Intel's top customer microprocessor bookings. I started giving him this information--\n\nBill Whitaker: So you started feeding him--\n\nRoomy Khan was so brazen, she used Intel's fax machine to send him confidential data about product demand. She says Rajaratnam referred to inside information as \"the edge.\" She was such a good inside source, she said he offered her money to stay at Intel.\n\nRoomy Khan: He said, 'Listen, I'll off - I'll give you, you know 100K just to stay there,' I don't remember the number he offered me. But he did offer me money to just stay there and \"Keep giving me information.\"\n\nBill Whitaker: And-- and keep--\n\nRoomy Khan: Yes. And I said, 'No, there's no way,'\n\nRoomy Khan came to the United States from Delhi, India, on a scholarship at age 23. She earned three graduate degrees before joining Intel. But she longed for the action of Wall Street and set out to build her own fortune. At the height of her success, she says she was worth $50 million.\n\nKhan moved into this $10 million gated estate in the heart of Silicon Valley. She was living the life she wanted where money was no object.\n\nRoomy Khan: Jewelry, painting. I mean, anything that you can think of, you know?\n\nBill Whitaker: You had it all?\n\nRoomy Khan: We had it all, yes.\n\nBill Whitaker: Sort of life we see in the movies with the hedge fund investors.\n\nOne purchase from that time still makes her light up.\n\nShe explained to us just how the biggest money could be made: when the predictions of Wall Street were at odds with the inside information.\n\nRoomy Khan: The most money you make is when your analysis is totally in an opposite to what your edge is telling you.\n\nRoomy Khan: Absolutely. If you have a really great source.\n\nRoomy Khan had a really good source who knew what was going on inside Google...a friend who worked for a firm that prepared Google's press releases and who told her the company's quarterly income would be lower than expected.\n\nRoomy Khan: And she told me they were going to miss the quarter.\n\nBill Whitaker: And you made money off of it.\n\nShe shared the information with Galleon chief, Raj Rajaratnam, who made $8 million betting against Google just before the price dropped.\n\nBill Whitaker: And you're making good money, but he's making far more. What's your motivation?\n\nRoomy Khan: Well, I had access to Raj so I had that access to the billionaire biggest hedge fund on Wall Street. And that was worth a lot to me.\n\nHer relationship with the hedge fund titan would be worth a lot to the government too. Roomy Khan didn't know the Securities and Exchange Commission -- the SEC -- had launched an investigation into Rajaratnam. Former SEC attorney Andrew Michaelson was tracking his texts and trades.\n\nAndrew Michaelson: We did see in Mr. Rajaratnam's instant messages communications where he would say, \"AMD's revenues are going to be X,\" before AMD itself announced them. And they were accurate. Mr. Rajaratnam's predictions were accurate.\n\nMichaelson joined the SEC in 2006 and this was one of his first cases. He remembers combing through stacks and stacks of Galleon's trading and phone records, instant messages and emails.\n\nBill Whitaker: How many documents are we talking about?\n\nAndrew Michaelson: Sometimes you'd have to sit there with a ruler to make sure that you're getting exactly who is talking, what phone number is calling which phone number at what time.\n\nMichaelson: We're connecting the dots. And then the next dot to connect is, \"Well, where's Raj Rajaratnam getting this information?\"\n\nFinally, after six months of searching, they found the needle in the haystack in a single, careless instant message from Roomy Khan.\n\nRoomy Khan: I texted him. And I said, \"Don't buy Polycom.\"\n\nRoomy Khan: And then-- in-- it was a text message.\n\nRoomy Khan: Yes. And then--and it said, \"Til I check the--the guidance.\n\nBill Whitaker: You're saying, \"Don't, don't do anything until I--\"\n\nBill Whitaker: Call my inside guy and get this inside information.\n\nRoomy Khan: Yes. Yes. Right.\n\nIt was the piece of evidence FBI special agent BJ Kang thought he could use to turn Roomy Khan into an informant against Rajaratnam.\n\nBJ Kang: She was an insider. She knew all the players. She worked for Galleon. She knew Raj.\n\nHe paid her a visit in November 2007.\n\nRoomy Khan: Two people knocked on my door and they flashed their badge and my heart sank because I just was like, \"Oh my God.''\n\nBJ Kang: She knew we were dead serious. She knew why we were there. And she knew this wasn't gonna go away.\n\nKang showed her the Polycom message she'd sent to Rajaratnam.\n\nRoomy Khan: And when they showed me this text message I knew this was over because it was very easy for them to connect me to the executive at Polycom.\n\nShe knew she had to cooperate. Starting in late 2007, she began to educate the Feds on the hidden world of some of Wall Street's biggest players.\n\nBJ Kang: We didn't have a very good understanding of what the hedge funds were doing, right? Um--\n\nBill Whitaker: --completely what you had.\n\nBJ Kang: Absolutely not. She kinda drew the-- drew out the roadmap for us to say, \"This is what they're doing. This is how they're doing it. This is the language that they use. And-- and here you go.\"\n\nAs part of her cooperation, she also reluctantly agreed to secretly record her phone conversations with her colleagues.\n\nBill Whitaker: And turned out to be a good liar.\n\nJonathan Streeter: And turned out to be a good liar. There are some people who can't do that very well, who can't get on the phone and lie to their former colleague on the phone and you know, that's - she was good at that.\n\nBut Jonathan Streeter, a former assistant U.S. Attorney, who was the trial counsel on the case, said Roomy Khan also lied to federal investigators.\n\nJonathan Streeter: Roomy had multiple instances, one after another where she had withheld information while she was cooperating or she had her gardener get a cell phone so that she could have secret phone calls with people.\n\nBill Whitaker: Why would you--\n\nRoomy Khan: --know. I was just so-- I just couldn't-- I just couldn't tell on all these people.\n\nJonathan Streeter: I told her that if she didn't tell us the truth about this, I was gonna make it my mission in life that she would spend a long time in jail.\n\nUltimately, Roomy Khan gave prosecutors what they wanted - enough evidence of insider trading that a judge allowed the government to tap Rajaratnam's cell phone. The investigation was the first time wiretaps were used in a significant way in an insider trading case.\n\nJonathan Streeter: Without Roomy-- you don't have a wiretap, and without a wiretap, you don't have a whole lot of other evidence. What you have is some circumstantial evidence that Rajaratnam made some well-timed trades and that he spoke to some people before he made those trades, but you don't have him on the phone, talking about inside information.\n\nIn one wiretap, Rajaratnam bragged to a colleague about getting valuable inside information from a Goldman Sachs board member, just two minutes before the market closed.\n\nRajaratnam (wire): I got a call at 3:58, right?\n\nRajaratnam (wire): Saying something good might happen to Goldman.\n\nThat \"something good\" was a $5 billion investment in Goldman by Warren Buffett that hadn't yet been announced to the public. Rajaratnam's hedge fund purchased more than 200,000 shares in those last minutes and made $840,000.\n\nIn another wiretap, he told a different colleague that he'd gotten inside information Goldman's earnings would be below market expectations.\n\nRajaratnam (wire): I heard yesterday from somebody who's on the Board of Goldman Sachs, that they are gonna lose $2 per share. The Street has them making $2.50.\n\nJonathan Streeter: That was quite possibly my favorite of the wiretap calls. So he's admitting he has a piece of inside information from a board member that's clearly different from what the rest of the world thinks. Right there, ticking off the elements of insider trading, he's given us a bunch of 'em.\n\nAnd in October 2009, U.S. Attorney Preet Bharara announced the arrests of Raj Rajaratnam and his colleagues.\n\nBharara (presser): They may have been privy to a lot of confidential information but there was one secret they did not know and that is that we were listening.\n\nThirty-two people were charged criminally or civilly in the case including members of Raj's inner circle - many were members of the business elite of South Asian descent, including Anil Kumar and Rajat Gupta, the former head of the McKinsey Consulting Group. The courts imposed more than $250 million in fines and penalties. In 2011, Rajaratnam was convicted of insider trading crimes and sentenced to 11 years in prison.\n\nAnita Raghavan: One of the astonishing things is that Raj was not caught earlier. He would boast about how he got inside information from corporate sources.\n\nAnita Raghavan, is a journalist who introduced us to Roomy Khan. She wrote a book, about the Galleon hedge fund, \"The Billionaire's Apprentice\" that tracked the downfall of Roomy Khan and her colleagues.\n\nBill Whitaker: What did the fall do to Roomy?\n\nAnita Raghavan: Everyone in the trading community distanced themselves from Roomy. They didn't want to have anything to do with her. She was the rat, she was the cooperator.\n\nThough she cooperated, Roomy Khan was sentenced to a year in prison for insider trading and for obstruction of justice, because of her lies.\n\nReleased in 2014, she now lives in Florida where she is struggling to rebuild her life. She's been unable to find a job. For all her early success, her advanced degrees, her multimillion dollar fortune - Roomy Khan says she allowed her ambition to get the better of her.\n\nRoomy Khan: As I look back, you know, I'm aghast at the choices I made. I had all the right the breaks. I was so fortunate. I landed in the United States. I was very fortunate and then I threw it all away."},
{"url": "http://www.pnas.org/content/early/2016/05/18/1601465113", "link_title": "Revealing a 5,000-y-old beer recipe in China", "sentiment": 0.1908730158730159, "text": "This research reveals a 5,000-y-old beer recipe in which broomcorn millet, barley, Job\u2019s tears, and tubers were fermented together. To our knowledge, our data provide the earliest direct evidence of in situ beer production in China, showing that an advanced beer-brewing technique was established around 5,000 y ago. For the first time, to our knowledge, we are able to identify the presence of barley in archaeological materials from China by applying a recently developed method based on phytolith morphometrics, predating macrobotanical remains of barley by 1,000 y. Our method successfully distinguishes the phytoliths of barley from those of its relative species in China.\n\nThe pottery vessels from the Mijiaya site reveal, to our knowledge, the first direct evidence of in situ beer making in China, based on the analyses of starch, phytolith, and chemical residues. Our data reveal a surprising beer recipe in which broomcorn millet (Panicum miliaceum), barley (Hordeum vulgare), Job\u2019s tears (Coix lacryma-jobi), and tubers were fermented together. The results indicate that people in China established advanced beer-brewing technology by using specialized tools and creating favorable fermentation conditions around 5,000 y ago. Our findings imply that early beer making may have motivated the initial translocation of barley from the Western Eurasia into the Central Plain of China before the crop became a part of agricultural subsistence in the region 3,000 y later."},
{"url": "http://qz.com/685262/scientists-say-theres-such-a-thing-as-ethical-amnesia-and-its-probably-happened-to-you/", "link_title": "Ethical Amnesia", "sentiment": 0.09694083694083694, "text": "Most of us like to think that we have moral standards, and there may be a psychological reason why.\n\nA study published (paywall) today (May 16) in the Proceedings of the National Academy of Sciences indicates that when we act unethically, we\u2019re more likely to remember these actions less clearly. Researchers from Northwestern University and Harvard University coined the term \u201cunethical amnesia\u201d to describe this phenomenon, which they believe stems from the fact that memories of ourselves acting in ways we shouldn\u2019t are uncomfortable.\n\n\u201cUnethical amnesia is driven by the desire to lower one\u2019s distress that comes from acting unethically and to maintain a positive self-image as a moral individual,\u201d the authors write in the paper.\n\nTo investigate, Maryam Kouchaki, a behavioral research specialist at the Kellogg School of Management at Northwestern University and her colleague Francesca Gino at Harvard Business School conducted nine separate studies with over 2,100 participants. Over the course of their work, they found that people remember the times they acted ethically, like playing a game fairly, more clearly than the times they probably cheated.\n\n\u201cWe speculated\u2026that people are limiting the retrieval of memories that threaten their moral self-concept and that is the reason we see pervasive ordinary unethical behaviors,\u201d Kouchaki wrote in an email.\n\nIntuitively, these results make sense. We don\u2019t like to think of ourselves as immoral people, and may come up with justifications (pawall) for our behavior that would indicate the contrary. The authors write that these results could indicate why certain acts of dishonesty are so pervasive, like hitching free rides on public transportation, stealing from the workplace, and even cheating on taxes.\n\nOf course, not all of our indiscretions are easy to forget, particularly when there are lasting ramifications to our actions. \u201cStrong consequences might reduce unethical amnesia with your rationale,\u201d Kouchaki wrote. But simultaneously, she speculated, the emotional pain caused by remembering severely negative consequences could work as even more motivation to forget that we acted immorally in the first place."},
{"url": "http://www.theguardian.com/lifeandstyle/2016/may/11/turmeric-latte-golden-milk-cult-following-alternative-coffee", "link_title": "Turmeric latte", "sentiment": 0.19543793195108985, "text": "It is a drink fit for Midas, with an ochre colour so vivid it doesn\u2019t need an Instagram filter. It\u2019s not made of gold, but it might as well be, given its cult following. \u201cGolden milk\u201d or turmeric latte \u2013 an unlikely combination of nut milk and juiced turmeric root \u2013 is 2016\u2019s drink of choice. In a new report on food trends in the US, Google singled out turmeric\u2019s ascent after searches for the spice increased by 56% from November 2015 to January 2016. And fuelling that rise is its use in lattes: \u201cgolden milk\u201d is among the top online searches associated with the spice. Turmeric lattes are now being sold at cafes from Sydney to San Francisco, and the drink is gaining fans in the UK.\n\nAt Modern Baker in Summertown, Oxford, sales of turmeric lattes \u2013 listed on the menu as \u201cGolden mylk\u201d (the \u201cy\u201d is health-speak for non-cow milk) now outnumber that of regular lattes. Turmeric lattes routinely feature in reviews for the York outpost of the Filmore & Union restaurant chain. Nama, a vegan restaurant in Notting Hill, west London, has noticed a surge in the turmeric latte\u2019s popularity recently, even though it has sold the drink for nearly two years. A prescient former employee used to whip them up for the staff, and they went down so well that the latte ended up on the menu.\n\n\u201cNobody was really serving them,\u201d Nama co-founder Irene Arango recalls. \u201cWe used to do little tastings at Nama and people got hooked.\u201d It is also, as Arango puts it, a pleasant way for health-conscious diners to get a fix of turmeric juice.\n\nAt first, it seems an odd concept. Turmeric is mostly known as a curry ingredient that leaves indelible yellow stains on appliances and fingernails. And, save for the aeration and the artistic lashes of cinnamon, the turmeric latte bears little similarity to its caffeinated namesake. But this is one of those trends whose provenance isn\u2019t just Gwyneth Paltrow\u2019s Goop website.\n\nAfter ghee, homemade yoghurt and coconut oil, turmeric is the latest health-food trend to originate from the south Asian pantry, another sign that the Indian subcontinent may be ahead of the hipster curve. Turmeric and milk is a fairly well-entrenched drink in the region\u2019s food culture, where it is considered a restorative. Turmeric is part of Ayurvedic medicine \u2013 a holistic, all-natural approach to health that has been practised for centuries in India \u2013 and a ubiquitous ingredient in curries and rice dishes.\n\nOn the south Asian recipe website Khana Pakana, a turmeric-and-milk combination, haldi doodh, is described as a drink women consume to lighten their skin. The spice is believed to help with everything from cancer to a cough, and is often given to children with a fever. The most commonly used recipe calls for turmeric powder mixed with milk and a dash of black pepper, as well as an optional addition of ghee.\n\nThis history seems to have filtered into the turmeric latte\u2019s current run of success. It is promoted for its health benefits \u2013 as an anti-inflammatory and an alternative to a caffeinated drink \u2013 and, Arango says, it is particularly popular with customers in the mornings. Modern Baker also makes one with an espresso shot, and an iced version.\n\nLike many trends, the turmeric latte may seem to have come out of nowhere. But it has been brewing for a while. The market research firm Mintel named turmeric as one of its foods to watch in 2016. It has done the rounds of the wellness circuit \u2013 the blogs, websites and Instagram accounts of \u201cclean eating\u201d advocates \u2013 for several months, and recipes for the drink abound on Pinterest.\n\nModern Baker\u2019s co-founder Melissa Sharp, who founded the bakery and cafe after an illness, came to the spice for health reasons. \u201cI knew about turmeric \u2026 I had seen a couple of places in London with the latte on their menus. And I did the research, and it was the most delicious thing,\u201d she says. Modern Baker is firmly on the turmeric bandwagon, using the spice in about two dozen products, including turmeric biscuits.\n\nDespite being recommended haldi doodh several times in Karachi, Pakistan, where I live, I\u2019ve never come around to the drink. But, one morning recently, I found myself stirring coconut milk, fresh ginger and cinnamon in a saucepan with a turmeric powder paste. It smells uncomfortably reminiscent of a Pakistani curry made with yoghurt, turmeric and gram flour. I spoon in some honey, start whisking and hope for the best. Despite my low expectations, the first sip is surprisingly pleasant. The tartness of the turmeric is dulled by the milk and cinnamon. I don\u2019t like the fibrous ginger, but after straining the drink is creamy, with the comforting quality of a warm, yet slightly sweet, soup. It tastes nice when it has cooled down, too.\n\nWhile haldi doodh is seen as the kind of old-school, antiquated drink a well-meaning relative would foist upon you, the turmeric latte is a world apart. \u201cThe trick is to use fresh cold-pressed turmeric juice,\u201d Arango says. Goop\u2019s recipe calls for almond milk, while Bon Appetit\u2019s iced version uses cashew. The Californian vegan restaurant chain Cafe Gratitude uses steamed almond milk with freshly squeezed turmeric juice and honey. Arango says coconut milk works better with turmeric, while Modern Baker primarily uses almond milk and varies it by adding coconut milk powder. Many recipes follow the south Asian playbook and use black pepper, which brings out the flavour of the turmeric.\n\nI still can\u2019t see myself ordering a turmeric latte out, though \u2013 this is one of the few times my Pakistani parents\u2019 \u201cwe can make this better and cheaper at home\u201d mantra is true. But, a day later, I find myself wondering what a caffeinated turmeric latte tastes like, and how I can get ahead of the next big south Asian trend."},
{"url": "http://www.bbc.com/news/world-asia-china-36365967", "link_title": "Chinese brewed beer 5,000 years ago, research suggests", "sentiment": 0.12947957839262186, "text": "Chinese drinkers may have been brewing beer as many as 5,000 years ago, new research suggests.\n\nUS and Chinese researchers say they found traces of barley, millet, grain, and tubers used in fermentation.\n\nIt was found on pottery discovered in Shaanxi province in northern China during an archaeological dig 10 years ago.\n\nIt would be the earliest known instance of beer-making in China and suggest a sophisticated approach.\n\nPublished in the US Proceedings of the National Academy of Sciences, the study also suggested that barley may have been used for booze before being used for food.\n\nThe pottery fragments were originally found in an archaeological dig in 2004-2006, but only had their residue analysed by Stanford University researchers in late 2015, confirming earlier speculation by Chinese scholars that they might have been used for brewing.\n\nThe find included pots and pottery funnels, covered with a residue of broomcorn millet, barley, a chewy grain known as Job's tears, and tubers.\n\nIt also included stoves that could have been used to heat and mash grains, as well as underground spaces that would have kept the brew at a cool, consistent fermentation temperature - and helped stop it going off quickly once it was made.\n\n\"The discovery of barley is a surprise,\" lead author Jiajing Wang of Stanford University told the BBC in an email, as it was previously thought the grain arrived in China 1,000 years later.\n\n\"This beer recipe indicates a mix of Chinese and Western traditions - barley from the West; millet, Job's tears and tubers from China.\"\n\nThe latest find would suggest that drinkers in China first began to develop a taste for beer around the same time as people in ancient Egypt and Iran - from where the barley may have come.\n\nThe exact taste of the ancient brew will remain a mystery however, as while the scientists say they know what ingredients were used, they don't know the quantities."},
{"url": "http://blog.kameleoon.com/en/best-practices/stopping-ab-tests-too-early/?utm_campaign=Conversion%20Matters%20EN%20May%202016&utm_source=hackernews", "link_title": "You're stopping your a/b tests too early", "sentiment": 0.16419085030196137, "text": "Are You Stopping Your A/B Tests Too Early?\n\nStopping A/B Tests too early is without a doubt the most common\u2014and one of the most potent, A/B Testing mistake.\n\nBut the answer is really not intuitive. We were going to write an article on when to stop your a/b test for both main statistical methods of A/B Testing, Frequentist, and Bayesian. But, we encountered two problems.\n\nFirst, most people doing A/B Testing don\u2019t know\u2014and don\u2019t care, whether their tool uses frequentist or bayesian statistics.\n\nSecond, when you dig around a bit in the different A/B Testing solutions, you find that no 2 softwares use exactly the same statistical method.\n\nSo, how could we write something helpful?\n\nHere is what we came up with. We will do our best to answer the following question: What concepts do I need to understand not to stop my A/B Test too early?\n\nThus, we will cover today:\n\nNote: None of these elements are stopping rules on their own. But having a better grasp of them will allow you to make better decisions.\n\nWhen your A/B Testing tool tells you something along the lines of: \u201cyour variation has X % chances to beat the control\u201d, it\u2019s actually giving you the statistical significance level.\n\nAnother way to put it: \u201cthere is 5% (1 in 20) chance that the result you see is completely random\u201d. Or \u201cthere is 5% chance that the difference in conversion measured between your control and variation is imaginary\u201d.\n\nYou want at minimum 95%. Not less. Yes, 80% chance do sounds like a solid winner but that\u2019s not why you\u2019re testing. You don\u2019t want just a \u201cwinner\u201d. You want a statistically valid result. Your time & money are at stake, so let\u2019s not gamble!\n\nFrom experience, it\u2019s not uncommon for a test to have a clear winner at 80% significance, and then it actually loses when you let it run properly.\n\nOkay\u2014so, if my tool tells me that my variation has 95% chance to beat the original, I\u2019m golden right? Well \u2026 no. Statistical significance doesn\u2019t imply statistical validity and isn\u2019t a stopping rule on its own.\n\nIf you did a fake test, with the same version of a page, an A/A test, you\u2019d have more than 70% chance that your test will reach 95% significance level at some point.\n\nyou got it, shoot for 95%+ significance level BUT don\u2019t stop your test just because it reached it.\n\nWhen you do A/B Testing you can\u2019t measure your \u201ctrue conversion rate\u201d because it\u2019s an ever-moving target.\n\nYou arbitrarily choose a portion of your audience with the following assumption: the selected visitors\u2019 behavior will correlate with what would have happened with your entire audience.\n\nKnow your audience. Conduct a thorough analysis of your traffic before launching your A/B Tests.\n\nHere is a couple of examples of things you need to know:\n\nThe problem is your traffic keeps evolving, so you won\u2019t know everything with 100% accuracy.\n\nSo, ask this: Is my sample representative of my entire audience, in proportions and composition?\n\nAnother issue if your sample is too small is the impact your outliers will have on your experiment. The smaller your sample is, the higher the variations between measures will be.\n\nHere\u2019s the data from tossing a coin 10 times. H (head), T (tail). We know the \u201ctrue\u201d probability of our coin is 50%. We repeat the toss 5 times and track the % of heads.\n\nThe outcomes vary from 30% to 80%.\n\nSame experience, but we toss the coin 100 times instead of 10.\n\nThe outcomes vary from 47% to 54%.\n\nThe larger your sample size is, the closer your result gets to the \u201ctrue\u201d value.\n\nIt\u2019s so much easier to grasp with an actual example\n\nWith conversion rates, you could have your variation winning by far the first day because you had just shot your newsletter and the majority of your traffic were your clients for example.\n\nThey like you considerably more than normal visitors, so they reacted positively to your experiment.\n\nShould you stop the test here, even with a significance level at 95%, you would have skewed results. The real result could be the exact opposite for all you know.\n\nHow big should your sample be ?\n\nThere is no magical number that will solve all your problems, sorry. It comes down to how much of an improvement you want to be able to detect. The bigger of a lift you want to detect, the smaller sample size you\u2019ll need.\n\nAnd even if you have Google-like traffic, it isn\u2019t a stopping condition on its own. We\u2019ll see that next.\n\nOne thing is true for all statistical methods though: the more data you collect, the more accurate or \u201ctrustworthy\u201d your results will be.\n\nBut it varies depending on the method your tool uses.\n\nLet me insist on the fact that those numbers might not be optimal for you if your tool doesn\u2019t use frequentist statistics. That said, it won\u2019t be detrimental on your results validity if you use them.\n\nTo determine sample size, we advise our clients to use a calculator like this one (we have one in our solution, but this one is extremely good too).\n\nIt gives an easy-to-read number, without you having to worry about the math too much. And it prevents you from being tempted to stop your test prematurely, as you\u2019ll know that till this sample size is reached, you shouldn\u2019t even look at your data.\n\nYou\u2019ll need to input the current conversion rate of your page, the minimum lift you want to track (i.e. what is the minimum improvement you\u2019d be happy with).\n\nWe then recommend at least 300 macro conversions (meaning your primary goal) per variation before even considering stopping the test.\n\nI\u2019ll repeat it again, It\u2019s not a magic number.\n\nWe sometimes shoot for 1000 conversions /variation if our client\u2019s traffic allows us to. The larger the better, as we saw earlier. It also could be a bit less if there is a considerable difference between the conversion rates of your control and variation.\n\nOkay, so if I have lots of traffic and a large enough sample size with 95% in 3 days it\u2019s great right?\n\nWelp, kudos on your traffic, but sorry no again \u2026\n\nYou already know that for emails and social media, there are optimal days (even hours) to post.\n\nPeople behave differently on given days and are influenced by a number of external events. Well, same thing for your conversion rates. Don\u2019t believe me, try. Run a conversion by day for a week you\u2019ll see how much it can vary from a day to another.\n\nThis means, if you started a test on a Thursday, end it on a Thursday. (We\u2019re not saying you should test for just one week.) Test for at least 2-3 weeks. More would be better though.\n\n1 to 2 business cycles would be great. As you\u2019ll get people that just heard of you and close to buying while accounting for most external factors (we\u2019ll talk more about those in our article on external validity threats) and sources of traffic.\n\nIf you must extend the duration of your test, extend it by a full week.\n\nTwo phenomenons to consider here:\n\nThis is also why the significance level isn\u2019t enough on its own. During a test, you\u2019ll most likely reach several times 95% before you can actually stop your test.\n\nMake sure your significance curve flattens out before calling it.\n\nSame thing with the conversion rates of your variations, wait till the fluctuations are negligible considering the situation and your current rates.\n\nFor tools that give you a confidence interval, for example \u201cvariation A has a conversion rate of 18,4% \u00b1 1,2% and Variation B 14,7% \u00b1 0,8%\u201d.\n\nMeaning the conversion rate of the variation A is between (18,4 \u2013 1,2) and (18,4 + 1,2), and conversion rate of the variation B is between (14,7 \u2013 0,8) and (14,7 + 0,8).\n\nIf the 2 intervals overlap, keep testing. Your confidence intervals will get more precise as you gather more data.\n\nSo, whatever you do, don\u2019t report on a test before it\u2019s actually over. To resist the temptation to stop a test, it\u2019s often best not to peek at the results before the end. If you\u2019re unsure, it\u2019s better to let it run a bit longer.\n\nTo stop an A/B Test, consider the following:\n\nOnly after taking all of those into account can you stop your test. Don\u2019t skip them, it\u2019d cost you money.\n\nAll right, that\u2019s all for today. We hope this article helped understand those concepts better and will reduce the risk of you calling a test too early.\n\nWe will definitely come back with additional content on this particular topic in the future.\n\nPS: Before you go, a couple of things I\u2019d like you to do:\n\nIt\u2019s extremely important for me to know that I write content both helpful and focused on what matters to you.\n\nI know it sounds cheesy and fake, but I feel like writing purely marketing, \u201cempty\u201d stuff\u2014just for the exercise, is a fat loss of time for everyone. Useless for you, and extremely un-enjoyable for me to write.\n\nPS2: If you missed the other articles in our series, here they are:"},
{"url": "https://medium.com/keep-learning-keep-growing/the-pain-you-feel-is-capitalism-dying-5cdbe06a936c#.bs4thezgs", "link_title": "The Pain You Feel Is Capitalism Dying", "sentiment": 0.05725706392373059, "text": "The Pain You Feel is Capitalism Dying\n\nIt can be very confusing to know that you won\u2019t find a decent job, pay off student loans or put in a down payment on a house in the next few years\u200a\u2014\u200aeven though you may have graduated from a top-tier university or secured glowing references from all those unpaid internships that got you to where you are today.\n\nEven if you are lucky enough to have all of this going for you, you\u2019ll still be one among hundreds of applicants for every job you apply for. And you\u2019ll still watch as the world becomes more unequal, with fewer paid opportunities to do what you feel called to do in your work or for your life path.\n\nWhat\u2019s more, you won\u2019t find much help from your friends because most (if not all) of them are going through the same thing. This is a painful and difficult time that is impacting all of us at once.\n\nThere will be people who tell you it\u2019s your fault. That you aren\u2019t trying hard enough. But those people are culprits in perpetuating a great lie of this period in history. The standard assumptions for how to be successful in life a few decades ago simply do not apply anymore. The guilt and shame you feel is the mental disease of late-stage capitalism. Embrace this truth and set yourself free.\n\nTo see how broken things have become you\u2019ll have to think systemically. Take note of the systems built up to create this situation and understand how it came to be\u200a\u2014\u200aso you\u2019ll see why it cannot possibly continue on its current path.\n\nFirst, a diagnosis of the problem:\n\nA Global Architecture of Wealth Extraction has been systematically built up to rig the economic game against you. This is why a tiny number of people (current count is 62) have more wealth amongst them than half the human population. Decades of those using tax havens to hide their wealth, unfair trade agreements designed to extract wealth from poor countries, banking regulations and austerity measures meant to destabilize entire economies so massive transfers of wealth can go from everyone else to a tiny financial elite, and election rules that all-but-guarantee only those who become whores to these financial pimps will ever sit in high office.\n\nSo yeah, it\u2019s okay to feel restless as capitalism winds itself down from these system-level harms to society.\n\nWhy do I say that capitalism (in its corporatist, wealth-extracting form) is dying? There\u2019s a long, detailed story that could be told about this. For the sake of brevity, I will answer with two essential pieces that show how business-as-usual is finished. It is physically impossible for it to continue much longer.\n\nReason 1: There Are No More Profits to Extract.\n\nAs eloquently described in the writings of Jeremy Rifkin and Paul Mason, the primary motivator for capitalists\u200a\u2014\u200ato extract wealth from consumer exchange in the form of monetary gain\u200a\u2014\u200ais crippled by the fact that the science of wealth extraction has become so advanced that every new wave brings diminishing returns. What is called \u201cmarginal cost\u201d by economists, the difference between how much it costs to produce something and what people are willing or able to pay for it, is nearly zero now for everything we manufacture or provide as a service. This zero marginal cost trend is breaking capitalism down by the unexpected outcome of its own spectacular success.\n\nAdd to this that most of the growth in the global economy in the last 40 years has been in speculative finance. The money system grows faster than the productive \u201creal\u201d economy\u200a\u2014\u200awith the predictable outcome of market crashes, financial collapse, and structural adjustments (wealth extraction) when the mismatch grows too large. What we end up with is bloated debt too large for everyone to pay back. Combined with the end game of wealth hoarding mentioned above, this is a death knell for capitalism as we\u2019ve known it in the last 100 years.\n\nReason 2: Damage Built Up in the Natural World\n\nThere is no such thing as an economy that exists without the physical world. The delusional idea that markets are separate from nature has guided mainstream economic policy for a long time\u200a\u2014\u200aand now we are seeing the consequences in mass extinctions, loss of topsoils, climate change, collapse of fish stocks in the world ocean, rising levels of pollution, and more.\n\nPhysicists would describe this as increasing entropy, which simply means the rise in social complexity of human economies comes with a corresponding deterioration of the larger natural environments they are embedded within. And we have crossed the unprecedented watermark of history in the 20th Century\u200a\u2014\u200awith exploding population growth, and the crossing of several essential planetary boundaries (any of which, if passed, will place our civilization in jeopardy). At current count, we have passed four of them.\n\nSo the nails are in the coffin for capitalism. What remains to be seen is whether this will take down our globalized civilization as well. I am hopeful, yet sober about our prospects. It\u2019s going to be a very turbulent time (for the rest of our lives) but I think we can make it through this restlessness by acknowledging that it\u2019s real, naming the architecture of wealth extraction that created these systemic harms, and dismantling this globalized system to release vital monetary resources for the emergence of a new, life-affirming paradigm for economic development.\n\nBut before we can begin this great work of our times, we must acknowledge the pain that a dying capitalist system creates in our personal lives. I know it hurts. It is quite natural to feel ashamed when you try really hard and do your best, yet still are unable to succeed. You\u2019ll need to change the rules of the game (a few of which I have outlined here). And doing this is going to require going through a healing process internally for yourself and with your friends.\n\nYou are not alone. All 7.4 billion of us alive today are going through this. We are doing it together. Now is the time to become fully aware of the systemic nature of what we are going through. The future will not be like the past. It is going to be painful and confusing at times. Yet the prospects for getting through this struggle are nothing less than a thriving planetary civilization that is inclusive and nourishing for all people while at the same time remaining in harmony with our home planet of Earth."},
{"url": "http://futurism.com/hiv-genes-have-been-cut-out-of-live-animals-using-crispr-2/", "link_title": "HIV Genes Have Been Cut Out of Live Animals Using CRISPR", "sentiment": 0.2236215538847118, "text": "For the first time ever, scientists were able to successfully cut out the HIV genes from live animals, and they had over a 50% success rate.\n\nA significant milestone was achieved today in the fight against HIV\u2014scientists led by Kamel Khalili of the Comprehensive NeuroAIDS Center at Temple University just reported that, for the first time, HIV genes have been successfully eliminated from the genomes of animals infected with the virus.\n\n\u201cIn a proof-of-concept study, we show that our gene editing technology can be effectively delivered to many organs of two small animal models and excise large fragments of viral DNA from the host cell genome,\u201d explained Khalili.\n\nAccording to Khalili\u2019s experiments, the method was successful in snipping out the virus in over 50% of the cells of each type. So while we still have some way to go before it can be successfully utilized as a treatment, the early work is promising.\n\nThe study, which was published in the journal Gene Therapy, was conducted on mice and rats. For their work, the team engineered the animals to incorporate the HIV genes in every cell in their bodies. Then, using\u00a0CRISPR, an extremely precise gene editing technique, the team created a pair of molecular scissors which they used to remove the viral genes that infected the brain, ear, liver, kidney, lungs, spleen, and even blood of the animals in question.\n\nLearn more about how CRISPR works in the video below:\n\nKhalili has previously used CRISPR to remove HIV from cells taken from HIV positive people, but the success of using the same technique on live animals, and not just their cells, is a significant step forward towards developing this method as a possible treatment (or perhaps even a cure) for HIV and AIDS patients.\n\nKhalili explains\u00a0how this method could translate for use in humans:\u00a0\u201cIf this technology gets into the clinic to treat human patients, it\u2019s not going to be very complicated. You don\u2019t have to bring the patient to the clinic and do a bone marrow transplant or all kinds of complicated technology. You can basically apply this to any setting.\u201d\n\nThe team is currently studying the ideal dosage and monitoring for possible side effects; but given that the CRISPR molecule specifically targets HIV genes, they believe that any adverse effects (such as off-target gene editing) in humans will be unlikely.\n\n\u201cThe first step is to permanently inactivate those viruses incorporated in cells,\u201d says Khalili. \u201cIf we can do that, and reach that level, then we may be able to functionally cure individuals or have a sterilizing cure.\u201d It will be a long road, but things are looking rather bright."},
{"url": "http://www.reuters.com/article/us-facebook-bias-idUSKCN0YE2R9", "link_title": "Facebook changes policies on 'Trending Topics' after criticism", "sentiment": 0.11771708683473389, "text": "SAN FRANCISCO Facebook Inc (FB.O) said on Monday that it had changed some of the procedures for its \"Trending Topics\" section after a news report alleging it suppressed conservative news prompted a U.S. Congressional demand for more transparency.\n\nThe company said an internal probe showed no evidence of political bias in the selection of news stories for Trending Topics, a feature that is separate from the main \"news feed\" where most Facebook users get their news.\n\nBut the world's largest social network said in a blogpost that it was introducing several changes, including elimination of a top-ten list of approved websites, more training and clearer guidelines to help human editors avoid ideological or political bias, and more robust review procedures.\n\nEarlier this month, a former Facebook contractor had accused the company's editors of deliberately suppressing conservative news. The allegations were reported by technology news website Gizmodo, which did not identify the ex-contractor.\n\nThe report led Republican Sen. John Thune to write a letter demanding that the company explain how it selects news articles for its Trending Topics list. (bit.ly/1Tvv3Nm)\n\nTwo days after Thune's letter, Facebook published a lengthy blogpost detailing how Trending Topics works even though it rarely discloses such practices. Previously, it had never discussed the inner workings of the feature, which displays topics and news articles in the top right hand corner of the desktop homepage for its more than 1.6 billion users.\n\nFacebook said its investigation showed that conservative and liberal topics were approved as trending topics at nearly identical rates. It said it was unable to substantiate any allegations of politically motivated suppression of particular subjects or sources.\n\nBut it did not rule out human error in selecting topics.\n\n\"Our investigation could not fully exclude the possibility of isolated improper actions or unintentional bias in the implementation of our guidelines or policies,\" Colin Stretch, Facebook's General Counsel, wrote in a company blogpost.\n\nFacebook Chief Executive Mark Zuckerberg met last week with more than a dozen conservative politicians and media personalities to discuss issues of trust in the social network.\n\nIn his letter, Thune had called on Facebook to respond to the criticism and sought answers by May 24 to several questions about its internal practices.\n\n\"Any attempt by a neutral and inclusive social media platform to censor or manipulate political discussion is an abuse of trust and inconsistent with the values of an open internet,\" Thune said."},
{"url": "https://medium.com/crowd-valley-collection/financial-markets-machine-learning-and-the-intelligent-robotic-future-8568985107da#.kllx91r8b", "link_title": "Financial Markets, Machine Learning and the Intelligent Robotic Future", "sentiment": 0.09095390524967994, "text": "Markets always move toward greater efficiency, history has taught us that much. Technological advancements in computing power has been remarkable, but the growth and pace of development keeps on accelerating. Have you thought through the extent of the plausible impact of artificial intelligence (AI) and machine learning in the financial services market? Go ahead, try.\n\nCognitive psychology teaches us that humans are easily fallible as it comes to assessing the extent to which events are within their control. Humans also easily overestimate their own importance in the events that befall them and in turn underestimate the impact of factors beyond our control, such as timing, the environment and sheer luck. Are we over estimating our importance in financial markets? Without a doubt.\n\nThrough the conversations we\u2019ve been privy to, there is hardly a sector that does not discuss the future implications of the electronification or digitization of value chains, some times using words such as commoditization. Companies such as law firms, consulting firms, investment banks have long debated the areas of their operations that are commoditized and that do not require specialized, unique knowledge. An IPO process has several components that are crucial and high value, and it has even more components that are tedious and ripe for commoditization. Same is true for the few business areas that an investment bank will be involved in.\n\nFinancial services by volume has several interesting occupations, least of which is trading. However, if you look at the complex, though logical and information rich environment,s what exactly is defensible for us humans that we can actually hold on to? Are we of the opinion that a human is required to execute a complex hedging strategy? Are we wrong, and if so by how wide a margin?\n\nAI and machine learning are still clear hype terms, but the practical implications with modern processing power continuously increasing is phenomenal and unprecedented. In raising these points in conversations with more traditional firms in financial services or market participants, a common dismissal is \u201cWe\u2019ve been using the basics, a phone and a relationship for decades. It worked then, it works now\u201d and debunks such as \u201cI\u2019m too old for those things\u201d. Not only do these represent demeaning way of looking at some of the most promising technological advancements, arguably they represent a deep misunderstanding of just how fundamental this change in landscape is.\n\nBy the way, none of this spells the end of human capacity, human creativity or human intelligence. Quite the contrary. But we\u2019ve yet to see any plausible and defensible logical argument just why this human brilliance should be spent barking orders to another human that enters numbers into a sheet. We should be focusing our minds on areas where we actually add value, not areas where we think we do.\n\nNone of the current market applications in digital finance make use of real AI, despite what hype around robo advisor services may imply. But in the next wave of applications, we are going to be seeing new models unfold that will dwarf our imagination and displace tens of thousands of professionals into new capacities and capabilities. Like all change it will be uncomfortable and fought against, but try arguing the contrary point. Can you justify all the roles humans play in financial markets? Why are we relevant?"},
{"url": "https://www.1843magazine.com/features/the-luxury-of-tears", "link_title": "People in richer societies cry more", "sentiment": 0.09137708607817306, "text": "When he became a father, Charles Darwin began taking notes on the emotional development of his children. Such record-making was part of the rhythm of the household. He logged the weather, his farts and sneezes, and the behaviour of the earthworms he kept in a jar on the piano. His offspring were too compelling a source of data to ignore.\n\nWillie was his first-born. Darwin tickled his feet with a spill of paper and watched for laughter. Annie arrived 14 months later. Darwin observed the moment when she first responded to her own reflection in the polished case of his fob watch; and her consternation when a wafer biscuit became stuck to her hand. It was the crying, though, that most aroused his curiosity. Darwin kept a careful record of these outbursts, noting when eyes were dry, when filled with tears \u2013 and concluded that though we may wail from the moment we emerge from the womb, it takes time to develop the facility for weeping. \u201cI first noticed this fact\u201d, he wrote, \u201cfrom having accidentally brushed with the cuff of my coat the open eye of one of my infants, when 77 days old, causing this eye to water freely; and though the child screamed violently, the other eye remained dry, or was only slightly suffused with tears.\u201d Crying, he decided, \u201crequired some practice\u201d.\n\nDarwin collected most of these data in the 1840s, when his children were young. It was the calmest decade of his life. The voyage of the Beagle was behind him; he had settled with his wife, Emma, in Down House, a comfortable villa in rural Kent. The reading public was devouring the published account of his South American travels, which detailed his tortoise-steak dinners on the Galapagos, his excavation of a fossilised giant sloth and his reflections on the human specimens he encountered on the way. On his desk lay the notes for \u201cOn the Origin of Species\u201d. When it appeared in 1859, the book erupted like a cultural Krakatoa. Over 150 years later, we are still living through its aftershock. It looms so large that we are inclined to forget its author\u2019s other published work. Most of all, perhaps, the book nourished by his domestic explorations in Kent, and which, through more subtle channels, also exerted a profound effect upon the future.\n\n\u201cThe Expression of the Emotions in Man and Animals\u201d (1872) compared the emotional subjects with whom Darwin shared his home with ones he had encountered on his travels \u2013 and hundreds more he would never meet. In a spectacular example of Victorian crowdsourcing, he fired off hundreds of letters and questionnaires to correspondents all over the world. He implored a biologist in Brazil to tell him whether South American monkeys wrinkled their eyes \u201cwhen they cry from grief or pain\u201d. From a phalanx of missionaries and doctors he drew reports on the weeping habits of the Australian Aboriginals. James Brooke, the Rajah of Sarawak, supplied emotional intelligence on the Dayaks of Borneo. Tristram Speedy, guardian to Prince Alamayu Simeon of Abyssinia, gave a long-distance lesson in east African passions.\n\nFrom their data, Darwin mined a number of influential conclusions. Emotions, he suggested, were facilitated by the act of expressing them. We don\u2019t cry because we are upset, rather the act of crying informs us that we are upset. In our neuroscientific age, when we\u2019re apt to regard a small firing in the brain as the first stage of all human processes, the proposal may seem bizarre, but it is not quite abandoned \u2013 a Japanese study from\u00a02007 drizzled its subjects with artificial tears and found, as Darwin would have expected, that many experienced feelings of sadness.\n\nDarwin\u2019s most tenacious idea, however, was a cultural one. A correspondent in New Zealand had told him the story of a Maori chief who \u201ccried like a child because the sailors spoilt his favourite cloak by powdering it with flour\u201d. He had observed similar behaviour from the deck of the Beagle, notably that of a Fuegian man, recently bereaved, who \u201calternately cried with hysterical violence, and laughed heartily at anything which amused him\u201d. Civilisation, he reasoned, had bred emotional temperance, and humans who lived beyond its borders were subject to fits of passion. \u201cSavages weep copiously from very slight causes,\u201d he concluded. \u201cEnglishmen rarely cry, except under the pressure of the acutest grief.\u201d\n\nToday, the remark sounds ludicrous. Darwin, though, did not write from a position of ignorance. He knew the pressure of the acutest grief. In 1851, his beloved daughter Annie sank from the world under the weight of tuberculosis. She was ten. (\u201cWe have lost the joy of the Household\u201d, wrote her father, \u201cand the solace of our old age.\u201d) And those savages? The language now offends, but the assumption it carries \u2013 that the inhabitants of rich Western nations shed fewer tears than citizens of the developing world \u2013 held firm until the beginning of the present decade.\n\nWhen I first visited Down House 20 years ago, I was ready to be moved. I was the sort of boy who fished in drains for sticklebacks and kept his fingers crossed during the Lord\u2019s Prayer. Darwin was my childhood hero. Standing in the modest little space of his study, it was easy to imagine the great man feeding slides into the microscope, squinting at his water-damaged notebooks, furrowing his appropriately simian brow. Easy, too, to feel moved to tears. Muslims weep before the Ka\u2019aba, Jews at the Wailing Wall. In Darwin\u2019s workroom, atheists can shed hot rational tears where a doubt-wracked Victorian naturalist sat down to revise the relationship between humanity and the universe.\n\nThe tears that swam in my eyes, however, were produced by something much more mundane. A three-legged stool \u2013 a low, plain, unremarkable thing mounted on brass castors so that its owner could scoot between his writing desk and his work-table without breaking the line of his thought. Its wooden sides were lined with scuff-marks, as if someone had dragged it hard against the wall. This, the guide explained, is exactly what had happened. Darwin rarely worked past lunchtime, giving over the rest of the day to activities with his family. Once the morning\u2019s studies were concluded, he surrendered the three-legged stool to his children, who punted it up and down the hallway, pretending that it was a boat. The Beagle, perhaps. This is the image that stirred my emotions \u2013 Charles Darwin, genius and really good dad.\n\nIn the context of \u201cThe Expression of the Emotions in Man and Animals\u201d, though, my response seemed an affront to Darwinism, brought to the very room in which its ideas were formulated. I hadn\u2019t cried from grief, acute or otherwise, but from a sentimental sense of connection with a great thinker whom I admired. Darwin\u2019s writings, however, seemed not to accommodate the possibility of such an emotion. Perhaps, in 1872, people just felt things differently.\n\nWe sound different from our ancestors. We wear different clothes, observe different philosophies, follow different ideas. Could certain ways of feeling have vanished along with Mother Bailey\u2019s Quieting Syrup and Capstan Filters, yielding to fresh moods and senses? A new generation of scholars working on the history of the emotions believes passionately that this is the case, and wants us to see our feelings not simply as what happens when a neurological circuit lights up in our brains, but as the products of bigger cultural and historical processes. Their first contention: the very idea of the emotions is a surprisingly young one.\n\n\u201cThe concept arrived from France in the early 19th century as a way of thinking about the body as a thing of reflexes and twitches, tears and shivers and trembles, that supplanted an older, more theological way of thinking,\u201d says Tiffany Watt Smith, once a director at the Royal Court Theatre, now a researcher at the Centre for the History of the Emotions at Queen Mary University of London. Before the discourse of the emotions took hold, she argues, people spoke of other phenomena \u2013 \u201cpassions\u201d, \u201cmoral sentiments\u201d, \u201caccidents of the soul\u201d \u2013 that were not always located within the human body. Ill winds blew no good upon the ancient Greeks, carrying flurries of unhappiness through the atmosphere. Fourth-century Christian hermits were plagued by acedia, a form of religious despair spread by demons that patrolled the desert between 11am and 4pm. Non-human organisms could also be afflicted by passions: in the Renaissance, palm trees became lovesick and horticulturists brokered arboreal marriages by entwining the leaves of proximate specimens.\n\nWatt Smith has compiled \u201cThe Book of Human Emotions\u201d, in which she gives the histories of 156 human feelings. Many of these you will have experienced \u2013 popular headline acts such as guilt, indignation and apathy. Others seem familiar, despite the exotic names \u2013 such as basorexia, the sudden urge to kiss someone; or matutolypea, the ill-temper that flourishes between the alarm-clock and the day\u2019s first cup of coffee. For Anglophone readers, some of her subjects are mysteries locked behind the door of someone else\u2019s culture. Amae, a Japanese term that describes the comfort felt when you surrender, temporarily, to the care and authority of a loved one. Liget, an angry enthusiasm that buzzes in the Ilongot tribe of the Philippines, pushing them to great feats of activity \u2013 sometimes agricultural, sometimes murderous. Awumbuk, a feeling of emptiness after visitors have departed, is experienced by the Baining people of Papua New Guinea. Departing guests, they theorise, leave behind a kind of heaviness as they go. (A bowl of water left out overnight absorbs this force.)\n\nPerhaps the most revealing entries are those on emotions that remain felt, but which have been substantially reconstructed. In the early 19th century, for instance, nostalgia was considered a terminal condition. Men in their 20s were thought particularly susceptible. During the American civil war, doctors scribbled the word on dozens of death certificates. In the 1830s, French medical authorities warned that excessive attachment to lost people and lost places could reduce the sufferer to a state of decrepitude. \u201cLittle by little his features become drawn; his face is creased with wrinkles; his hair falls out, his body is emaciated, his legs tremble under him; a slow fever saps his strength\u2026his discourse becomes incoherent; his fever becomes ever greater, and soon he succumbs.\u201d\n\nBy the middle of the 19th century, it had fallen from the diagnostic repertoire. Technology, doctors asserted, had cured it. If you pined for the scenes of youth, you could get on a train and spend an afternoon running about in them. If you were sick with the melancholy remembrance of your childhood nurse, you could send her a telegram enquiring after her health. \u201cHappily,\u201d reflected one specialist, \u201cnostalgia diminishes day by day; by descending little by little among the masses, instruction will develop the intelligence of people, making them more and more capable of struggling against the disease.\u201d\n\nFor most of us, that struggle is over \u2013 so over, in fact, that we consider nostalgia a pleasure, and cultivate the condition without suffering anything worse than a hefty bill from eBay. Science, too, has changed its tune. In 2012, an American study measured the therapeutic value of the feeling: \u201cNostalgia makes people feel loved and valued and increases perceptions of social support when people are lonely.\u201d The following year, British researchers found something even more tangible: \u201con a physical level\u201d, their leader reported, \u201cnostalgia literally makes us feel warmer.\u201d\n\nThe Centre for the History of the Emotions is at the heart of London\u2019s East End, on a campus that enfolds one of Britain\u2019s oldest Jewish cemeteries. Its academics share their findings on a collective blog. Some work on despair, others disgust. One is considering shame and anger in early-modern Spain. Another is examining Busman\u2019s Stomach, a gastric disorder affecting bus drivers in the 1930s, ascribed to carbon-monoxide fumes but probably psychosomatic. (The Marxist biologist J.B.S. Haldane prescribed reading Lenin as a cure.) A founder member, Jules Evans, has found an unexpected enthusiasm in Korea for the ideas of the Greek Stoics, who treated emotions like beliefs or opinions that could be revised. He is now engaged in a global survey of religious ecstasy, which includes a study of the alarmingly transmissible passions of Islamic State and al-Qaeda. Queen Mary\u2019s researchers enjoy happy relations with a small but thriving coalition of similar institutions \u2013 the Max Planck Institute in Berlin, the Nordic Network for Intimacy Research, and the Australian Research Council\u2019s Centre of Excellence for the History of Emotions, the biggest and most lavishly funded, with sites in Adelaide, Brisbane, Melbourne, Perth and Sydney.\n\nThe director of the London Centre, Thomas Dixon, seems a man in excellent emotional health \u2013 friendly, full of infectious enthusiasm for his work, and, when I arrive, waiting at the door of his building with milky tea and two kinds of cake. He ought to look content. His last publication, \u201cWeeping Britannia\u201d, a critical history of British emotional restraint, was one of the most lauded history books of 2015. Its thesis is excitingly revisionist. It takes that most familiar of emotional concepts \u2013 the British stiff upper lip \u2013 and reveals that it was a historical blip. The phrase, it seems, was coined in America, and only became fully associated with Britain during the first half of the 20th century, as an increasingly militarised and imperial national culture absorbed the shock of global conflict. Britain before this period is, he suggests, better characterised as a wet-cheeked, passionate nation in which tears enjoyed an elevated status. Politicians advanced their arguments by shedding parliamentary tears. Monarchs wept to demonstrate their sincerity. Fans of Henry Mackenzie\u2019s influential novel \u201cThe Man of Feeling\u201d (1771) read how its gentleman-hero offered his \u201ctribute of tears\u201d to beggars, orphans and prostitutes, and they attempted to emulate his sensitivity. \u201cWeeping\u201d, Dixon writes, \u201cwas a moral and religious activity; something to be cultivated, tutored, practised, learned, performed.\u201d It is a history of which Darwin seems to have known little.\n\nIn a poem reflecting on the traumas of the French revolution, William Blake asserted that \u201cthe tear is an intellectual thing\u201d. This has become a mantra for Dixon and his colleagues. Blake\u2019s line suggests that weeping is not where thought ends \u2013 a state to which we are reduced once more complex processes have broken down \u2013 but evidence that we have computed a mass of cognitive data about the world around us and come to a considered conclusion. It\u2019s a challenge to the language of continence and incontinence that attends our conversations about emotion \u2013 our tendency to apologise for our tears as we might apologise for losing control of our bladders. \u201cWhen our bodies respond emotionally\u201d, says Dixon, \u201cwe are thinking with our bodies\u201d. The history of the emotions, therefore, is like the history of ideas.\n\nWhen Dixon began his work on weeping, he put Charles Darwin\u2019s study of Willie, Annie and their siblings at the top of his reading list. He even intended to follow Darwin\u2019s example and use his son as an experimental subject \u2013 but the stresses of early parenthood soon put paid to that. Perhaps it was just as well. In 2011, experimental evidence emerged that disrupted the assumptions of \u201cThe Expression of the Emotions in Man and Animals\u201d and supported the deeper historical evidence that Dixon was accumulating.\n\nIn the 20th century, most research followed the path hacked by Darwin. In 1906, the American psychologist Alvin Borgquist considered the fruits of his own global survey of explorers and missionaries, and asserted that \u201ctears are more frequently shed among the lower races of mankind than among civilised people.\u201d Borquist\u2019s terminology may not have survived the 20th century, but the assumption of Western emotional dryness proved extraordinarily durable. A narrative built up across the disciplines. The Dutch historian Johan Huizinga published \u201cThe Waning of the Middle Ages\u201d (1919), which tracked the disappearance of the wild, carnivalesque side of medieval life \u2013 the kind you see in Bruegel paintings in which masked grotesques caper around Flemish squares. The German sociologist Norbert Elias published \u201cThe Civilising Process\u201d (1939), which described the growth of a culture of politeness \u2013 a new empire of the cooler emotions that colonised and pacified older, more chaotic ways of being. (A key event in the Elias timeline is the 16th-century debut of the fork.)\n\nThe modern era was seemingly content with this narrative. Then, in 2011, a team of Dutch clinical psychologists produced a study that consigned it to the out-tray of history. Ad Vingerhoets and his team examined data from 37 countries \u2013 the results of interviews in which respondents had told stories of their lachrymal lives. Their conclusions would have brought tears to Darwin\u2019s eyes. \u201cIndividuals living in more affluent, democratic, extroverted, and individualistic countries,\u201d they wrote, \u201ctend to report to cry more often.\u201d Although people enduring unenviable economic circumstances might be more plagued by depression, those from richer cultures shed more tears. Australasian and American men emerged as the weepiest in the world; their Nigerian, Bulgarian and Malaysian counterparts the most dry eyed. Women in Sweden outcried those in Ghana and Nepal. The female populations of countries where gender equality was highest wept more copiously than those where it was lower. The evidence also showed \u2013 contrary to centuries of stereotyping \u2013 that the inhabitants of colder climates wept more frequently than those who lived in warmer zones. Tears, the study suggested, were not evidence of primitivism, as they had been for Darwin. They were not even good indicators of distress. Rather than being the habit of the wretched of the Earth, weeping appeared to be an indicator of privilege \u2013 a membership perk enjoyed in some of the world\u2019s most comfortable and liveable societies. \u201cIf you live in really distressing and difficult circumstances, crying is a luxury,\u201d says Dixon. \u201cWe know when we have been bereaved, we might be so shocked or traumatised that tears don\u2019t come. So perhaps we should see tears as a sign of moderate grief, of bearable negative emotion. If you are enduring extreme distress or extreme hardship, that is not the time for tears.\u201d\n\nIn countries visited by war or famine, the observation might not seem so counterintuitive. Dorte Jessen, head of the Jordan arm of the World Food Programme\u2019s response to the Syrian refugee crisis, has spent over a decade looking into the tearless eyes of those in the direst need. During the 2011 famine in the Horn of Africa, she was based in the sprawling refugee camp in Dadaab, Kenya, 60 miles from the border with Somalia. Early in her assignment, she recalls, she watched a mother and her two young children receiving emergency rations \u2013 sachets containing a sweet mixture of peanut paste, vegetable fat and cocoa. Just a few steps from the distribution point, the mother ripped open one of the packets and handed it to her oldest child. \u201cThey didn\u2019t talk or express any emotion. They just kept walking,\u201d Jessen notes. \u201cOnce you are past a certain point of exhaustion, there is simply no energy to spare to get emotional.\u201d\n\nIn 1890, the philosopher William James drew a distinction between the \u201ccrying fit\u201d \u2013 a psychological event accompanied by \u201ca certain pungent pleasure\u201d \u2013 and the much less bearable sensation of \u201cdry and shrunken sorrow\u201d. Some experiences, it seems, are too bleak for tears. Former inmates of Nazi concentration camps have reported, sometimes guiltily, that they did not weep during their ordeal. At a war-crimes trial in May 2015, Susan Pollock, a Hungarian Holocaust survivor, recalled her dry eyes as she watched her mother being despatched to the gas chamber. \u201cI wasn\u2019t crying,\u201d she said. \u201cI just wanted to recede into myself, never to be seen.\u201d\n\nWe might imagine that on the flood map of world history, certain infamous fields in eastern Europe would register as brackish lakes. But the greater volume of those tears, I suspect, would have been shed not by the victims but by those who came as an act of remembrance, once the furnaces had cooled. Pollock considered herself temporarily dehumanised by her life in the concentration camp. But in an Auschwitz or a Treblinka, what would be the purpose of tears? They would be as superfluous as a pair of diamond earrings.\n\nYou don\u2019t have to invoke Holocaust and famine to discern the luxurious nature of weeping, or the existence of an economy of emotion in which some are privileged to demonstrate their feelings and others are not. It\u2019s there on the literary and historical record, legible in the scorn that Odysseus feels for his son Telemachus until his boy has given him a big Greek man-hug and produced proper princely tears; in the response of the 19th-century French doctors who were baffled to observe the nervous, weepy symptoms of hysteria \u2013 the malady of refined females \u2013 in muscular working-class railway workers who had been injured on the tracks. It\u2019s there, too, in contemporary culture. If politicians cry, the value of their tears is assessed by commentators who act like jewellers, squinting for signs of fakery and paste. When, in the first week of this year, Barack Obama wept as he urged Congress to support tighter gun controls, the cameras caught every scintilla of his emotion, their clicks cascading like rain on the White House roof. Sympathetic observers celebrated these as moral tears, saltwater proof of the truth of his arguments. His enemies went on Twitter and Fox News to make allegations about onion juice on his fingers.\n\nStatesmen and -women occupy a professional field in which the expression of emotion is permissible. For others, tears are unaffordable. The medical dispatchers who answer emergency calls receive training to help them discount their emotions as they advise those who have swallowed pills, or whose babies have stopped breathing. We would consider it a dereliction of duty if surgeons, nurses, police officers and soldiers wept during working hours. They have surrendered their right to cry in the same way that other employees might sign away their expectation of fixed hours or sick pay. Their restraint gives us the space to express our pain or gratitude, which we buy from them through taxation.\n\nAs the world develops, so do its passions \u2013 often in ways that are not immediately comprehensible to its inhabitants. Social media are generating new rituals of anger and indignation. Reports of weeping icons have become bullets in the propaganda war between Ukraine and Russia. In Japan, an activity called rui-katsu (tear-seeking) has developed, in which customers gather to watch weepies or pay for a wet-eyed escort to come to the workplace to embrace them. In China, older people bristle at the new emotional culture that they perceive leaking in from the West \u2013 exemplified by the moment in 2010 when a contestant on the reality show \u201cFei Cheng Wu Rao\u201d asked a prospective partner if she\u2019d be happy to go out for a date on a bicycle. \u201cI\u2019d rather cry in a ,\u201d was the reply. It was an off-the-cuff remark, but seemed to articulate something about the future of China \u2013 and, therefore, the future of the world.\n\nThe history of the emotions is a young discipline. It is at the very beginning of its investigation into the long story of our feelings. \u201cAre we\u201d, asks Thomas Dixon, \u201cwriting the history of something that has always been the same fundamentally in the human mind being expressed and interpreted in different ways? Or are we, as most of us who do it would think, discovering the historicity of the human mind? Discovering that you can\u2019t feel just any old way; you only feel the way you do because of your language, your experience, your family, your upbringing, your social institutions, your political institutions.\u201d If we could acquire that perspective upon ourselves, how would it change us? Who would we become, if we could chart the flow of oceans of tears, measure the dry breadth of those famine lands, experience our feelings as events in history as well as in our bodies? People, hopefully, who might look back upon these times, their scuffed old artefacts and antique ideas, and shed a generous and sentimental tear."},
{"url": "http://blog.shaunfinglas.co.uk/2016/05/your-job-isnt-to-write-code.html", "link_title": "Your Job Isn't to Write Code", "sentiment": 0.12726265982079937, "text": "Solving problems is the role of software developers first and foremost. The most interesting aspect is that in many cases it is possible to perform this role without writing a single line of code.\n\nI once worked with a digital dashboard which monitored applications. One of the yet to be implemented features was a key to highlight which each chart related to. During this period many employees would ask which graph related to which feature. The solution was a few weeks a way so as a temporary fix I stuck a post it note to the screen. This was by no means the solution, but it was good enough for the time being. The questions went away and eventually the dash was updated to include a digital version. Total lines of code? Zero.\n\nA common experience that many developers encounter is solving a problem while not actually at the computer, programming. In fact this technique of simply taking a break such as going for a walk can yield some impressive results. One of my fondest memories of this trick was using shampoo in the shower to walk through a buggy A* implementation using the bathroom tiles. After returning to the task sometime after, the stupid mistake stood out. Lines of code to figure out the fix? Zero.\n\nJust the other week I began furiously updating an existing application to change how a core feature worked. The solution was not going to be quick, but it seemed like a good idea. About halfway in I reverted the changes. After further thought it turns out there was a much better solution. One that would not introduce risk to the current project's goals. Total lines of code? Minus one hundred, give or take.\n\nThis lack of code is not a bad thing. In all three examples the goal was complete. You can solve problems with a single line, or thousands, it actually does not matter. If you switch your thinking to focus on completing goals or hitting targets, you are still rewarded with a feeling of accomplishment. The slack time you gain can simply be redirected to other areas or personal improvement.\n\nMany wise developers have said this before. The role of a software developer is to solve problems, not write code. This is not new, unfortunately a younger, naive version of myself ignored this advice."},
{"url": "http://www.irrlicht3d.org/pivot/entry.php?id=1504", "link_title": "The Android SDK Manager is now blatantly lying", "sentiment": 0.02, "text": "a4z (Book Sale Statist\u2026): it's a pity that you publ\u2026\n\nhalan (HearthStone and N\u2026): HS fan, it looks like you\u2026\n\nHS fan (HearthStone and N\u2026): So wait, you find a card \u2026\n\nmarcos (HearthStone and N\u2026): they do this to mobile ga\u2026\n\nShane (HearthStone and N\u2026): I agree. Much of the gami\u2026\n\nniko (Dynamic Day-Night\u2026): Well, you can change the \u2026\n\nTazo (Dynamic Day-Night\u2026): Visually, I would say tha\u2026\n\nxaos (Updated eBook pri\u2026): That is indeed an appreci\u2026\n\nniko (Book Sale Statist\u2026): I'm not allowed to. But I\u2026\n\nKai (Book Sale Statist\u2026): I bought your book too a \u2026\n\n"},
{"url": "https://medium.com/@cyware/why-is-cyber-security-so-important-1ecdc78ecd7d#.nmmtgigz9", "link_title": "Why is cybersecurity so important?", "sentiment": 0.16584770114942526, "text": "Cyber security news have claimed its spot on the front pages in the recent weeks. The importance of \u2018Cyber security Awareness\u2019 is looming over everyone\u2019s head like an \u2018elephant in the room\u2019 which have to be addressed. The rate of cyber crimes is increasing in a rapid manner and the attackers are compromising private and public sector alike, including sensitive industries like health and banking.\n\nWe are hacking our daily life with the help of the Internet by using it for many actions like communication, entertainment, shopping and ultimately to gain knowledge. Along with the comforts and convenience, it also attracts many dangerous actors to your comfort zone. Since the incorporation of services like money transaction, the \u2018cyberspace\u2019 has become a very easy spot for criminals. The cyber crimes are getting more and more sophisticated each day.\n\nWe have been exposed to many types cyber-threats after the evolution of the Internet. There are many \u2018potential dangers\u2019 involved in almost all activities we do in the Internet. Internet\u2019s massive size and infrastructure is home for many dangerous actors and malicious programs. Identifying the potential vulnerabilities and acting accordingly is the only way to stay safe online. In order to identify the potential dangers in the web, one must stay tuned with the latest news and updates from the cybersecurity industry.\n\nCyber criminals are evolving at a very aggressive pace, they are innovating and using versatile modes of attack every day. And when it comes to cybersecurity, criminals always have the upper hand. They get the advantage of being anonymous and spontaneous, which makes cyber attacks very dangerous. Our \u2018cyber-behavior\u2019 has become a very key aspect of our daily life. It\u2019s our responsibility to adopt a good \u2018cyber-hygiene\u2019, because cyber-attacks have the potential to change our life forever. Your last \u2018Facebook post\u2019 can lead you to a potential danger, and that\u2019s just one scenario. With the smartphone evolution, the attack vector of cyber-crimes has increased to a very dangerous level. Today, everyone carries a \u2018potential target\u2019 in their pockets. And we depend on smartphones for many personal and professional activities. The cybercriminals are changing their tactics to more \u2018targeted attacks\u2019 and nobody is free from the threats of cybercrimes.\n\nCyber security awareness has become a basic requirement these days, especially with the increasing dependency of the human race to cyberspace. We buy groceries online, we book tickets online, we transfer money online and the list is on and on. Staying safe in the Internet is a skill, a skill which everyone needs to learn because prevention is way better than cure. Cyber security is considered as the responsibility of information security experts but we have reached a stage where cyber security has become everyone\u2019s responsibility. Educating yourselves with the latest changes in the cyber security and getting familiar with the cyber security industry practices are very important to your security.\n\nCyber security awareness is not an easy task. In fact, Cyber security is a massive field which includes many branches like \u2018Threat detection\u2019, \u2018Network security\u2019, etc. To conceive the knowledge to understand and respond to the cyber-threats is very tricky and involves a lot of hard work. That\u2019s one of the main challenges in bringing cyber security awareness to the public. There is no benchmark approach to cyber security awareness and there are no shortcuts. It\u2019s a continuous process of learning and understanding.\n\nLet us be cyber aware!"},
{"url": "http://arstechnica.com/science/2016/05/antibiotics-side-effects-include-immune-disease-and-fewer-brain-cells/", "link_title": "Antibiotics\u2019 side-effects include immune disease and fewer brain cells", "sentiment": 0.06999291860402972, "text": "In some situations, antibiotics are lifesavers. In others, however, they do more harm than good. For instance, when antibiotics are used too much or for the wrong illnesses, the drugs only end up killing helpful microbes and spawning drug-resistant superbugs. To figure out the proper times to use antibiotics, doctors need to carefully weigh the risks and benefits of each situation. But, sadly, that calculation is extremely tricky\u2014if not impossible\u2014because scientists still aren\u2019t sure what all of the risks are.\n\nWith two new studies, researchers added to the tally. In general, both studies found that when antibiotics kill off microbes in the gut, the immune system gets thrown out of balance and can cause unexpected health problems. In one of the studies, certain types of antibiotics appeared to spur an inflammatory condition in humans that can sabotage life-saving transplants. In the second study, a long course of antibiotics seemed to stymy the birth of brain cells in adult mice, which led to memory problems.\n\nWhile the studies focus on disparate treatment situations, the studies both serve to highlight the unexpected risks of blasting the body\u2019s complex microbial communities\u2014and how careful doctors should be when using weapons of mass microbial destruction, such as antibiotics.\n\nIn the first study, researchers analyzed the records of 857 patients who received hematopoietic stem cell transplants, a treatment generally used for blood and bone marrow cancers. Antibiotics are often given to prevent or treat infections that can arise from the transplant, but doctors select from a wide variety of types of antibiotics to give their patients. The researchers picked out 12 of the most common types of antibiotics used and looked to see if patient health varied depending on which antibiotic they took. They did.\n\nTwo drug combinations in particular\u2014a regimen of piperacillin and tazobactam as well as a combo of imipenem and cilastatin\u2014linked with patients having a higher risk of developing graft-versus-host disease (GVHD), a life-threatening inflammatory condition in which the transplanted cells treat the recipient\u2019s body as a foreign enemy and mount an attack.\n\nThose antibiotic combinations are considered \u201cbroad spectrum,\u201d meaning they can massacre many different types of microbes, particularly helpful anaerobic microbes. Studying the patients\u2019 fecal samples from before and after the transplant, the researchers noted that the two antibiotic treatments led to significant disruptions in the patients\u2019 gut microbiomes.\n\nIn follow-up mouse studies, the researchers recreated the results with the two antibiotics. They also noted that the drugs thinned the mucosal barrier in the rodents\u2019 colons, lowered the number of certain immune cells, and set the stage for intestinal inflammation seen during GVHD.\n\nWhile the authors don\u2019t prove causation and would need to back up the results in further trials, they note that their results \u201csuggest that selecting antibiotics with a more limited spectrum of activity (especially against anaerobes) could prevent microbiota injury and thus reduce GVHD in the colon and GVHD-associated mortality.\u201d The study is published in the journal of Science Translational Medicine.\n\nIn the second study in Cell Reports, researchers sprinkled broad spectrum antibiotics into the drinking water of mice for seven weeks. Then, based on previously-established connections between the gut, the immune system, and the brain, the researchers looked for changes in brain cell development the rodent\u2019s noggins. In the hippocampus, which plays an important role in memory, there was a slow down in brain cell production in drugged mice compared to controls. And those drugged mice performed relatively poorly on memory tests.\n\nWhen the researchers looked closer, they noted that the antibiotic-treated mice had fewer Ly6Chi monocytes\u2014white blood cells recruited to inflamed tissue\u2014in their bone marrow, blood, and brains. When researchers looked at mice with low levels of Ly6Chi monocytes that hadn\u2019t been treated with antibiotics, they saw the same drop in brain cell production. And when the researchers added the cells back to the antibiotic-treated mice, they saw the rodent\u2019s brains rebound.\u00a0In cell experiments, the researchers noted that the monocytes can help brain cells develop.\n\nInterestingly, mouse minds damaged by antibiotics could also be saved by exercise and drinking a cocktail of helpful bacteria. The findings, while only in mice and await validation in further trials, suggest so far that antibiotics may take a significant toll on the brain. The study also hints that future strategies to manipulate the microbiome could improve brain function."},
{"url": "http://www.dw.com/en/china-to-build-sudans-first-nuclear-reactor/a-19279015", "link_title": "China to build nuclear reactor in Sudan", "sentiment": 0.14485294117647057, "text": "Chinese media confirmed Tuesday the Asian country had signed a framework agreement with Sudan to build the African nation's first nuclear reactor.\n\nThe official Chinese news agency Xinhua reported the deal had been signed during a three-day visit to Sudan by China's top energy official, Nur Bekri.\n\nThe report did not specify what kind of reactor would be built in Sudan, but Reuters speculated it might be a Hualong 1 as currently promoted by China on overseas markets.\n\nBloomberg said the reactor would be built by China National Nuclear Corp. (CNNC). With its current power capacity of little over 3,000 megawatts, nuclear energy is set to help Sudan meet its growing demand.\n\nCNNC is also planning to build reactors in Argentina and Pakistan, while China General Nuclear Power Corp. (CGN) is focusing on its own project in Kenya.\n\nChina aims to become a globally dominant player in the nuclear sector, using its ambitious domestic reactor program to develop the necessary experience and expertise."},
{"url": "http://www.cocoawithlove.com/blog/2016/05/19/random-numbers.html", "link_title": "Random number generators in Swift", "sentiment": 0.026372212237167608, "text": "What\u2019s the best general purpose random number generating algorithm available?\n\nIn this article I\u2019ll present a protocol and use it to implement 8 different random number generating algorithms. Implementations will include wrappers around Mac/iOS built-in algorithms, my own implementations in Swift of some popular algorithms and some corresponding C implementations for comparison. The implementations of the protocol all seed from /dev/urandom by default, can generate data of arbitrary size (although I\u2019ll focus on 64-bit integer generation) and offer conversion to (preserving up to 52-bits of randomness in the significand).\n\nAs an aside, my Swift implementation of the Mersenne Twister ended up 20% faster than the official mt19937-64.c implementation. Curious to understand what I had done, I ended up \u201cfixing\u201d the C version to be just as fast as the Swift version. Yes, it\u2019s true: with a little tuning, C can be just as fast as Swift.\n\nWelcome to C with love.\n\nMy primary use for random numbers is in fuzz testing; deliberately sending mixed and garbled inputs to my functions (to look for data handling errors) or running large numbers of threads with different timing offsets and data sizes (to look for timing or thread-safety bugs).\n\nIt\u2019s difficult to know exactly what performance or quality I require from random numbers in this scenario \u2013 most \u201cgood quality\u201d options would probably suffice \u2013 but what I do require is:\n\nHistorically, I\u2019ve used a C implementation of the Mersenne Twister. I don\u2019t have any particular problems with it but the algorithm is nearing its 20th birthday so I was curious to see what else was around.\n\nThe C standard library on the Mac and iOS contains a few different functions for random number generation:\n\nIf you need cryptographically secure random numbers, the only option you should consider is /dev/[u]random.\n\nOn Mac and iOS, both /dev/random and /dev/urandom are identical and use the Yarrow algorithm in conjunction with bits accumulated from hardware entropy sources. The existence of two different names is largely for cross-compatibility with Linux where the different devices have historically had a complicated range of distinct security considerations with general advice leaning towards /dev/urandom. I\u2019ve used /dev/urandom to minimize portability problems.\n\nThe problem is that reading from /dev/urandom is slow. In my testing, it is between 100 and 1000 times slower than other generators and uses additional system resources on top of the userspace resources of typical random number generators.\n\nThe end result is that /dev/urandom, while useful for setting initial values, is a poor choice for a general-use random number generator.\n\nThe , and the various functions are all variations of linear congruential generator. This means that invocation mutates the internal state according to the following equation:\n\nIn the case of and , a is 16807, c is 0 and m is . Looks pretty simple, doesn\u2019t it? It is. These algorithms don\u2019t give a good distribution, don\u2019t have a very long period and can get worse with poor seed choices.\n\nEven ignoring quality considerations, I consider all of these functions \u2013 except - to be useless due to the following problems:\n\nEven with , the algorithm needs to be run twice to generate a single 64-bit integer. This causes it to be roughly half the speed it should ideally be.\n\nThe standard \u201cgood\u201d random number generator I see used in Swift is . It is generally high quality but it is slow due to the use of large amounts of state, locks on the global data and periodic mixing of additional entropy from /dev/urandom.\n\nThe \u201carc4\u201d in the name is because the algorithm is \u201callegedly\u201d compatible with the RC4 algorithm developed by RSA Labs. Like the official RC4, was originally intended for use as a cryptographic random number generator. While the implementation in Mac/iOS doesn\u2019t suffer the same problems that made the RC4 implementation in WEP vulnerable to trivial attacks, the output of should be treated as no longer cryptographically secure.\n\nWhat does that mean?\n\nprovides good quality but is about 5 times slower than algorithms of equivalent quality. It uses global state and can\u2019t be directly seeded for debugging purposes or other situations requiring repeatability.\n\nI\u2019m going to look at some high quality, simple, fast random number generators, implement them in Swift and see how they compare.\n\nA linear-feedback shift register (LFSR) is just a series of shift and XOR operations. Wikipedia has a very clear animated GIF of the operation generating a random sequence from a 4-bit number.\n\nBy carefully selecting the feedback points, the shift amounts and combining multiple registers, you can get very long cycles, good distribution and low predicatability. Researcher Pierre L\u2019Ecuyer gives some tables of values for \u201cTausworthe\u201d style linear-feedback shift register generators in his paper Tables of Maximally-Equidistributed Combined LFSR Generators.\n\nIn the code I present as part of this article, I give two variants, and \u2013 with periods approximately equal to and respectively.\n\nThe Mersenne Twister was a huge advancement when it was introduced by M Matsumoto and T Nishimura in 1997 and it remains the random generator that newer non-cryptographic generators are compared against (the algorithm isn\u2019t cryptographic because you can observe just 624 values and from that point, predict the sequence).\n\nSo the Mersenne Twister fails the \u201cunpredictable\u201d test but it is well tested, has a good distribution, very long period and is fairly fast. But there are some caveats.\n\nIn a tight loop, the Mersenne Twister is within a factor of 2 of the fastest algorithms tested. However, the standard Mersenne Twister uses 2496 bytes of internal storage. That might not seem like a lot of space on a modern computer but it is big enough to put additional burdern on your L1 cache.\n\nOn the quality front: the Mersenne Twister has some known problems with entering \u201czero\u201d states (situations where it\u2019s internal state contains a large number of zeros and the generator get \u201cstuck\u201d).\n\nThe Well equidistributed long-period linear algorithm (WELL) comes from F. Panneton, P. L\u2019Ecuyer, and M. Matsumoto \u2013 two out of three of these names have already appeared in this article (it\u2019s a small academic cabal, I guess). Like the Mersenne Twister, it is based on linear recurrences modulo 2 over a finite binary field. The WELL algorithm handles poor seeds and states better than the Mersenne Twister (\u201cescaping states with a large number of zeros\u201d). It is otherwise similar quality to the Mersenne Twister and uses significantly less memory.\n\nSadly, only 32-bit variants of the algorithm exist. This means it needs to run twice to produce the 64-bit values that I\u2019m using for performance comparison. The end result is that the implementation I used is about twice as slow as equivalent quality algorithms. The much smaller 64 byte internal state of the WELL algorithm \u2013 versus the 2496 byte state of Mersenne Twister \u2013 might mean that the real difference is somewhat less, depending on your program, although it\u2019s difficult to tell.\n\nThere has been a battle in the last three years between Melissa O\u2019Neill (creator of PCG) and Sebastiano Vigna (creator of xorshift+ and xorshift*). Both authors have relied on automated statistical quality tests to experiment with different variations on themes to find heavily refined versions of their respective approaches \u2013 O\u2019Neill applying block ciphers on top of linear congruential generators and Vigna performing variations on xorshift.\n\nThe latest release from Sebastiano Vigna (in collaboration with David Blackman) is xoroshiro128plus \u2013 a specialized linear-feedback shift register. Xoroshiro claims to be the fastest algorithm to fair well on the TestU01 set of quality tests for random number generators and claims to provide a better statistical distribution on the PracRand tests than previous xorshift algorithms.\n\nWhile the period of the generator is fairly low ( ), it\u2019s easily high enough for any common purpose.\n\nI defined the following protocol and provided implementations of the protocol for each of the algorithms:\n\nThe advantage with these protocols: a generator need only implement or and all the remaining functions are automatically provided (although I\u2019ve implemented optimized versions of in all cases to ensure a fair comparison).\n\nAll algorithms were used to generate 100 million 64-bit values. CwlRandom.swift was statically linked with the testing bundle (rather than dynamically linked through the CwlUtils.framework) for performance reasons. The and implementations were implemented in the same file as the tests so the extra function call layer for these tests would be inlined away.\n\nThese are the timing results:\n\nOn my 2.67Ghz Nehalem Mac Pro, performance ranges from 300 million per second for to 813 thousand per second for .\n\nThese numbers show the Swift version of faster than but from run to run, they trade places. Any difference between them is within the margin of error of this test setup.\n\nSo the Swift implementation of MersenneTwister ended up 20% faster than the mt19937-64.c C implementation. Hooray, Swift is the fastest!\n\nThe Mersenne Twister is made up of two parts:\n\nThe \u201cxor-shift-mask\u201d has very little wiggle room. Here\u2019s the C implementation:\n\nThere is a minor performance issue with this code but there\u2019s no significant room for refactoring here. The real difference between my Swift code and the C is in the \u201ctwist\u201d steps.\n\nA simple Swift implementation of \u201ctwist\u201d would look like this:\n\nThis code would work but unfortunately, is not a fast operation and at 100 million per second speeds, the ternary conditional operator is also too slow (don\u2019t worry about the part, that\u2019s a constant and is optimized away). Avoiding these requires restructuring the loop so that they\u2019re not required.\n\nThe mt19937-64.c implementation breaks the loop apart into two halves and follows up with an epilogue to handle the final position:\n\nThis code contains no division or modulo (remember: the is a constant and optimized away) and no conditionals or ternary operators (except the loops themselves). But there\u2019s a weird array (which is at index 0 and at index 1) and worse: the is fully iterated multiple times since the and accesses each walk the values from the other loop.\n\nI took a different approach to optimize the \u201ctwist\u201d code for my implementation. My code walks both halves of the loop simultaneously, using two different indexes, offset by :\n\nThe epilogue needs to handle two indexes but the array is only traversed once, we\u2019re hitting half as many loop conditions and a simple multiply by or (optimized to bitwise arithmetic) is used to eliminate the ternary operator conditional.\n\nIf there were no other elements at play, this alone would improve performance 7% versus the mt19937-64.c implementation.\n\nBut there\u2019s another advantage: with these two iterations (using the index and the index) side-by-side in the same loop, the compiler can automatically optimize to SIMD instructions to give us an extra 10% boost.\n\nThere\u2019s another 3% performance difference but to understand that, we\u2019ll need to make the C and Swift code more similar.\n\nSwift is faster but it\u2019s not an apples-to-apples comparison. Is it possible to make a true comparison? Where both C and Swift follow the same logic?\n\nLet\u2019s try changing the C code to:\n\nAs expected, this brings the C to within 3% of Swift.\n\nSo what\u2019s the cause of the remaining difference?\n\nAt this point, the only differences are a few types in C that are in Swift. We need to move the C code to a more consistent 64-bit everywhere with instead of .\n\nThe remaining difference? The use of the postincrement operator I showed in the first line of the C \u201cxor-shift-mask\u201d code. We need to change:\n\nIt might seem like this shouldn\u2019t make any difference but it does affect the generated assembly and appears to result in a 1-2% difference.\n\nBoth the C and Swift are now the same. I don\u2019t just mean \u201cthey take the same time\u201d, I mean they are compiled to literally the same instructions.\n\nHooray, C is also the fastest!\n\nThe CwlRandom.swift file is fully self-contained so you can just copy the file, if that\u2019s all you need.\n\nOtherwise, the ReadMe.md file for the project contains detailed information on cloning the whole repository and adding the framework it produces to your own projects.\n\nIt looks like Xoroshiro is the best general purpose algorithm currently available. Low memory (just 128 bits of storage), extremely high performance (1.2 nanoseconds per 64-bit number, after subtracting baseline overheads) and very well distributed (beating other algorithms on a range of automated tests). Mersenne Twister might still be a better choice for highly conservative projects unwilling to switch to such a new algorithm, but the current generation of statistically tested algorithms brings a baseline of assurance from the outset that previous generations lacked.\n\nOf course, if you only need a few random numbers in your program and you don\u2019t really care about multithreading or repeatability then these alternative algorithms are unnecessary \u2013 there\u2019s no problem with the built-in or even reading from /dev/urandom (you could generate hundreds of numbers from /dev/urandom per second without overheads reaching 1%). However, the protocol I\u2019ve presented makes it easier to seed and generate 64-bit values or from these sources too.\n\nGetting C-level performance in Swift for numerical algorithms is quirky but not particularly difficult. If you limit yourself to value types (no classes or existentials), use unsafe pointers and tuples instead of arrays, use overflow discarding operators / / instead of normal / / , use or for your loops, then Swift and clang C will generally compile to identical instructions.\n\nIt\u2019s not as though C is maximally performant without a little contortion. Using types for indexes on 64-bit systems should be avoided and so should common idioms like inline use of the postincrement operator."},
{"url": "http://www.theage.com.au/victoria/gay-men-receive-apology-more-than-30-years-after-homosexuality-decriminalised-20160524-gp2m4o.html", "link_title": "An Australian parliament apologises for criminalising homosexuality", "sentiment": 0.04228869047619048, "text": "Peter McEwan was just 17 when the police caught him caressing\u00a0another man behind some bushes on Brighton beach in 1967.\n\nIt was enough for him to receive a criminal conviction while he was still finishing year 12.\n\nHe said police had written a confession for him and he agreed to sign it.\n\n\"I was an innocent boy. I trusted the police,\" he said.\n\nLater his story was splashed on the front page of The Truth newspaper.\n\nToday in parliament Premier Daniel Andrews apologised to Mr McEwan and other men who were convicted under homophobic laws that have now been scrapped.\n\n\"There was a time in our history when we turned thousands of ordinary young men into criminals,\" Mr Andrews said. \"And it was profoundly and unimaginably wrong.\"\n\nHe apologised for the laws the parliament had passed and the \"lives ruined\".\n\n\"It all started here. It will end here, too.\"\n\nMr Andrews raised the individual cases of several men affected by the\u00a0laws. He said he also learnt of two women convicted for offensive behaviour in the 1970s for holding hands on a tram.\n\n\"It's about time and it is right,\" My Guy said.\n\nMr McEwan sat in parliament's public gallery with some of his siblings to hear the apology. Other men who received criminal convictions or were charged with offences were also present alongside their supporters.\n\nThey sat quietly, some nodding gently or occasionally rubbing their eyes.\n\nPolice Chief Commissioner Graham Ashton was present and posed in photographs with the men on the steps of parliament before the apology.\n\nHuman Rights Law Centre director of advocacy Anna Brown said there were young gay and lesbian people around Australia struggling with their sexuality.\n\n\"This apology will help ease the shame and stigma felt by these young people who are finding their place in the world,\" she said.\n\nJustice Connect manager of project development Alan Yang said he hoped the official apology would raise awareness and encourage people affected by the former laws to apply to have their records wiped clean.\n\n\"This is our one and only chance to get this in the public eye,\" he said.\n\nMr Yang said free and confidential legal advice was available through the Human Rights Law Centre for people wanting their records expunged.\n\nHe said the bigoted laws claimed many victims. In one case, 14-year-old Tom Anderson was forced to perform sexual acts on his employer but when he reported the abuse to police they charged him with buggery.\n\nAlthough no conviction was recorded against him the experience was highly traumatic and had a major impact on his life.\n\nThe former Coalition government, under then premier Denis Napthine, introduced laws to expunge the criminal records of men convicted of gay sex before homosexuality was decriminalised in Victoria in the 1980s."},
{"url": "https://campustechnology.com/articles/2016/05/16/how-blockchain-will-disrupt-the-higher-education-transcript.aspx", "link_title": "How Blockchain Will Disrupt the Higher Education Transcript", "sentiment": 0.10655809244518925, "text": "Blockchain technology could offer a more learner-centered alternative to traditional credentialing.\n\nLast year, the MIT Media Lab began issuing digital certificates to the participants in its Director's Fellows program. The authentication behind the certificates relies on blockchain technology, best known for its connection to the cryptocurrency bitcoin.\n\nIn a blog post, Philipp Schmidt, director of learning innovation at the Media Lab, described how blockchain works: \"In essence, it is a just a distributed ledger to record transactions. What makes it special is that it is durable, time-stamped, transparent and decentralized. Those characteristics are equally useful for managing financial transactions as for a system of reputation. In fact, you can think of reputation as a type of currency for social capital, rather than financial capital.\"\n\nThe technology has tremendous potential for higher education, according to Phil Long, chief innovation officer and associate vice provost for learning sciences at the University of Texas at Austin. In a May 12 Future Trends Forum video chat hosted by consultant and futurist Bryan Alexander, Long pointed to credentialing as an obvious first place to apply blockchain in higher ed.\n\nA transcript is the record of what a student has accomplished at a university. The document is managed and controlled by the institution, not the student. In contrast, Long said, blockchain has the potential of providing an immutable record of an individual learner's accomplishment that can be disclosed in a public context. \"The single thing that attracted me most is the potential it has to reaffirm the learner's ownership of their own record,\" he said.\n\nToday when you get a credential from an institution, you receive a piece of paper, but ultimately anybody who wants to verify that credential goes back to the source. Yet new models of higher education are complicating that process, Long pointed out. \"The notion that you will go to one university for all the training you need for your career is quite pass\u00e9. In the future, many learners will have earned credentials from a range of institutions,\" he said. For anyone to validate those records, they are going to have to go back to every single source. \"Is [blockchain] a way that that kind of intermediary chain can be broken?\" Long asked. With blockchain technology, he said, \"the assertion of the authenticity is made at the time it is created; there is no need to validate it thereafter.\"\n\nBlockchain is scaring a lot of people, he added, \"because its fundamental characteristic is the elimination of intermediaries \u2014 which is one of the reasons the financial industry became so excited when bitcoin was created.\"\n\nOne forum attendee asked Long if blockchain could help provide credentialing for MOOCs and other types of learning outside brick-and-mortar institutions.\n\nIf an institution is confident in asserting that the individual who has completed a MOOC has achieved certain accomplishments, the answer is absolutely yes, Long said. \"The learner could have a record of their learning from MOOCs, professional development activities and universities.\"\n\nExtending the idea further, Long referenced the work of a colleague from France, Serge Ravet, who suggests blockchain could be used to exchange social capital among individuals. For example, he said, the Future Trends Forum is a kind of learning community. \"If you are getting value out of the way in which Bryan Alexander has pulled together members of the community in these sessions, in Serge's view, you would have a Twitter-like application where through the hashtag you could offer to Bryan a social piece of capital that says 'I valued this interaction' and it would go into Bryan's blockchain.\" And there would be an accumulation of that chain of affirmation and reputation that he would accrue \u2014 in essence the community validating itself. \"That is sort of the anarchistic extreme of this idea,\" Long said."},
{"url": "https://blog.torproject.org/blog/mid-2016-tor-bug-retrospective-lessons-future-coding", "link_title": "Mid-2016 Tor bug retrospective, with lessons for future coding", "sentiment": 0.1309443095419987, "text": "Programs have bugs because developers make mistakes. Generally, when we discover a serious bug, we try to fix it as soon as we can and move on. But many groups have found it helpful to pause periodically and look for trends in the bugs they have discovered or fixed over the course of their projects. By finding trends, we can try to identify ways to develop our software better.\n\nI recently did an informal review of our major bugs from the last few years. (I'm calling it \"informal\" rather than \"formal\" mainly because I made up the process as I went along.)\n\nMy goals were to see if we're right in our understanding of what causes bugs in Tor, and what approaches to avoid bugs and limit their impact would be most effective.\n\nBy reviewing all the bugs and looking for patterns, I'm hoping that we can test some of our operating hypotheses about what allows severe bugs to happen, and what practices would prevent them. If this information is reasonably accurate, it should help us use our time and resources more effectively to write our code more safely over the coming years.\n\nI took an inventory of \"severe bugs\" from three sources:\n\nFor each of these cases, I assessed \"is this severe\" and \"is this really a bug\" more or less ad hoc, erring on the side of inclusion. I wound up with 70 tickets.\n\nAt this point, I did a hand-examination of each ticket, asking these questions:\n\nI then used a set of keywords to group tickets by similar causes or potential prevention methods.\n\nFinally, I grouped tickets by keywords, looking for the keywords that had the largest number of tickets associated.\n\nConsider this an exploratory exercise, not a scientific finding. We should look into formalizing the methodology more and giving it more process the next time we do it, for these reasons:\n\nWe've believed for a while that we can reduce the number of bugs that make it into the wild by using more tests on our codebase. This seems broadly true, but incomplete.\n\nFirst, it seems that only about half of our severe bugs appeared to be the kind of thing that better tests would have caught. The other half involved logic errors and design oversights that would probably have made it through testing.\n\nSecond, it seems that in some cases, our existing tests were adequate to the job, if only we had automated them better, or had run them more consistently, more rigorously, or under more conditions.\n\nIn all cases, of course, automation isn't quite enough. We must also have the automated tests run regularly (daily?), and make sure that the results are available to developers in a convenient way.\n\nRecommendation 1.1 Run our automated unit tests under more code-hardening methodologies.\n\nThis includes --enable-expensive-hardening under GCC and clang, valgrind with leak checking turned on, and anything else we can find.\n\nBugs where running tests under hardening or valgrind might have helped include: #13104, #14821, #17401, #17404, #18454.\n\nRecommendation 1.2: Also run test-network and test-stem in an automated environment.\n\nThese checks can detect a lot of problems, but right now we only try the stem tests in automated builds, and we don't try them with hardening.\n\nCases where a suitably extended (or completely vanilla) stem or chutney test case might have helped include: #8746, #9296, #10465, #10849, #11200, #13698,\n\n #15245, #15801, #16247, #16248, #17668, #17702, #17772, #18116, #18318, and\n\n #18517.\n\nRecommendation 1.3: Automate use of static analysis tools with Tor.\n\nThere were some cases where we found a bug using a static analysis tool later than we might have, because the static analysis tool had to be hand-launched. We can get faster bug resolution by automatically running all the static analysis tools we use. (We've already done this.)\n\nStatic analyzers might have caught: #13477 and #18454, at very little effort on our part.\n\nRecommendation 1.4: Continue requiring unit tests for new code, and writing unit tests for old code.\n\nUntested code had bugs at a higher rate than tested code.\n\nBugs where plain old unit tests might have helped include: #11824,\n\n #12195, #13066, #13151, #15083, #16400, #17041, #17668, #17702, #17772,\n\n #17876, #18162, and #18318.\n\nRecommendation 1.5: Get more users to try out our nightly builds.\n\nHaving more users of our nightly builds would help us notice more bugs on the git master branch before those bugs appear in stable or alpha releases.\n\nHaving users for our nightly builds would have prevented #11200 entirely.\n\nRecommendation 1.6: Whenever possible, write integration tests for new features.\n\nFeatures that lack integration tests via Chutney or some other mechanism tend to have bugs that last longer than other bugs before anybody notices them.\n\nRecommendation 1.7: We need more tests about shutting down busy clients and relays.\n\nOur code tends to have a fair number of corner cases concerning shutting down at the wrong time, and crashing or asserting rather than exiting cleanly.\n\nCareful shutdown tests might have caught #8746 and #18116.\n\nC is notoriously tricky, and we've put a fair amount of effort into avoiding its nastier failure modes. The C-related errors that did cause problems for us were not the ones I would have expected: Buffer overflows generally got caught very quickly. There are other C warts we've been less successful at avoiding.\n\nWe had a few cases where signed integer overflow (or unsigned overflow) could cause bad bugs in our code, some resulting in heap corruption.\n\nPerhaps we should prefer using unsigned integers everywhere we don't actually need signed integers? But although unsigned overflow isn't undefined behavior, it's still usually a bug when it's not intentional. So maybe preferring unsigned values wouldn't be so great.\n\nPerhaps smartlists and similar data structures should use internally instead of int for size and capacity. (Their use of int in their APIs isn't easy to change because of the rest of the codebase.)\n\nOur work on using -ftrapv throughout our code by default (in #17983) should help turn subtle signed overflow errors into crashes.\n\nInteger overflow was behind bugs #13104 and #18162.\n\nWe have two styles of container: Those that are specialized for a given type, and those that store void*. In nearly all cases, the void* ones have involved programmers making mistakes about what type they actually contained, in some case causing hard-to-debug issues.\n\nBug #18454 is one example of a void-punning problem.\n\nRecommendation 2.3: Continue to forbid new hand-written parsing code in C.\n\nThis caused fewer issues than you'd think (only a few ones for binary encoding and parsing, and only one for text parsing), but they were particularly troublesome.\n\nOutside of hand-written parsing code, memory violations are less frequent than you'd think.\n\nExamples of these bugs include #4168, #17668, #13151, #15202, #15601, #15823, and #17404.\n\nRecommendation 2.4: In the long-term, using a higher-level language than C would be wise.\n\nThis is a longer term project, however, and would have to happen after we get more module separation.\n\nBugs that would be more difficult (or impossible) to cause in a safe language include: #9296, #9602, #11743, #12694, #13104, #13477, #15202, #15823,\n\n #15901, #17041, #17401, #17404, #18162, and #18454.\n\nMany of our more subtle errors were caused by objects being in states that we didn't think they could actually be in at the same time, usually on error cases, shutdown paths, or other parts of the codebase not directly tied to the dominant path.\n\nRecommendation 3.1: For every object type, we should have a very clear understanding of its lifetime, who creates it, who destroys it, and how long it is guaranteed to last.\n\nWe should document this for every type, and try to make sure that the documentation is simple and easy to verify.\n\nWe could also consider more reference-counting or handles (qv) to avoid lifetime problems.\n\nRecommendation 3.2: State machines, particularly in error handling, need to be documented and/or simplified.\n\nWe have numerous implicit state machines scattered throughout our codebase, but few of them are explicitly expressed or documented as state machines. We should have a standard way to express and create new state machines to ensure their correctness, and to better analyze them.\n\nThis might be as simple as documenting state machines and noticing cases where transitions aren't considered, or might be more complicated.\n\nA great many of our crash bugs were caused by assertions that did not actually need to be assertions. Some part of the code was violating an invariant, but rather than exiting the program, we could have simply had the function that noticed the problem exit with an error, fix up the invariant, or recover in some other way.\n\nWe've recently added a family of nonfatal assertion (#18613) functions; we should use them wherever reasonable.\n\nRecommendation 5.1: all backward compatibility code should have a timeout date.\n\nOn several occasions we added backward compatibility code to keep an old version of Tor working, but left it enabled for longer than we needed to. This code has tended not to get the same regular attention it deserves, and has also tended to hold surprising deviations from the specification. We should audit the code that's there today and see what we can remove, and we should never add new code of this kind without adding a ticket and a comment planning to remove it.\n\nLess focus on backward compatibility would have prevented bugs #1038, bug\n\n #9777, and bug #13426.\n\nIn several cases, the original author of a piece of buggy code scheduled it for removal with an \"XXX02...\" comment, or noted some potential problem with it. If we had been more vigilant about searching for and categorizing these cases, we could have removed or fixed this obsolete code before it had caused severe bugs.\n\nThere are 407 instances of XXX, TODO, ???, FFF, or FIXME in src/common and src/or right now; we should get on auditing those and recategorizing them as \"fix now, open a ticket\", \"fix later, open a ticket\", or something else.\n\nBugs of this kind include #1038, #11648, and #17702.\n\nI don't have a firm set of conclusions here, other than to maybe make sure that our tests specifically correspond to what the spec says?\n\nRecommendation 7.1: Cull the options; remove ones we don't need/use/advise.\n\nSeveral severe bugs could only occur when the user specifically set one or more options which, on reflection, nobody should really set. In most cases, these options were added for fairly good reasons (such as protocol migration or bug workarounds), but they no longer serve a good purpose.\n\nWe should go through all of our settings and see what we can disable or deprecate. This may also allow us to get rid of more code.\n\nBugs caused by seldom-used or ill-advised settings for options include:\n\n #8387, #10849, #15245, #16069, and #17674.\n\nRecommendation 7.2:Possibly, deprecate using a single Tor for many roles at a time.\n\nMany bugs were related to running a HS+relay or HS+client or client+relay configuration. Maybe we should recommend separate processes for these deployment scenarios.\n\n#9819 is one bug of this kind.\n\nLarge features merged at the end of release, predictably, caused big bugs down the line.\n\nThe discussion and history on bugs #7912 and #10777 indicate that waiting until very late in the cycle was probably not as good an idea as it seemed at the time.\n\nRecommendation 9.1: We should favor a single convention for return values, and not accept code that doesn't follow it.\n\nWe have had a few bugs caused by differing return-value conventions on similar functions. Our most common convention has been to have a negative value indicate failure and zero to indicate success. When 0 indicates failure and positive values indicate success, we usually can expect to have a bug in the calling code someplace.\n\nBug #16360 was caused by a misunderstanding of this kind, and could have been much worse than it really was.\n\nRecommendation 10.1: Callgraph complexity hits us a lot -- particular in code where a calling function assumes that a called function will not make certain changes in other structures.\n\nWe should, whenever possible, simplify our callgraph to remove cycles, and to limit maximum call depth.\n\nBugs resulting from, or worsened by, complexit y in the callgraph, include\n\n #4900, #13698, #16013, and #17752.\n\nSeveral hard-to-diagnose bugs were called by code where we identified targets for some operation and simultaneously performed that operation. In general, we should probably have our default approach involve identifying the items to operate on first, and operating on them afterwards. We might want to operate on them immediately afterwards, or schedule the operation for higher in the mainloop.\n\nBugs #16013 and #17752 were caused by the modify-in-iterator pattern.\n\nRecommendation 10.3: Perhaps we should have a container-freezer.\n\nWe have code that supports removing a member from a smartlist or hashtable while iterating over it.... but adding or removing members through other means at the same time won't work. What's more, debugging the results is annoyingly difficult. Perhaps we should have our code catch such attempts and give an assertion failure.\n\nBugs #16013 and #17752 were caused by the modify-in-iterator pattern.\n\nRecommendation 10.4: Duplicated code is trouble; different functions that do the same thing differently are trouble.\n\nThis is well-known, but nonetheless, we have a few cases where we grew two functions to do similar things, patched one to solve a problem or add a feature, but forgot to patch the other.\n\nCode duplication was at issue in bugs #12195, #13066, and #17772.\n\nRecommendation 11.1: Our path-selection/node-selection code is very complex, and needs more testing and rewrites.\n\nMore than in most other areas, we found bugs in the code that selects paths and nodes. This code is hard to test in part because it's randomized, and in part because it looks at several global structures including the nodelist, the microdescriptor set, the networkstatus, and state related to guard nodes. We should look at ways to simplify our logic here as much as possible.\n\nThis code was related to bugs #9777, #10777, #13066, #16247, #17674, and\n\n #17772.\n\nSome time over the next month or so, we should re-scan this document for actionable items to improve our future code practices, create bugtracker tickets for those items, and try to sort them by their benefit-to-effort ratio. We should also make a plan to try this again in a year or two."},
{"url": "http://electrek.co/2016/05/23/tesla-founder-marc-tarpenning-hydrogen-fuel-cells-scam/", "link_title": "Tesla co-founder says hydrogen fuel cells are a \u2018scam\u2019", "sentiment": 0.058406491384432595, "text": "People affiliated with Tesla have often been outspoken about how hydrogen fuel cells are simply a bad solution to the sustainable transport issue. Just last week we reported on Tesla co-founder and CTO, JB Straubel, going on a quick rant about hydrogen and saying that fuel cells will soon be \u201cirrelevant\u201d in the transportation industry.\n\nNow another Tesla co-founder, Marc Tarpenning, went a little further than Straubel and called hydrogen fuel cells a \u201cscam\u201d. He also said out loud what many in the industry are thinking \u2013 that energy companies are supporting the technology for its inefficiency.\n\nTarpenning is not with Tesla anymore, but he is one of the original co-founders with his long-time friend and business partner, Martin Eberhard. He was on the Internet History Podcast last week to talk about his ventures and he explained in great details the foundings of his two companies; NuvoMedia and Tesla Motors.\n\nThe podcast is worth listening in full \u2013 see below (Tesla starts at about 25 minutes in), but I found the rant about hydrogen fuel cells particularly interesting.\n\nAfter selling NuvoMedia, a maker of e-book readers, he and Eberhard first investigated alternative fuel sources in order to start a company to solve the sustainable transport issue. They looked into hydrogen fuel cells and quickly dismissed it:\n\nIf your goal is to reduce energy consumption, petrol or whatever resource, you want to use it as efficiently as possible. You don\u2019t want to pick something that consumes\u00a0a lot for whatever reason, and hydrogen is uniquely bad. There\u2019s a saying in the auto industry that hydrogen is the future of transportation and always will be. It\u2019s a scam as far as I can tell because the energy equation is terrible. It\u2019s just terrible. People will say that hydrogen is the most abundant element in the universe, but it\u2019s abundant out there in the universe not here. We live on a planet where hydrogen is super reactive \u2013 it\u2019s bound up into everything. It\u2019s bound up into water, wood and everything else. They only way that you get hydrogen requires you to pour energy into it to break it from the chemical bonds. Electrolysis is the most commen method. You put electricity in water and it separates it, but you are pouring energy in order to make hydrogen, and then you have to compress it and that takes energy, and then you have to transport it to wherever you actually need it, which is really difficult because hydrogen is much harder to work with than gasoline or even natural gas \u2013 and natural gas is not that easy. And then you ultimately have to place it into a car where you\u2019ll have a very high-pressure vessel which offers its own safety issues \u2013 and that\u2019s only to convert it back again to electricity to make the car go because hydrogen fuel cell cars are really electric cars. They just have an extraordinary bad battery. Hydrogen is an energy carrier and not a primary fuel source on this planet. Maybe out somewhere in the universe, but not on a terrestrial planet. When you add that all up, it turns out that the amount of energy per kilometer driven is just terrible. It\u2019s way worse than almost anything else you can come up with \u2013 which I always suspected is one of the reasons why the energy companies have long been big proponents of it. When we were raising money the first time, we had very carefully gone through the math to understand fuel cells because there was a bunch of money going into fuel cells at the time and we also looked at biofuels and ethanols \u2013 we sort of went down the whole list to figure out what the most energy efficient system was \u2013 which turned out to be battery electric cars.\n\nTarpenning then described how they included a slide about hydrogen fuel cells in their powerpoint presentation when trying to raise money from venture capitalists for Tesla. The slide was of course about why Tesla would use batteries to power its vehicles and not hydrogen.\n\nHe says that half of the VCs would ask them to skip the slide saying that they already ran their own cost analysis and they are aware of the inefficiency of the system, but the other half of the VCs would go really quiet and then start asking more questions. Marc explained that those VCs had already invested in fuel cell companies, which most went out of business by now.\n\nTarpenning left Tesla in 2008 after an internal power-struggle saw his co-founder Martin Eberhard ousted as CEO and Elon Musk took over not long after. He is still involved in the electric vehicle industry. He\u00a0invested, alongside Eberhard, in electric motorcycle maker Alta Motors in 2014.\n\nWe also recently reported on him joining the advisory board of \u2018self-driving vehicles for industry\u2019 company: Clearpath Robotics."},
{"url": "https://www.theguardian.com/technology/2016/may/23/copyright-law-internet-mumsnet", "link_title": "How copyright law is being misused to remove material from the internet", "sentiment": 0.0030776736924277783, "text": "Writing a bad review online has always run a small risk of opening yourself up to a defamation claim. But few would expect to be told that they had to delete their review or face a lawsuit over another part of the law: copyright infringement.\n\nYet that\u2019s what happened to Annabelle Narey after she posted a negative review of a building firm on Mumsnet.\n\nNarey, who is the head of programme at an international children\u2019s charity, had turned to London-based BuildTeam for a side return extension, but almost six months later, the relationship had turned acrimonious. The build, which was only supposed to take 10\u201314 weeks, was still unfinished, she wrote. \u201cOn Christmas day a ceiling fell down in an upstairs bedroom,\u201d she says, apparently due to an issue with the plumbing. \u201cMercifully no one was hurt. [That] there seem to be so many glowing reports out there it is frankly curious. Proceed at your own risk,\u201d the review concluded.\n\nBuildTeam disputes her account. In a letter sent to Mumsnet, which the site passed on to Narey, the builders complained that the comments were defamatory. They say it is \u201cuntrue\u201d that the ceiling fell down due to an issue with plumbing, and cited a total of 11 statements they claimed were defamatory.\n\nMumsnet, following UK law on libel accusations, passed the letter on to Narey and offered her the chance to delete the post or get in touch with BuildTeam to sort out the matter.\n\n\u201cBuildTeam have been in touch persistently with us at Mumsnet since mid-March, asking for the thread to be removed,\u201d a spokeswoman said. \u201cWe\u2019re keen to defend our posters\u2019 freedom of speech and to ask complainants to follow due process, so previously we had referred them to Section 5 of the 2013 Defamation Act.\u201d\n\nBy this point, the thread on Mumsnet had grown to include other posters claiming to have had bad experiences with the building firm. Some of them decided to remove the posts in response to the legal threats from BuildTeam, but Narey wanted to keep hers up.\n\nBuildTeam says that \u201cat no point has \u2026 Build Team Holborn Ltd stated that they are to pursue a defamation claim against any individual. Enquiries were made to the relevant web hosts as to their position for such posts being made, thus resulting in the relevant documentation being lodged with the aforementioned hosts. At present Build Team Holborn Ltd are currently assessing the situation and/or their options in respect of reserving their rights should any action be required in the future.\u201d\n\nNarey says that after she heard from Mumsnet about the defamation claims, BuildTeam got in touch personally to ask for the post to be removed. \u201cStaff even came to our house holding printouts of it. They never acknowledged the contents or made any apology, but distanced themselves from the context of the review, asking only for it to be taken down,\u201d she said.\n\nBut in April, the decision was made for her, in a very peculiar way. Mumsnet received a warning from Google: a takedown request had been made under the American Digital Millennium Copyright Act (DMCA), alleging that copyrighted material was posted without a licence on the thread.\n\nAs soon as the DMCA takedown request had been filed, Google de-listed the entire thread. All 126 posts are now not discoverable when a user searches Google for BuildTeam \u2013 or any other terms. The search company told Mumsnet it could make a counterclaim, if it was certain no infringement had taken place, but since the site couldn\u2019t verify that its users weren\u2019t actually posting copyrighted material, it would have opened it up to further legal pressure.\n\nIn fact, no copyright infringement had occurred at all. Instead, something weirder had happened. At some point after Narey posted her comments on Mumsnet, someone had copied the entire text of one of her posts and pasted it, verbatim, to a spammy blog titled \u201cHome Improvement Tips and Tricks\u201d. The post, headlined \u201cBuildteam interior designers\u201d was backdated to September 14 2015, three months before Narey had written it, and was signed by a \u201cDouglas Bush\u201d of South Bend, Indiana. The website was registered to someone quite different, though: Muhammed Ashraf, from Faisalabad, Pakistan.\n\nQuite why Douglas Bush or Muhammed Ashraf would be reviewing a builder based in Clapham is not explained in \u201chis\u201d post. BuildTeam says it has no idea why Narey\u2019s review was reposted, but that it had nothing to do with it. \u201cAt no material times have we any knowledge of why this false DCMA take down was filed, nor have we contracted any reputation management firms, or any individual or a group to take such action on our behalf. Finally, and in conjunction to the above, we have never spoken with a \u2018Douglas Bush,\u2019 or a \u2018Muhammed Ashraf.\u2019\u201d\n\nWhoever sent the takedown request, Mumsnet was forced to make a choice: either leave the post up, and accept being delisted; fight the delisting and open themselves up to the same legal threats made against Google; or delete the post themselves, and ask the post to be relisted on the search engine.\n\n\u201cAlthough we understood the user\u2019s argument that something odd had happened, we weren\u2019t in a position to explain what - our hope was that by zapping one post we might ensure that the thread remained listed.\u201d\n\nMumsnet deleted the post, and asked Google to reinstate the thread, but a month later, they received final word from the search firm: \u201c\u2018Google has decided not to take action based on our policies concerning content removal and reinstatement\u2019 which (it turned out) meant that they had delisted the entire thread\u201d.\n\nThe motivation of Ashraf can only be guessed at, but censorship using the DMCA is common online. The act allows web hosts a certain amount of immunity from claims of copyright infringement through what is known as the \u201csafe harbour\u201d rules: in essence, a host isn\u2019t responsible for hosting infringing material provided they didn\u2019t know about it when it went up, and took it down as soon as they were told about it.\n\nIn practice, however, this means that web hosts (and the term is broadly interpreted, meaning sites like YouTube, Twitter and Google count) are forced to develop a hair-trigger over claims of copyright infringement, assuming guilt and asking the accused to prove their innocence.\n\nAs such, a very easy way to remove something from the internet is to accuse its creator of infringing copyright. Worse, the potential downside of such a false claim is minimal: the accused would have to first file a counterclaim, proving they own the copyright; then file a private lawsuit, and prove material damage; and then track down the offending party to actually recover any monies granted by the court.\n\nThat doesn\u2019t happen all that often.\n\nBut in recent years, big web companies have started funding lawsuits themselves, to fill the gap in the law and tilt the scales a bit further in favour of content creators wrongly accused.\n\nOliver Hotham is one beneficiary of that change. In 2013, the journalist posted an interview with \u201cStraight Pride UK\u201d, a homophobic group that expressed support for anti-gay polices in Russia. Seemingly embarrassed by their own statements, Straight Pride UK then filed a takedown request with Hotham\u2019s blog host, Wordpress.com, claiming that they owned the copyright to the answers they gave Hotham, and they had not intended the text to be published. \u201cStraight Pride UK thought as he was a student that we would add fun to it, dress it up and make him feel like a reporter by adding \u2018press release\u2019 to the document,\u201d the group\u2019s spokesman, who went by the name \u201cPeter Sidorove\u201d, told the Guardian at the time.\n\nAutomattic, the parent company of Wordpress.com, called the takedown request \u201ccensorship using the DMCA\u201d, and vowed to fight it. Eventually, Hotham and Automattic were victorious, with a Californian judge granting over $20,000 in damages, but it was a hollow victory: Sidorove and Straight Pride UK had disappeared off the face of the earth, leaving little chance of the money being paid out.\n\nIn November, YouTube announced a similar plan, to \u201coffer legal support to a handful of videos that we believe represent clear fair uses which have been subject to DMCA takedowns\u201d.\n\n\u201cWe\u2019re doing this because we recognise that creators can be intimidated by the DMCA\u2019s counter notification process, and the potential for litigation that comes with it,\u201d Fred von Lohmann, Google\u2019s copyright legal director, wrote.\n\nBut the company can\u2019t offer legal support for every video on YouTube, nor even every video with an obvious case. And when it comes to takedown requests for Google Search, the numbers are staggering: the company received 88m copyright takedowns in the last month. Despite that, a Google spokesperson said that \u201cwe use a variety of techniques to try to identify [fraudulent] claims, and when we do identify possible fraud, we push back very strongly against the claim. Indeed, we do this for millions of URLs every year.\u201d\n\nGoogle is aware of cases like Narey\u2019s, and is looking at how to improve fraud detection, but there\u2019s a limit to what it can do in general. The scale is too large for it to take the sort of personal approach that Automattic did with Hotham\u2019s case, and ultimately the law doesn\u2019t allow for it to hit back against fraudulent claims without some involvement of the accused \u2013 which, technically, was Mumsnet, not Narey. And while Mumsnet was offered the chance to file a counterclaim, the forum couldn\u2019t, because it too couldn\u2019t be certain the claim actually was fraudulent.\n\nFor Narey, it\u2019s a bit late. \u201cFor the law governing the internet to allow decisions regarding my integrity to be taken without any investigation at all seems shocking,\u201d she says. \u201cI have no ambition other than to bring our experience to public attention.\u201d"},
{"url": "http://www.recode.net/2016/5/23/11746112/amazon-price-drop-adjustment-refunds", "link_title": "Amazon stopped giving refunds when an item's price drops after you purchase it", "sentiment": 0.046130952380952384, "text": "The company that prides itself on customer centricity may have just alienated some customers.\n\nAmazon has been known to give refunds if the price of an item drops after a purchase and the customer notifies customer service. But it appears that Amazon stopped providing these refunds earlier this month, except for televisions, according to price-tracking companies and customer postings on Reddit.\n\nThe move may have something to do with the rise of startups that track prices for Amazon customers and automatically request refunds when appropriate. One of them, a Santa Monica-based startup called\u00a0Earny that is backed by the startup incubator Science, first pointed out the change to Recode.\n\nEarny scours a customer's email inbox for digital receipts, and then continuously checks the price on a retailer's website to see if it drops. Brooklyn-based\u00a0Paribus, another price-drop tool, also noticed the recent change.\n\nAmazon spokeswoman Julie Law told Recode the policy was always limited to televisions and that any customer who has received refunds on other products was granted an \"exception.\" But it's clear that those exceptions were previously given out freely, and now they are not.\n\nIt's also clear that startups like Earny and Paribus, which require users to hand over their Amazon account credentials, are on Amazon's radar.\n\n\"[W]e take customer security very seriously and want to remind them not to share their Amazon account credentials with anyone,\" Law said."},
{"url": "http://techcrunch.com/2016/05/18/google-takes-a-new-approach-to-native-apps-with-instant-apps-for-android/", "link_title": "Instant Apps for Android", "sentiment": 0.07887321012321011, "text": "Mobile apps often provide a better user experience than browser-based web apps, but you first have to find them, download them, and then try not to forget you installed them. Now, Google wants us to rethink what mobile apps are and how we interact with them.\n\nInstant Apps, a new Android feature\u00a0Google announced at its I/O developer conference today but plans to roll out very slowly, wants to bridge this gap between mobile apps and web apps by allowing you to use native apps almost instantly \u2014 even when you haven\u2019t previously installed them \u2014 simply by tapping on a URL.\n\nTypically, downloading and installing an app would take a while, but with Instant Apps, developers will have to partition their apps into small, runnable parts that can start within a few seconds.\n\n\u201cInstant Apps is really about re-thinking where apps are going,\u201d Google VP of Engineering for Android Dave Burke told me. The idea behind Instant Apps is to make the native app experience as convenient as surfing to a web site. \u201cWeb pages are ephemeral,\u201d he said. \u201cThey appear, you use them, and never think about them again.\u201d Apps, he argued, have lots of friction and often you only want an app to perform one action or to get a specific piece of information.\n\nAs\u00a0Google\u2019s Michael Siliski and Ficus Kirkpatrick\u00a0told me, the idea here is to allow a mobile experience to start in about the same time it would take to render a standard mobile web page. While the team is still working out the limits, Siliski and Kirkpatrick expects downloads for Instant Apps to clock in at under 4 megabytes.\n\nHere is what that would look like in practice: say you are in a new city and want to pay for parking with whatever parking app the local municipality is using. You hold your phone to the parking meter, the built-in NFC chip reads the info, and the native app appears almost instantaneously. There is no need to download the app or even log in (or to uninstall it later).\n\nThe focus here, they argued, is on making the experience as seamless as possible.\u00a0Because you\u2019re already logging in to your phone, the app will allow you to pay with your Google Wallet account and then disappear again, for example. \u201cThere are classes of apps that are better as a native app but aren\u2019t being experienced like this because of the install hurdle,\u201d\u00a0Siliski said.\n\nGoogle also worked with BuzzFeed on an early test that lets you open up an instant version of the Buzzfeed Video app\u00a0to watch a video, and with B&H to show how developers could use this feature for an ad hoc online shopping experience.\n\nDevelopers will have to do some work to enable all of this new functionality, of course. Kirkpatrick noted that this is more of an upgrade to an existing app than a complete rewrite, though. They can still use the same source code and some developers may be able to implement Instant App support in as little as a day (assuming they have a very basic app, of course).\n\nApps will run in a secure sandbox and\u00a0that this feature, once released, will work all the way back to Android Jelly Bean.\n\nBurke connected the idea behind this new kind of app with the current hype around bots. They, too, promise to make it easier to perform some actions without having to install a new application for each task. As Burke argued, though, bots\u00a0aren\u2019t really all that convenient in\u00a0practice (yet) given that you have to go through multiple steps and maybe type a bunch of sentences to get anything done. Instant Apps, he said, gives you \u201cthe good stuff from bots without the negatives.\u201d\n\nIt\u2019ll likely be a while before you\u2019ll see any Instant Apps in the wild, though. Google is announcing this feature today to gather input from developers. It has already worked with a few early testers and the team feels it\u2019s ready to provide today\u2019s sneak peek at the functionality. The capability to use Instant Apps will roll out to users later this year and Google plans to expand the set of developers with access to this feature over the course of next year.\n\nWhy so slow? The company argues that this is a tricky feature to get right. \u201cThere are a lot of changes to the developer experience and we want to make sure we get this right,\u201d Siliski said.\n\nGoogle in the past has\u00a0experimented with a number of ways to bridge the gap between the web and native applications, including surfacing content from Android and iOS apps in search and even streaming virtualized apps from the cloud. But Instant Apps is about making it more seamless and natural to download then use real, native applications.\n\nThe move comes at a time when the app stores are overrun with content, and consumers are becoming less inclined to seek out and try new applications. A 2015 study indicated that consumers spend 85 percent of their time on smartphones using apps, but only use a small handful of third-party apps on a regular basis.\n\nThis shift in consumer behavior means that it\u2019s difficult for developers to get their app onto users\u2019 devices\u00a0\u2013 something that Instant Apps could help to address, if implemented correctly."},
{"url": "https://motherboard.vice.com/read/notorious-hacker-phineas-fishers-is-trying-to-start-a-hack-back-political-movement", "link_title": "A Notorious Hacker Is Trying to Start a \u2018Hack Back\u2019 Political Movement", "sentiment": 0.15331697254232463, "text": "In August of 2014, a hacker shook the cybersecurity world by exposing the secrets of the infamous government surveillance vendor Gamma Group, the makers of the spyware FinFisher.\n\nThe hacker jokingly called himself Phineas Fisher, publicizing the hack and taunting the company on Twitter. He also wrote a detailed guide on how he breached Gamma\u2014not to brag, the hacker wrote, but to demystify hacking and \u201cto hopefully inform and inspire you to go out and hack shit.\u201d\n\nThen, Phineas Fisher went dark. For almost a year, his public profiles remained silent. Given that he had just upset a company that sold tools to dozens of spy and police all over the world, it seemed like a wise move.\n\nThen, slightly less than a year later, Phineas Fisher came back with a bang. This time, he hacked into the computers of Gamma\u2019s competitor Hacking Team, another company mostly known for selling spyware to questionable governments around the world.\n\n\u201cGamma and [Hacking Team] down, a few more to go :),\u201d he tweeted.\n\nHe followed the hack by taunting Hacking Team through the company\u2019s own Twitter account, and then, once again, he went dark for months, until he revealed how he got into Hacking Team. Once again, he called others to action. \u201cHacking is a powerful tool. Let\u2019s learn and fight!\u201d he wrote.\n\nOn Twitter, he proclaimed on his profile that \u201cour keyboard is our weapon.\u201d\n\nLast week, he hit his third victim, the union of the Catalan police Mossos D\u2019Esquadra. Accompanying his new hack, he also released a 39-minute how-to video with anti-police songs playing in the background, in which he showed how he got the data.\n\nWhile his latest victim seems like a small catch compared to FinFisher and Hacking Team, it\u2019s in line with his political views as an \u201canarchist revolutionary,\u201d and with his larger goal of inspiring other hacktivists to \u201chack back,\u201d as he puts it in his guides.\n\n\u201cEverything doesn't have to be big,\u201d Phineas Fisher told me in an email. \u201cI wanted to strike a small blow at the system, teach a bit of hacking with the video, and inspire people to take action.\u201d\n\nHis plan might very well be working. With his rare, targeted, and almost \u201csurgical\u201d strikes, Phineas Fisher has a very good chance of inspiring a new generation of hacktivists and \u201csetting the stage for other hackers to follow in his footsteps,\u201d according to Biella Coleman, a professor at McGill University in Montreal who's well-known for her study of hackers, hacktivism, and Anonymous.\n\nMustafa Al-Bassam, a security researcher and former LulzSec hacker, agreed, saying that the hacker\u2019s \u201cstrategy and message bares resemblance to many past Anonymous operations, but he's been arguably more effective at it doing it alone.\u201d\n\nPhineas Fisher is \u201cone of the most intelligent hackers I've seen,\u201d and \u201cone of the most inspiring to people in the hacktivist community in recent times\u2014possibly of all time,\u201d Al-Bassam told me in an online chat.\n\nColeman said that unlike Anonymous and LulzSec-inspired hackers, Phineas Fisher has been better at choosing targets and justifying his actions with more rounded and sophisticated political and ethical views. For Coleman, the big challenge for the hacker now will be to balance his need not to get caught, with his goal of inspiring others to join his cause.\n\nPhineas Fisher, on his part, seems not to be in a rush, though he\u2019s been considerably more active online in the last few weeks.\n\n\u201cHacking takes time to learn and get good at,\u201d he said, revealing that he first got interested in it when he read an article on Anonymous and LulzSec hacker Jeremy Hammond in 2012. In any case, \u201cit\u2019s impossible to know yet\u201d whether his call to hack back has been working, and actually, he doesn\u2019t really want to start a \u201cformal movement\u201d a la Anonymous or LulzSec.\n\n\u201cBut I don't want to be the lone hacker fighting the system,\u201d he told me. \u201cI want to inspire others to take similar action, and try to provide the information so they can learn how.\u201d"},
{"url": "http://factordaily.com/wittyfeed-viral-content/", "link_title": "Inside the Indian BuzzFeed competitor you\u2019ve never heard of", "sentiment": 0.12307203653650359, "text": "It is well past midnight and Parveen Singhal roars on a black Royal Enfield 500cc Thunderbird through the streets of Indore. His friends and relatives try to keep pace with him on their motorbikes and a car but it is Singhal, 21, who pulls first into their destination: Sarafa, a food street by night that is open until 2am.\n\nThe motley group of young men, bubbly girls, and even Singhal\u2019s parents who preferred to ride in a Maruti Swift car, feast on dahiwada and papdi chaat \u2014 call it North Indian fast food, if you will \u2014 from shops lining the street. It\u2019s all washed down with banta, a flavoured soda often rich in colour.\n\nAn hour later, Singhal hits the sack exhausted, ending a long Sunday and knowing he has a long day (and week) ahead at WittyFeed, a company he co-founded with his elder brother and his friend.\n\nWhat does WittyFeed do and why did I fly in to Indore write about it? WittyFeed, owned by Vatsana Technologies, is into viral content. The kind of content that is massively shareable and generates a lot of interest instantly, primarily on social media.\n\nAs marketing dollars get vacuumed into digital media and advertisers look for the largest catchment of eyeballs, viral content sites \u2014 global leaders are BuzzFeed, Upworthy and ViralNova \u2014 have tasted both popularity and prosperity. On the Internet, most publishers chase traffic aggressively. The more number of people on your site, the more money you can make.\n\nIn terms of pure traffic, WittyFeed might be India\u2019s answer to BuzzFeed,\u00a0raking in numbers that digital rivals can only dream of. In April, the site clocked over 82 million visits \u2014 nearly four times that of ScoopWhoop, a more visible Indian viral content site. Data from SimilarWeb, which measures website traffic, shows that ScoopWhoop had 22.9 million visits in April, while WittyFeed clocked 82.7 million visits. BuzzFeed, the world\u2019s largest viral content site, of course, is far ahead \u2014 it had 259 million visits the same month.\n\nA typical week at WittyFeed begins on a Sunday with a huddle where team leaders update the rest of the company about the week gone by and their plans for the week ahead. \u201cThere is no point taking a holiday on a Sunday because you can\u2019t do anything here,\u201d says Vinay Singhal, CEO and co-founder, and Parveen\u2019s elder brother.\n\nAt a recent team meeting, an employee pipes up, \u201cLast week 300 stories were created and nine of them crossed one million page views.\u201d Another presents monthly statistics: \u201cWe had 74.3 million traffic last month, it was a little less than the previous month because of the block.\u201d The block referred to Facebook. Nearly 40% of WittyFeed\u2019s traffic comes from the world\u2019s largest social network, and on April 27, Facebook blocked WittyFeed. \u201cFacebook mistook us for a spam site and blocked all our links,\u201d says CEO Singhal, who is 26.\n\nToday, things are back to normal at the 60-member company and traffic is picking up. Singhal reckons that the site will easily clock 90 to 100 million visitors in May. These numbers put them well past some of largest viral websites in the world like Upworthy or ScoopWhoop.\n\nTo those who doubt WittyFeed\u2019s numbers, Singhal confidently shows off the site\u2019s Google Analytics page which displays its traffic statistics. \u00a0In the last six months, WittyFeed has clocked nearly 1.5 billion page views and over 170 million users. Alexa, another service that ranks websites based on their popularity, counts WittyFeed among the top 200 sites in the world. That puts the Indore site ahead of Forbes.com or even WashingtonPost.com. In India, only a few websites such as Indiatimes.com, run by the Times Group, and online retailers like Amazon and Flipkart are ahead of WittyFeed in terms of traffic.\n\nSome of these numbers may need to be taken with a pinch of salt (for instance, you can doctor Google Analytics numbers by having a bot \u201cvisit\u201d your site frequently) but it is clear that WittyFeed\u2019s numbers cannot be brushed aside as cooked up. Traffic tracker ComScore, which most marketers use to apportion advertising dollars, puts WittyFeed\u2019s traffic from smartphones in India ahead of BuzzFeed\u00a0India.\u00a0ComScore estimates WittyFeed\u2019s traffic originating on Indian mobile phones at 5.85 million users versus BuzzFeed\u2019s 2.56 million in India. ComScore also doesn\u2019t report worldwide numbers for smartphones.\n\nThe traffic reflects in Vatsana Technologies\u2019s cash flows. The company clocked revenues of \u20b926 crore in the year toMarch 31, 2015, says Singhal who started the website in September 2014 with two other co-founders \u2014 his batchmate from engineering college, Shashank Vaishnav, 25, and brother Parveen. Vatsana had revenues from other small businesses earlier. \u201cWe have a 25-30% margin on the revenues that we make,\u201d says Singhal.\n\nHowever unlikely it might seem, for the three, the mission is to overtake BuzzFeed at some point. After taking care of their expenses, they reinvest all of the money back into the business \u2014 hiring more people and improving infrastructure. Their current office is spread over about 1,800 sq ft, bursting at the seams with over 60 employees sitting in rows facing the same side. Soon, the team will move to a 10,000 sq ft office in the same half-empty mall it operates out of currently, complete with a gym, cafeteria and a work area which can seat 150 employees. Singhal says the WittyFeed team will be 100-strong by end of the year.\n\n\u201cNumbers wise, BuzzFeed is the god of the industry,\u201d says Singhal. With mostly user generated content, It will be an uphill battle for WittyFeed to match up to the content quality of BuzzFeed. \u201cBuzzFeed has a certain tonality and quality associated with it which is the best in the industry,\u201d said Sandeep Amar, the CEO of India.com, a leading news portal. BuzzFeed, which started as a viral content site focused on just attracting eyeballs has, in recent years, tried to raise the quality of its content and even has put together an investigative journalism team.\n\nWittyFeed has ambitions of launching sponsored stories and branded content, much like how BuzzFeed makes money. It is slowly trying to improve the quality of its content, as well. \u201cWe don\u2019t want to stay a viral content site. We want to become a holistic media company like Vice. They are a different class altogether,\u201d says Singhal. \u201cThat is the company we look up to and want to be like in three to five years.\u201d Vice is among the world\u2019s largest new media companies focussed on millennials and is known for its edgy, high quality content. WittyFeed recently hired Alok Vani, a media professional with over 15 years of experience at media houses such as CNBC and India TV, to help craft its content strategy.\n\nThe Singhal boys success reflects on their family. They have moved from their Haryana village, Noonsar, to a new house in Palasia, an affluent locality in Indore. (Noonsar lies about 170 km west of New Delhi.)\n\nAfter finishing Class 10 in 2006, Vinay went to Kota, Rajasthan to study further. He took up biology because he wanted to become a doctor. \u201cI was like everyone is becoming a doctor, I\u2019ll also do that,\u201d he says, oblivious that he might be loud in the Cafe Coffee Day we are at. He comes across the archetypal young marwari businessman \u2014 his father used to run a general store in their village \u2014 and his risk-taking nature extends to the things that he has committed to.\n\nSometime last year, he married the woman he loved \u2014 his schoolmate from Noonsar. Most urban Indian wouldn\u2019t bat an eyelid at this, but in traditional families, marrying a woman from another caste would have meant being ostracised \u2014 and sometimes even killed to \u201cprotect\u201d the family honour. After carefully planning for almost six months, the two got married. \u201cWhy should I not do something I want to because some ass***e in society doesn\u2019t want me to?\u201d asks Singhal.\n\nMost of his worldview was formed after he left his village. \u201cThose were the two years I was exposed to the outside world for the first time,\u201d he says. Singhal soon realised that he was passionate about computers. He found inspiration in tech luminaries like Bill Gates. \u201cI was so naive that I thought I\u2019ll make a better OS (operating system) and become richer than him,\u201d he says.\n\nSoon he dropped his plans to become a doctor and studied to become an engineer. Singhal met his co-founder, Vaishnav, in the Tamil Nadu- based Sri Ramaswamy Memorial University, better known as SRM University, where he studied computer science. His younger brother, Parveen, was still in school at the time. The four years of engineering education in Tamil Nadu was when Singhal caught up with the rest of the world. \u201cI watched 150 hollywood movies on my laptop in one year,\u201d he says, chuckling. It helped him get better at English and understand the outer world better.\n\nIn his first year at SRM, Singhal set up a website called BadlegaBharat.com (translates to India will change) with six college mates. But they needed money. \u201cEven to do patriotic activities you need money.\u201d The group decided to design and set up websites to make money and came up with the name Vatsana by putting the first letter of everyone in the group together (Vatsana also means trust in Thai).\n\nAs work picked up, Singhal discovered that coding was not his cup of tea. He was better at business. For two years they made and sold websites, mostly for relatives, friends and seniors from college. In the meantime, the group fell apart due to various reasons. \u201cWe had our differences,\u201d Singhal says. Parveen, meanwhile, fresh out of school,had joined SRM to do a bachelors in Information Systems Management. \u201cHe introduced social media to the company,\u201d recalls the CEO brother. Parveen is now responsible for content and traffic at WittyFeed.\n\nIn 2012, Vatsana launched a Facebook page called \u201cAmazing things in the world\u201d, which is where the WittyFeed trio tasted blood. \u201cIt was targeted to bring positivity to the newsfeed,\u201d says Singhal, who graduated from SRM with 7.5 points. In six months, the page had one million fans without spending a\u00a0single penny\u00a0on advertising. Today, it has 4.2 million Likes. The page is an endless stream of amazing photographs and videos from around the world, like baby elephants taking a mud bath at the Pinnawala Elephant Orphanage in Sri Lanka. \u201cWe used to reply to nearly 200 messages every day,\u201d says Singhal.\n\nSoon, the three came up with the idea of creating a platform where everyone could create their own stories. TheStupidStation.com was created and traffic was driven to the site from their Facebook page. \u201cWhen our classmates had taken up a package of \u20b93 lakh for a year, we were making \u00a0\u20b98-10 lakh every month,\u201d Singhal recalls. The traffic was driven to the site mostly from Facebook.\n\nIn April 2014, they moved to Indore from Chennai because it was Vaishnav\u2019s hometown and it cheaper to run a startup out of. \u201cWe come here, work hard for a few years and we\u2019ll be probably kings of the city,\u201d he says of the thinking behind the move to Indore. It was hard to brand TheStupidStation as a serious media player so they abandoned the site and launched another site called evrystry.com. That didn\u2019t stick either. Then came the idea of WittyFeed, a name that would \u201croll off the tongue,\u201d and was easy to remember.\n\nWhat\u2019s working really well for WittyFeed is the traffic that comes from Facebook. Its stories, mostly about celebrities, shocking facts, grotesque videos or pictures get shared quite a bit. For instance, a story headlined: \u201cHe walks into women\u2019s locker room and gets the shock of his life,\u201d has over 300,000 views. Another one: \u201cPrevent unwanted pregnancy using these shocking methods and no condoms at all\u201d has 200,000 views on the site. A creepy video of a man pulling out an ingrown hair out of his beard has hundreds of thousands of views. To be fair, BuzzFeed also does this kind of content.\n\nThis, however, is probably not enough\u00a0to get the kind of numbers WittyFeed claims.\n\nThe site has figured out a nifty hack to drive traffic. At first, they drove traffic to WittyFeed from their own \u201cAmazing things in the world\u201d Facebook Page. But then they figured that other Pages with a large following could help them drive a lot more traffic. So they set up a site called Viral9 on which people with a large social media presence, or influencers, can sign up. Influencers can then pick up links to WittyFeed content and share it on their Facebook pages. WittyFeed pays the influencers a cut, depending on how much traffic they generate.\n\nThis was a breakthrough for WittyFeed. Nearly 6,000 influencers including the likes of Womansera, a fortnightly women\u2019s interest magazine and Women\u2019s Rights News have signed up to share WittyFeed\u2019s content. An influencer gets $7 for generating 1,000 views from countries like the US, UK, Canada or Australia. Outside these countries, they get $2 per 1,000 views.\n\nBut signing up influencers to share content meant that WittyFeed needed more content in the first place. Which is why WittyFeed created a platform for authors to create content with ease. An in-house editorial team approves the content and pushes it on to WittyFeed. Authors are paid according to the performance of their stories. Nearly 200 such authors create 80% of WittyFeed\u2019s content. Vaishnav, the co-founder and CTO of WittyFeed, built the platform with less than a dozen engineers and lots of help from startup folks he met in Chennai. \u201cPeople like Harishankaran of HackerRank and Vivek Durai from Chennai\u2019s startup ecosystem have really helped us,\u201d said Vaishnav. HackerRank, is backed by top investors like Vinod Khosla and Silicon Valley-based incubator Y Combinator. Durai runs Termsheet.io, a platform which helps early stage startups raise funds.\n\nThe team also has figured out other ways to make content viral. By reading up closely on BuzzFeed\u2019s strategy as well as applying their own hacks, they have managed to piece together almost a formulaic approach to viral content. \u201cCelebrity content is mostly viral. Then relatable content is viral (for instance a story on what your eye color says about your personality),\u201d says Singhal.\n\n\u201cVirality ka jo scene hai, has three factors\u2026firstly, it should be clickable. Secondly it should be shareable and the third is seeding of content,\u201d explains Singhal. Clickability comes from making people curious with just a smart headline and a good thumbnail. Then comes the content itself. \u201cThe sole purpose of the article is to generate an emotion..any emotion. When that happens, you will share it.\u201d\n\nBut for these two things to happen, you need to push the content first in front of people. \u201cIf you seed the content to the right kind of audience in the right size, it will go through the clickability and sharabilty cycle and become viral,\u201d says Singhal with the conviction of an expert. Virality is also science, adds Vaishnav. BuzzFeed has understood this well and calls it the Process for Optimizing and Understanding Network Diffusion, he points out.\n\nStill, some industry executives are baffled by the numbers shared by WittyFeed. \u201cI\u2019ve never heard of them before,\u201d says a New Delhi media executive, requesting anonymity because he works for one of India\u2019s highest traffic news sites. After visiting WittyFeed, he points out that the site makes users click multiple times to finish reading a story breaking the user experience badly. \u201cWe don\u2019t want to do that. But [to change that] first we need to get our content quality and branding up,\u201d says Singhal.\n\n\u201cThey seem to have cracked the problem from both sides very well. Their cost of acquiring content is low because of the user-generated model and their distribution strategy is also very interesting,\u201d says Sreejith Sivanandan, former Senior Director and Asia Head \u00a0of Publisher Services at AOL. That said, he adds, big-spending brands and advertisers tend to shy away from sites that have too much user-generated content and low editorial authority. Advertisements on WittyFeed \u00a0include those from \u00a0SpiceJet, Firstcry and Google \u2014 mostly served by ad networks run by Google and Rocket Fuel.\n\nThe Monday after the Sarafa outing, Parveen walks into office late in the morning. Vaishnav has\u00a0just woken up from a nap on the sofa after coding through the night with his team to fix some bugs on a new version of WittyFeed. Back in his cabin, which is shared by Shashank, Vani and Vinay, Parveen plans his next piece of viral content for the week: a video for mother\u2019s day that is just around the corner. It\u2019s a simple collation of videos of a bunch of a people saying \u201cmother fuc**r\u201d or similar abuses in different languages. And, ends with a question \u201cHow does it feel when someone says this to your mother?\u201d The English is not quite Wren & Martin,\u00a0but \u2014 \u00a0predictably \u2014 the video has nearly 175,000 views on Facebook.\n\nMaybe BuzzFeed had better watch out. Maybe."},
{"url": "http://uk.pcmag.com/news/77521/start-experimenting-with-googles-science-journal-app", "link_title": "Start Experimenting with Google's 'Science Journal' App", "sentiment": 0.21933865683865683, "text": "Are you a big data nerd? Are your kids at that perfect age where they want to conduct all sorts of little science experiments to learn more about the world and/or blow up some baking-soda volcanos? Curious just what, exactly, your Android phone measures?\n\nGoogle's new app, Science Journal, helps turn your Android device into a tricorder of sorts. With it, you can use your smartphone's sensors to gather data, and the Android app helps you visualize and graph this data in an easy-to-understand way. So, for example, you could use the app to measure sound in a particular area over a period of time, or the movement of the device's internal accelerometers depending on what you're doing (spinning your phone in a circle, perhaps).\n\nThe app doesn't let you do a ton of measurements so far, but Google is working to expand its functionality. It's also partnering with San Francisco's Exploratorium to develop external kits that can be used in conjunction with the Science Journal app\u2014which include various microcontrollers and other sensors.\n\n\"Though we love seeing visitors on our museum floor exploring everything from sound to speed to color, what we love even more is inspiring a world of curious learners. We're excited about making hands-on exploration accessible to people in a place where they already are\u2014their mobile devices. Every time you have a mobile device in your hand is an opportunity to ask questions about the world around you. We hope you'll take it,\" the Exploratorium said in a\u00a0blog post.\n\nGoogle sent out 120,000 different kits to local science museums as part of its Google Field Trip Days initiative, which allows students from underserved communities to attend a local museum for free\u2014including transportation and lunch. Google also sent out 350,000 different pairs of safety glasses to schools, makerspaces, and Maker Faires worldwide, to help protect younger scientists working on more ambitious projects.\n\n\"We're excited to nurture an open ecosystem where people everywhere can use Science Journal to create their own activities, integrate their own sensors and even build kits of their own. To that end, we have released the microcontroller firmware code on GitHub and will be open sourcing the Android app later this summer. We're eager to work with hardware vendors, science educators and the open source community to continue improving Science Journal,\" reads Google's announcement."},
{"url": "https://vikashkoushik.com/the-secret-to-figuring-the-root-cause-to-any-problem-a7191a9e07af?source=latest", "link_title": "5 Whys - The Secret to Figuring the Root Cause to Your Problems", "sentiment": 0.11144418013229895, "text": "The 5 Whys\u200a\u2014\u200aThe Secret To Figuring The Root Cause To Any Problem\n\nThe 5 Whys method was first introduced by Sakichi Toyoda during the evolution of Toyota\u2019s manufacturing methodologies. By figuring the root cause of his problem, not only was Sakichi able to solve the problem, he was also able to understand the situation at his company better.\n\nThat\u2019s exactly what this method is all about. Except, you have to ask yourself \u201cWhy?\u201d only 5 times and not a million times.\n\nIf you notice, by the time they\u2019ve asked us the question \u201cWhy?\u201d for the 3rd or 4th time, we\u2019re put in a situation where we have to think really hard to give a solid answer.\n\nMostly we end up giving some random answer or just tell \u201cHoney, that\u2019s how things work.\u201d so we can end their interrogation quickly and get back to what we were doing.\n\nWe\u2019re probably thinking, \u201cWow! This guy is smart!\u201d or, \u201cFor the love of god, will you leave it already?\u201d.\n\nHave you noticed kids? They have a habit of asking the question \u201cWhy?\u201d a million times!\n\nThis was when I was introduced to a method called 5 Whys .\n\nMy confidence started going down, shifting into doubt. A mild panic began. Once so confident, now I was second guessing everything.\n\nI felt so stupid! I asked too many questions that didn\u2019t help me. What was I thinking? Oh, there I go with another question. Vikash, WHAT\u2019S UP WITH YOU AND THE QUESTIONS?\n\nI was clueless. I had so many questions in my head for which I didn\u2019t have answers.\n\nI sat on my favourite chair trying to analyse the mistakes I made. Few minutes passed. I had nothing. Nothing but frustration.\n\nThe campaign followed the AIDA model . The image was so powerful that it would make any user go \u201cWoah! What\u2019s this?\u201d and catch their attention. It had a copy that would make him desire the product. Everything about the campaign felt so perfect. Yet, it didn\u2019t move the needle.\n\nLast year while working on my startup , I was running a couple of marketing campaigns. Unfortunately, it didn\u2019t turn out the way I would\u2019ve liked it to.\n\nI think it\u2019s possible that you\u2019ve been there. After all, we are human beings and we do make mistakes. I\u2019ve been through this a couple of times (probably even more). It has never been easy getting past those 10\u201315 minutes that usually feel like hours.\n\nHave you ever sat on a sofa with elbows on your knees and hands on your chin, so low in confidence that you started doubting all decisions that you ever took?\n\nHave you ever been through a rough patch so bad, that you felt like you weren\u2019t getting anywhere in life?\n\nThe interesting thing about the word \u201cWhy?\u201d is that it can be beneficial for folks sitting on either side of the table\u200a\u2014\u200athe ones asking the question and the ones trying to answer it.\n\nThe University of Pennsylvania\u2019s Wharton School of Business conducted an experiment to understand the power of asking this simple question\u200a\u2014\u200a\u201cWhy?\u201d. The researchers divided a call center that solicits donations into three and explained to only one of the three groups about the importance of their work\u200a\u2014\u200athe \u201cWhy?\u201d.\n\nBy the end of the month, the researchers noticed that the group that knew why they were making those calls and its importance were able to relate to the problem better. This allowed them to convey the message more effectively, which in turn helped them raise more than double the amount than the other two groups.\n\nHere\u2019s a common example of how asking \u201cWhy?\u201d can bring clarity and why it\u2019s important.\n\nEver noticed entrepreneurs pitching to their potential customers or investors? From the outside, it may seem like entrepreneurs are pitching what their product is and how it solves a problem. What they\u2019re actually doing on a very higher level is selling their vision.\n\nWhen entrepreneurs try to answer questions that are in the lines of \u201cWhy is this problem so important that it needs to be solved?\u201d, what comes out of it is the vision that entrepreneurs have for their company. A vision for a new world that they believe will be much better than the existing one.\n\nWhen investors or customers give their money, among many others, one important reason is because they believe in the entrepreneur\u2019s vision. They believe that the world will be a better place to live when this problem is solved.\n\nSimon Sinek in his amazing TEDx Talk, talks about the importance of explaining the \u201cWhy?\u201d. He goes on to say:\n\nNow that we understand the importance of asking \u201cWhy?\u201d, let\u2019s look at what went wrong with my marketing experiment.\n\n1. Why were our conversion rates so poor from this campaign?\n\nI immediately went to Google Analytics and drilled in to see how people from my ad campaign were behaving. It turned out that they weren\u2019t spending enough time on our site.\n\n2. Why weren\u2019t users spending enough time on our site?\n\nMy initial thought was maybe people didn\u2019t like the product. But how can that be? Users from other channels were converting.\n\nThey were actually PAYING!\n\nDoes this mean this channel didn\u2019t work for us? Should I experiment with other channels? I didn\u2019t have a solid answer. I wanted to be sure that this channel wouldn\u2019t move the needle.\n\nSo, I took the blame and said to myself, \u201cIt must be something that I did in this campaign that is causing the poor conversion rates. What I do know is, at the moment, users from this channel don\u2019t like our product.\u201d\n\n3. Why did the users from this channel not like our product?\n\nThe targeted users were a perfect fit for our user persona. So I knew that wasn\u2019t the problem.\n\nMaybe the users coming from this channel didn\u2019t fully understand our product.\n\n4. Why did the users not fully understand our product?\n\nMaybe I wasn\u2019t clear with the copy in our campaign. Users probably couldn\u2019t relate my campaign\u2019s copy with my landing page\u2019s copy.\n\n5. Why were the users not able to relate to my campaign\u2019s copy with my landing page?\n\nThere was no continuity. The copy I had in my campaign didn\u2019t align with the message that was being delivered on my website.\n\nHow could I say something in my ad campaign, bring users to my site and not help them relate my website\u2019s copy to what was said in the ad?\n\nAfter realising this, I couldn\u2019t blame anyone. The poor conversion numbers made sense. It felt obvious.\n\nI made a blunder and it looks like this is something that a lot of people do!\n\nIn a recent podcast with Unbounce, Joanna Wiebe from Copy Hackers talks about a very similar experience she had with Twitter. Yes, Twitter!\n\nTwitter sends its users a mail asking them to promote their tweets to get a better reach and conversion. Joanna received one such mail asking her to set up a \u201cWebsite Card\u201d for her Twitter account. When she clicked on the call to action button on her email that read\u200a\u2014\u200a\u201cCreate a Website Card\u201d\u200a\u2014\u200ashe was taken to her Twitter account\u2019s in-app page. On this page, the only word that was even remotely close to what she had read in her mail\u200a\u2014\u200a\u201cCreate a Website Card\u201d\u200a\u2014\u200awas \u201ccredit card\u201d!\n\nShe wasn\u2019t eased into the selling process.\n\nShe was (obviously) confused. A normal user probably wouldn\u2019t have known what a \u201cWebsite Card\u201d meant. And all of a sudden, even without explaining the meaning of a Website Card, Twitter was asking its users for their credit card details!?\n\nWhat Twitter could have done to make it easier for its users to understand their offering, was to have a Landing Page that matched with what they were trying to sell\u200a\u2014\u200ain this case, their advertising platform\u200a\u2014\u200awhy one should use it, and success stories of how people who used it in the past.\n\nI believe Facebook does a better job at this. Facebook even to this date follow the basics of building a kick-ass landing page to help its users understand their offering and ultimately convert.\n\nEvery time you see an ad from Facebook asking you to try their advertising platform, you\u2019re automatically taken to a landing page (similar to the screenshot below). This page describes the different types of ads that are available to you, how it can be beneficial, and how it has helped other businesses."},
{"url": "https://www.stevesouders.com/blog/2016/05/23/http-archive-switches-to-chrome/", "link_title": "HTTP Archive switches to Chrome", "sentiment": 0.22000752870318083, "text": "The HTTP Archive crawls the world\u2019s top URLs twice each month and records detailed information like the number of HTTP requests, the most popular image formats, and the use of gzip compression. In addition to aggregate stats, the HTTP Archive has the same data for individual websites plus images and video of the site loading. It\u2019s built on top of WebPageTest (yay\u00a0Pat!), and all our code and data is open source. HTTP Archive is part of the Internet Archive\u00a0and is made possible thanks to our sponsors:\u00a0Google, Mozilla, New Relic, O\u2019Reilly Media, Etsy, dynaTrace, Instart Logic,\u00a0Catchpoint Systems, Fastly, SOASTA mPulse, and Hosting Facts.\n\nI started the HTTP Archive in November 2010. Even though I worked\u00a0at Google, I decided to use Internet Explorer 8 to gather the data because I wanted the data to represent the typical user experience\u00a0and IE 8 was the world\u2019s most popular browser. Later, testing\u00a0switched to IE 9 when it became the most popular browser. Chrome\u2019s popularity has been growing, so we started parallel testing with Chrome last year in anticipation of switching over.\u00a0This month, it\u00a0was determined that\u00a0Chrome is\u00a0the world\u2019s most popular browser.\n\nIn May 2011, I launched HTTP Archive Mobile. This testing was done with real iPhones. It\u00a0started by testing\u00a01,000 URLs and has\u00a0\u201cscaled up\u201d to 5,000 URLs. I put that in quotes because 5,000 URLs is far short of the 500,000 URLs being tested on desktop. Pat hosts these iPhones at home. We\u2019ve found that maintaining real\u00a0mobile devices for large scale testing is costly, unreliable, and time-consuming. For the last year we\u2019ve talked about how to track mobile data in a way that would allow us to scale to 1M URLs. We decided emulating Android using Chrome\u2019s mobile emulation features was the best option, and started parallel testing in this mode early last year.\n\nToday, we\u2019re announcing our switch from IE 9 and real iPhones to Chrome and emulated Android as the test agents for HTTP Archive.\n\nWe swapped in the new Chrome and emulated Android data starting March 1 2016. In other words, if you go to HTTP Archive\u00a0the\u00a0data starting from\u00a0March 1 2016 is from Chrome, and everything prior is from Internet Explorer. Similarly, if you go to HTTP Archive Mobile\u00a0the\u00a0data starting from\u00a0March 1 2016 is from emulated Android, and everything prior is from real iPhones. For purposes of comparison, we\u2019re temporarily maintaining HTTP Archive IE and HTTP Archive iPhone\u00a0where you can see the data from those test agents up to the current day. We\u2019ll keep doing this testing through June.\n\nThis switchover opens the way for us to expand both our desktop and mobile testing to the top 1 million URLs worldwide. It also lowers our hardware and maintenance costs, and allows us to use the world\u2019s most popular browser. Take a look today at our aggregate trends and see what\u00a0stats we have for\u00a0your website."},
{"url": "http://hackaday.com/2016/05/23/the-man-who-didnt-invent-the-personal-computer/", "link_title": "The Man Who Didn't Invent the Personal Computer", "sentiment": 0.11015917736992119, "text": "[John Blankenbaker] did not invent\u00a0the personal computer.\u00a0Museums, computer historians, and authors have other realities in mind\u00a0when they say [John]\u2019s invention, the KENBAK-1, was the first electronic, commercially available computer that was not a kit, and available to the general population.\n\nIn a way, it\u2019s almost to the KENBAK\u2019s detriment that it is labelled the first personal computer. It was, after all, a computer from before the age of the microprocessor. It is possibly the simplest machine ever sold and an architecturally unique machine that has more in common with the ENIAC than any other machine built in the last thirty years..\n\nThe story of the creation of this ancient computer has never been told until now. [John], a surprisingly spry\u00a0octogenarian, told the story of his career and the development of the first personal computer at the Vintage Computer Festival East last month. This is his story of not inventing the personal computer.\n\n[John] began his career as an intern working at the National Bureau of Standards in Washington, D.C. He was assigned to the SEAC, a project to build a\u00a0relatively small computer for the US Government before a much larger computer could be completed.\n\nThe SEAC was a small computer, but it was not by any means simple. There were over seven hundred vacuum tubes in the racks, tens of thousands of very expensive diodes for all of the logic. The memory was a set of mercury delay lines \u2014 tubes, filled with liquid mercury, with acoustic transducers at each end. These transducers would send pings through the mercury, representing zeros or ones. Because of the speed of sound through this mercury, a small amount of information could be stored, like a gigantic shift register.\n\nThere were only a handful of instructions available for this computer \u2013 addition, subtraction, multiplication, division, comparison, and an instruction for input and output. There weren\u2019t many instructions, but it was enough; SEAC was the first stored program computer in the United States, and the only computer in Washington, D.C. Everything from calculations needed by Los Alamos to navigation tables for the new LORAN network were created on SEAC.\n\nAfter working for the National Bureau of Standards, [John] moved on to Hughes Aircraft. His work on the SEAC made him extremely valuable for another small computer \u2013 an airborne computer, built in 1952. This was only a few years after the development and commercialization of the transistor, and as such this airborne computer used the latest vacuum tubes, arguably the most technologically advanced vacuum tubes ever made. All the memory was contained in a rotating magnetic drum, and most of the logic was implemented with diodes so expensive they were stored in a safe.\n\nThis was the very beginning of the computer age, and as such semiconductors were in short supply. Adding a flip-flop to a design meant the cost of this computer would increase by $500. It was the time of small computer systems, and the architectural limitations of early computers wasn\u2019t because they couldn\u2019t build them bigger; these computers were small because anything bigger would cost too much.\n\nIn 1970, [John] found himself unemployed, with $6000 in his bank account from a severance package. He decided if he was ever going to build a small computer, this was the time. He wanted an educational computer \u2014 a machine that would teach people how to use a computer. It would have to be cheap, and something that a single person could operate. A personal computer, if you will.\n\n[John] settled on a very basic computer, with an architecture not unlike the SEAC he built 20 years before. There would be three registers, A, B, and X. Five addressing modes complimented a handful of instructions \u2013 add, subtract, load, store, AND, OR, a few jumps, conditionals, I/O, and of course a NOP. The memory would be two shift registers configured as serial memory, a much smaller and less toxic version of SEAC\u2019s mercury delay line memory.\n\nWith the right architecture in hand, [John] began building his computer. To keep costs low, he went with off-the-shelf parts. The were 132 standard TTL logic ICs, accompanied by two 1024-bit MOS shift registers. The machine had 256 bytes of memory (although the word size was not eight bits), and no ROM. It wasn\u2019t fast, it wasn\u2019t powerful, but anyone could program it by punching ones and zeros into the front panel.\n\nWith a design and a few tweaks to the circuit, [John] had a computer. The plan always was to manufacture this computer. That meant sales, and that meant a market.\n\nIt\u2019s easy to see the ideal market for the earliest computers would be scientists, academics, and engineers when looking back on nearly fifty years of computer history. Universities were\u00a0Digital Equipment Corporation\u2019s largest market after the government, after all. [John] designed the KENBAK as a computer trainer, and decided teachers \u2014 high school and community college teachers \u2014 would be the market where the utility of a personal computer would be apparent.\n\nIn May of 1971, [John] trundled out his booth to a convention of high school mathematics teachers nearby. The computer worked, and he had a few programs loaded into the memory, the most memorable of which would give the day of the week for any date in the 1900s. There are eight lights across the face of the KENBAK, and with just a little bit of code, this computer would blink the first light for Monday, the second light for Tuesday, all the way down the line until an invalid date would light up the eighth light.\n\nApparently, and to [John]\u2019s great surprise, high school mathematics teachers don\u2019t know the rules for the calendar. Nevertheless, he did get a lot of interest from private individuals, colleges, and universities. KENBAKs were shipped around the country, to France, Italy, Mexico, and Canada. Only forty or so KENBAKs were ever sold,\u00a0with seven units ending up at a technical college in Nova Scotia. Canada.\n\nIn the mid-1980s, long after selling his inventory off and moving across the country to Pennsylvania,\u00a0The Computer Museum in Boston \u2014 the forerunner of the Computer History Museum in Mountain View, California \u2014 announced they were trying to find the\u00a0first personal computer. These submissions (or more cynically; these donations to the museum) would be judged by a panel of technologists including [Woz], [David Bunnell], the publisher of\u00a0PC World,\u00a0and\u00a0[Oliver Strimpel].\n\nOf course, any attempt to find the first personal computer eventually becomes an argument of adjectives.\u00a0Computers had existed before 1970, going back to [Konrad Zuse]\u2019s Z3, the\u00a0Colossus\u00a0at Bletchley Park, or later the programmable ENIAC. These computers were each firsts in their own way, and technically the Computer Museum produced two winners for their \u2018first computer\u2019 contest; the French Micral from 1973 was\u00a0called, \u201cthe first commercially available microprocessor based computer.\u201d\n\nThe KENBAK, according to the Boston Computer Museum, The Computer History Museum, and [Woz] himself, was the first electronic,\u00a0commercially available computer\u00a0that was\u00a0not a kit and available to the general population.\n\nOf course, [John] will never say he invented the personal computer. He says there were people who had built computers out of logic chips before him. Those people just didn\u2019t commercialize their computers. There were computers sold to the public before the KENBAK, as well. [John] remembers a computer sold by FAO Schwartz that had a drum with pins and holes, and would complete operations in an automated fashion.\n\n\u201cOne of the things that people sometimes say about this is that I invented it. I always say there was no invention, it\u2019s just logic, only ones and twos, that\u2019s all it is,\u201d [John] said closing his talk at the Vintage Computer Festival, \u201cthere was no radical thing that I invented. I just programmed a computer.\u201d"},
{"url": "https://modelviewculture.com/pieces/why-i-founded-geek-moms-company-my-journey-as-a-black-mom-in-tech", "link_title": "Why I Founded Geek Moms and Company: My Journey as a Black Mom in Tech", "sentiment": 0.11212915872006779, "text": "\u201cYou will always be barefoot, pregnant and stupid.\u201d\n\nAbout nine years ago was the first time my mom said this to my face. She always thought that none of the lessons she taught me would ever stick. This one did. I had no idea where my life was heading, but I knew I had to prove her wrong.\n\nAt the time I was 19 and pregnant with my second child. I had just broken up with my ex and was looking for a place to stay, homeless and begging my step-mom to let me stay with her. I was the kid with a million ideas: from trying to self-publish my poetry, to selling natural oils, even recording my so-called singing in hopes of being able to perform at venues. But I\u2019d done nothing with those dreams. My mom just wanted me to get a \u201cnormal\u201d job. She wanted me to go back to Home Depot and work my way up to becoming a manager. I flat out refused.\n\nSince then, I hated every job I ever got and I failed at every business I ever attempted. I hated working for other people: getting blamed for things I didn\u2019t do, paid crappy money UNDER THE TABLE, ripped off because the drawer was short 5 cents. My last straw was working for a hair shop where I was not only discriminated against for being Nigerian, but was paying $150 a week for a booth fee and 50% of the money I made for every client I booked.\n\nI made more money being a kitchen hair braider, so I went back to it. I booked a lot of clients at home, but I was selling myself short and did way too many hours of great service for cheap pay. I thought I had no other choice, even if standing for 12 hours+ a day, pregnant with my fourth child was killing me. My now-husband hated watching me work. He argued with me to stop taking so many clients, but I wouldn\u2019t listen until one day my fingers completely cramped up in excruciating pain. I had to find a better way.\n\nI had a Nikon D40 and a Craigslist ad: I was going to find work as a photographer. I got hired for a few small gigs; most I had to do for free. Then one day I saw an ad: \u201cAre you creative? Do you love Technology?Join i.c.stars\u201d.\n\nIt turned out i.c.stars was a hub for Chicago-area, low-income young adults to develop skills in business and tech. I had beamed over the word CREATIVITY, so I applied. When they called me in for an interview, the process was nerve wrecking. They were asking strange questions: Did I want to provide jobs for others? Did I want to be a Change Agent? Did I want to be a Leader in my Community?\n\nNever before that moment, had I EVER thought about \u201cMY COMMUNITY.\u201d I was homeless several times, living off of public aid, just trying survive. Community? I could barely help myself.\n\nI looked around at the people who were interviewing me. They were serious. Meanwhile I was asking myself, Do I have this option? Do they really think I can do these things? It hit me hard. I went home after the interview, thinking I hadn\u2019t made it in.\n\nThen I got the email: accepted.\n\nAs an intern in the program, I would spend 4 months dedicating my life to being a change agent, understanding the world of tech through project-based learning, and developing a solution for a client company. The first week was intense. I had no clue what was going on, but I felt a change. I remember the day Sandee, the founder of i.c.stars gave a talk about the power we had. She taught me that being in the technology space, I could be a driving force in the community, making a real impact. I\u2019ve held on to that idea ever since.\n\nAfter i.c.stars, I got an internship, for a QA position, at one of the hardest technology companies to interview for. When I finished the internship, I had to interview again to get the full time position. Each interview evaluated a different skill set. This was real life and the real world of working in technology: I couldn\u2019t mess this up. I was so afraid that it was going to get taken away from me for any reason, just to be given to someone who was more deserving, you know, someone with more privilege than me.\n\nI was interviewing along with with my best friend and a few other people. I knew she was going to get in for sure: she\u2019s super smart, dedicated to her craft and just overall a great person. But me? HA!\n\nAt least it was great while it lasted.\n\nAfter the interviews, I was pulled into the room, and I received my offer letter. The recruiter sat me down and told me, \u201cNo one can take this from you, this is your job and you deserve it\u201d. I broke down in tears. No one can take this from you. That statement replaced all the doubt, it replaced my insecurities, it replaced the \u201cbarefoot and pregnant\u201d quote in my head.\n\nOnce I was in, I learned about imposter syndrome: I definitely had it. I was a black mom of 4 with no college degree. I needed to be on top of my studies. More books, more classes, more late nights and avoid all \u201cCollege conversations\u201d, \u201cPolitics conversations\u201d and \u201cCurrent Events conversations\u201d. I was so scared to be found out, for people to notice that I snuck into this job under the radar, to be forced to go back to work as a cashier at Home Depot.\n\nIt took a year for me to complete shake those thoughts. I worked on many client projects and survived, dammit.\n\nI can do this.\n\nBut once I conquered imposter syndrome, it was time to face life again: I was pregnant.\n\nWith child number 5, I was now the pregnant mom at work. The one who had to leave early for appointments, the one too nauseated to fly to a client, too nauseated to show up at my home office, the one running late every morning because I needed 12 more hours of sleep. I turned into the mom with excuses. I truly thought I was going to get fired, until I made it to maternity leave.\n\nBy the time I got back to work, I was better. I felt better about my capabilities as a consultant. I had a clearer vision on my mission. I was ready and it showed. I got positive feedback that boosted my confidence; in return I worked harder. But I soon realized the consequences. My kids were left out the picture. I had to travel; my kids needed me and I wasn\u2019t there. I supported them from a distance, but to me it was still wrong to be so far away.\n\nI still don\u2019t like talking about it: the guilt doesn\u2019t go away and I\u2019m still traveling. I have to. This job has given me so much. I can give my kids experiences rather than buying them a toy. I can show them what mommy does for a living and they are welcomed with open arms. I can take them to workshops, festivals and fairs that I can finally afford to go to when I\u2019m in town. So many blessings that I can pass down to my kids, but I still needed to give them my time. It still kills me, but I\u2019m getting better with it.\n\nMy side project keeps me sane. Geek Moms & Company was founded while I was pregnant with my 5th kid, it\u2019s my thing that I can\u2019t let go, baby number 4 and 1/2. I wanted to give more women opportunities to grow and conquer, while doing what I loved to do: sharing their stories through content creation. So, Geek Moms & Company was born: a platform for women in the technology space. Our mission is to influence and inspire others to share authentic stories and embrace new challenges. We provide digital media services for women business owners, women-led startups and entrepreneurs, along with resources to access opportunities for growth.\n\nThe original problem I wanted to address was that there were not enough black women in technology. Is this due to lack of opportunities? Lack of resources? Lack of confidence? After some research, I learned that there are many factors. After living it, and working on The Black Tech Activist, a YouTube project I started, I saw the solution. Being present, believe it or not, is making an impact. \u00a0A colleague of mine, who faces discrimination everyday at her job, is still showing up to work and has fallen in love with the tech community. I asked her why, and that\u2019s when it came in full circle: people see us in this space. For now it\u2019s small, but we\u2019re here. The more we grow, the more people will see us, and as we make strides in our careers, other women who are trying to find a way will see that this is a possibility for them, too.\n\nAfter hearing that, I decided that I want more people to see us. I can do that with social media, with documentaries, with blogging and visual content. I can help other women tell their stories, I can help moms tell their stories.\n\nAs I work my 9 to 5, and I work on Geek Moms & Company, I quickly realize that there is no balance and no extra time. Everyone is suffering. I had to learn how to bring it all together. It\u2019s hard, it\u2019s exhausting but it\u2019s possible. Get the right support system who believes in you no matter what happens. Don\u2019t be afraid to stand behind YOUR passion. Show the process. Be open. Be real.\n\nThe kids are screaming, and I show it in my videos while I\u2019m recording. Who has time to reshoot? This is life.\n\nI want my babies to see my hard work, I want them to see themselves on my websites, in my videos, in every aspect of my life. I want them to be proud of me."},
{"url": "http://motherboard.vice.com/read/researchers-use-developer-biometrics-to-predict-code-quality", "link_title": "Researchers Use Developer Biometrics to Predict Code Quality", "sentiment": 0.006150793650793657, "text": "Informatics researchers from the University of Zurich have developed a not at all sinister-sounding system capable of predicting the quality of code produced by developers based on their biometric data. By using heart rate information, for example, they were able to quantify the difficulty a given programmer had in producing a piece of software. This information could then be used to identify likely sections of bad code. Pre-crime for software debugging, in other words.\n\nThe Zurich researchers, Sebastian C M\u00fcller and Thomas Fritz, describe their work in a paper presented this week at the 38th International Conference on Software Engineering in Austin.\n\nHere's the not at all sinister way that M\u00fcller and Fritze open their paper: \"A commonly accepted principle in software evolution is that delaying software quality concerns, such as defects or poor understandability of the code, increases the cost of fixing them. Ward Cunningham even went as far as stating that 'every minute spent on not-quite-right code counts as interest on that debt.'\u201d\n\nA common first line of defense against bugs and just poor quality code is the code review, the duo notes. Basically, one developer writes some code and then passes it on to someone else, who sort of acts like a code editor. The reviewer/editor looks over the completed code for defects and places where the code might be improved.\n\nThis is a costly system, however: Code reviews take time and people. Automated review systems exist and sort of work, but they run up against two barriers. \"First, they are predominantly based on metrics, such as code churn or module size, that can only be collected after a code change is completed and often require access to further information, such as the history of the code,\" the paper notes. \"Second, they do not take the individual differences between developers comprehending code into account, such as the ones that exist between novices and experts.\"\n\nEnter biometrics. By looking at the programmer as they program, rather than the code after the programmer is done writing it, the system described by the Zurich researchers finds code quality issues as the code is being produced. Previous research has already show that heart rate variability (HRV) or electrodermal activity (EDA) can be accurate indicators of task difficulty or difficulty in comprehending code snippets. The more difficult the task, the higher the cognitive load and chances for errors to be made.\n\nThey tested this out with two teams of developers from Canada and Switzerland. Biometric data was collected as they wrote software and this was correlated with interviews with the developers and human-based code reviews.\n\nThe paper at least notes (barely) the obvious privacy concerns with this, but it doesn't go much further than that. It's more than just a matter of privacy, really, but of general invasiveness (which is a bit different) and the really glaring, obvious problem of monitoring programmer stress as they program itself sounding like the most stressful scenario ever\u2014like a coding interview that never ends where you also happen to be naked. Like that."},
{"url": "http://arstechnica.com/tech-policy/2016/05/googles-closing-argument-android-was-built-from-scratch-the-fair-way/", "link_title": "Google\u2019s closing argument: Android was built from scratch, the fair way", "sentiment": 0.19285431235431236, "text": "SAN FRANCISCO\u2014Google attorney Robert Van Nest made his closing argument to a panel of jurors here today, asking them to clear\u00a0Android of copyright infringement\u00a0allegations as a matter of \"fairness and fair use.\"\n\n\"This is a very important case, not only for Google but for innovation and technology in general,\" Van Nest told the jury.\u00a0\"What Google\u00a0engineers did was nothing out of that mainstream. They\u00a0built Android from scratch, using new Google\u00a0technology,\u00a0and adapted technology\u00a0from open sources. Android was a remarkable thing, a brand-new platform for innovation.\"\n\nVan Nest's hour-long\u00a0closing argument, and his later rebuttal,\u00a0was\u00a0Google's\u00a0final fusillade before\u00a0this six-year-old lawsuit goes to the jury. Oracle\u00a0has argued\u00a0that Google's use of 37 Java APIs in Android infringes copyrights that Oracle\u00a0acquired when it purchased Sun Microsystems. An appeals court has already found that APIs can indeed be copyrighted. Unless the ten-person\u00a0jury empaneled\u00a0in San Francisco finds that Google's use of APIs was \"fair use,\" Oracle will win damages, and the company is\u00a0hoping to\u00a0ask for as much as\u00a0$9 billion.\n\nThe Java language was \"open and free,\" and the APIs \"have always been treated by Sun as open and free, along with the language,\" Van Nest said. Android\u00a0was a brand-new use for the Java APIs, \"a use that\u00a0no other company, before or since, has been able to achieve.\"\n\nAnd Sun never complained about Android, he said.\u00a0Oracle CEO Larry Ellison didn't, either\u2014at first.\u00a0\"it wasn\u2019t until later that Mr.\u00a0Ellison\u00a0changed his mind,\" said Van Nest. \"It was after he had tried to use Java to build his own smartphone and failed to do it.\"\n\n\"Now we\u2019re in a situation where Oracle, which had no investment in Android, took none of the risk\u2014they want all the credit and a lot of the money,\" he said. \"And that\u2019s not fair.\"\n\n\"It takes strength and courage to stand up to Google. That's what Oracle has done.\" Van Nest focused on four key witnesses: former Sun Microsystems CEO Jonathan Schwartz, former Google CEO Eric Schmidt, former Android chief Andy Rubin, and Alphabet CEO Larry Page. He pulled up Schwartz's picture first, and reminded the jurors that he didn't have any problem with Android.\n\n\u201cIt was completely\u2014you know, it was fair,\" Van Nest said, quoting Schwartz's testimony to the jury.\u00a0\"They weren\u2019t asking us to put our logo on it, and they weren\u2019t asking us to call it Java or bless or endorse it.\u201d\n\nThen Van Nest moved to parry an attack he knows is coming from Oracle, which will give a closing statement immediately after him. Oracle is\u00a0going to run through Google and Sun internal e-mails about how licensing was required, part of a failed negotiation between the companies that took place before Android was launched. It's a red herring, he told the jury.\n\n\"That has nothing to do with the\u00a0Java API declarations,\" Van Nest said. \"Those discussions happened much earlier,\" in 2005 and 2006.\n\nAndroid was a classic example of fair use, Van Nest\u00a0said, linking evidence with the jury instructions. First, it\u00a0was utterly transformative, he argued, noting that even\u00a0Oracle's own expert witness teaches Android programming to his computer science students at Duke University.\n\nGoogle engineers used APIs from Java SE, a product for desktops that was never intended for smartphones. The API declarations used were \"purely functional,\" Van Nest said,\u00a0another factor that favors fair use. Packages had names like java.io and java.security, while classes had names like getDateAndTime and ZipInputStream.\n\n\"Android has helped Java,\" Van Nest said. Developers can write programs for Android without learning a new language. He pointed to deposition testimony of an Oracle VP calling Android a \"positive for the mobile phone market.\"\n\nVan Nest completed the circle by noting that Google was following standard industry practice, another factor he urged the jury to consider\u00a0as weighing in favor of fair use.\u00a0\"Every single computer programmer or scientist\" that testified had an example of using Java APIs, including Apache Harmony and GNU Classpath.\n\n\"There was no hue and cry in 2007,\" when Google published Android's\u00a0software development kit, along with all of the APIs used, new and old, he said. \"As long as you write your own implementing code, that's fine. You're OK.\"\n\nThat's why Google wouldn't pay up when it was approached by Oracle in 2010.\u00a0\"We will not pay for code that we are not using, or license IP that we strongly believe we are not violating, and that you refuse to enumerate,\" Google said, quoting Google's response to\u00a0Oracle CEO Safra Catz.\n\n\"Google's position then is the same as Google's position now,\" he said. \"Android is a fair use of free and open API declarations that have been open for years.\u00a0You can't come around, three, four, or five years later and say, we want to go back to the\u00a0start and change everything. That's not fair, and that's not right.\"\n\nOracle will make its closing statement next, and the jury will begin deliberations after that. If the jury finds in Oracle's favor, the case will enter a separate damages phase.\n\nMore from the Oracle v. Google trial:"},
{"url": "https://medium.com/@preslavrachev/the-role-of-the-full-stack-developer-in-large-projects-a368e18485d5#.a6hvykpxx", "link_title": "The Role of the Full Stack Developer in Large Projects", "sentiment": 0.14570522032612193, "text": "A little less than two decades ago, things were simple. Software developers were primarily engaged with developing software end-to-end. There was no major distinction between the people creating the UI, and those developing the backend. Most applications were either desktop monoliths, or desktop \u201cclients\u201d that communicated with some kind of server backend, over the network:\n\nIn either case, early development tools provided enough building blocks, for developers to create everything under the same project. This resulted in homogenous, but boring, and unappealingly looking applications. Back in those days, Web was, for the most part, a bunch of static pages, linking to one another. With some eccentric exceptions, most of them resembled the look and feel of their blocky Windows counterparts.\n\nA few years later, during the early and mid-2000s, came the Web 2.0 revolution, which swept everything away. Rather than remaining single-purpose monoliths, software applications turned into distributed \u201cservices\u201d. The traditional model of installing hundreds of megabytes on one\u2019s computer, got replaced by typing the URL of one\u2019s favourite web app in a Web browser. With time, the relevance of the underlying operating system diminished. In a little less than a decade, the browser became the OS\u200a\u2014\u200afor many, the one and only window kept open at all times.\n\nWeb 2.0, and the following foray into mobile, forced significant changes, into how the new wave of software applications was supposed to be built. For once, backend and frontend development became separate disciplines, each requiring a different set of skills and expertise. In fact, in recent years, the gap between frontend and backend has become became so huge, that a new discipline has arrived, intended to glue those pieces back together\u200a\u2014\u200athe role of the full stack developer.\n\nIf you look at job listings nowadays, it seems like just about everyone is looking for developers who can work on all aspects of a project. Many companies call this a \u201cfull stack\u201d position, and in fact many organizations already have opened such \u201cfull stack\u201d positions across their teams. I find this idea of a \u201cfull stack\u201d developer being a \u201cknow-it-all\u201d whiz, flawed, and dare believe that relatively few organizations have managed to raise the new role to its full potential. This post tries to shed some light on where the full stack developer should ideally stay within a team, hoping to help dissolve those misconceptions.\n\nThere are two very contradicting misconceptions that get tossed around, when it comes to defining what a full stack developer is, and the full stack is supposed to do:\n\nNeither of these is 100% wrong, but neither is correct either. A full stack developer is not a whiz kid, who could magically replace a team of backend and frontend experts. No. Though a full stack developer is supposed to understand both worlds, her role is not to replace, but help bring those two groups closer together. In fact, a good full-stack developer is a bit like the bass player in a rock band:\n\nPlaying bass in a rock band is an often underrated, but an incredibly important role. Unlike the vocalist or the guitar player, the bassist takes little credit for her performance. With the exception of jazz and funk music, the bassist rarely does have a prime time on her own. She won\u2019t always pull out a breathtaking solo, the way the lead guitar would do. She also won\u2019t always engage the fans, the way only the vocalist could do. Yet, a good bass player will always be there, gluing the entire band together.\n\nHave you ever seen a good rock band with a bad bass player? Or worse, a self-respecting rock band with no bass player at all? No? Well, there are a few good reasons for this:\n\nFirst of all, there is a huge sound spectrum gap between drums at the low end, and high-pitched guitars and vocals at the other one. This gap is usually filled by the bass. Taking the bass out of the equation, leaves a shallow tune, left of \u201csoul and spirit\u201d. You can still listen to it, but it will always feel as if something is missing.\n\nHaving a skilled bass player in the band though, is just as important for the other band mates, as it is for the audience. One of the hardest things in playing in a band, is playing in tune and sync with the others. This is where the role of the bass player as a coordinator and a hub between the players is so important. She keeps the tempo and rhythm set by the drummer at all times, laying out a foundation of the main melody, which stays pretty much the same throughout the song. This allows the guitar player to pull off a beautiful solo, without fear of the sound breaking loose.\n\nA software team nowadays, looks pretty similar to the rock band, I have been describing so far. You usually have pure backend and frontend developers, covering their own spectra of tasks. If the project at hand is well specified, both sides typically agree on an API and keep doing what they do best, sharing little concern of how the other side is doing things.\n\nAs we all know though, very few things in life work in their ideal state. Much of the time, developers spend clearing out the misunderstandings in their communication. They do this at lengthy meetings. If you have been in such a meeting, you know how little respect a frontend guy would pay to your detailed proposals for speeding up the backend. Let\u2019s be honest though, when it comes to frontend guys explaining their problems, it\u2019s your turn to start looking at your phone. That\u2019s just the reality. More often than not, the visions of the two sides deviate, and the only way to bring them back on track are more meetings, and refactoring efforts."},
{"url": "https://medium.com/@nashvail/nerding-out-with-bezier-curves-6e3c0bc48e2f#.vr9im62b7", "link_title": "Nerding Out with Bezier Curves", "sentiment": 0.15078616898148142, "text": "Bezier curves are very special curves. The math and the idea behind them blew me away and you should get ready to be blown away too. Since you\u2019re reading this I assume you are a designer or a developer and have dealt with Bezier curves before, especially Cubic Bezier curves, we\u2019ll get to what Cubic Bezier curves are in a second. Now these curves are used in a variety of places, to create vector graphics, animation paths, animation easing curves e.t.c only because they are so easy to control. You don\u2019t need to know a whole lot of Math, none at all to bend these curves to your whims. Think if Bezier curves didn\u2019t exist and people had to come up with unique Mathematical functions for curves for let\u2019s say drawing vector graphics like fonts for example, a nightmare of course. Alright, time for some Math. I\u2019ll start with the general formula for Bezier curves, it's quite daunting at the first sight, but we\u2019ll make our way through. \u201cWhoa! Whoa! Whoa! Einstein!\u201d. Hey wait, don\u2019t click away. It\u2019s easy, look, I made it so colorful \ud83d\ude42. Let us start breaking the formula down. You can skip over the parts you already know. B because it\u2019s a Bezier curve. As mentioned earlier in the article about parametric form of curves, t is a parameter. You plug in t and out comes x and y, a point. We\u2019ll soon see how that works with the equation above. It\u2019ll be good to mention here that for Bezier curves the value of t should be between 0 and 1, both included. This symbol, \u03a3, in Mathematics is called the summation operator. The way it works is like this, on the right of this symbol is an expression with a variable i, and i can only hold integer values. On the top and bottom of the symbol we write the limits of i. For each value of i the expression to the right is evaluated and added to the total until i reaches n. Just a shorter notation for something longer. Alright, looks like we\u2019re clear with sigma. This C here, is the C from Permutations and Combinations. Let\u2019s have an impromptu Combinations lesson shall we. Now, in the formula this part is what is called a Binomial coefficient. The way to read nCi is like this, n Choose i. Which is to say given n items in how many ways can you choose i items out of it ( n is always greater than or equal to i). Okay, that might not have made a lot of sense, consider this example\u00a0: I have 3 items a circle, a square and a triangle. Therefore here, n = 3. In how many ways can I choose 2(i = 2) items out of the 3. In the language of Mathematics that would be written as 3C2 ( 3 Choose 2). The answer is 3.\n\nAnd when i is 0, there is just one way to choose 0 items out of n, 1, to choose none at all. Instead of drawing sketches and figuring out the answer to a given Combination expression, there is this generalized formula. This is the important bit. In the general formula for Bezier curve there is t, i and n. We haven\u2019t really touched on what n is. n is what is called the degree of the Bezier curve. n is what decides whether a Bezier curve is linear or quadratic or cubic or something else. You see, if you have used the pen tool before, you click at two distinct locations to create two distinct points and then you control the curve that is formed between the two points using handles. A Bezier curve is always going to have at least two anchor points, and the remaining are control points that are used to control the shape of the curve. Also, what you call handles are just the control points connected by a line to a anchor point, they\u2019re just there to provide a better mental model. So when you adjust the handles, in reality you are simply changing the coordinates of the control points. Let us get rid of all the accessories and focus on the core. The curve you see in the image above is a Cubic Bezier curve, or in other words the degree of the Bezier curve shown above is 3, or in the general formula for Bezier Curves you plug n = 3. n = 1 gives you a linear Bezier curve with two anchor points P0 and P1 and no control points, so it essentially ends up being a straight line. n = 2 gives you a quadratic Bezier Curve with two anchor points P0 and P2 and one control point P1 and similarly n = 3 gives you a Cubic Bezier curve with two anchor points P0 and P3 and two control points P1 and P2. The higher the n, the more complicated shapes can be drawn. Now we\u2019re going to form from the general equation the equation for Cubic Bezier curve which involves substituting n = 3 in the general formula. The equation we will get will be in the variable t which as mentioned earlier is a parameter whose value varies between 0 and 1. Also, for the equation we will need 4 Pis ( read: Pee eyes ) P0, P1, P2 and P3. The choice of these points is up to us, after all when we draw vector graphics say using the pen tool we choose the position of anchor points and the control points don\u2019t we? After the changes our equation for Cubic Bezier curve looks something like this.\n\nYou are about to witness something very special about these equations. To recap, the mentioned equation is the parametric form of the Bezier curve with the parameter t which can hold values varying between 0 and 1. A curve is a collection of points. Each unique t you pass in to B gives a unique point that builds up the whole Bezier curve. The magical thing about the equation is that when t = 0, B(0) = P0 and when t = 1, B(1) = P3, therefore, the extreme values of t, 0 and 1 give the extreme most points of the curve which are of course the anchor points. This is not true just for cubic Bezier curves, for a curve of degree n B(0) = P0 and B(1) = Pn. For any other value of t between 0 and 1 (e.g t = 0.2 in figure above) you get a point which builds up the curve. Since the whole equation is dependent on the position of the Pis (Pee eyes) changing their position changes the shape of the curve. And that is how Bezier curves work. Now that we know the Math behind Bezier curves let\u2019s put that knowledge to some use. I have created a simple JavaScript program that renders a cubic Bezier curve, there is no UI to interact with it because I didn\u2019t want the logic to fade away in all of the UI code and also because I am lazy. But that doesn\u2019t mean you can\u2019t interact with it\u00a0:).\n\nWas that a little too much to take in? We started out with defining what curves are, from there we moved to points and how they are the building blocks of a curve. Then we went on to Bezier curves and understood the Math to find points that make a Bezier curve up. I hope you learnt something and leaving this article smarter than when you started reading it. The code for the little custom Cubic Bezier engine can be found in this GitHub repo. If you liked what you read or didn\u2019t let me know. Also if you have a second do recommend and share this article for others to discover. Thanks for reading. Have a good one!"},
{"url": "https://cdn.ampproject.org/c/boingboing.net/2016/05/23/monitoring-programmers-stres.html/amp", "link_title": "Monitoring programmers stress levels accurately predicts quality of their code", "sentiment": 0.1958333333333334, "text": "In Using (bio)metrics to predict code quality online, presented at the ACM's 38th International Conference on Software Engineering, two Swiss researchers presented their work on monitoring programmers' biometrics to predict the quality of the code they were writing.\n\nThe researchers showed that programmers' stress levels were a better predictor of the quality (and thus the maintainability) of their code than the more costly process of code-review, where another programmer checks a colleague's work before it is put into production, in order to ensure that it will be clear to future maintainers of the system.\n\nThe finding means that programmers wearing biometric monitors can have the code they need to rework automatically flagged as they write it, and conduct their own code review after the fact. Motherboard's Michael Byrne argues that this is privacy-invasive and could well introduce its own stress.\n\nBut if programmers' biometrics are being measured for their own benefit, especially passively (say, by using a webcam's infrared capacity to measure heartrate and the capacitive touchpad to measure skin conductivity), then system could work more like a spellchecker: after the day's work is done, you run it through a quick check and the machine asks you to look again at potential trouble-spots.\n\nAmongst other results, our study shows that biometrics outperform more traditional metrics and a naive classifier in predicting a developer\u2019s perceived difficulty of code elements while working on these elements. Our analysis also shows that code elements that are perceived more difficult by developers also end up having more quality concerns found in peer code reviews, which supports our initial assumption. In addition, the results show that biometrics helped to automatically detect 50 percent of the bugs found in code reviews and outperformed traditional metrics in predicting all quality concerns found in code reviews.\n\nUsing (bio)metrics to predict code quality online [Sebastian C. M\u00fcller and Thomas Fritz/Proceedings of the 38th International Conference on Software Engineering]"},
{"url": "https://www.youtube.com/watch?v=zqrpKUTMXgY", "link_title": "Brain and Enlightment", "sentiment": 0.13333333333333333, "text": "Neurotheology 3 - This lecture, by neuroscientist and author Todd Murphy, explores an hypothesis about what happens in the brain when a person attains enlightenment. Using concepts in neurotheology developed by Michael A. Persinger (inventor of the God Helmet), Murphy (inventor of the 8 Coil Shakti neural stimulation system) explores the brain's role in enlightenment as understood in Buddhism. The talk also looks at the self (or sense of self), and how it's place as a brain function allows it to be flexible enough to change as a person becomes enlightened. It examines a few case histories, including those of Ramana Maharishi, Bhagwan Shree Rajneesh, The Buddha, and some others.\n\n\n\nNov. 8th 2013 - Todd Murphy's book \"Sacred Pathways: The Brain's Role in Religious and Mystic Experiences\" (foreword by His Holiness, The Dalai Lama) is now available. It has a more in-depth discussion of enlightenment than this video lecture. Here is the URL: \n\nhttp://www.shaktitechnology.com/sacre..."},
{"url": "http://priceonomics.com/what-is-the-internets-favorite-book/", "link_title": "What Is the Internet\u2019s Favorite Book? (Data: Goodreads)", "sentiment": 0.19123231632801022, "text": "Which is the better book: War and Peace or installment one of The Hunger Games?\n\nIf you ask a book reviewer or look at any of the \u201cBest Book\u201d lists compiled by \u00a0critics, you would say War and Peace. But what if you asked everyday readers on the Internet?\n\nOver four million member of the website Goodreads members have rated the first installment of The Hunger Games on a 1-5 scale, and it has received an average score of 4.36. It currently sits atop a Goodreads crowdsourced list of \u201cBest Books Ever\u201d. By comparison, the average score given by the 150,000 people who have rated War and Peace is 4.10, and it ranks 724th on the \u201cBest Books Ever list\u201d.\n\nIt\u2019s no surprise that on a crowdsourced ratings site, a briskly paced young adult novel beat out a dense, 1,000 page philosophical epic about Napoleon\u2019s invasion of Russia. And it\u2019s probably not exactly the same group of people rating the Hunger Games and War and Peace. They likely have different backgrounds and expectations in literature.\n\nStill, Goodreads ratings provide a glimpse into the literature that people actually like the most, and how that might differ from the critics. We know what the literati think from the variety of literary prizes and lists of books you must read before you die. But what do the people say? We collected the ratings for tens of thousands of books on Goodreads to find out.\n\nOur analysis shows that the books people are most exuberant about include Calvin and Hobbes, manga and South American poetry. And if you want to read a classic blessed by both the critics and the Internet, you should pick up a novel by Robert Graves or Vladimir Nabokov\u2014and avoid James Joyce like the plague. We also found that George R.R. Martin is the king of fantasy and Nigella Lawson the queen of cookbooks, and that Goodreads ratings suggest that David Foster Wallace is the 57th most loved fiction writer.\n\nThe website Goodreads was launched in 2007 as a social platform for readers to share their book recommendations and catalogue what they have read. The website now has over 20 million users and contains the world\u2019s largest repository of book ratings. Well over 500 million ratings have been given to the more than 10 million books listed on the site.\n\nRather than collect the rating for every single book, we chose to collect data on the ten most popular books by each of the 9,000 authors who appear on Goodreads\u2019 Best Books Ever list \u2013 a list which is independent of user ratings and voted on by particularly active members. We ended up with a dataset of over 23,000 notable books.\n\nWith this data in hand, we turned to the question of which books the Internet think are best.\n\nThe following table displays the twenty highest rated books in our dataset. We only included books with over 2,000 ratings. For reference, the average book gets a rating of 3.98.\n\nWith a score of 4.81, the highest rated book is Bill Watterson\u2019s The Complete Calvin and Hobbes. Over 27,000 users have rated the book, and considerably more than 80% of them have given it five stars. It scores just ahead of the ESV Study Bible \u2013 an English Standard Version of the Bible with evangelical commentary.\n\nFive of the top twenty rated books are Calvin and Hobbes\u2019 collections. The dominance of Bill Watterson\u2019s comics about a young boy and his stuffed tiger can be looked at in two ways. One, almost everyone who reads this book loves it, or two, Calvin and Hobbes fans are unusually devoted to the strip and likely to vote on Goodreads. It is likely some combination of the two.\n\nComics are well represented on this list of Goodreads most loved books. \u201cToda Mafalda\u201d, which is third on the list, is a collection of comic strips by the Argentinian cartoonist Quino. \u201cThe Complete Far Side\u201d, a collection of work by the cartoonist Gary Larson, is 16th. Religious and fantasy novels also do well.\n\nThe books that receive the most ratings on Goodreads, such as the Harry Potter and the Game of Thrones series, do not appear on this list. In fact, none of the 55 books that have received over a million ratings score above 4.6.\n\nAre the most popular books also well loved? The chart below displays the number of ratings and average rating for every book in our dataset.\n\nThe five most reviewed books on Goodreads are the first books in The Hunger Games, Harry Potter and Twilight series, and the novels To Kill a Mockingbird and The Great Gatsby. The highest performing of these megahits is Harry Potter and the Sorcerer\u2019s Stone, which has a rating of 4.42.\n\nThe Harry Potter series also has the most highly rated book that has over one million ratings: Harry Potter and the Deathly Hallows has nearly 1.6 million reviews and scores a 4.59. Apparently, if you get to the seventh installment in a series, you probably really like it.\n\nBeyond looking at the Internet\u2019s favorite books, we also analyzed who Goodreads has anointed as the greatest author. We measured this by looking at the average ratings of authors\u2019 books. We consider only authors with five books or more on Goodreads.\n\nAs you might expect given the adoration of Calvin and Hobbes, Bill Watterson is number one on the list of most loved authors. He is followed by a diverse set of writers (and one religious institution).\n\nHiromu Arakawa and eight other authors on this list are Japanese manga artists. Manga is a type of Japanese comic book/graphic novel that is increasingly popular internationally. Manga authors like Arakawa, Hiro Fujiwara, Yana Toboso and Yosihiki Nakamura consistently publish books that get rave reviews.\n\nAshley Antoinette \u2013 third on the list \u2013 is the author of the \u201cracy\u201d and popular Prada Plan novels. Antoinette\u2019s books, which she frequently co-authors with her husband JaQuavis Coleman, all have thousands of ratings with an average above 4.5.\n\nThe Church of Latter-day Saints is the only non individual author to make the list. Books published by this institution, such as the Book of Mormon and True to the Faith, all score highly on Goodreads. While other religious institutions also publish books, none have such fervent support.\n\nOur list of top twenty authors is dominated by just a few genres: comics, religion and young adult. The readers of genres like business, history and memoir seem to be more critical. The following list shows the top author of each genre \u2013 we assigned authors to the first genre listed on their Goodreads author page. ChickLit is a genre name given by Goodreads, not Priceonomics.\n\nMost genres are topped by a famous author. Gabriel Garcia Marquez once called Pablo Neruda the greatest poet of the 20th Century, and the Internet agrees. Gore Vidal, a celebrated biographer of Abraham Lincoln and Aaron Burr, tops his genre. George R. R. Martin beats out J.R.R. Tolkien for the top spot in Fantasy.\n\nOther authors are less well known. It is not Dr. Seuss or Maurice Sendak who tops the Children's book genre, but Patricia Polacco. Polacco\u2019s five most popular books all get ratings over 4.3, and her most popular book, the autobiographical Thank You, Mr. Falker, receives a 4.51. The most highly rated Science Fiction author is Cassandra Clare. Clare\u2019s novel City of Bones has received over one million reviews, and her ratings are consistently better than luminaries like Isaac Asimov and Ursula K. Le Guin.\n\nThus far, our most highly rated lists do not include history\u2019s most celebrated authors like Shakespeare, Jane Austen, Charles Dickens, and George Eliot. We wondered if they would show up if we focused exclusively on authors who Goodreads designates as part of the genre \u201cfiction\u201d.\n\nThis list includes writers of contemporary pop fiction like Ashley Antoinette and romance novelists Tiffany Reisz and C.L. Stone, but also members of the literary canon. The Argentine short-story writer Jorge Luis Borges and the French novelist Marcel Proust, who are considered among modern literature's most influential authors, both make the top ten.\n\nSome other notable fiction authors also appear in the top 100: Flannery O\u2019Connor (25), James Baldwin (26), Arthur Conan Doyle (29), Lee Child (43), Fyodor Dostoyevsky (52), David Foster Wallace (56), Nicholas Sparks (68), Kurt Vonnegut (73) and Alice Munro (100).\n\nStill, very few authors of the classics make the list. The most hallowed authors are not the Internet\u2019s favorites.\n\nSo which classics are also crowd-pleasers? To assess which classics people actually like, we looked at the average rating of the top 50 books on thegreatestbooks.org, a list that combines the results from more than one hundred other best books lists. (The list has a heavy European and North American bias.) Books are ranked by their Goodreads rating.\n\nThe highest rated classic is the Collected Fiction of Jorge Luis Borges. Borges\u2019s Collected Fiction is one of three short story collections that top the list, along with the works of Anton Chekhov and Franz Kafka. It\u2019s a bit of a surprise to see the works of these challenging authors perform so well. Maybe we would all be happier with our literary lives if we read more short stories.\n\nIt\u2019s worth noting that Goodreads profiles are public to your friends, so some of these ratings may be impacted by social pressure or a desire to exhibit a certain sensibility. The strong ratings of these tougher classics could be due to literary snobs who pretend to like In Search of Lost Time.\n\nDostoevsky\u2019s The Brothers Karamazov and Jane Austen\u2019s Pride and Prejudice are the top two novels on the list. Pride and Prejudice is the only classic novel with over one million ratings and an average rating above 4.2. Elizabeth and Darcy bring the quantity and quality.\n\nHeart of Darkness and Moby Dick receive the worst ratings among the classics. Herman Melville and Joseph Conrad\u2019s dark stories of insane men may be celebrated by literary critics, but the Internet is not impressed. Generally, older books perform worse on Goodreads. Ulysses, The Odyssey, Gulliver\u2019s Travels, and the Canterbury Tales are all among the ten worst rated classics.\n\nThese old books rate quite poorly, but their low scores pale in comparison to the very least liked books. The following table shows the twenty books with the lowest ratings that were rated at least 2,000 times.\n\nThe most loathed book in the dataset is L. Ron Hubbard\u2019s Dianetics: The Modern Science of Mental Health with a rating of 2.29. Hubbard is the founder of the Church of Scientology. His book is on his theory of psychotherapy.\n\nThe book which best combines popularity and dislike is One Night at the Call Center by the Indian author Chetan Bhagat. Bhagat\u2019s books, which mostly focus on India\u2019s \u201cnew\u201d young middle class, are wildly popular in India, but also disdained in literary circles.\n\nJust like restaurants and mechanics, books are now at the mercy of online rating systems. It may seem perverse to critics to take a piece of literature and reduce to a 1-5 rating, but millions of of readers on Goodreads to just that.\n\nOur examination of these ratings show that the books people rate highest are generally not those that win the Man Booker or Pulitzer Prize. They are crowd-pleasers like Calvin and Hobbes, Japanese comic books and other books with devoted followings. When the Internet decides, James Joyce doesn\u2019t hold a candle to J.K. Rowling.\n\nOur next article looks at how Subaru created targeted ads for lesbians to gain a foothold in the American car market.\u00a0To get notified when we post it\u00a0\u00a0\u2192\u00a0\u00a0join our email list.\n\nNote:\u00a0If you\u2019re a company that wants to work with Priceonomics to turn your data into great stories, learn more about the\u00a0Priceonomics Data Studio."},
{"url": "http://arstechnica.com/cars/2016/05/four-hundred-miles-with-teslas-autopilot-forced-me-to-trust-the-machine/", "link_title": "Four hundred miles with Tesla\u2019s autopilot forced me to trust the machine", "sentiment": 0.05659414972974295, "text": "A few weeks ago, I finally tried Tesla Motors' \"autopilot\" feature. A Tesla rep and I tooled around Houston's I-45 in a Model X crossover SUV for 15 minutes, just long enough to test the vehicle's adaptive cruise/automatic lane-keeping wizardry. Once I toggled on the autopilot, the rep relaxed by checking e-mail on her phone. This sent a clear message: keep an eye on the dumb journalist when he's driving the $140,000 SUV, but once the machine takes over, everything\u2019s fine.\n\nAs we pulled back into the showroom (or whatever Texas\u2019 insane dealership protection laws demand Tesla call the places it\u2019s not allowed to sell or service vehicles), I told the rep that I was driving to Austin soon; Autopilot would be just the thing for the long stretches of empty road out on I-10 and TX-71. Without missing a beat, she offered me a loaner Model S.\n\nArs has officially driven a Model S with autopilot before, but only under controlled circumstances. The Austin trip\u00a0would let me\u00a0take the car out for nearly four hundred miles of driving in a big mix of traffic scenarios. Plus, I'd get to log more cockpit time in a Tesla. Of course I said yes. Who wouldn\u2019t?\n\nI\u2019ve driven two previous Model S sedans on the approximately 400 mile (about 640km) Houston-Austin round trip\u2014first a 2013 model P85+ and more recently a dual-motor P85D. Both trips were fun, since both times Tesla provided the currently-most-powerful version of its sedan to hoon around in. True to form, the company again loaned me its top-end car: a dual-motor P90D with a 90kWh battery and the much ballyhooed \"ludicrous mode\" acceleration option (caution: there\u2019s some NSFW language in that linked video).\n\nThe P90D looks and behaves much like previous Model S sedans.\u00a0Its exterior is studded with sensors that can detect both the road and\u00a0other vehicles, though unlike the P85D I drove, this P90D had the software installed to take full advantage of those sensors. (The entire\u00a0Model S fleet produced with these extra sensors over the the past year has since been software-upgraded to match the new vehicle\u2019s capabilities.)\n\nAs I gingerly placed my butt in the matte leather driver\u2019s seat of the pearly white P90D, another friendly Tesla rep gave me an unnecessary tour\u00a0of the cockpit. Yes, I knew how the climate control worked. Yes, I knew how to open the charge port. Yes, I knew how to engage the auto-cruise and auto-steer. I was an old hand at Tesla-ing. Let\u2019s get this show on the road!\n\nMy first jaunt was a 30-mile (48km) drive from the Houston Galleria showroom to my home on the southeast side of town. Houston\u2019s west 610 loop is in a constant state of rush hour, but the traffic mid-day was as light as it gets. It flowed along at maybe 40 miles per hour (64km/h) until the loop bent east and things opened up. Once I merged with traffic on the loop, I pulled the cruise stalk toward me twice to engage auto-cruise and auto-steer.\n\nThe car had already seen a 60mph (100km/h) speed limit sign, so it set that as its target speed. Since traffic was moving slower than the limit, my Tesla\u00a0patiently paced the car ahead\u00a0at a near-constant four car lengths. I twisted the end of the cruise stalk forward and reduced that distance to two car lengths, which curbed\u00a0the constant flow of people jerking their cars angrily into the lane ahead of me (woe betide you if you leave a space in Houston traffic). I forced myself to relax and hovered my hands over the wheel.\n\nThe car was driving itself. All I needed was a leather jacket and a two-way radio watch, and I'd be living out the Knight Rider fantasy I'd had since age 5.\n\nWell, OK, this system isn't revolutionary; adaptive cruise control (where your car paces itself with the car ahead) has existed for a decade or more, and automatic lane-keeping is at least as old. Other car makers\u00a0like Volvo, Mercedes, and Audi\u00a0offer similar auto-cruise and auto-steer packages.\n\nBut if Tesla isn't the first OEM to the autopilot table, it is certainly the loudest. The company dominates the marketing war\u2014to the point that even folks who don't particularly care about cars know the company's story.\n\nThat evening, when I caught up with my next-door neighbor\u2014not a particularly tech-centric person\u2014his eyes lit up. \"That's the electric car that drives itself, right?\" he asked. He wanted a test drive.\n\nI was due at my downtown Austin destination the next day at noon, so I set out at 7:30am with a fully charged car. From most points in Houston to most points in Austin takes three hours, but my trips begin\u00a0with the disadvantage of starting\u00a0southeast of Houston and having to fight through the city\u2019s morning rush hour (easily adding an extra 60-90 minutes). I would also need at least 20-30 minutes at the Tesla Supercharger in Columbus, Texas to top off the battery.\n\nFor all the car's software wizardry, calling its autopilot system an \"autopilot system\" isn\u2019t really accurate. The car is very good at following the road and keeping pace with the car in front of it, but it does not follow the path you program into the navigation system nor do any other kind of advanced self-guidance. So even though I programmed the Columbus Supercharger as my first destination, I stayed hands-on with the car as I drove out of my neighborhood and onto the I-45 service road. Auto-steer had to wait for the freeway, where I toggled it on.\n\nUnfortunately, I toggled it right back off. The portion of I-45 near my home is undergoing a multi-year $6 billion rebuild, with a long stretch spanning several exits being destroyed, reconfigured, widened, and otherwise \"improved.\" The result is a multi-mile segment of road that snakes back and forth across what will eventually be both sides of the finished project. For now, there's no consistent surfacing, no real lane markers, and nothing for the lane-keeping system to \"see.\" I drove human-style until I could bail off of I-45 and onto the westbound Sam Houston Tollway.\n\nReferred to by locals as \"Beltway 8,\" \"the beltway,\" or sometimes \"the Houston Speedway\" (people drive fast!), my section of the Sam Houston Tollway remains\u00a0relatively un-cluttered for long stretches during the morning. I toggled on the auto-cruise and auto-steer, dialed the target speed up to 90mph and the desired follow distance to four cars, parked myself in the right lane, and pulled my hands slowly away from the wheel.\n\nIt takes a while to get used to this feeling. Instead of serving as the primary means of direction for a car, you're now a meat-based backup and failsafe system. Instincts and impulses formed by more than two decades behind the wheel scream out a warning\u2014\"GRAB THE WHEEL NOW OR YOU'LL DIE\"\u2014while the rational forebrain fights back. Eventually, the voices quiet as the car starts to prove itself. When the road curves, the car follows. If the car is ever going too fast to negotiate the curve, it slows down and then accelerates smoothly back out of the turn.\n\nLane keeping was almost excellent. The car kept itself squarely planted in the middle of the lane without ping-ponging between the edges like some earlier systems did. The car would still sometimes \"lunge\" at exit ramps when lane markings would fall away to the right or left, though the behavior\u00a0was never enough to make me grab the wheel and disengage. (Tesla's 7.1 software update was supposed to eliminate the issue, though it hasn't completely.) Other than the occasional lunge, however, the car was solid as long as there remained even the slightest trace of visible lane markings.\n\nSitting in the right lane on the tollway meant I frequently encountered other cars merging from the service road. Most of the time, the Model S would see them\u2014with the speedometer display constantly updating to show silhouettes of other cars in their approximate positions around me\u2014and slow down. The braking was perhaps a little later and a little harder than I would have done, but it was smooth and controlled every time. (The braking interval can be adjusted in the car\u2019s settings). However, sometimes the person merging onto the road would be more beside me than in front of me. In those cases, I could force the\u00a0system to initiate an automatic lane change by pushing up and holding the turn signal stalk, or I could scoot out of the way by manually mashing the accelerator."},
{"url": "http://www.theregister.co.uk/2015/01/08/erik_meijer_agile_is_a_cancer_we_have_to_eliminate_from_the_industry/", "link_title": "AGILE must be destroyed, once and for all", "sentiment": 0.11616876310272536, "text": "Comment A couple of months back, Dutch computer scientist Erik Meijer gave an outspoken and distinctly anti-Agile talk at the Reaktor Dev Day in Finland.\n\n\u201cAgile is a cancer that we have to eliminate from the industry,\" said Meijer; harsh words for a methodology that started in the nineties as a lightweight alternative to bureaucratic and inflexible approaches to software development.\n\nThe Agile Manifesto is a statement from a number of development gurus espousing four principles:\n\nWhat is wrong with Agile? Meijer\u2019s thesis is that \u201cwe talk too much about code, we don\u2019t write enough code.\u201d\n\nStand-up meetings in Scrum (a common Agile approach) are an annoying interruption at best, or at worst one of the mechanisms of \u201csubtle control\u201d, where managers drive a team while giving the illusion of shared leadership. \u201cWe should end Scrum and Agile,\u201d he says. \u201cWe are developers. We write code.\u201d\n\nEven test driven development, where developers write tests first to subsequently verify the behaviour of their code, comes in for a beating. \u201cThis is so ridiculous. Do you think you can model the real failures that happen in production? No,\u201d says Meijer.\n\nHe advocates instead a \u201cmove fast and break things\u201d model, where software is deployed and errors fixed as they are discovered.\n\nMeijer also managed to include a jab or two at Microsoft, his former employers, for \u201carrogance and deafness to the environment,\u201d first with the Office Ribbon, and second with Windows 8. \u201cIt ignored the feedback loop, and it just produced output.\u201d\n\nHowever, not all of Meijer\u2019s arguments stand up. The talk has been debunked by technical architect Nic Ferrier, formerly at ThoughtWorks, a software development company which uses Agile techniques.\n\n\u201cIt's not Agile that sucks, and dragging programmers down. It's the inability of programmers and business people to understand each other,\u201d he said.\n\nLike Meijer though, Ferrier is no fan of Scrum. \u201cSprints are utterly ridiculous ways to produce software,\u201d he says, referring to the short-term targets which form part of Scrum methodology.\n\nHere's the thing. If you go to developer conferences and hear the like of Kent Beck or Martin Fowler (both Manifesto signatories) speak, you discover that the essence of Agile is less a particular way of developing software, and more an insistence that team members communicate with and respect each other, and that the \u201cteam\u201d includes every stakeholder, including the users.\n\nIf Agile has failed, it is because of its success. As soon as smart Agile development teams started delivering good results, companies started coming up with tools to help others do it right. Businesses could buy into the illusion that Agile is a tool you can buy, and vendors were happy to profit.\n\nIs Agile bunk? No, but much of what is sold as Agile has little to do with what you find in the Agile Manifesto.\u00ae"},
{"url": "http://venturebeat.com/2016/05/22/how-fanduel-grew-from-humble-scottish-startup-into-an-american-fantasy-sports-giant", "link_title": "How FanDuel grew from humble Scottish startup into American fantasy sports giant", "sentiment": 0.11877472201001613, "text": "The daily fantasy sports (DFS) industry has taken one hell of a beating these past seven months, with states from New York and Illinois to Nevada concluding that \u201cbetting\u201d on virtual sports teams compiled from real pro and college athletes may not be a game of skill after all \u2014 it could be illegal gambling.\n\nThe two most prominent players in the saga have been Boston-based DraftKings \u2014 founded in 2011 and with $445 million in VC cash to its name so far \u2014 and FanDuel, which has raised north of $360 million in financing since its inception in 2009. The DFS industry in North America sits in something of an awkward position, with some states declaring it illegal gambling, some passing new legislation to accommodate it, and some still to make their minds up either way. And this surely can\u2019t be good for business.\n\nIn FanDuel\u2019s latest annual report, announced just this week, auditor Deloitte highlighted the \u201cuncertainty\u201d created by this situation:\n\nThough it\u2019s continuing to challenge the attorneys general preliminary decisions, FanDuel has already suspended operations in both Nevada and New York, which may not bode well for its bottom line. But FanDuel cofounder and CEO Nigel Eccles poured tepid water on such notions. \u201cThe loss of turnover in the states in which the group has suspended operations pending further legal clarity represents less than 20 percent of total turnover for the 18 months ended June 30, 2015,\u201d he said.\n\nFanDuel\u2019s predicament helps highlight how much can change in a year. In July, the New York-headquartered company cemented its unicorn status when it raised $275 million from big names such as Google Capital and Time Warner. One week later, it made its first acquisition. Less than 12 weeks later, New York\u2019s attorney general was knocking on its door after reports surfaced that a DraftKings employee had used inside knowledge to scoop $350,000 in a FanDuel fantasy football contest. The employee in question was soon cleared, but this event \u2014 in conjunction with an eye-popping amount of TV advertising that catapulted fantasy sports into the mainstream \u2014 opened the floodgates for scrutiny.\n\nVentureBeat\u00a0paid a visit to Eccles in his home city (hint: not New York) to learn more\u00a0on where the company came from, where it\u2019s at, and where things could go from here.\n\nBefore its rise to daily fantasy sports supremacy, FanDuel was actually known as Hubdub, a news-prediction platform based in Edinburgh, Scotland, with about\u00a0seven employees. Hubdub let users bet with virtual money on the outcome of key events \u2014 for example the presidential election. Though the company received VC investment in its former incarnation, Hubdub closed\u00a0in 2010 to focus all its efforts on a separate product it had launched a year earlier \u2014 FanDuel.\n\n\u201cIt was a classic pivot,\u201d said Eccles. \u201c2008 was an election year, and we thought it would be cool to build a prediction market where you could predict the outcome of running news stories. And the election was one of the biggest ones. Users loved it; we had a ton of engagement. But the fatal flaw was that there wasn\u2019t really a business model. So even though we were getting the scale in users, we just couldn\u2019t figure out how we would turn it into an interesting business.\u201d\n\nThe company then went Stateside to South by Southwest (SXSW) in Austin, and it set about figuring out how to move the business forward. \u201cWe said we seem to know how to build games that people like to engage with, but we really want something with more of a robust business model,\u201d added Eccles. \u201cAnd we also wanted something that people were willing to pay for.\u201d\n\nNoting that sports was one of the most active categories on Hubdub at that point, and fantasy sports in general was a significant market, Eccles and co. scanned existing fantasy sports products and concluded it had room for improvement. For starters, they weren\u2019t particularly mobile-friendly at a time when the smartphone and accompanying app ecosystems were starting to snowball.\n\n\u201cWe thought we could make them a lot better, we could make it more mobile friendly, and we could make it faster \u2014 and that was the genesis to FanDuel,\u201d said Eccles. \u201cEven the very earliest version, in 2009, we designed it for mobile. And now, over 80 percent of our users are mobile.\u201d\n\nWhat Hubdub had were effectively two products \u2014 the main Hubdub prediction platform, and its new FanDuel offshoot. But with fresh investors on board, the team was nervous about pivoting so soon, so it continued with both products in parallel to avoid shocking its\u00a0backers. \u201cIt was much easier to say, \u2018let\u2019s work on two things,\u2019 but once we started coding the new product, I don\u2019t think we ever touched Hubdub again,\u201d said Eccles.\n\nThe world was in a state of financial uncertainty at that point, with many companies missing their numbers, but after explaining to their investors what it wanted to do, it received the greenlight to go all-in on FanDuel. \u201cEventually they just said, \u2018We backed the team, there\u2019s five of you, we always knew it was super-early stage so if the team wants to go in that direction, let\u2019s see where you go,\u201d he added.\n\nToday, FanDuel counts 360 employees around the world. Eccles dividing his time between its two biggest offices in Edinburgh, where engineers represent the majority of the 140-strong headcount; and New York, the company\u2019s official headquarters and operational hub for 160 people. Orlando, Los Angeles, and Glasgow (Scotland) make up the remaining employees, covering roles such as marketing, customer support, and engineering.\n\nGiven that FanDuel is essentially split between two key hubs on either side of a large ocean, does this cause any operational headaches?\n\n\u201cThe challenge hasn\u2019t been so much operations, the challenge has been more inter-office communications,\u201d said Eccles. \u201cFrom the East Coast, there\u2019s a 5-hour time difference, so you\u2019re only starting on emails from New York at around lunch time, and for me, 10 p.m. is when my emails start to settle down. Sure, there are a lot of evening calls, and a lot of early morning calls from New York. That can be hard, getting everyone together in one room.\u201d"},
{"url": "http://fivethirtyeight.com/features/trumps-scorning-of-data-may-not-hurt-him-but-itll-hurt-the-gop/", "link_title": "Trump's scorning of data will hurt the GOP", "sentiment": 0.133819232893307, "text": "Donald Trump during a campaign event on Thursday in Lawrenceville, New Jersey.\n\nData doesn\u2019t win elections; candidates do. Presumptive Republican nominee Donald Trump bet on that idea last week when he announced his plan to rely on his personality and rallies in the general election instead of collecting data on voters. Trump has a point: The effect of \u201cbig data\u201d and improved analytics on elections is often overhyped. Even David Plouffe \u2014 the architect of President Obama\u2019s 2008 and 2012 campaigns, the most data-savvy in history \u2014 agreed that Obama\u2019s \u201cdata processing machine\u201d was not responsible for his wins.\n\nBut Republicans are worried, and for good reason: Trump\u2019s assumption that the sole value of data is to win more votes is too narrow. His decision to limit the role of data probably won\u2019t be the deciding factor in the 2016 election, but data organization and access are an investment in the future of the party. A presidential campaign presents a rare opportunity to cultivate the next generation of talent and collect a ton of new data on voters, and Trump\u2019s refusal to do so means that Republicans may need to wait until 2020 or beyond to even the playing field with Democrats.\n\nPresidential elections, when turnout routinely exceeds midterm and off-cycle elections, produce an influx of data to public voter files and staffers who earn experience analyzing that data. Two examples of innovations in data-driven strategy \u2014 direct mail after 1964 and digital data analytics after 2004 \u2014 demonstrate that even losing presidential campaigns have paid huge dividends.\n\nTrump\u2019s candidacy is often compared to the 1964 campaign of Barry Goldwater, and generally not as a compliment. Goldwater lost to Lyndon Johnson by nearly 23 percentage points, receiving only 27 million votes out of 70 million cast. Despite this historic loss, Goldwater\u2019s campaign was a launching point for the national conservative movement largely because of the data it produced and the innovations it enabled.\n\nDirect mail was a new tool for campaigns at the time. It was used to market a message directly to voters while raising money, much as online advertisements and emails do for today\u2019s candidates. Messages are only as useful as the audience they reach, however, and the Republican Party was unorganized in most of the South and West. The \u201cDraft Goldwater\u201d committee, comprised of conservative activists who wanted to steer the Republican Party rightward, sought out \u201csilent supporters\u201d to become delegates and donors for Goldwater \u2014 creating reliable national lists of conservative Republicans in the process.\n\nFor these conservative activists, the Goldwater campaign was not a failure \u2014 it identified 27 million Americans willing to support a solidly conservative candidate, and generated data on where to find them. One activist, Richard Viguerie, went to the clerk of the House of Representatives and copied (by hand) the names of donors who gave Goldwater $50 or more, forming the basis for his mailing list. In his book, Viguerie claims that he used this data to send 70 million letters per year and 1 billion pieces of conservative direct mail between 1974 and 1980, when conservative Ronald Reagan was elected president. Goldwater\u2019s ideological legacy was secured using strategies that relied upon the data generated by his failed campaign.\n\nPolitical data has moved far beyond card catalogues and mailing lists. Making sense of today\u2019s data requires experience with database software and statistical modeling. The software used by campaigns cannot be bought \u201coff the shelf\u201d or assembled quickly because it relies on data controlled and managed by the parties.\n\nIn 2005, after winning two consecutive presidential elections, the Republican Party\u2019s data was more centralized and better organized than Democrats\u2019. This success led to a sense of complacency among Republicans. Meanwhile, after two straight losses, Democrats were open to new campaigning techniques at the same time that online and digital tools were emerging.\n\nLike conservatives post-Goldwater, the Democrats used their data effectively after a loss in a presidential election. In this case, failed 2004 Democratic candidate Howard Dean \u2014 whose campaign was known for its innovative use of the internet \u2014 became chair of the Democratic National Committee in 2005. He prioritized the creation of a national voter database with one company in charge (NGP VAN). This investment made it easier for future candidates to incorporate data analytics into their strategy, giving them incentive to hire employees who could innovate and build on each other\u2019s work \u2014 even after elections ended. Since 2004, Democrats have founded 67 firms and organizations dedicated to digital campaigning and data analytics, while former Republican staffers founded 13. These firms apply their expertise down the ballot by providing services to Democratic gubernatorial, Senate and House candidates, all of whom work from the same dataset.\n\nDemocrats now hold a substantial expertise advantage in digital data-driven campaigning, and the GOP admitted as much in their 2012 election post-mortem. John McCain hired only 15 data staffers in 2008, compared with Obama\u2019s 131. To his credit, Mitt Romney increased the number of data hires to 87 in 2012. (Obama had 342). In 2016, Republicans were positioned to build on this effort and narrow the analysis gap between the parties, pivoting off of two consecutive losses into an innovative data strategy \u2014 just like in 1964 and 2004.\n\nBut Republicans seem set to squander the opportunity. Trump currently employs as few as two staffers dedicated to data, according to reports. (The Trump campaign did not respond to a request to confirm the number of staffers it has devoted to data.) The Republican Party has not consolidated data as Democrats did, instead relying on \u201ca jumble of firms not always working in concert.\u201d Independent firms such as i360, funded by the Koch brothers, have not integrated with the party database.\n\nMeanwhile, Democratic consolidation and expertise building continues. Hillary Clinton set out to assemble a data team three times the size of Obama\u2019s formidable 2012 operation. Several members of her digital and analytics leadership team worked for Civis Analytics, BlueLabs, and Blue State Digital \u2014 all firms founded by former Obama or Dean employees. After this election, win or lose, Clinton\u2019s data staff will be positioned to bring their expertise back to these companies or found new ones while Republicans continue to chase the data advantage they wasted after 2004.\n\nListen to our podcast special on the history of political data in U.S. elections."},
{"url": "https://blog.prototypo.io/memoize-immutable-efficient-memoizer-for-redux-and-other-immutable-environments-59277fefa45f#.8j221gdn2", "link_title": "Memoize-Immutable: efficient memoizer for Redux and other immutable environments", "sentiment": 0.059952963702963706, "text": "The role of a memoizer is to increase the efficiency of a function:\n\nHere\u2019s a basic implementation of a memoizer and usage example.\n\nThe main limitation of this memoizer is that it only works for functions that take a single primitive argument, such as one number or one string.\n\nMemoizers that accept any number of arguments of any kind usually rely on JSON.stringify() and\u00a0.toString() methods to serialize each arguments, then concatenate all these strings to generate a unique cache key (see Ramda\u2019s memoize.js and dependencies for example).\n\nThis serialization can take a lot of time, especially for complex and deep objects. But this is usually the only way to create a reliable cache key, unless you\u2019re working with immutable data\u2026\n\nModern frontend frameworks help you manage the state of your application, and update its UI when the state changes. Consider an application that displays the family tree of the House of Windsor:\n\nWhere each member of the family is represented as a plain object:\n\nIf prince William was to be beheaded this year (excuse my French), in AngularJS and similar frameworks, the state would be mutated with:\n\nAnd inside the framework, detecting that change would basically require:\n\nBoth operations can be quite complex, this is why Redux opted for an immutable state: when a property deep in the state has to change, instead of modifying it directly, you re-create all \u201cparent\u201d objects that contain that property:\n\nDetecting changes is then much easier:\n\nAn added benefit of an immutable environment is that the same === operator can be used to compare the arguments of a memoized function!\n\nIn our case, the fact that our memoizer used to serialize arguments was making memoization less efficient than no memoization at all. This was especially frustrating when we knew that arguments could be compared with ===.\n\nFortunately, ES2015 has brought us a new kind of object that can leverage immutability to solve a part of our problem: Map, a simple key/value map that accepts non-primitive values as keys, without serializing them:\n\nPerfect! Although this could only be used as a cache when memoizing a function that accepts a single argument. Using the `arguments` array as the cache key wouldn\u2019t work, as it will never be === to the next `arguments` passed to the function.\n\nThe first solution we came up with was to cache results in nested maps:\n\nAnd it worked\u00a0\u2026except for one detail.\n\nControlling the size of a Map is straightforward, but controlling the size of nested Maps is much more complicated and inefficient! Worse, by using a Map, all arguments would be kept in memory, even if they correspond to a part of the state that is no longer used in the app.\n\nI exposed my problem to @LasseFister, and he soon suggested to use two different maps:\n\nThis allowed us to build a single-level cache map whose size can be controlled very easily.\n\nWe compared different strategies to limit the size of the cache used by our memoize functions. We realized that, when using a native Map as a cache, we could use our app for a long while without noticing any increase in the memory used by the browser tab. We then compared the performances of this native Map with the performances of two different LRU cache implementations (one built on top of Map, and one built with a simple object and a double linked list).\n\nTry our cache benchmark in your browser.\n\nThe results show that the two LRU solutions have comparable performances, but are 4 to 6 times slower than a native Map. So we decided not to implement any cache limiting algorithm in memoize-immutable, and leave this up to the user.\n\nIn our app, we provide our own native Map to the memoizer function, but keep a reference to it and clear it entirely every thirty minutes. You can start by having a look at our implementation and giving this solution a try!"},
{"url": "https://www.youtube.com/watch?v=tmQPOtR92O8", "link_title": "How to Cut Video and Merge Them Together| MacX Video Converter Pro", "sentiment": 0.2743055555555555, "text": "Free Trial:\n\nMac:http://www.macxdvd.com/mac-video-conv...\n\nWindows:http://www.macxdvd.com/macx-hd-video-...\n\n\n\nLooking for an easy-use video making tools to cut the unwanted video parts and merge them together to create your own video shows? Here is an easy way for you!\n\n\n\nIn this video, we are going to show you how to cut off the unwanted frames, trim videos and merge clips together making your own video story with MacX Video Converter Pro.\n\n\n\nFirst, open MacX Video Converter Pro. Import the video. Click the edit bottom to trim the target videos. You can simply drag on the time line or input the exact start time and end time. If those video clips are in different resolution, click Crop& Expand bottom to adjust the width and height make them keep same resolution. After editing work is done, selected merge all the video, then hit the run bottom. Your video will be done in seconds.\n\n\n\n Don\u2019t forget to subscribe our official Youtube channel to get more tips and tricks. Thanks for watching!"},
{"url": "http://fridgerator.github.io/2016/05/24/fantasy-football-genetic-algorithm-in-crystal.html", "link_title": "Fantasy Football Genetic Algorithm in Crystal", "sentiment": 0.13308270676691733, "text": "Inspired by a recent talk at Nebraska.code() conference - Artificial Intelligence: A Crash Course from Josh Durham over at Beyond the Scores, I set out to try some AI / Machine learning of my own.\n\nPerhaps one of the more interesting topics in the field, IMO, is the Genetic Algorithm - emulating biological evolution over a data set using natural selection, mutation and breeding. I\u2019m not going to pretend to be an expert on the topic, to the contrary I am a complete noob and suggestions on how to improve my code are very welcome.\n\nAnd now the hardest part, finding a suitable application for testing and creating the algorithm.\n\nLast year I started playing fantasy football, using a Rails app I created that allows me to track my team and make efficient recuitments / trades based on the data from the Fantasy Football Nerd API. I also tried my hand at FanDuel and wrote some brute-force functions (not really knowing much about linear algebra) to try to build the best team with the highest expected points while staying under the salary cap. But thats boring and took a long time, a reeeeally long time if I used the entire data set - billions of possible combinations.\n\nThis idea isn\u2019t unique or novel in any way, a quick search returns dozens of others that have applied some kind of genetic algorithm to the fantasy football knapsack problem. The one thing that does make this unique, is that its written in Crystal ;)\n\nMy genetic population is a list of randomly generated teams, each containing 9 players (quarterback, two running backs, three wide receivers, a tight end, kicker and defence). Links to and classes.\n\nThe class, or - the chromosome, contains several important methods:\n\nThe main loop (here) creates a population of 10,000 teams and evolves it a total of 80 times.\n\nCheck out the beast in action: http://recordit.co/lu8ZEV916D\n\nThe salaries right now are randomly generated, as I don\u2019t have actual data to use since we\u2019re not in football season. And I haven\u2019t yet done much tuning of the parameters: changing the population size, number of itterations, mutation percent, etc.\n\nAgain, feel free to leave feedback on how this can be improved. I will probably continue to tweak and modify the algorithm so its ready come football season."},
{"url": "https://webkit.org/blog/6240/ecmascript-6-proper-tail-calls-in-webkit/", "link_title": "ECMAScript 6 Proper Tail Calls in WebKit", "sentiment": 0.10965651412079984, "text": "Proper Tail Calls (PTC) is a new feature in the ECMAScript 6 language. This feature was added to facilitate recursive programming patterns, both for direct and indirect recursion. Various other design patterns can benefit from PTC as well, such as code that wraps some functionality where the wrapping code directly returns the result of what it wraps. Through the use of PTC, the amount of memory needed to run code is reduced. In deeply recursive code, PTC enables code to run that would otherwise throw a stack overflow exception.\n\nTypically when calling a function, stack space is allocated for the data associated with making a function call. This data includes the return address, prior stack pointer, arguments to the function, and space for the function\u2019s local values. This space is called a stack frame. A call made to a function in tail position will reuse the stack space of the calling function. A function call is in tail position if the following criteria are met:\n\nWhen a function call is in tail position, ECMAScript 6 mandates that such a call must reuse the stack space of its own frame instead of pushing another frame onto the call stack. To emphasize, ECMAScript 6 requires that a call in tail position will reuse the caller\u2019s stack space. The calling function\u2019s frame is called a tail deleted frame as it is no longer on the stack once it makes a tail call. This means that the tail deleted function will not show up in a stack trace. It is important to note that PTC differs from Tail Call Optimization, which is a discretionary optimization that many optimizing compilers will make for various performance reasons.\n\nPTC was added to ECMAScript primarily to reuse stack space. The reuse of the stack memory allows for recursive and tail call coding patterns common to functional programming and other programming paradigms. Using PTC, a program could make an unbounded number of consecutive tail calls without unboundedly growing the stack.\n\nPTC provides other benefits as well. Programs that utilize PTC can see a reduced memory footprint because the garbage collector will be more likely to collect certain local objects. Programs that utilize PTC can also see an improvement in speed because there is less processing when returning from a tail called function.\n\nReduced stack usage can provide benefits in other ways as well. Modern computing devices incorporate tiered memory caches to reduce latency in memory accesses. Although these caches are generous in size, they are still finite. Reducing stack usage through the use of PTC also reduces the amount of cache space needed, freeing up cache space for other memory accesses.\n\nConsider a function that allocates a local object, but that object is never made visible to other code. The only references to such a local object will be through a pointer in the function\u2019s stack frame or in a register that the function is using. Should the JavaScript virtual machine need to garbage collect memory, it will find a reference to such a local object by scanning the stack and the contents of the CPU\u2019s registers. If that function makes a call to another function and that call is not a tail call, then any local objects of the calling function will not be collected until the calling function itself returns. However, if a function makes a tail call to another function, all local objects of the calling function can be garbage collected because there are no more stack references to the object.\n\nAnother benefit of PTC is that when a leaf function returns, it bypasses all intermediate tail called functions and returns directly to the first caller that didn\u2019t make a tail call. This eliminates all of the return processing of those intermediate functions. The deeper the call chain of successive tail calls, the more performance benefit this provides. This works for both direct and mutual recursion.\n\nThere are many algorithms that are best written using recursion. Many of those algorithms naturally take advantage of PTC, while others may require some reworking. Consider writing a program to compute the greatest common divisor (GCD) function using Euclid\u2019s algorithm. The translation of Euclid\u2019s algorithm into a program that utilizes PTC is simple, elegant, and natural:\n\nThe natural translation of other recursive mathematical functions can lead to recursive calls that are not in tail position. For example, a program that computes factorial (N!) is commonly written as:\n\nIn this function, the recursive call to is not in tail position because the return statement computes and returns the product of and the result of the recursive call. As a reminder, to be in tail position, the return value of the called function must be the only thing returned by the calling function. With a little modification, we can rewrite to utilize PTC as follows:\n\nThis change puts the recursive call to factorial in tail position which allows the function to take advantage of PTC. The number of recursive calls and arithmetic operations is the same for both versions.\n\nThis next example involves the functions , and which are used to calculate the square roots of numbers using Newton\u2019s Iterative method:\n\nThe top function, , determines if the result will be a real or imaginary number and then calls , which sets up the iterative square process and returns the result of . The call to in is not in tail position since additional processing is done on the result of the call. All other function calls are tail position.\n\nSuppose is called with 99 as the argument. It will call , which will subsequently call . Using Web Inspector, we observed that calls back to itself 6 times before returning a result. That result is returned directly back to computeSquareRoot, where it is converted to a string, saving the processing of 7 returns.\n\nThis last example shows the type of functional programming that PTC enables:\n\nThe function searches the list using tail recursion. For a list the size of 100,000 elements given in this example, most browsers will run out of stack memory and throw an exception. In strict mode, where can take advantage of PTC, the program runs just fine. It is also interesting to note that even with a list size small enough to allow this code to run without PTC, using PTC results in the code running 2.5x faster.\n\nThere are a couple subtle, but minor issues to be aware of when using PTC. Remember that PTC is only available in strict mode and only for calls made from tail position. The other notable change involves the generation of stack traces. There are some non-standard ECMAScript features in JavaScript that work differently in the presence of PTC. These include and the object\u2019s stack trace. For example, say a tail called function throws an object; the function that catches that Error will not see the function that called in the object\u2019s stack trace. As a general rule, the object\u2019s stack trace will not include a function that made a tail call to another function. We call such frames tail deleted frames because its as if they are deleted from the stack when making a call.\n\nBecause PTC places a strict resource guarantee on stack usage, JavaScriptCore cannot keep around information for all tail deleted frames. Keeping around any extra resources, no matter how small, for an unbounded number of tail deleted frames is not possible because it could use an unbounded amount of memory and eventually exhaust the memory limits of the program. Given that tail calls do not keep around any information in the program\u2019s executing state, debugging tail calls can be challenging when using an interactive debugging tool. Without any added machinery, the debugger will not know if a tail call did or did not occur. Because we want to make debugging tail calls inside Web Inspector a great experience, we have implemented mechanisms inside JavaScriptCore to keep around a shadow stack that will display a finite number, currently 128, tail deleted frames. This allows us to both provide strict guarantees on memory usage and to provide users of PTC the benefit of seeing the most important stack frames in their program inside Web Inspector.\n\nWe call our shadow stack implementation ShadowChicken. The name is an homage to the CHICKEN scheme interpreter. ShadowChicken uses a logging mechanism for constructing its shadow stack. The log works as follows:\n\nTo construct the shadow stack, ShadowChicken takes two inputs:\n\nGiven these two inputs, ShadowChicken is able to construct a shadow stack that includes tail-deleted frames. It will reconcile the machine stack with its log. Because the log has tail packets for when tail calls occurred, it is able to use the log to insert tail-deleted stack frames into the shadow stack to represent frames that were only present on the machine stack before a tail call occurred. ShadowChicken uses a constant amount of memory on top of the memory your program already uses. The log is fixed in size. Whenever ShadowChicken runs out of space in the log, it will perform its reconciliation algorithm to update its internal data structure about the state of the shadow stack. The shadow stack will contain at most 128 tail deleted frames, a number we believe strikes a good balance between programs that intentionally take advantage of PTC and programs that just happen to use PTC without the explicit intention of doing so.\n\nBecause JavaScriptCore has the machinery for constructing a shadow stack, Web Inspector can use JavaScriptCore\u2019s shadow stack in its debugger. This allows users of Web Inspector to interact with tail deleted stack frames as if they are real stack frames that are on the current machine stack.\n\nLet\u2019s see some interactions with Web Inspector and the iterative square root code to compute the square root of 99. After setting a breakpoint in and invoking , Web Inspector shows that we are paused, ready to return the result.\n\nWeb Inspector not only shows the frame we\u2019re stopped in and the original caller, , but also shows the seven tail deleted frames. These are highlighted in the above image. Tail deleted frames in Web Inspector show up with a gray \u0192 icon to their left. As the next image shows, the variables in tail deleted frames can be examined as if the frame were a normal frame. The next image shows Web Inspector inspecting a tail deleted frame one level up from the leaf frame.\n\nIn this image, the local variables (circled) can be examined. The highlighted line in the program also shows where the tail deleted frame made a tail call from.\n\nThe next image shows what happens when single stepping from the breakpoint.\n\nWith on click of the step into button, Web Inspector now shows that we have returned directly to where made the first tail call.\n\nWebKit has ECMAScript 6 Proper Tail Calls and web developers are encouraged to take advantage of them to design programs in ways that were not possible before. Existing code can benefit as well. Web Inspector makes developing and debugging with PTC straightforward and intuitive.\n\nThe current Safari Technology Preview Release 4 has support for PTC. However, Web Inspector was only recently hooked up to work with ShadowChicken and therefore it is not in Safari Technology Preview Release 4. It is expected to be in the next Safari Technology Preview release. You can also try out ShadowChicken in Web Inspector by using a WebKit Nightly build.\n\nThis article was cowritten with Saam Barati from the JavaScriptCore team. We invite comments and feedback on WebKit\u2019s PTC implementation. Feel free to get in touch with @msaboff, @saambarati, and @jonathandavis on Twitter."},
{"url": "http://padrinorb.com/", "link_title": "Padrino \u2013 the elegant ruby web framework, based on Sinatra", "sentiment": 0.12072788322788323, "text": "The Padrino code base has been kept simple and easy to understand, maintain and enhance. The generator for each new project creates a clean and compact directory structure keeping your code simple and well organized.\n\nPadrino strives to adhere to the following basic principles:\n\nThis framework can be used with ease for web development for a project of any size from your lightweight json web service to a large full-stack web application!\n\nPadrino ships with an Admin Interface that includes the following features:\n\nBuilding on our experience in developing web applications, we designed a framework that meets all the requirements for creating a top notch web application in a clean, concise and simple environment, with minimal deadline delays.\n\nWe provide you with the following out of the box:\n\nPadrino is ORM/ODM, JavaScript, testing, rendering, and mocking agnostic supporting the use of any number of available libraries.\n\nThe available components and their defaults are listed below:\n\nJust create the project with the usual generator command and pass in your preferred components!\n\nMany people love the simplicity and expressiveness of Sinatra but quickly find themselves missing a great deal of functionality provided by other web frameworks such as Rails when building non-trivial applications.\n\nSinatra acts as a thin layer on top of Rack itself and the \"micro\"-framework is kept light introducing complexities only when required by the particular application.\n\nStarting from this assumption, we have developed a different approach to a web development framework. We expand on Sinatra through the addition of standard libraries including helpers, components, and other functionality that are needed in a framework suitable for arbitrarily complex web applications."},
{"url": "https://marcusblankenship.com/resources-i-love/", "link_title": "Resources I Love", "sentiment": 0.33342424242424246, "text": "These are resources I have, use, and recommend. If there\u2019s resources, books, blogs or newsletters you enjoy, please email them to marcus@marcusblankenship.com. Thank you!\n\nThese books might be written for all kinds of leaders, but we have a lot to learn from them. \u00a0All of them are worth your time, I promise. \u00a0In fact, much of my writing and training is inspired directly from these books!\n\nHumble Inquiry: The Gentle Art of Asking Instead of Telling \u2013 Recommended by Marcus\n\n\n\nThe Truth About Leadership \u2013 Recommended by Amy\n\n\n\nLead from the Heart \u2013 Recommended by Jason\n\n\n\nIt\u2019s Okay to be the Boss \u2013 Recommended by Eliza\n\n\n\nThe Five Dysfunctions of a Team \u2013 Recommended by Marcus\n\n\n\nMore and more people are writing about leading programmers, or creating great software teams, and that makes me happy! \u00a0I have a lot more books to go in this section, but this is a good start.\n\nAre Your Lights On? \u2013 Recommended by Scott\n\n\n\nIn order to manage others, we must manage ourself first. \u00a0Our time. \u00a0Our attitude. \u00a0Our reactions. \u00a0Our focus. \u00a0These books have helped me do that for many years.\n\nWho Moved My Cheese? An Amazing Way to Deal with Change in your Work and Life \u2013 Recommended by Stef\n\n\n\nEat that Frog! 21 Great Ways to Stop Procrastinating and Get More Done in Less Time \u2013 Recommended by Stef\n\n\n\nThe War of Art \u2013 Recommended by Marcus\n\n\n\nThe 3 Secrets to Effective Time Investment\u2013 Recommended by Marcus\n\n\n\nIf you don\u2019t think Selling and Negotiating are part of your job, think again. You have to sell your boss on ideas, sell clients on technical directions, and sell you team on following you every day. Reading these books BEFORE you need them will save you a mountain of heart ache, I promise.\n\nThe Power of a Positive No: Save The Deal Save The Relationship and Still Say No \u2013 Recommended by Stef\n\n\n\nGetting Past No: Negotiating in Difficult Situations \u2013 Recommended by Stef\n\n\n\nGetting to Yes: Negotiating Agreement Without Giving In \u2013 Recommended by Stef\n\n"},
{"url": "http://isse.sourceforge.net/", "link_title": "ISSE: An Interactive Sound Source Separation Editor", "sentiment": 0.1364935064935065, "text": "In applications such as audio denoising, music transcription, music remixing, and audio-based forensics, it is desirable to decompose a single- or stereo-channel recording into its respective sources. To perform such tasks, we present ISSE - an interactive source separation editor (pronounced \"ice\").\n\nISSE is an open-source, freely available, cross-platform audio editing tool that allows a user to perform source separation by painting on time-frequency visualizations of sound. The software leverages both a new user interaction paradigm and machine learning-based separation algorithm that \"learns\" from human feedback (e.g. painting annotations) to perform separation. For more information, please see the about and demos sections of the website and the demo video below."},
{"url": "https://blog.dominodatalab.com/data-science-just-one-way-donald-trump-different/", "link_title": "Data Science: Just One More Way That Donald Trump Is Different", "sentiment": 0.20921556122448978, "text": "If you\u2019re a data scientist interested in working in politics, don\u2019t bother applying to the Trump campaign \u2014 but Hillary has a job for you.\n\nIn a recent AP interview Donald Trump discounted the value of data as a campaign strategy tool. His plan is to win based on the strength of his personality. The presumptive Republican nominee pointed to Obama\u2019s personality, saying \u201cObama got the votes much more so than his data processing machine, and I think the same is true with me.\u201d\n\nAs with many things Trump, it\u2019s an unconventional opinion. Most political professionals view Obama\u2019s 2008 campaign as groundbreaking in its use of data, especially to get out the vote. Obama\u2019s focus on the right battlegrounds during the primaries got him delegates that Hillary Clinton hadn\u2019t even realized were in play.\n\nFor any major political campaign, data science is the brain of the operation. The questions that data scientists can answer cover every single aspect of the campaign: where to fundraise, which states to fight for, which message for the audience, who are the swing voters. On these, data trumps opinion every time.\n\nStrategic questions aside, a big part of a political campaign is just straight up marketing. Candidates spend tens of millions of dollars on advertising. Political ads run alongside ads for cars, diapers and beer. Data science is the way companies target advertising at the right audience. It\u2019s the difference between carpet bombing and smart bombs: far more impact for far less money.\n\nFor Trump, who has raised less than half of what Hillary Clinton has raised, it would make sense to try to stretch his advertising dollar. Instead, to date, he\u2019s benefited tremendously from free press. His campaign spent less than \u2153 of what Ted Cruz did, and has spent only about 20% of what the Clinton campaign has.\n\nBut as the campaign shifts to the general election, the free press advantage is likely to shrink. Also, his ability to control the press has come in large part because of his willingness to make unconventional statements. An inability to run a data driven ad campaign would become a tremendous liability if he needed to deliver a focused message.\n\nThe Clinton Campaign, on the other hand, is looking for even more data scientists to help them figure out how to best allocate resources to get the best outcomes. Their analysts work on fundraising, advertising, geographic analysis and more. There are currently more than a dozen open jobs on the analytics team, from beginner to experienced researcher. Some of the interesting jobs include:\n\nData scientists who want to help Donald Trump \u201cmake America great again\u201d may find a wall between them and their candidate. The Clinton Campaign, on the other hand, is ready to use data to make decisions.\n\nUPDATE: In an article on fivethirtyeight.com this morning, Joshua Darr explores the impact Trump\u2019s decision to limit the role of data in his campaign will have on the Republican Party. Not in terms of influencing the outcome of the election, but rather that Presidential elections present a rare opportunity to both collect data at far higher volumes than other elections, and to cultivate the next generation of talent capable of leveraging that data."},
{"url": "https://www.entrepreneur.com/article/276303", "link_title": "Peter Thiel: On Things Not Yet Done (2016 Commencement)", "sentiment": 0.13307481595617193, "text": "Watch Peter Thiel's 2016 Hamilton College Commencement address and read the transcript below (or read a list of highlights from other 2016 commencement speeches).\n\nThank you so much for the kind introduction. It\u2019s a tremendous honor to be here.\n\nLike most graduation speakers my main qualification would seem to be that I am one of the few people who are even more clueless about what is going on in your lives than your parents and your professors.\n\nMost of you are about 21 or 22 years old.\u00a0You\u2019re about to begin working. I haven\u2019t worked for anybody for 21 years. But if I try to give a reason for why it makes sense for me to speak here today, I would say it\u2019s because thinking about the future is what I do for a living. And this is a commencement. It\u2019s a new beginning. As a technology investor, I invest in new beginnings. I believe in what hasn\u2019t yet been seen or been done.\n\nThis is not what I set out to do when I began my career. When I was sitting where you are, back in 1989, I would\u2019ve told you that I wanted to be a lawyer. I didn\u2019t really know what lawyers do all day, but I knew they first had to go to law school, and school was familiar to me.\n\nI had been competitively tracked from middle school to high school to college, and by going straight to law school, I knew I would be competing at the same kinds of tests I\u2019d been taking ever since I was a kid, but I could tell everyone that I was now doing it for the sake of becoming a professional adult.\n\nI did well enough in law school to be hired by a big New York law firm, but it turned out to be a very strange place. From the outside, everybody wanted to get in,\u00a0and from the inside, everybody wanted to get out.\n\nWhen I left the firm, after seven months and three days, my coworkers were surprised. One of them told me that he hadn\u2019t known it was possible to escape from Alcatraz. Now that might sound odd, because all you had to do to escape was walk through the front door and not come back. But people really did find it very hard to leave, because so much of their identity was wrapped up in having won the competitions to get there in the first place.\n\nJust as I was leaving the law firm, I got an interview for a Supreme Court clerkship. This is sort of the top prize you can get as a lawyer. It was the absolute last stage of the competition. But I lost. At the time I was totally devastated. It seemed just like the end of the world.\n\nAbout a decade later, I ran into an old friend -- someone who had helped me prepare for the Supreme Court interview, whom I hadn\u2019t seen in years. His first words to me were not, you know, 'Hi Peter'\u00a0or 'How are you doing?'\u00a0but rather, 'So, aren\u2019t you glad you didn\u2019t get that clerkship?'\u00a0Because if I hadn\u2019t lost that last competition, we both knew that I never would have left the track laid down since middle school, I wouldn\u2019t have moved to California and co-founded a startup, I wouldn\u2019t have done anything new.\n\nLooking back at my ambition to become a lawyer, it looks less like a plan for the future and more like an alibi for the present. It was a way to explain to anyone who would ask --\u00a0to my parents, to my peers\u00a0and most of all to myself --\u00a0that there was no need to worry. I was perfectly on track. But it turned out in retrospect that my biggest problem was taking the track without thinking really hard about where it was going.\n\nWhen I co-founded a technology startup, we took the opposite approach. We consciously set out to change the direction of the world: very definite, very big plans. Our goal was nothing less than to replace the U.S. dollar by creating a new digital currency.\n\nWe had a young team. When we started, I was the only person over 23 years old. When we released our first product, the first users were simply the 24 people who worked at our company. Outside, there were millions of people working in the global financial industry, and when we told some of them about our plans we noticed a clear pattern: the more experience someone had in banking, the more certain they were that our venture could never succeed.\n\nThey were wrong. People around the world now rely on PayPal to move more than $200 billion every year. We did fail at our greater goal. The dollar\u2019s still dominant. We didn\u2019t succeed in taking over the whole world, but we did create a successful company in the process. And more importantly, we learned that\u00a0while doing new things is difficult, it is far from impossible.\n\nAt this moment in your life you know fewer limits, fewer taboos\u00a0and fewer fears than you will ever in the future. So do not squander your ignorance. Go out and do what your teachers and parents thought could not be done --\u00a0and what they never thought of doing.\n\nNow this is not to say that we should assume there is no value in teaching and tradition. And here we can take inspiration from a graduate of Hamilton College, the illustrious Ezra Pound, class of 1905. Pound was a poet, and he was also a prophet of sorts, and he announced his mission in three words: 'Make it new.'\u00a0When Pound said 'make it new,'\u00a0he was talking about the old. He wanted to recover what was best in tradition --\u00a0and render it fresh.\n\nHere at Hamilton, in America\u00a0and that part of the world called the West, we are all part of an unusual kind of tradition. The tradition we\u2019ve inherited is itself about doing new things. The new science of Francis Bacon and Isaac Newton discovered truths that had never been written down in books. Our whole continent is a new world. The founders of this country set out to create what they called 'a new order for the ages.' America is the frontier country. We are not true to our own tradition unless we seek what is new.\n\nSo how are we doing? How much is new today? It is a clich\u00e9 to say that we are living through a time of rapid change, but it is an open secret that the truth is closer to stagnation. Computers are getting faster and smartphones are somewhat new. But on the other hand, jets are slower, trains are breaking down, houses are expensive\u00a0and incomes are flat.\n\nToday the word technology\u00a0means information technology. The so-called tech industry builds computers and software. But in the 1960s, technology\u00a0had a more expansive meaning and meant not just computers, but also airplanes, medicines, fertilizers, materials, space travel --\u00a0all sorts of things. Technology was advancing on every front and leading to a world of underwater cities, vacations on the moon\u00a0and energy too cheap to meter.\n\nWe\u2019ve all heard America described as a 'developed country,' setting it apart from countries that are still developing. This description pretends to be neutral. But I find it far from neutral. Because it suggests that our tradition of making new things is over. When we say we are developed, we\u2019re saying, 'that\u2019s it.'\u00a0That for us, history is over. We are saying that everything there is to do has already been done, and now the only thing left is for others in the world to catch up. And in this view, the 1960s vision of a fantastic and far better future was just a mistake.\n\nI think we should strongly refuse this temptation to assume that our history is over. Of course, if we choose to believe that we\u2019re powerless to do anything that is not familiar, we will be right, but only in a sort of self-fulfilling way. We should not, however, blame nature. It will only be our own fault.\n\nFamiliar tracks and traditions are like clich\u00e9s --\u00a0they are everywhere, they may sometimes be correct, but often they are justified by nothing except constant repetition. Let me end today by questioning two clich\u00e9s in particular:\n\nThe first comes from Shakespeare, who wrote this well-known piece of advice: 'To thine own self be true.'\u00a0Now Shakespeare wrote that, but he didn\u2019t say it. He put it in the mouth of a character named Polonius, who Hamlet accurately describes as a tedious old fool, even though Polonius was senior counselor to the King of Denmark.\n\nAnd so, in reality, Shakespeare is telling us two things.\u00a0First, do not be true to yourself. How do you know you even have such a thing as a self? Your self might be motivated by competition with others, like I was. You need to discipline your self, to cultivate it and care for it. Not to follow it blindly. Second, Shakespeare\u2019s saying that you should be skeptical of advice, even from your elders. Polonius is a father speaking to his daughter, but his advice is terrible. Here Shakespeare\u2019s a faithful example of our western tradition, which does not honor what is merely inherited.\n\nThe other clich\u00e9 goes like this: 'Live each day as if it were your last.'\u00a0The best way to take this as advice is to do exactly the opposite. Live each day as if you will live forever. That means, first and foremost, that you should treat the people around you as if they too will be around for a very long time to come. The choices that you make today matter, because their consequences will grow greater and greater.\n\nThat is what Einstein was getting at when he supposedly said that compound interest is the most powerful force in the universe. This isn\u2019t just about finance or money, but it\u2019s about the idea that you\u2019ll get the best returns in life from investing your time in building durable friendships and long-lasting relationships.\n\nIn one sense, all of you are here today because you were approved by the admissions office of Hamilton to pursue a course of study, which is now over. In another sense, you are here because you found a group of friends to sustain you along the way, and those friendships will continue. If you take care of them, they will compound in the years ahead.\n\nEverything that you have done so far has had some kind of formal ending, some kind of graduation. You should, and I hope that you will, take time today to celebrate all that you\u2019ve achieved so far. But remember that today\u2019s commencement is not the beginning of one more thing that will end. It is the beginning of forever. And I won\u2019t delay you any further in getting on with it. Thank you."},
{"url": "https://blog.mightysignal.com/new-app-report-2915840dd675#.xc3hz8xzc", "link_title": "The average iOS app released today is 31.3 MB (and other mobile trends)", "sentiment": 0.044055944055944055, "text": "Here are the new iOS apps released 5/12\u20135/18. In total there were 16,613.\n\nThe average size of an app released was 31.3 MB. Using Verizon\u2019s average LTE speed, the average time it takes to download an app released last week is 20-50 seconds. That is in good coverage areas; with mobile speeds internationally being slower, downloading an app released last week takes much longer abroad. The next billion users will be mobile and international, so that poses a problem. For example, based on Brazil\u2019s fastest 3g speed of 2.3 Mbps, downloading an app released last week would take on average 108 seconds; Brazil currently increasing its LTE coverage, however.\n\nGoogle\u2019s solution is Instant Apps, announced last week, which lets consumers experience bits of native apps without downloading them. This doesn\u2019t speed up download times, but it does build a bridge to native app experiences.\n\nRhapsody released Rhapsody VR, an app to experience shows in VR.\n\nSDKs include Amplitude, Unity\n\niTunes, MightySignal\n\nSketchfab released its first app, Sketchfab VR. They\u2019ve raised 9.5M and are based in New York.\n\nSDKs include Unity\n\niTunes, MightySignal\n\nJuicero, the juicing company that raise 70M, released its first mobile app.\n\nSDKs include Fabric, Mixpanel, New Relic, Segment\n\niTunes, MightySignal\n\nViber Media, the makers of the popular chat app, released a new game, Viber Emperoros. This is the third week in a row Viber has released a game.\n\nSDKs include Adjust, Appsflyer, Facebook, Flurry, Admob, Google Analytics, Hockey\n\niTunes, MightySignal\n\nGannett Co., Inc. released The Detroit News eEdition.\n\nSDKs include comScore, Facebook, Localytics, Adobe, Admob\n\niTunes, MightySignal\n\nFor questions, please contact us here."},
{"url": "http://www.theguardian.com/lifeandstyle/2016/may/23/microsoft-windows-ten-10-software-update-problems-iplayer", "link_title": "Microsoft has ruined my day, and possibly my life", "sentiment": -0.03228896103896105, "text": "I\u2019m sitting at my computer, getting on with my work, when suddenly an unseen hand takes over. It stops my work and starts an update, which I can not stop, never asked for and do not want, from Windows 7 to Windows 10. Red alert! Where is my work? What the hell is happening to my computer?\n\n\u201cYour computer?\u201d says Clayden scornfully, and he is right. I\u2019m sitting in my home, at my desk, using a computer that I think belongs to me, but it doesn\u2019t. It seems to belong to Microsoft, because they\u2019re in charge of it. They\u2019ve poked their way into my living room, stopped me working, sabotaged my printer, which was specially set up to match Windows 7, stopped the sound on iPlayer, wasted hours of my time while I fiddle about trying to find files and sort out how this wretched new system works. They\u2019ve given me a fright, ruined my day, and possibly my life, because I think I\u2019ve lost the bulk of my documents.\n\nAn annoying little notice rears up in the corner of my screen. \u201cI\u2019m Cortana,\u201d it says. \u201cAsk me anything.\u201d Here\u2019s a machine telling me what to do, and I\u2019m falling for it. Because I just wrote: \u201cIt says\u201d. No, it doesn\u2019t. It can\u2019t speak. It\u2019s a sodding machine, taking over.\n\nI moan at Fielding, who\u2019s cheesed off with his Mac, which has also sabotaged his iPlayer. He must update it. It\u2019s only four years old. But this is the age of built-in obsolescence, AKA rubbish, guaranteed not to last.\n\n\u201cAre you sure you haven\u2019t turned yours off?\u201d he asks. No. It\u2019s on. I can see the little lights glimmering. There\u2019s just no sound or sense to it.\n\nHow dare Microsoft intrude on my private life like this? What a bloody cheek. And I am not alone. They\u2019ve been doing these updates since July last year, although they claim they don\u2019t install them without users\u2019 permission. Whatever\u2019s going on, they need to fix it like they promised. Because it\u2019s still happening. So I\u2019m stuck here, waiting for them to come and fix it. How long do you think they\u2019ll be?"},
{"url": "http://www.theverge.com/circuitbreaker/2016/5/23/11757590/new-macbook-pro-rumor-oled-touch-bar", "link_title": "Major MacBook Pro revamp could replace function keys with OLED touch bar", "sentiment": 0.18214756258234518, "text": "Apple is preparing a serious overhaul of the MacBook Pro line for later this year, according to a research note from well-connected KGI Securities analyst Ming-chi Kuo\u00a0reported on by MacRumors. The laptops are set to come in 13-inch and 15-inch models, with a thinner and lighter design enabled by a new MacBook-esque keyboard and revamped hinges. The new MacBook Pros will also reportedly use Thunderbolt 3-enabled USB-C ports.\n\nAll of that is kind of predictable. What is not predictable is Kuo's assertion that the MacBook Pros will substitute the keyboard's physical function keys with an \"OLED display touch bar,\" which sounds deeply wild with shades of the original Razer Blade. The laptops are also said to integrate Apple's Touch ID fingerprint scanners. Kuo says they're the \"most significant upgrade ever undertaken by Apple\" and are planned for the fourth quarter of 2016, with a 13-inch regular MacBook apparently also on the cards.\n\nKuo has a good, if imperfect, record for\u00a0predicting this kind of thing, and the specificity of this rumor makes it notable. If true, however, it means we're unlikely to see much in the way of meaningful MacBook updates at Apple's annual Worldwide Developers Conference (WWDC)\u00a0next month."},
{"url": "http://metanomalies.com/life-stories/", "link_title": "Life-stories", "sentiment": 0.0653008962868118, "text": "Around people whose name lifts to prominence in the regard of public awareness accrete histories and life stories of folk tale kinds, often anecdotal, often celebratory or shocking; or both.\n\nChopin and George Sand as an item perhaps; or else, Elvis maybe, truck driver, stopping off at Sun Records?\n\nI want to consider stories like these and see where they lead in regard to our own apprehension and understanding about our awareness of and grip upon our own lives as we see them as discrete individuals.\n\nLet\u2019s call these stories all celebratory; whether notorious or not.\u00a0 They are an act of remembrance and are shared communally amongst one\u2019s contemporaries within the circles one inhabits and on the mind map one draws as one\u2019s preferred personal territory. \u00a0As such they are items used by us in our shaping and constructing of our personal identities. We might be variously aware and conscious of our using them in this way, sometimes doing so deliberately and carefully; sometimes having done so without actually having taken note of having done so.\n\nOur identities as we make them might be looked on as being a collection of stories a person tells to himself about himself.\n\nAs items of community they are also building blocks; able to secure qualification for us to enter a social group or a mental fraternity: Once again, with some deliberation, or else without our realisation of it.\u00a0 William Blake said famously that \u2018Milton was of the devil\u2019s party without knowing it\u2019.\u00a0 Blake also said with vehemence: \u2018I must create a system of my own or else be enslaved by another man\u2019s\u2019.\u00a0 Blake then was intensely aware of the use and purpose of our lives to be, as another poet said: \u2018And I will make my soul\u2019.\n\nThese stories that we tell to ourselves about ourselves might from another angle be considered as being part of our Sartrean \u2018essence\u2019. This Sartre saw as a quality not present in our existence from the very first, but instead always following on from, (and being created over time in us) our having come into the world at our births.\n\nSartre felt that this essence we create for ourselves necessarily engenders in us a \u2018bad faith\u2019.\u00a0 He felt that because we are born as creatures without direction, meaning, and purpose; and because that we clutch for lifelines to which we cling to make sense of things, and by them fill this lack; that ours is a desperate case in which we stop a hole in our leaky ships of life by any material that comes to hand, and so build out of scraps of expedience something which we like to believe that and say that we are.\n\nSartre felt that our making, fabricating, ourselves like this was an act of human weakness; an act of grasping at straws, and so essentially unauthentic; and so creates lives always lived out in falsehood, even when lived out with a subjective sincerity. This, then, is how Sartre tells us a person generates \u2018bad faith\u2019 in his life.\n\nYou can see that Sartre and Blake owned mental lives at poles opposed to one another in many respects.\u00a0 Creative persons and people of belief generally do own a life of thought in polar opposition to Sartre\u2019s existential pessimism.\n\nThere is a further consideration to be made in an attempt at distinguishing a Sartrean \u2018bad faith\u2019 among those of us who own to, and model themselves around, and upon, instructive and celebratory stories.\n\nWhen one knows one is telling oneself untruths in the stories one embraces as one\u2019s model for one\u2019s own life; perhaps Sartre is right; that there is a level of knowing self-deception which is being used by certain of us to \u2018justify\u2019 our attitudes and actions to ourselves; and in despite of ourselves?\u00a0\u00a0 This surely is \u2018bad faith\u2019 in any language?\n\nYet again, there are actors and storytellers themselves; whose livings are to portray and bring to life dramatic persona whose, actions, thoughts, words, done and spoken; all which as an actor or storyteller they does not believe in, nor might these be manifestations of stories held dear within their own life-choice stories?\u00a0 Are acting and storytelling then exemplary items of \u2018bad faith\u2019 in their performers and performance?\n\nReaders, movie-goers, opera-lovers, ballad, theatre and TV lovers; are they all at rock-bottom only indulgent mendacious self-deluders, because they will engage with and identify themselves in, and live out stories as they unfold as if they themselves were the very subject of them?\n\nAnd then, are the historical stories we use to add a brick or a slate to our self-created identities, which help inform us of whom and what we are, are these also wadding and padding for our selves which we use to help us by way of illusion out of a quandary of existence?\u00a0 And this regardless of the truth value or credibility of such tales as told to ourselves?\n\nThe George Sand and Chopin; the Elvis at Sun Studios stories; is it legitimate to own such stories within ourselves? Or are we all frauds at bottom?\n\nOur emotional lives tell us we are not frauds and that we are good to own such stories to ourselves.\u00a0 Our lives of reasoning might tell some of us something other than this; as Sartre\u2019s did to himself; that the stories we tell ourselves and own as ours are flimsy props in a theatre of the absurd of our senseless lives.\n\nHowever, the logic for considering connection and harmony between our emotional and reasoning lives seems clear. It rests on the status of our emotional lives; on whether these are prior in importance and in value to our rational lives; on whether these aspects of us are, or must necessarily be, in conflict with one another, completely or else in part; and on what might be the highest eminence of human value and whether this eminence might be characterised as being rational and/or emotional.\n\nWhat might be an answer to these questions?\u00a0 A viable, self-consistent, and practicable answer which satisfies all bases?\u00a0 A marriage of thought and feeling; a marriage not of \u2018heterogeneous ideas\u2019 and feelings \u2018yoked together by violence\u2019; but perhaps it might be \u2018a marriage made in Heaven\u2019?\n\nWhat would be the qualities asked of in such a \u2018marriage\u2019: conciliation, peace, forgiveness, mercy, grace, faith \u2013 and to crown all \u2013 love?\u00a0 These are feelings; they are also concepts.\u00a0 They are feelings which when felt in us generate harmony; and act to harmonise one\u2019s thought \u2013 to itself and to ones feelings.\u00a0 And so the stories we should tell to ourselves then, so as to assume them as the building blocks creating our identities, seem to be stories which carry as their messages of import these conciliatory qualities.\n\nYou may look upon this startling fact as being an accident of nature; that such qualities when sought out for and practiced with endeavour, tend ever towards peace and joy toward, and for, others, and also in oneself.\u00a0 It\u2019s a pretty strange coincidence of nature that it should be so?"},
{"url": "https://www.youtube.com/watch?v=r6Rp-uo6HmI", "link_title": "Why Snow and Confetti Ruin YouTube Video Quality [video]", "sentiment": 0.05714285714285715, "text": "Your sports team wins. The confetti drops. And suddenly, the video quality falls apart. Why? Let's talk about interframe compression, bitrate, and unnecessary green screen effects.\n\n\n\nI'm at https://tomscott.com\n\non Twitter at http://twitter.com/tomscott\n\non Facebook at http://facebook.com/tomscott\n\nand on Instagram as @tomscottgo\n\n\n\nYou might also like: How Green Screen Worked Before Computers: https://www.youtube.com/watch?v=msPCQ...\n\n\n\nThis uses one Creative Commons by-attribution photo, \"Sony Trinitron\" by Ant\u00edfama, available here: http://flic.kr/p/6DBMYn"},
{"url": "http://blog.rishankjhavar.com/tough-books/", "link_title": "SEVEN INCREDIBLY TOUGH BOOKS TO READ: PARTH KHARE", "sentiment": 0.11846245596245597, "text": "Ever decided to read a book belonging to the hall of fame of literature, ordered it in a jiffy, opened it and then, well, sighed? We know the feeling of reading tough books. So does Parth Khare, our\u00a0guest blogger for today:\n\nWhile there might be plenty of Twilight and such stories catering to the those with simpler tastes, there are books for the connoisseurs, for the hardcore, for those erudite enough to fathom its prolixity and obfuscation (see what I\u00a0did there?). Here are the literature hall of famers who make your science major paper seem easier than Archies.\n\nJames Joyce, take a bow. No book in the history of English literature has exacted a consensus out of scholars and people, except probably Finnegans Wake. It is hard. Hard\u00a0in the way that comprehending the entirety of the universe is, and you are more likely to do that than to get through the labyrinth that this book represents.\u00a0A strange style, stranger language, Irish babble, portmanteau words, neological puns and onomatopoeia of the toughest kind (use of sounds in words), here\u2019s a quote that sums everything up: \u2018The gracehopper was always jigging ajog, hoppy on akkant of his joyicity.\u2018 No joy there, Joyce.\n\nT.S Eliot wrote some knotty, tough books himself (more on that later) but he described this one as \u2018altogether alive but demanding something of a reader that an ordinary novel reader isn\u2019t prepared to give.\u2019 High praise indeed from the father of modern English poetry, but here\u2019s the catch, we can\u2019t give what we don\u2019t have, we just can\u2019t. This book is awash with prose that is both torturous and tortuous to the untrained (sometimes even trained) mind with its Gothic prose style, its discursive and rambling nature and philosophical musings by a character who in his spare time likes to dress himself up in nighties and curls of blonde hair (why didn\u2019t our philosophy teacher try that in those boring classes?). It is essentially a collection of monologues and ideas from the different characters which make you want to say, \u2018Mate, you\u2019ve got some unresolved issues there\u2019. From the book itself, \u2018you pound the liver out of a goose to get a pate, you pound the muscles of a man\u2019s cardia to get a philosopher.\u2018 And you pound your mind out to understand this book, despite its greatness.\n\nGo ahead, say it out loud. You have heard of this book. There are\u00a0a lot of allusions to this book abound in popular culture (ever heard of \u2018death by water\u2019, or \u2018dead man\u2019s alley\u2019? Yes, right). Released in 1922, this book encompasses the apprehensions of one of the most respected poet, critic and dramatist, Thomas Stearns Eliot. He was worried about the degradation of culture in the world and that the younger generation no longer pays due importance to the classics and old school greatness. He believed that modern poetry should come from reading old legendary books, and boy, did the wasteland come from them and smack you right in your (progressive) head. Here\u2019s something unique. The lines in this poem change languages. That\u2019s right, you heard me. So one time you are skiing in some mountains and next, you have a German line on your hands, followed by French and even (yes!) Sanskrit. It is chock\u00a0full of allusions and it is a deliberate attempt to make you go back and read proper literature. A collection of 5 poems, it is sure to make you cry out your eyes. In Eliot\u2019s own learned words \u201cI will show you fear in a handful of dust\u201d. Quiver.\n\nThe old fox of Konigsberg as Kant was known in his hey days, comes the book of all philosophy books. The Critique of Pure Reason did many things in the philosophical world. It solved metaphysical riddles, it aligned God with reason (yes, strange as it sounds), it answered or attempted to answer the mind vs. body